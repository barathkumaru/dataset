ID,TITLE,ABSTRACT,Computer Science,Physics,Mathematics,Statistics,Quantitative Biology,Quantitative Finance
13854,A Deep Neural Architecture for Sentence-level Sentiment Classification in Twitter Social Networking,"  This paper introduces a novel deep learning framework including a
lexicon-based approach for sentence-level prediction of sentiment label
distribution. We propose to first apply semantic rules and then use a Deep
Convolutional Neural Network (DeepCNN) for character-level embeddings in order
to increase information for word-level embedding. After that, a Bidirectional
Long Short-Term Memory Network (Bi-LSTM) produces a sentence-wide feature
representation from the word-level embedding. We evaluate our approach on three
Twitter sentiment classification datasets. Experimental results show that our
model can improve the classification accuracy of sentence-level sentiment
analysis in Twitter social networking.
",1,0,0,0,0,0
14166,"Multi-band characterization of the hot Jupiters: WASP-5b, WASP-44b and WASP-46b","  We have carried out a campaign to characterize the hot Jupiters WASP-5b,
WASP-44b and WASP-46b using multiband photometry collected at the
Observatório do Pico Dos Dias in Brazil. We have determined the planetary
physical properties and new transit ephemerides for these systems. The new
orbital parameters and physical properties of WASP-5b and WASP-44b are
consistent with previous estimates. In the case of WASP-46b, there is some
quota of disagreement between previous results. We provide a new determination
of the radius of this planet and help clarify the previous differences. We also
studied the transit time variations including our new measurements. No clear
variation from a linear trend was found for the systems WASP-5b and WASP-44b.
In the case of WASP-46b, we found evidence of deviations indicating the
presence of a companion but statistical analysis of the existing times points
to a signal due to the sampling rather than a new planet. Finally, we studied
the fractional radius variation as a function of wavelength for these systems.
The broad-band spectrums of WASP-5b and WASP-44b are mostly flat. In the case
of WASP-46b we found a trend, but further measurements are necessary to confirm
this finding.
",0,1,0,0,0,0
18190,Explicit polynomial sequences with maximal spaces of partial derivatives and a question of K. Mulmuley,"  We answer a question of K. Mulmuley: In [Efremenko-Landsberg-Schenck-Weyman]
it was shown that the method of shifted partial derivatives cannot be used to
separate the padded permanent from the determinant. Mulmuley asked if this
""no-go"" result could be extended to a model without padding. We prove this is
indeed the case using the iterated matrix multiplication polynomial. We also
provide several examples of polynomials with maximal space of partial
derivatives, including the complete symmetric polynomials. We apply Koszul
flattenings to these polynomials to have the first explicit sequence of
polynomials with symmetric border rank lower bounds higher than the bounds
attainable via partial derivatives.
",1,0,1,0,0,0
15452,Alternative Semantic Representations for Zero-Shot Human Action Recognition,"  A proper semantic representation for encoding side information is key to the
success of zero-shot learning. In this paper, we explore two alternative
semantic representations especially for zero-shot human action recognition:
textual descriptions of human actions and deep features extracted from still
images relevant to human actions. Such side information are accessible on Web
with little cost, which paves a new way in gaining side information for
large-scale zero-shot human action recognition. We investigate different
encoding methods to generate semantic representations for human actions from
such side information. Based on our zero-shot visual recognition method, we
conducted experiments on UCF101 and HMDB51 to evaluate two proposed semantic
representations . The results suggest that our proposed text- and image-based
semantic representations outperform traditional attributes and word vectors
considerably for zero-shot human action recognition. In particular, the
image-based semantic representations yield the favourable performance even
though the representation is extracted from a small number of images per class.
",1,0,0,0,0,0
3817,Effective Description of Higher-Order Scalar-Tensor Theories,"  Most existing theories of dark energy and/or modified gravity, involving a
scalar degree of freedom, can be conveniently described within the framework of
the Effective Theory of Dark Energy, based on the unitary gauge where the
scalar field is uniform. We extend this effective approach by allowing the
Lagrangian in unitary gauge to depend on the time derivative of the lapse
function. Although this dependence generically signals the presence of an extra
scalar degree of freedom, theories that contain only one propagating scalar
degree of freedom, in addition to the usual tensor modes, can be constructed by
requiring the initial Lagrangian to be degenerate. Starting from a general
quadratic action, we derive the dispersion relations for the linear
perturbations around Minkowski and a cosmological background. Our analysis
directly applies to the recently introduced Degenerate Higher-Order
Scalar-Tensor (DHOST) theories. For these theories, we find that one cannot
recover a Poisson-like equation in the static linear regime except for the
subclass that includes the Horndeski and so-called ""beyond Horndeski"" theories.
We also discuss Lorentz-breaking models inspired by Horava gravity.
",0,1,0,0,0,0
20326,Some Investigations about the Properties of Maximum Likelihood Estimations Based on Lower Record Values for a Sub-Family of the Exponential Family,"  Here, in this paper it has been considered a sub family of exponential
family. Maximum likelihood estimations (MLE) for the parameter of this family,
probability density function, and cumulative density function based on a sample
and based on lower record values have been obtained. It has been considered
Mean Square Error (MSE) as a criterion for determining which is better in
different situations. Additionally, it has been proved some theories about the
relations between MLE based on lower record values and based on a random
sample. Also, some interesting asymptotically properties for these estimations
have been shown during some theories.
",0,0,1,1,0,0
14799,Low-shot learning with large-scale diffusion,"  This paper considers the problem of inferring image labels from images when
only a few annotated examples are available at training time. This setup is
often referred to as low-shot learning, where a standard approach is to
re-train the last few layers of a convolutional neural network learned on
separate classes for which training examples are abundant. We consider a
semi-supervised setting based on a large collection of images to support label
propagation. This is possible by leveraging the recent advances on large-scale
similarity graph construction.
We show that despite its conceptual simplicity, scaling label propagation up
to hundred millions of images leads to state of the art accuracy in the
low-shot learning regime.
",1,0,0,1,0,0
14087,An educational distributed Cosmic Ray detector network based on ArduSiPM,"  The advent of microcontrollers with enough CPU power and with analog and
digital peripherals makes possible to design a complete particle detector with
relative acquisition system around one microcontroller chip. The existence of a
world wide data infrastructure as internet allows for devising a distributed
network of cheap detectors capable to elaborate and send data or respond to
settings commands. The internet infrastructure enables to distribute the
absolute time (with precision of few milliseconds), to the simple devices far
apart, with few milliseconds precision, from a few meters to thousands of
kilometres. So it is possible to create a crowdsourcing experiment of citizen
science that use small scintillation-based particle detectors to monitor the
high energetic cosmic ray and the radiation environment.
",0,1,0,0,0,0
6087,A constrained control-planning strategy for redundant manipulators,"  This paper presents an interconnected control-planning strategy for redundant
manipulators, subject to system and environmental constraints. The method
incorporates low-level control characteristics and high-level planning
components into a robust strategy for manipulators acting in complex
environments, subject to joint limits. This strategy is formulated using an
adaptive control rule, the estimated dynamic model of the robotic system and
the nullspace of the linearized constraints. A path is generated that takes
into account the capabilities of the platform. The proposed method is
computationally efficient, enabling its implementation on a real multi-body
robotic system. Through experimental results with a 7 DOF manipulator, we
demonstrate the performance of the method in real-world scenarios.
",1,0,0,0,0,0
1318,Principal Boundary on Riemannian Manifolds,"  We revisit the classification problem and focus on nonlinear methods for
classification on manifolds. For multivariate datasets lying on an embedded
nonlinear Riemannian manifold within the higher-dimensional space, our aim is
to acquire a classification boundary between the classes with labels. Motivated
by the principal flow [Panaretos, Pham and Yao, 2014], a curve that moves along
a path of the maximum variation of the data, we introduce the principal
boundary. From the classification perspective, the principal boundary is
defined as an optimal curve that moves in between the principal flows traced
out from two classes of the data, and at any point on the boundary, it
maximizes the margin between the two classes. We estimate the boundary in
quality with its direction supervised by the two principal flows. We show that
the principal boundary yields the usual decision boundary found by the support
vector machine, in the sense that locally, the two boundaries coincide. By
means of examples, we illustrate how to find, use and interpret the principal
boundary.
",1,0,0,1,0,0
18780,Lower Bounds on Regret for Noisy Gaussian Process Bandit Optimization,"  In this paper, we consider the problem of sequentially optimizing a black-box
function $f$ based on noisy samples and bandit feedback. We assume that $f$ is
smooth in the sense of having a bounded norm in some reproducing kernel Hilbert
space (RKHS), yielding a commonly-considered non-Bayesian form of Gaussian
process bandit optimization. We provide algorithm-independent lower bounds on
the simple regret, measuring the suboptimality of a single point reported after
$T$ rounds, and on the cumulative regret, measuring the sum of regrets over the
$T$ chosen points. For the isotropic squared-exponential kernel in $d$
dimensions, we find that an average simple regret of $\epsilon$ requires $T =
\Omega\big(\frac{1}{\epsilon^2} (\log\frac{1}{\epsilon})^{d/2}\big)$, and the
average cumulative regret is at least $\Omega\big( \sqrt{T(\log T)^{d/2}}
\big)$, thus matching existing upper bounds up to the replacement of $d/2$ by
$2d+O(1)$ in both cases. For the Matérn-$\nu$ kernel, we give analogous
bounds of the form $\Omega\big( (\frac{1}{\epsilon})^{2+d/\nu}\big)$ and
$\Omega\big( T^{\frac{\nu + d}{2\nu + d}} \big)$, and discuss the resulting
gaps to the existing upper bounds.
",1,0,0,1,0,0
5776,Directed negative-weight percolation,"  We consider a directed variant of the negative-weight percolation model in a
two-dimensional, periodic, square lattice. The problem exhibits edge weights
which are taken from a distribution that allows for both positive and negative
values. Additionally, in this model variant all edges are directed. For a given
realization of the disorder, a minimally weighted loop/path configuration is
determined by performing a non-trivial transformation of the original lattice
into a minimum weight perfect matching problem. For this problem, fast
polynomial-time algorithms are available, thus we could study large systems
with high accuracy. Depending on the fraction of negatively and positively
weighted edges in the lattice, a continuous phase transition can be identified,
whose characterizing critical exponents we have estimated by a finite-size
scaling analyses of the numerically obtained data. We observe a strong change
of the universality class with respect to standard directed percolation, as
well as with respect to undirected negative-weight percolation. Furthermore,
the relation to directed polymers in random media is illustrated.
",0,1,0,0,0,0
7755,A note on knot concordance and involutive knot Floer homology,"  We prove that if two knots are concordant, their involutive knot Floer
complexes satisfy a certain type of stable equivalence.
",0,0,1,0,0,0
17034,Can scientists and their institutions become their own open access publishers?,"  This article offers a personal perspective on the current state of academic
publishing, and posits that the scientific community is beset with journals
that contribute little valuable knowledge, overload the community's capacity
for high-quality peer review, and waste resources. Open access publishing can
offer solutions that benefit researchers and other information users, as well
as institutions and funders, but commercial journal publishers have influenced
open access policies and practices in ways that favor their economic interests
over those of other stakeholders in knowledge creation and sharing. One way to
free research from constraints on access is the diamond route of open access
publishing, in which institutions and funders that produce new knowledge
reclaim responsibility for publication via institutional journals or other open
platforms. I argue that research journals (especially those published for
profit) may no longer be fit for purpose, and hope that readers will consider
whether the time has come to put responsibility for publishing back into the
hands of researchers and their institutions. The potential advantages and
challenges involved in a shift away from for-profit journals in favor of
institutional open access publishing are explored.
",1,0,0,0,0,0
15642,Recent progress in the Zimmer program,"  This paper can be viewed as a sequel to the author's long survey on the
Zimmer program \cite{F11} published in 2011. The sequel focuses on recent rapid
progress on certain aspects of the program particularly concerning rigidity of
Anosov actions and Zimmer's conjecture that there are no actions in low
dimensions. Some emphasis is put on the surprising connections between these
two different sets of developments and also on the key connections and ideas
for future research that arise from these works taken together.
",0,0,1,0,0,0
11341,Guarantees for Spectral Clustering with Fairness Constraints,"  Given the widespread popularity of spectral clustering (SC) for partitioning
graph data, we study a version of constrained SC in which we try to incorporate
the fairness notion proposed by Chierichetti et al. (2017). According to this
notion, a clustering is fair if every demographic group is approximately
proportionally represented in each cluster. To this end, we develop variants of
both normalized and unnormalized constrained SC and show that they help find
fairer clusterings on both synthetic and real data. We also provide a rigorous
theoretical analysis of our algorithms. While there have been efforts to
incorporate various constraints into the SC framework, theoretically analyzing
them is a challenging problem. We overcome this by proposing a natural variant
of the stochastic block model where h groups have strong inter-group
connectivity, but also exhibit a ""natural"" clustering structure which is fair.
We prove that our algorithms can recover this fair clustering with high
probability.
",1,0,0,1,0,0
945,Total-positivity preservers,"  We prove that the only entrywise transforms of rectangular matrices which
preserve total positivity or total non-negativity are either constant or
linear. This follows from an extended classification of preservers of these two
properties for matrices of fixed dimension. We also prove that the same
assertions hold upon working only with symmetric matrices; for total-positivity
preservers our proofs proceed through solving two totally positive completion
problems.
",0,0,1,0,0,0
19903,Multiple Source Domain Adaptation with Adversarial Training of Neural Networks,"  While domain adaptation has been actively researched in recent years, most
theoretical results and algorithms focus on the single-source-single-target
adaptation setting. Naive application of such algorithms on multiple source
domain adaptation problem may lead to suboptimal solutions. As a step toward
bridging the gap, we propose a new generalization bound for domain adaptation
when there are multiple source domains with labeled instances and one target
domain with unlabeled instances. Compared with existing bounds, the new bound
does not require expert knowledge about the target distribution, nor the
optimal combination rule for multisource domains. Interestingly, our theory
also leads to an efficient learning strategy using adversarial neural networks:
we show how to interpret it as learning feature representations that are
invariant to the multiple domain shifts while still being discriminative for
the learning task. To this end, we propose two models, both of which we call
multisource domain adversarial networks (MDANs): the first model optimizes
directly our bound, while the second model is a smoothed approximation of the
first one, leading to a more data-efficient and task-adaptive model. The
optimization tasks of both models are minimax saddle point problems that can be
optimized by adversarial training. To demonstrate the effectiveness of MDANs,
we conduct extensive experiments showing superior adaptation performance on
three real-world datasets: sentiment analysis, digit classification, and
vehicle counting.
",1,0,0,1,0,0
2790,Chaos and thermalization in small quantum systems,"  Chaos and ergodicity are the cornerstones of statistical physics and
thermodynamics. While classically even small systems like a particle in a
two-dimensional cavity, can exhibit chaotic behavior and thereby relax to a
microcanonical ensemble, quantum systems formally can not. Recent theoretical
breakthroughs and, in particular, the eigenstate thermalization hypothesis
(ETH) however indicate that quantum systems can also thermalize. In fact ETH
provided us with a framework connecting microscopic models and macroscopic
phenomena, based on the notion of highly entangled quantum states. Such
thermalization was beautifully demonstrated experimentally by A. Kaufman et.
al. who studied relaxation dynamics of a small lattice system of interacting
bosonic particles. By directly measuring the entanglement entropy of
subsystems, as well as other observables, they showed that after the initial
transient time the system locally relaxes to a thermal ensemble while globally
maintaining a zero-entropy pure state.
",0,1,0,0,0,0
644,Deep Multi-User Reinforcement Learning for Distributed Dynamic Spectrum Access,"  We consider the problem of dynamic spectrum access for network utility
maximization in multichannel wireless networks. The shared bandwidth is divided
into K orthogonal channels. In the beginning of each time slot, each user
selects a channel and transmits a packet with a certain transmission
probability. After each time slot, each user that has transmitted a packet
receives a local observation indicating whether its packet was successfully
delivered or not (i.e., ACK signal). The objective is a multi-user strategy for
accessing the spectrum that maximizes a certain network utility in a
distributed manner without online coordination or message exchanges between
users. Obtaining an optimal solution for the spectrum access problem is
computationally expensive in general due to the large state space and partial
observability of the states. To tackle this problem, we develop a novel
distributed dynamic spectrum access algorithm based on deep multi-user
reinforcement leaning. Specifically, at each time slot, each user maps its
current state to spectrum access actions based on a trained deep-Q network used
to maximize the objective function. Game theoretic analysis of the system
dynamics is developed for establishing design principles for the implementation
of the algorithm. Experimental results demonstrate strong performance of the
algorithm.
",1,0,0,0,0,0
13823,Hidden chiral symmetries in BDI multichannel Kitaev chains,"  Realistic implementations of the Kitaev chain require, in general, the
introduction of extra internal degrees of freedom. In the present work, we
discuss the presence of hidden BDI symmetries for free Hamiltonians describing
systems with an arbitrary number of internal degrees of freedom. We generalize
results of a spinfull Kitaev chain to construct a Hamiltonian with $n$ internal
degrees of freedom and obtain the corresponding hidden chiral symmetry. As an
explicit application of this generalized result, we exploit by analytical and
numerical calculations the case of a spinful 2-band Kitaev chain, which can
host up to 4 Majorana bound states. We also observe the appearence of minigap
states, when chiral symmetry is broken.
",0,1,0,0,0,0
8908,Statistics students' identification of inferential model elements within contexts of their own invention,"  Statistical thinking partially depends upon an iterative process by which
essential features of a problem setting are identified and mapped onto an
abstract model or archetype, and then translated back into the context of the
original problem setting (Wild and Pfannkuch 1999). Assessment in introductory
statistics often relies on tasks that present students with data in context and
expects them to choose and describe an appropriate model. This study explores
post-secondary student responses to an alternative task that prompts students
to clearly identify a sample, population, statistic, and parameter using a
context of their own invention. The data include free text narrative responses
of a random sample of 500 students from a sample of more than 1600 introductory
statistics students. Results suggest that students' responses often portrayed
sample and population accurately. Portrayals of statistic and parameter were
less reliable and were associated with descriptions of a wide variety of other
concepts. Responses frequently attributed a variable of some kind to the
statistic, or a study design detail to the parameter. Implications for
instruction and research are discussed, including a call for emphasis on a
modeling paradigm in introductory statistics.
",0,0,0,1,0,0
5996,RPC: A Large-Scale Retail Product Checkout Dataset,"  Over recent years, emerging interest has occurred in integrating computer
vision technology into the retail industry. Automatic checkout (ACO) is one of
the critical problems in this area which aims to automatically generate the
shopping list from the images of the products to purchase. The main challenge
of this problem comes from the large scale and the fine-grained nature of the
product categories as well as the difficulty for collecting training images
that reflect the realistic checkout scenarios due to continuous update of the
products. Despite its significant practical and research value, this problem is
not extensively studied in the computer vision community, largely due to the
lack of a high-quality dataset. To fill this gap, in this work we propose a new
dataset to facilitate relevant research. Our dataset enjoys the following
characteristics: (1) It is by far the largest dataset in terms of both product
image quantity and product categories. (2) It includes single-product images
taken in a controlled environment and multi-product images taken by the
checkout system. (3) It provides different levels of annotations for the
check-out images. Comparing with the existing datasets, ours is closer to the
realistic setting and can derive a variety of research problems. Besides the
dataset, we also benchmark the performance on this dataset with various
approaches. The dataset and related resources can be found at
\url{this https URL}.
",1,0,0,0,0,0
14328,Improving pairwise comparison models using Empirical Bayes shrinkage,"  Comparison data arises in many important contexts, e.g. shopping, web clicks,
or sports competitions. Typically we are given a dataset of comparisons and
wish to train a model to make predictions about the outcome of unseen
comparisons. In many cases available datasets have relatively few comparisons
(e.g. there are only so many NFL games per year) or efficiency is important
(e.g. we want to quickly estimate the relative appeal of a product). In such
settings it is well known that shrinkage estimators outperform maximum
likelihood estimators. A complicating matter is that standard comparison models
such as the conditional multinomial logit model are only models of conditional
outcomes (who wins) and not of comparisons themselves (who competes). As such,
different models of the comparison process lead to different shrinkage
estimators. In this work we derive a collection of methods for estimating the
pairwise uncertainty of pairwise predictions based on different assumptions
about the comparison process. These uncertainty estimates allow us both to
examine model uncertainty as well as perform Empirical Bayes shrinkage
estimation of the model parameters. We demonstrate that our shrunk estimators
outperform standard maximum likelihood methods on real comparison data from
online comparison surveys as well as from several sports contexts.
",1,0,0,1,0,0
14208,Topological conjugacy of topological Markov shifts and Ruelle algebras,"  We will characterize topologically conjugate two-sided topological Markov
shifts $(\bar{X}_A,\bar{\sigma}_A)$ in terms of the associated asymptotic
Ruelle $C^*$-algebras ${\mathcal{R}}_A$ with its commutative $C^*$-subalgebras
$C(\bar{X}_A)$ and the canonical circle actions. We will also show that
extended Ruelle algebras ${\widetilde{\mathcal{R}}}_A$, which are purely
infinite version of the asymptotic Ruelle algebras, with its commutative
$C^*$-subalgebras $C(\bar{X}_A)$ and the canonical torus actions $\gamma^A$ are
complete invariants for topological conjugacy of two-sided topological Markov
shifts. We then have a computable topological conjugacy invariant, written in
terms of the underlying matrix, of a two-sided topological Markov shift by
using K-theory of the extended Ruelle algebra. The diagonal action of
$\gamma^A$ has a unique KMS-state on ${\widetilde{\mathcal{R}}}_A$, which is an
extension of the Parry measure on $\bar{X}_A$.
",0,0,1,0,0,0
4968,Minority carrier diffusion lengths and mobilities in low-doped n-InGaAs for focal plane array applications,"  The hole diffusion length in n-InGaAs is extracted for two samples of
different doping concentrations using a set of long and thin diffused junction
diodes separated by various distances on the order of the diffusion length. The
methodology is described, including the ensuing analysis which yields diffusion
lengths between 70 - 85 um at room temperature for doping concentrations in the
range of 5 - 9 x 10^15 cm-3. The analysis also provides insight into the
minority carrier mobility which is a parameter not commonly reported in the
literature. Hole mobilities on the order of 500 - 750 cm2/Vs are reported for
the aforementioned doping range, which are comparable albeit longer than the
majority hole mobility for the same doping magnitude in p-InGaAs. A radiative
recombination coefficient of (0.5-0.2)x10^-10 cm^-3s^-1 is also extracted from
the ensuing analysis for an InGaAs thickness of 2.7 um. Preliminary evidence is
also given for both heavy and light hole diffusion. The dark current of
InP/InGaAs p-i-n photodetectors with 25 and 15 um pitches are then calibrated
to device simulations and correlated to the extracted diffusion lengths and
doping concentrations. An effective Shockley-Read-Hall lifetime of between
90-200 us provides the best fit to the dark current of these structures.
",0,1,0,0,0,0
3150,United Nations Digital Blue Helmets as a Starting Point for Cyber Peacekeeping,"  Prior works, such as the Tallinn manual on the international law applicable
to cyber warfare, focus on the circumstances of cyber warfare. Many
organizations are considering how to conduct cyber warfare, but few have
discussed methods to reduce, or even prevent, cyber conflict. A recent series
of publications started developing the framework of Cyber Peacekeeping (CPK)
and its legal requirements. These works assessed the current state of
organizations such as ITU IMPACT, NATO CCDCOE and Shanghai Cooperation
Organization, and found that they did not satisfy requirements to effectively
host CPK activities. An assessment of organizations currently working in the
areas related to CPK found that the United Nations (UN) has mandates and
organizational structures that appear to somewhat overlap the needs of CPK.
However, the UN's current approach to Peacekeeping cannot be directly mapped to
cyberspace. In this research we analyze the development of traditional
Peacekeeping in the United Nations, and current initiatives in cyberspace.
Specifically, we will compare the proposed CPK framework with the recent
initiative of the United Nations named the 'Digital Blue Helmets' as well as
with other projects in the UN which helps to predict and mitigate conflicts.
Our goal is to find practical recommendations for the implementation of the CPK
framework in the United Nations, and to examine how responsibilities defined in
the CPK framework overlap with those of the 'Digital Blue Helmets' and the
Global Pulse program.
",1,0,0,0,0,0
4226,The loss surface of deep and wide neural networks,"  While the optimization problem behind deep neural networks is highly
non-convex, it is frequently observed in practice that training deep networks
seems possible without getting stuck in suboptimal points. It has been argued
that this is the case as all local minima are close to being globally optimal.
We show that this is (almost) true, in fact almost all local minima are
globally optimal, for a fully connected network with squared loss and analytic
activation function given that the number of hidden units of one layer of the
network is larger than the number of training points and the network structure
from this layer on is pyramidal.
",1,0,0,1,0,0
10732,Empirical study on social groups in pedestrian evacuation dynamics,"  Pedestrian crowds often include social groups, i.e. pedestrians that walk
together because of social relationships. They show characteristic
configurations and influence the dynamics of the entire crowd. In order to
investigate the impact of social groups on evacuations we performed an
empirical study with pupils. Several evacuation runs with groups of different
sizes and different interactions were performed. New group parameters are
introduced which allow to describe the dynamics of the groups and the
configuration of the group members quantitatively. The analysis shows a
possible decrease of evacuation times for large groups due to self-ordering
effects. Social groups can be approximated as ellipses that orientate along
their direction of motion. Furthermore, explicitly cooperative behaviour among
group members leads to a stronger aggregation of group members and an
intermittent way of evacuation.
",0,1,0,0,0,0
698,Multi-User Multi-Armed Bandits for Uncoordinated Spectrum Access,"  A multi-user multi-armed bandit (MAB) framework is used to develop algorithms
for uncoordinated spectrum access. The number of users is assumed to be unknown
to each user. A stochastic setting is first considered, where the rewards on a
channel are the same for each user. In contrast to prior work, it is assumed
that the number of users can possibly exceed the number of channels, and that
rewards can be non-zero even under collisions. The proposed algorithm consists
of an estimation phase and an allocation phase. It is shown that if every user
adopts the algorithm, the system wide regret is constant with time with high
probability. The regret guarantees hold for any number of users and channels,
in particular, even when the number of users is less than the number of
channels. Next, an adversarial multi-user MAB framework is considered, where
the rewards on the channels are user-dependent. It is assumed that the number
of users is less than the number of channels, and that the users receive zero
reward on collision. The proposed algorithm combines the Exp3.P algorithm
developed in prior work for single user adversarial bandits with a collision
resolution mechanism to achieve sub-linear regret. It is shown that if every
user employs the proposed algorithm, the system wide regret is of the order
$O(T^\frac{3}{4})$ over a horizon of time $T$. The algorithms in both
stochastic and adversarial scenarios are extended to the dynamic case where the
number of users in the system evolves over time and are shown to lead to
sub-linear regret.
",0,0,0,1,0,0
5475,On recurrence in G-spaces,"  We introduce and analyze the following general concept of recurrence. Let $G$
be a group and let $X$ be a G-space with the action $G\times X\longrightarrow
X$, $(g,x)\longmapsto gx$. For a family $\mathfrak{F}$ of subset of $X$ and
$A\in \mathfrak{F}$, we denote $\Delta_{\mathfrak{F}}(A)=\{g\in G: gB\subseteq
A$ for some $B\in \mathfrak{F}, \ B\subseteq A\}$, and say that a subset $R$ of
$G$ is $\mathfrak{F}$-recurrent if $R\bigcap \Delta_{\mathfrak{F}}
(A)\neq\emptyset$ for each $A\in \mathfrak{F}$.
",0,0,1,0,0,0
9794,The Supernova -- Supernova Remnant Connection,"  Many aspects of the progenitor systems, environments, and explosion dynamics
of the various subtypes of supernovae are difficult to investigate at
extragalactic distances where they are observed as unresolved sources.
Alternatively, young supernova remnants in our own galaxy and in the Large and
Small Magellanic Clouds offer opportunities to resolve, measure, and track
expanding stellar ejecta in fine detail, but the handful that are known exhibit
widely different properties that reflect the diversity of their parent
explosions and local circumstellar and interstellar environments. A way of
complementing both supernova and supernova remnant research is to establish
strong empirical links between the two separate stages of stellar explosions.
Here we briefly review recent progress in the development of
supernova---supernova remnant connections, paying special attention to
connections made through the study of ""middle-aged"" (10-100 yr) supernovae and
young (< 1000 yr) supernova remnants. We highlight how this approach can
uniquely inform several key areas of supernova research, including the origins
of explosive mixing, high-velocity jets, and the formation of dust in the
ejecta.
",0,1,0,0,0,0
9238,BaFe2(As1-xPx)2 (x = 0.22-0.42) thin films grown on practical metal-tape substrates and their critical current densities,"  We optimized the substrate temperature (Ts) and phosphorus concentration (x)
of BaFe2(As1-xPx)2 films on practical metal-tape substrates for pulsed laser
deposition from the viewpoints of crystallinity, superconductor critical
temperature (Tc), and critical current density (Jc). It was found that the
optimum Ts and x values are 1050 degree C and x = 0.28, respectively. The
optimized film exhibits Tc_onset = 26.6 and Tc_zero = 22.4 K along with a high
self-field Jc at 4 K (~1 MA/cm2) and relatively isotropic Jc under magnetic
fields up to 9 T. Unexpectedly, we found that lower crystallinity samples,
which were grown at a higher Ts of 1250 degree C than the optimized Ts = 1050
degree C, exhibit higher Jc along the ab plane under high magnetic fields than
the optimized samples. The presence of horizontal defects that act as strong
vortex pinning centers, such as stacking faults, are a possible origin of the
high Jc values in the poor crystallinity samples.
",0,1,0,0,0,0
7254,Bar formation in the Milky Way type galaxies,"  Many barred galaxies, possibly including the Milky Way, have cusps in the
centres. There is a widespread belief, however, that usual bar instability
taking place in bulgeless galaxy models is impossible for the cuspy models,
because of the presence of the inner Lindblad resonance for any pattern speed.
At the same time there are numerical evidences that the bar instability can
form a bar. We analyse this discrepancy, by accurate and diverse N-body
simulations and using the calculation of normal modes. We show that bar
formation in cuspy galaxies can be explained by taking into account the disc
thickness. The exponential growth time is moderate for typical current disc
masses (about 250 Myr), but considerably increases (factor 2 or more) upon
substitution of the live halo and bulge with a rigid halo/bulge potential;
meanwhile pattern speeds remain almost the same. Normal mode analysis with
different disc mass favours a young bar hypothesis, according to which the bar
instability saturated only recently.
",0,1,0,0,0,0
5278,Do You Want Your Autonomous Car To Drive Like You?,"  With progress in enabling autonomous cars to drive safely on the road, it is
time to start asking how they should be driving. A common answer is that they
should be adopting their users' driving style. This makes the assumption that
users want their autonomous cars to drive like they drive - aggressive drivers
want aggressive cars, defensive drivers want defensive cars. In this paper, we
put that assumption to the test. We find that users tend to prefer a
significantly more defensive driving style than their own. Interestingly, they
prefer the style they think is their own, even though their actual driving
style tends to be more aggressive. We also find that preferences do depend on
the specific driving scenario, opening the door for new ways of learning
driving style preference.
",1,0,0,0,0,0
12649,Crystalline Electric Field Randomness in the Triangular Lattice Spin-Liquid YbMgGaO$_4$,"  We apply moderate-high-energy inelastic neutron scattering (INS) measurements
to investigate Yb$^{3+}$ crystalline electric field (CEF) levels in the
triangular spin-liquid candidate YbMgGaO$_4$. Three CEF excitations from the
ground-state Kramers doublet are centered at the energies $\hbar \omega$ = 39,
61, and 97\,meV in agreement with the effective \mbox{spin-1/2} $g$-factors and
experimental heat capacity, but reveal sizable broadening. We argue that this
broadening originates from the site mixing between Mg$^{2+}$ and Ga$^{3+}$
giving rise to a distribution of Yb--O distances and orientations and, thus, of
CEF parameters that account for the peculiar energy profile of the CEF
excitations. The CEF randomness gives rise to a distribution of the effective
spin-1/2 $g$-factors and explains the unprecedented broadening of low-energy
magnetic excitations in the fully polarized ferromagnetic phase of YbMgGaO$_4$,
although a distribution of magnetic couplings due to the Mg/Ga disorder may be
important as well.
",0,1,0,0,0,0
10366,From Relational Data to Graphs: Inferring Significant Links using Generalized Hypergeometric Ensembles,"  The inference of network topologies from relational data is an important
problem in data analysis. Exemplary applications include the reconstruction of
social ties from data on human interactions, the inference of gene
co-expression networks from DNA microarray data, or the learning of semantic
relationships based on co-occurrences of words in documents. Solving these
problems requires techniques to infer significant links in noisy relational
data. In this short paper, we propose a new statistical modeling framework to
address this challenge. It builds on generalized hypergeometric ensembles, a
class of generative stochastic models that give rise to analytically tractable
probability spaces of directed, multi-edge graphs. We show how this framework
can be used to assess the significance of links in noisy relational data. We
illustrate our method in two data sets capturing spatio-temporal proximity
relations between actors in a social system. The results show that our
analytical framework provides a new approach to infer significant links from
relational data, with interesting perspectives for the mining of data on social
systems.
",1,1,0,1,0,0
17702,Aggregation and Disaggregation of Energetic Flexibility from Distributed Energy Resources,"  A variety of energy resources has been identified as being flexible in their
electric energy consumption or generation. This energetic flexibility can be
used for various purposes such as minimizing energy procurement costs or
providing ancillary services to power grids. To fully leverage the flexibility
available from distributed small-scale resources, their flexibility must be
quantified and aggregated.
This paper introduces a generic and scalable approach for flexible energy
systems to quantitatively describe and price their flexibility based on
zonotopic sets. The description proposed allows aggregators to efficiently pool
the flexibility of large numbers of systems and to make control and market
decisions on the aggregate level. In addition, an algorithm is presented that
distributes aggregate-level control decisions among the individual systems of
the pool in an economically fair and computationally efficient way. Finally, it
is shown how the zonotopic description of flexibility enables an efficient
computation of aggregate regulation power bid-curves.
",1,0,0,0,0,0
18365,The Geometry and Topology of Data and Information for Analytics of Processes and Behaviours: Building on Bourdieu and Addressing New Societal Challenges,"  We begin by summarizing the relevance and importance of inductive analytics
based on the geometry and topology of data and information. Contemporary issues
are then discussed. These include how sampling data for representativity is
increasingly to be questioned. While we can always avail of analytics from a
""bag of tools and techniques"", in the application of machine learning and
predictive analytics, nonetheless we present the case for Bourdieu and
Benzécri-based science of data, as follows. This is to construct bridges
between data sources and position-taking, and decision-making. There is summary
presentation of a few case studies, illustrating and exemplifying application
domains.
",1,0,0,0,0,0
9857,Strong Completeness and the Finite Model Property for Bi-Intuitionistic Stable Tense Logics,"  Bi-Intuitionistic Stable Tense Logics (BIST Logics) are tense logics with a
Kripke semantics where worlds in a frame are equipped with a pre-order as well
as with an accessibility relation which is 'stable' with respect to this
pre-order. BIST logics are extensions of a logic, BiSKt, which arose in the
semantic context of hypergraphs, since a special case of the pre-order can
represent the incidence structure of a hypergraph. In this paper we provide,
for the first time, a Hilbert-style axiomatisation of BISKt and prove the
strong completeness of BiSKt. We go on to prove strong completeness of a class
of BIST logics obtained by extending BiSKt by formulas of a certain form.
Moreover we show that the finite model property and the decidability hold for a
class of BIST logics.
",1,0,1,0,0,0
7321,Data-Driven Stochastic Robust Optimization: A General Computational Framework and Algorithm for Optimization under Uncertainty in the Big Data Era,"  A novel data-driven stochastic robust optimization (DDSRO) framework is
proposed for optimization under uncertainty leveraging labeled multi-class
uncertainty data. Uncertainty data in large datasets are often collected from
various conditions, which are encoded by class labels. Machine learning methods
including Dirichlet process mixture model and maximum likelihood estimation are
employed for uncertainty modeling. A DDSRO framework is further proposed based
on the data-driven uncertainty model through a bi-level optimization structure.
The outer optimization problem follows a two-stage stochastic programming
approach to optimize the expected objective across different data classes;
adaptive robust optimization is nested as the inner problem to ensure the
robustness of the solution while maintaining computational tractability. A
decomposition-based algorithm is further developed to solve the resulting
multi-level optimization problem efficiently. Case studies on process network
design and planning are presented to demonstrate the applicability of the
proposed framework and algorithm.
",1,0,1,0,0,0
14259,Cohesion-based Online Actor-Critic Reinforcement Learning for mHealth Intervention,"  In the wake of the vast population of smart device users worldwide, mobile
health (mHealth) technologies are hopeful to generate positive and wide
influence on people's health. They are able to provide flexible, affordable and
portable health guides to device users. Current online decision-making methods
for mHealth assume that the users are completely heterogeneous. They share no
information among users and learn a separate policy for each user. However,
data for each user is very limited in size to support the separate online
learning, leading to unstable policies that contain lots of variances. Besides,
we find the truth that a user may be similar with some, but not all, users, and
connected users tend to have similar behaviors. In this paper, we propose a
network cohesion constrained (actor-critic) Reinforcement Learning (RL) method
for mHealth. The goal is to explore how to share information among similar
users to better convert the limited user information into sharper learned
policies. To the best of our knowledge, this is the first online actor-critic
RL for mHealth and first network cohesion constrained (actor-critic) RL method
in all applications. The network cohesion is important to derive effective
policies. We come up with a novel method to learn the network by using the warm
start trajectory, which directly reflects the users' property. The optimization
of our model is difficult and very different from the general supervised
learning due to the indirect observation of values. As a contribution, we
propose two algorithms for the proposed online RLs. Apart from mHealth, the
proposed methods can be easily applied or adapted to other health-related
tasks. Extensive experiment results on the HeartSteps dataset demonstrates that
in a variety of parameter settings, the proposed two methods obtain obvious
improvements over the state-of-the-art methods.
",1,0,0,0,0,0
18212,On the Global Fluctuations of Block Gaussian Matrices,"  In this paper we study the global fluctuations of block Gaussian matrices
within the framework of second-order free probability theory. In order to
compute the second-order Cauchy transform of these matrices, we introduce a
matricial second-order conditional expectation and compute the matricial
second-order Cauchy transform of a certain type of non-commutative random
variables. As a by-product, using the linearization technique, we obtain the
second-order Cauchy transform of non-commutative rational functions evaluated
on selfadjoint Gaussian matrices.
",0,0,1,0,0,0
7358,Interleaved Group Convolutions for Deep Neural Networks,"  In this paper, we present a simple and modularized neural network
architecture, named interleaved group convolutional neural networks (IGCNets).
The main point lies in a novel building block, a pair of two successive
interleaved group convolutions: primary group convolution and secondary group
convolution. The two group convolutions are complementary: (i) the convolution
on each partition in primary group convolution is a spatial convolution, while
on each partition in secondary group convolution, the convolution is a
point-wise convolution; (ii) the channels in the same secondary partition come
from different primary partitions. We discuss one representative advantage:
Wider than a regular convolution with the number of parameters and the
computation complexity preserved. We also show that regular convolutions, group
convolution with summation fusion, and the Xception block are special cases of
interleaved group convolutions. Empirical results over standard benchmarks,
CIFAR-$10$, CIFAR-$100$, SVHN and ImageNet demonstrate that our networks are
more efficient in using parameters and computation complexity with similar or
higher accuracy.
",1,0,0,0,0,0
11793,Algebraic relations between solutions of Painlevé equations,"  We calculate model theoretic ranks of Painlevé equations in this article,
showing in particular, that any equation in any of the Painlevé families has
Morley rank one, extending results of Nagloo and Pillay (2011). We show that
the type of the generic solution of any equation in the second Painlevé
family is geometrically trivial, extending a result of Nagloo (2015).
We also establish the orthogonality of various pairs of equations in the
Painlevé families, showing at least generically, that all instances of
nonorthogonality between equations in the same Painlevé family come from
classically studied B{ä}cklund transformations. For instance, we show that if
at least one of $\alpha, \beta$ is transcendental, then $P_{II} (\alpha)$ is
nonorthogonal to $P_{II} ( \beta )$ if and only if $\alpha+ \beta \in \mathbb
Z$ or $\alpha - \beta \in \mathbb Z$. Our results have concrete interpretations
in terms of characterizing the algebraic relations between solutions of
Painlevé equations. We give similar results for orthogonality relations
between equations in different Painlevé families, and formulate some general
questions which extend conjectures of Nagloo and Pillay (2011) on transcendence
and algebraic independence of solutions to Painlevé equations. We also apply
our analysis of ranks to establish some orthogonality results for pairs of
Painlevé equations from different families. For instance, we answer several
open questions of Nagloo (2016), and in the process answer a question of Boalch
(2012).
",0,0,1,0,0,0
14518,ISeeU: Visually interpretable deep learning for mortality prediction inside the ICU,"  To improve the performance of Intensive Care Units (ICUs), the field of
bio-statistics has developed scores which try to predict the likelihood of
negative outcomes. These help evaluate the effectiveness of treatments and
clinical practice, and also help to identify patients with unexpected outcomes.
However, they have been shown by several studies to offer sub-optimal
performance. Alternatively, Deep Learning offers state of the art capabilities
in certain prediction tasks and research suggests deep neural networks are able
to outperform traditional techniques. Nevertheless, a main impediment for the
adoption of Deep Learning in healthcare is its reduced interpretability, for in
this field it is crucial to gain insight on the why of predictions, to assure
that models are actually learning relevant features instead of spurious
correlations. To address this, we propose a deep multi-scale convolutional
architecture trained on the Medical Information Mart for Intensive Care III
(MIMIC-III) for mortality prediction, and the use of concepts from coalitional
game theory to construct visual explanations aimed to show how important these
inputs are deemed by the network. Our results show our model attains state of
the art performance while remaining interpretable. Supporting code can be found
at this https URL.
",1,0,0,1,0,0
538,A giant planet undergoing extreme ultraviolet irradiation by its hot massive-star host,"  The amount of ultraviolet irradiation and ablation experienced by a planet
depends strongly on the temperature of its host star. Of the thousands of
extra-solar planets now known, only four giant planets have been found that
transit hot, A-type stars (temperatures of 7300-10,000K), and none are known to
transit even hotter B-type stars. WASP-33 is an A-type star with a temperature
of ~7430K, which hosts the hottest known transiting planet; the planet is
itself as hot as a red dwarf star of type M. The planet displays a large heat
differential between its day-side and night-side, and is highly inflated,
traits that have been linked to high insolation. However, even at the
temperature of WASP-33b's day-side, its atmosphere likely resembles the
molecule-dominated atmospheres of other planets, and at the level of
ultraviolet irradiation it experiences, its atmosphere is unlikely to be
significantly ablated over the lifetime of its star. Here we report
observations of the bright star HD 195689, which reveal a close-in (orbital
period ~1.48 days) transiting giant planet, KELT-9b. At ~10,170K, the host star
is at the dividing line between stars of type A and B, and we measure the
KELT-9b's day-side temperature to be ~4600K. This is as hot as stars of stellar
type K4. The molecules in K stars are entirely dissociated, and thus the
primary sources of opacity in the day-side atmosphere of KELT-9b are likely
atomic metals. Furthermore, KELT-9b receives ~700 times more extreme
ultraviolet radiation (wavelengths shorter than 91.2 nanometers) than WASP-33b,
leading to a predicted range of mass-loss rates that could leave the planet
largely stripped of its envelope during the main-sequence lifetime of the host
star.
",0,1,0,0,0,0
10284,Battery Degradation Maps for Power System Optimization and as a Benchmark Reference,"  This paper presents a novel method to describe battery degradation. We use
the concept of degradation maps to model the incremental charge capacity loss
as a function of discrete battery control actions and state of charge. The maps
can be scaled to represent any battery system in size and power. Their convex
piece-wise affine representations allow for tractable optimal control
formulations and can be used in power system simulations to incorporate battery
degradation. The map parameters for different battery technologies are
published making them an useful basis to benchmark different battery
technologies in case studies.
",1,0,0,0,0,0
2407,Convergence rate bounds for a proximal ADMM with over-relaxation stepsize parameter for solving nonconvex linearly constrained problems,"  This paper establishes convergence rate bounds for a variant of the proximal
alternating direction method of multipliers (ADMM) for solving nonconvex
linearly constrained optimization problems. The variant of the proximal ADMM
allows the inclusion of an over-relaxation stepsize parameter belonging to the
interval $(0,2)$. To the best of our knowledge, all related papers in the
literature only consider the case where the over-relaxation parameter lies in
the interval $(0,(1+\sqrt{5})/2)$.
",0,0,1,0,0,0
17381,Design and implementation of dynamic logic gates and R-S flip-flop using quasiperiodically driven Murali-Lakshmanan-Chua circuit,"  We report the propagation of a square wave signal in a quasi-periodically
driven Murali-Lakshmanan-Chua (QPDMLC) circuit system. It is observed that
signal propagation is possible only above a certain threshold strength of the
square wave or digital signal and all the values above the threshold amplitude
are termed as 'region of signal propagation'. Then, we extend this region of
signal propagation to perform various logical operations like AND/NAND/OR/NOR
and hence it is also designated as the 'region of logical operation'. Based on
this region, we propose implementing the dynamic logic gates, namely
AND/NAND/OR/NOR, which can be decided by the asymmetrical input square waves
without altering the system parameters. Further, we show that a single QPDMLC
system will produce simultaneously two outputs which are complementary to each
other. As a result, a single QPDMLC system yields either AND as well as NAND or
OR as well as NOR gates simultaneously. Then we combine the corresponding two
QPDMLC systems in a cross-coupled way and report that its dynamics mimics that
of fundamental R-S flip-flop circuit. All these phenomena have been explained
with analytical solutions of the circuit equations characterizing the system
and finally the results are compared with the corresponding numerical and
experimental analysis.
",0,1,0,0,0,0
16111,Small-Scale Challenges to the $Λ$CDM Paradigm,"  The dark energy plus cold dark matter ($\Lambda$CDM) cosmological model has
been a demonstrably successful framework for predicting and explaining the
large-scale structure of Universe and its evolution with time. Yet on length
scales smaller than $\sim 1$ Mpc and mass scales smaller than $\sim 10^{11}
M_{\odot}$, the theory faces a number of challenges. For example, the observed
cores of many dark-matter dominated galaxies are both less dense and less cuspy
than naively predicted in $\Lambda$CDM. The number of small galaxies and dwarf
satellites in the Local Group is also far below the predicted count of low-mass
dark matter halos and subhalos within similar volumes. These issues underlie
the most well-documented problems with $\Lambda$CDM: Cusp/Core, Missing
Satellites, and Too-Big-to-Fail. The key question is whether a better
understanding of baryon physics, dark matter physics, or both will be required
to meet these challenges. Other anomalies, including the observed planar and
orbital configurations of Local Group satellites and the tight baryonic/dark
matter scaling relations obeyed by the galaxy population, have been less
thoroughly explored in the context of $\Lambda$CDM theory. Future surveys to
discover faint, distant dwarf galaxies and to precisely measure their masses
and density structure hold promising avenues for testing possible solutions to
the small-scale challenges going forward. Observational programs to constrain
or discover and characterize the number of truly dark low-mass halos are among
the most important, and achievable, goals in this field over then next decade.
These efforts will either further verify the $\Lambda$CDM paradigm or demand a
substantial revision in our understanding of the nature of dark matter.
",0,1,0,0,0,0
3132,Tailoring Heterovalent Interface Formation with Light,"  Integrating different semiconductor materials into an epitaxial device
structure offers additional degrees of freedom to select for optimal material
properties in each layer. However, interface between materials with different
valences (i.e. III-V, II-VI and IV semiconductors) can be difficult to form
with high quality. Using ZnSe/GaAs as a model system, we explore the use of UV
illumination during heterovalent interface growth by molecular beam epitaxy as
a way to modify the interface properties. We find that UV illumination alters
the mixture of chemical bonds at the interface, permitting the formation of
Ga-Se bonds that help to passivate the underlying GaAs layer. Illumination also
helps to reduce defects in the ZnSe epilayer. These results suggest that
moderate UV illumination during growth may be used as a way to improve the
optical properties of both the GaAs and ZnSe layers on either side of the
interface.
",0,1,0,0,0,0
12283,On the conjecture of Jeśmanowicz,"  We give a survey on some results covering the last 60 years concerning
Jeśmanowicz' conjecture. Moreover, we conclude the survey with a new result
by showing that the special Diophantine equation $$(20k)^x+(99k)^y=(101k)^z$$
has no solution other than $(x,y,z)=(2,2,2)$.
",0,0,1,0,0,0
13436,Kernel Recursive ABC: Point Estimation with Intractable Likelihood,"  We propose a novel approach to parameter estimation for simulator-based
statistical models with intractable likelihood. Our proposed method involves
recursive application of kernel ABC and kernel herding to the same observed
data. We provide a theoretical explanation regarding why the approach works,
showing (for the population setting) that, under a certain assumption, point
estimates obtained with this method converge to the true parameter, as
recursion proceeds. We have conducted a variety of numerical experiments,
including parameter estimation for a real-world pedestrian flow simulator, and
show that in most cases our method outperforms existing approaches.
",0,0,0,1,0,0
16487,Kinematically Redundant Octahedral Motion Platform for Virtual Reality Simulations,"  We propose a novel design of a parallel manipulator of Stewart Gough type for
virtual reality application of single individuals; i.e. an omni-directional
treadmill is mounted on the motion platform in order to improve VR immersion by
giving feedback to the human body. For this purpose we modify the well-known
octahedral manipulator in a way that it has one degree of kinematical
redundancy; namely an equiform reconfigurability of the base. The instantaneous
kinematics and singularities of this mechanism are studied, where especially
""unavoidable singularities"" are characterized. These are poses of the motion
platform, which can only be realized by singular configurations of the
mechanism despite its kinematic redundancy.
",1,0,0,0,0,0
11678,Annealed limit theorems for the ising model on random regular graphs,"  In a recent paper [15], Giardin{à}, Giberti, Hofstad, Prioriello have
proved a law of large number and a central limit theorem with respect to the
annealed measure for the magnetization of the Ising model on some random graphs
including the random 2-regular graph. We present a new proof of their results,
which applies to all random regular graphs. In addition, we prove the existence
of annealed pressure in the case of configuration model random graphs.
",0,1,1,0,0,0
4655,Towards Neural Phrase-based Machine Translation,"  In this paper, we present Neural Phrase-based Machine Translation (NPMT). Our
method explicitly models the phrase structures in output sequences using
Sleep-WAke Networks (SWAN), a recently proposed segmentation-based sequence
modeling method. To mitigate the monotonic alignment requirement of SWAN, we
introduce a new layer to perform (soft) local reordering of input sequences.
Different from existing neural machine translation (NMT) approaches, NPMT does
not use attention-based decoding mechanisms. Instead, it directly outputs
phrases in a sequential order and can decode in linear time. Our experiments
show that NPMT achieves superior performances on IWSLT 2014
German-English/English-German and IWSLT 2015 English-Vietnamese machine
translation tasks compared with strong NMT baselines. We also observe that our
method produces meaningful phrases in output languages.
",1,0,0,1,0,0
10930,Modalities in homotopy type theory,"  Univalent homotopy type theory (HoTT) may be seen as a language for the
category of $\infty$-groupoids. It is being developed as a new foundation for
mathematics and as an internal language for (elementary) higher toposes. We
develop the theory of factorization systems, reflective subuniverses, and
modalities in homotopy type theory, including their construction using a
""localization"" higher inductive type. This produces in particular the
($n$-connected, $n$-truncated) factorization system as well as internal
presentations of subtoposes, through lex modalities. We also develop the
semantics of these constructions.
",1,0,1,0,0,0
16561,Multi-SpaM: a Maximum-Likelihood approach to Phylogeny reconstruction based on Multiple Spaced-Word Matches,"  Motivation: Word-based or `alignment-free' methods for phylogeny
reconstruction are much faster than traditional approaches, but they are
generally less accurate. Most of these methods calculate pairwise distances for
a set of input sequences, for example from word frequencies, from so-called
spaced-word matches or from the average length of common substrings.
Results: In this paper, we propose the first word-based approach to tree
reconstruction that is based on multiple sequence comparison and Maximum
Likelihood. Our algorithm first samples small, gap-free alignments involving
four taxa each. For each of these alignments, it then calculates a quartet tree
and, finally, the program Quartet MaxCut is used to infer a super tree topology
for the full set of input taxa from the calculated quartet trees. Experimental
results show that trees calculated with our approach are of high quality.
Availability: The source code of the program is available at
this https URL
Contact: thomas.dencker@stud.uni-goettingen.de
",0,0,0,0,1,0
5089,Factorisation of the product of Dirichlet series of completely multiplicative functions,"  In the first chapter, we will present a computation of the square value of
the module of L functions associated to a Dirichlet character. This computation
suggests to ask if a certain ring of arithmetic multiplicative functions exists
and if it is unique. This search has led to the construction of that ring in
chapter two. Finally, in the third chapter, we will present some propositions
associated with this ring. The result below is one of the main results of this
work :
For F and G two completely multiplicative functions, $ s $ a complex number
such as the dirichlet series $ D(F,s) $ and $ D(G,s) $ converge :
$ \forall F,G \in \mathbb{M}_{c} : D(F,s) \times D(G,s) = D(F \times G,2s)
\times D(F \square G,s) $
where the operation $ \square $ is defined in chapter two as the sum of the
previously mentioned ring. Here are some similar versions, with $ s = x+iy $ :
$ \forall F, G \in \mathbb{M}_{c} : ~ D(F,s) \times D(G,\overline{s}) = D(F
\times G,2x) \times D(\frac{F}{\text{Id}_{e}^{iy}} \square
\frac{G}{\text{Id}_{e}^{-iy}}, x) $
$ \forall F, G \in \mathbb{M}_{c} : ~ |D(F,s)|^{2} = D(|F|^{2},2x) \times
D(\frac{F}{\text{Id}_{e}^{iy}} \square \overline{\frac{F}{\text{Id}_{e}^{iy}}},
x) $
",0,0,1,0,0,0
11177,Gradient Flows in Uncertainty Propagation and Filtering of Linear Gaussian Systems,"  The purpose of this work is mostly expository and aims to elucidate the
Jordan-Kinderlehrer-Otto (JKO) scheme for uncertainty propagation, and a
variant, the Laugesen-Mehta-Meyn-Raginsky (LMMR) scheme for filtering. We point
out that these variational schemes can be understood as proximal operators in
the space of density functions, realizing gradient flows. These schemes hold
the promise of leading to efficient ways for solving the Fokker-Planck equation
as well as the equations of non-linear filtering. Our aim in this paper is to
develop in detail the underlying ideas in the setting of linear stochastic
systems with Gaussian noise and recover known results.
",1,0,1,0,0,0
10643,Bi-$s^*$-concave distributions,"  We introduce a new shape-constrained class of distribution functions on R,
the bi-$s^*$-concave class. In parallel to results of Dümbgen, Kolesnyk, and
Wilke (2017) for what they called the class of bi-log-concave distribution
functions, we show that every s-concave density f has a bi-$s^*$-concave
distribution function $F$ and that every bi-$s^*$-concave distribution function
satisfies $\gamma (F) \le 1/(1+s)$ where finiteness of $$ \gamma (F) \equiv
\sup_{x} F(x) (1-F(x)) \frac{| f' (x)|}{f^2 (x)}, $$ the Csörgő -
Révész constant of F, plays an important role in the theory of quantile
processes on $R$.
",0,0,1,1,0,0
18992,Learning Word Embeddings from the Portuguese Twitter Stream: A Study of some Practical Aspects,"  This paper describes a preliminary study for producing and distributing a
large-scale database of embeddings from the Portuguese Twitter stream. We start
by experimenting with a relatively small sample and focusing on three
challenges: volume of training data, vocabulary size and intrinsic evaluation
metrics. Using a single GPU, we were able to scale up vocabulary size from 2048
words embedded and 500K training examples to 32768 words over 10M training
examples while keeping a stable validation loss and approximately linear trend
on training time per epoch. We also observed that using less than 50\% of the
available training examples for each vocabulary size might result in
overfitting. Results on intrinsic evaluation show promising performance for a
vocabulary size of 32768 words. Nevertheless, intrinsic evaluation metrics
suffer from over-sensitivity to their corresponding cosine similarity
thresholds, indicating that a wider range of metrics need to be developed to
track progress.
",1,0,0,0,0,0
18247,Width-$k$ Generalizations of Classical Permutation Statistics,"  We introduce new natural generalizations of the classical descent and
inversion statistics for permutations, called width-$k$ descents and width-$k$
inversions. These variations induce generalizations of the excedance and major
statistics, providing a framework in which the most well-known
equidistributivity results for classical statistics are paralleled. We explore
additional relationships among the statistics providing specific formulas in
certain special cases. Moreover, we explore the behavior of these width-$k$
statistics in the context of pattern avoidance.
",0,0,1,0,0,0
13890,The Merging Path Plot: adaptive fusing of k-groups with likelihood-based model selection,"  There are many statistical tests that verify the null hypothesis: the
variable of interest has the same distribution among k-groups. But once the
null hypothesis is rejected, how to present the structure of dissimilarity
between groups? In this article, we introduce The Merging Path Plot - a
methodology, and factorMerger - an R package, for exploration and visualization
of k-group dissimilarities. Comparison of k-groups is one of the most important
issues in exploratory analyses and it has zillions of applications. The
classical solution is to test a~null hypothesis that observations from all
groups come from the same distribution. If the global null hypothesis is
rejected, a~more detailed analysis of differences among pairs of groups is
performed. The traditional approach is to use pairwise post hoc tests in order
to verify which groups differ significantly. However, this approach fails with
a large number of groups in both interpretation and visualization layer.
The~Merging Path Plot methodology solves this problem by using an
easy-to-understand description of dissimilarity among groups based on
Likelihood Ratio Test (LRT) statistic.
",1,0,0,1,0,0
20155,A local weighted Axler-Zheng theorem in $\mathbb{C}^n$,"  The well-known Axler-Zheng theorem characterizes compactness of finite sums
of finite products of Toeplitz operators on the unit disk in terms of the
Berezin transform of these operators. Subsequently this theorem was generalized
to other domains and appeared in different forms, including domains in
$\mathbb{C}^n$ on which the $\overline{\partial}$-Neumann operator $N$ is
compact. In this work we remove the assumption on $N$, and we study weighted
Bergman spaces on smooth bounded pseudoconvex domains. We prove a local version
of the Axler-Zheng theorem characterizing compactness of Toeplitz operators in
the algebra generated by symbols continuous up to the boundary in terms of the
behavior of the Berezin transform at strongly pseudoconvex points. We employ a
Forelli-Rudin type inflation method to handle the weights.
",0,0,1,0,0,0
8484,"On the Dynamics of Supermassive Black Holes in Gas-Rich, Star-Forming Galaxies: the Case for Nuclear Star Cluster Coevolution","  We introduce a new model for the formation and evolution of supermassive
black holes (SMBHs) in the RAMSES code using sink particles, improving over
previous work the treatment of gas accretion and dynamical evolution. This new
model is tested against a suite of high-resolution simulations of an isolated,
gas-rich, cooling halo. We study the effect of various feedback models on the
SMBH growth and its dynamics within the galaxy.
In runs without any feedback, the SMBH is trapped within a massive bulge and
is therefore able to grow quickly, but only if the seed mass is chosen larger
than the minimum Jeans mass resolved by the simulation. We demonstrate that, in
the absence of supernovae (SN) feedback, the maximum SMBH mass is reached when
Active Galactic Nucleus (AGN) heating balances gas cooling in the nuclear
region.
When our efficient SN feedback is included, it completely prevents bulge
formation, so that massive gas clumps can perturb the SMBH orbit, and reduce
the accretion rate significantly. To overcome this issue, we propose an
observationally motivated model for the joint evolution of the SMBH and a
parent nuclear star cluster (NSC), which allows the SMBH to remain in the
nuclear region, grow fast and resist external perturbations. In this scenario,
however, SN feedback controls the gas supply and the maximum SMBH mass now
depends on the balance between AGN heating and gravity. We conclude that
SMBH/NSC co-evolution is crucial for the growth of SMBH in high-z galaxies, the
progenitors of massive elliptical today.
",0,1,0,0,0,0
8050,Submolecular-resolution non-invasive imaging of interfacial water with atomic force microscopy,"  Scanning probe microscopy (SPM) has been extensively applied to probe
interfacial water in many interdisciplinary fields but the disturbance of the
probes on the hydrogen-bonding structure of water has remained an intractable
problem. Here we report submolecular-resolution imaging of the water clusters
on a NaCl(001) surface within the nearly non-invasive region by a qPlus-based
noncontact atomic force microscopy. Comparison with theoretical simulations
reveals that the key lies in probing the weak high-order electrostatic force
between the quadrupole-like CO-terminated tip and the polar water molecules at
large tip-water distances. This interaction allows the imaging and structural
determination of the weakly bonded water clusters and even of their metastable
states without inducing any disturbance. This work may open up new possibility
of studying the intrinsic structure and electrostatics of ice or water on bulk
insulating surfaces, ion hydration and biological water with atomic precision.
",0,1,0,0,0,0
4542,Automata-Guided Hierarchical Reinforcement Learning for Skill Composition,"  Skills learned through (deep) reinforcement learning often generalizes poorly
across domains and re-training is necessary when presented with a new task. We
present a framework that combines techniques in \textit{formal methods} with
\textit{reinforcement learning} (RL). The methods we provide allows for
convenient specification of tasks with logical expressions, learns hierarchical
policies (meta-controller and low-level controllers) with well-defined
intrinsic rewards, and construct new skills from existing ones with little to
no additional exploration. We evaluate the proposed methods in a simple grid
world simulation as well as a more complicated kitchen environment in AI2Thor
",1,0,0,0,0,0
18980,Parametric Identification Using Weighted Null-Space Fitting,"  In identification of dynamical systems, the prediction error method using a
quadratic cost function provides asymptotically efficient estimates under
Gaussian noise and additional mild assumptions, but in general it requires
solving a non-convex optimization problem. An alternative class of methods uses
a non-parametric model as intermediate step to obtain the model of interest.
Weighted null-space fitting (WNSF) belongs to this class. It is a weighted
least-squares method consisting of three steps. In the first step, a high-order
ARX model is estimated. In a second least-squares step, this high-order
estimate is reduced to a parametric estimate. In the third step, weighted least
squares is used to reduce the variance of the estimates. The method is flexible
in parametrization and suitable for both open- and closed-loop data. In this
paper, we show that WNSF provides estimates with the same asymptotic properties
as PEM with a quadratic cost function when the model orders are chosen
according to the true system. Also, simulation studies indicate that WNSF may
be competitive with state-of-the-art methods.
",1,0,0,0,0,0
20824,Towards Robust Interpretability with Self-Explaining Neural Networks,"  Most recent work on interpretability of complex machine learning models has
focused on estimating $\textit{a posteriori}$ explanations for previously
trained models around specific predictions. $\textit{Self-explaining}$ models
where interpretability plays a key role already during learning have received
much less attention. We propose three desiderata for explanations in general --
explicitness, faithfulness, and stability -- and show that existing methods do
not satisfy them. In response, we design self-explaining models in stages,
progressively generalizing linear classifiers to complex yet architecturally
explicit models. Faithfulness and stability are enforced via regularization
specifically tailored to such models. Experimental results across various
benchmark datasets show that our framework offers a promising direction for
reconciling model complexity and interpretability.
",0,0,0,1,0,0
8991,Linear Convergence of An Iterative Phase Retrieval Algorithm with Data Reuse,"  Phase retrieval has been an attractive but difficult problem rising from
physical science, and there has been a gap between state-of-the-art theoretical
convergence analyses and the corresponding efficient retrieval methods.
Firstly, these analyses all assume that the sensing vectors and the iterative
updates are independent, which only fits the ideal model with infinite
measurements but not the reality, where data are limited and have to be reused.
Secondly, the empirical results of some efficient methods, such as the
randomized Kaczmarz method, show linear convergence, which is beyond existing
theoretical explanations considering its randomness and reuse of data. In this
work, we study for the first time, without the independence assumption, the
convergence behavior of the randomized Kaczmarz method for phase retrieval.
Specifically, beginning from taking expectation of the squared estimation error
with respect to the index of measurement by fixing the sensing vector and the
error in the previous step, we discard the independence assumption, rigorously
derive the upper and lower bounds of the reduction of the mean squared error,
and prove the linear convergence. This work fills the gap between a fast
converging algorithm and its theoretical understanding. The proposed
methodology may contribute to the study of other iterative algorithms for phase
retrieval and other problems in the broad area of signal processing and machine
learning.
",1,0,0,0,0,0
5592,Instability of pulses in gradient reaction-diffusion systems: A symplectic approach,"  In a scalar reaction-diffusion equation, it is known that the stability of a
steady state can be determined from the Maslov index, a topological invariant
that counts the state's critical points. In particular, this implies that pulse
solutions are unstable. We extend this picture to pulses in reaction-diffusion
systems with gradient nonlinearity. In particular, we associate a Maslov index
to any asymptotically constant state, generalizing existing definitions of the
Maslov index for homoclinic orbits. It is shown that this index equals the
number of unstable eigenvalues for the linearized evolution equation. Finally,
we use a symmetry argument to show that any pulse solution must have nonzero
Maslov index, and hence be unstable.
",0,0,1,0,0,0
17557,"Sharp gradient estimate for heat kernels on $RCD^*(K,N)$ metric measure spaces","  In this paper, we will establish an elliptic local Li-Yau gradient estimate
for weak solutions of the heat equation on metric measure spaces with
generalized Ricci curvature bounded from below. One of its main applications is
a sharp gradient estimate for the logarithm of heat kernels. These results seem
new even for smooth Riemannian manifolds.
",0,0,1,0,0,0
9952,Time evolution of the Luttinger model with nonuniform temperature profile,"  We study the time evolution of a one-dimensional interacting fermion system
described by the Luttinger model starting from a nonequilibrium state defined
by a smooth temperature profile $T(x)$. As a specific example we consider the
case when $T(x)$ is equal to $T_L$ ($T_R$) far to the left (right). Using a
series expansion in $\epsilon = 2(T_{R} - T_{L})/(T_{L}+T_{R})$, we compute the
energy density, the heat current density, and the fermion two-point correlation
function for all times $t \geq 0$. For local (delta-function) interactions, the
first two are computed to all orders, giving simple exact expressions involving
the Schwarzian derivative of the integral of $T(x)$. For nonlocal interactions,
breaking scale invariance, we compute the nonequilibrium steady state (NESS) to
all orders and the evolution to first order in $\epsilon$. The heat current in
the NESS is universal even when conformal invariance is broken by the
interactions, and its dependence on $T_{L,R}$ agrees with numerical results for
the $XXZ$ spin chain. Moreover, our analytical formulas predict peaks at short
times in the transition region between different temperatures and show
dispersion effects that, even if nonuniversal, are qualitatively similar to
ones observed in numerical simulations for related models, such as spin chains
and interacting lattice fermions.
",0,1,1,0,0,0
3614,Indoor UAV scheduling with Restful Task Assignment Algorithm,"  Research in UAV scheduling has obtained an emerging interest from scientists
in the optimization field. When the scheduling itself has established a strong
root since the 19th century, works on UAV scheduling in indoor environment has
come forth in the latest decade. Several works on scheduling UAV operations in
indoor (two and three dimensional) and outdoor environments are reported. In
this paper, a further study on UAV scheduling in three dimensional indoor
environment is investigated. Dealing with indoor environment\textemdash where
humans, UAVs, and other elements or infrastructures are likely to coexist in
the same space\textemdash draws attention towards the safety of the operations.
In relation to the battery level, a preserved battery level leads to safer
operations, promoting the UAV to have a decent remaining power level. A
methodology which consists of a heuristic approach based on Restful Task
Assignment Algorithm, incorporated with Particle Swarm Optimization Algorithm,
is proposed. The motivation is to preserve the battery level throughout the
operations, which promotes less possibility in having failed UAVs on duty. This
methodology is tested with 54 benchmark datasets stressing on 4 different
aspects: geographical distance, number of tasks, number of predecessors, and
slack time. The test results and their characteristics in regard to the
proposed methodology are discussed and presented.
",1,0,0,0,0,0
4251,Weighted estimates for positive operators and Doob maximal operators on filtered measure spaces,"  We characterize strong type and weak type inequalities with two weights for
positive operators on filtered measure spaces. These estimates are
probabilistic analogues of two-weight inequalities for positive operators
associated to the dyadic cubes in $\mathbb R^n$ due to Lacey, Sawyer and
Uriarte-Tuero \cite{LaSaUr}. Several mixed bounds for the Doob maximal operator
on filtered measure spaces are also obtained. In fact, Hytönen-Pérez
type and Lerner-Moen type norm estimates for Doob maximal operator are
established. Our approaches are mainly based on the construction of principal
sets.
",0,0,1,0,0,0
9523,A study of existing Ontologies in the IoT-domain,"  Several domains have adopted the increasing use of IoT-based devices to
collect sensor data for generating abstractions and perceptions of the real
world. This sensor data is multi-modal and heterogeneous in nature. This
heterogeneity induces interoperability issues while developing cross-domain
applications, thereby restricting the possibility of reusing sensor data to
develop new applications. As a solution to this, semantic approaches have been
proposed in the literature to tackle problems related to interoperability of
sensor data. Several ontologies have been proposed to handle different aspects
of IoT-based sensor data collection, ranging from discovering the IoT sensors
for data collection to applying reasoning on the collected sensor data for
drawing inferences. In this paper, we survey these existing semantic ontologies
to provide an overview of the recent developments in this field. We highlight
the fundamental ontological concepts (e.g., sensor-capabilities and
context-awareness) required for an IoT-based application, and survey the
existing ontologies which include these concepts. Based on our study, we also
identify the shortcomings of currently available ontologies, which serves as a
stepping stone to state the need for a common unified ontology for the IoT
domain.
",1,0,0,0,0,0
5856,Restricted Boltzmann Machines for Robust and Fast Latent Truth Discovery,"  We address the problem of latent truth discovery, LTD for short, where the
goal is to discover the underlying true values of entity attributes in the
presence of noisy, conflicting or incomplete information. Despite a multitude
of algorithms to address the LTD problem that can be found in literature, only
little is known about their overall performance with respect to effectiveness
(in terms of truth discovery capabilities), efficiency and robustness. A
practical LTD approach should satisfy all these characteristics so that it can
be applied to heterogeneous datasets of varying quality and degrees of
cleanliness.
We propose a novel algorithm for LTD that satisfies the above requirements.
The proposed model is based on Restricted Boltzmann Machines, thus coined
LTD-RBM. In extensive experiments on various heterogeneous and publicly
available datasets, LTD-RBM is superior to state-of-the-art LTD techniques in
terms of an overall consideration of effectiveness, efficiency and robustness.
",0,0,0,1,0,0
14897,Tuning of Interlayer Coupling in Large-Area Graphene/WSe2 van der Waals Heterostructure via Ion Irradiation: Optical Evidences and Photonic Applications,"  Van der Waals (vdW) heterostructures are receiving great attentions due to
their intriguing properties and potentials in many research fields. The flow of
charge carriers in vdW heterostructures can be efficiently rectified by the
inter-layer coupling between neighboring layers, offering a rich collection of
functionalities and a mechanism for designing atomically thin devices.
Nevertheless, non-uniform contact in larger-area heterostructures reduces the
device efficiency. In this work, ion irradiation had been verified as an
efficient technique to enhance the contact and interlayer coupling in the newly
developed graphene/WSe2 hetero-structure with a large area of 10 mm x 10 mm.
During the ion irradiation process, the morphology of monolayer graphene had
been modified, promoting the contact with WSe2. Experimental evidences of the
tunable interlayer electron transfer are displayed by investigation of
photoluminescence and ultrafast absorption of the irradiated heterostructure.
Besides, we have found that in graphene/WSe2 heterostructure, graphene serves
as a fast channel for the photo-excited carriers to relax in WSe2, and the
nonlinear absorption of WSe2 could be effectively tuned by the carrier transfer
process in graphene, enabling specific optical absorption of the
heterostructure in comparison with separated graphene or WSe2. On the basis of
these new findings, by applying the ion beam modified graphene/WSe2
heterostructure as a saturable absorber, Q-switched pulsed lasing with
optimized performance has been realized in a Nd:YAG waveguide cavity. This work
paves the way towards developing novel devices based on large-area
heterostructures by using ion beam irradiation.
",0,1,0,0,0,0
4592,Scalable Joint Models for Reliable Uncertainty-Aware Event Prediction,"  Missing data and noisy observations pose significant challenges for reliably
predicting events from irregularly sampled multivariate time series
(longitudinal) data. Imputation methods, which are typically used for
completing the data prior to event prediction, lack a principled mechanism to
account for the uncertainty due to missingness. Alternatively, state-of-the-art
joint modeling techniques can be used for jointly modeling the longitudinal and
event data and compute event probabilities conditioned on the longitudinal
observations. These approaches, however, make strong parametric assumptions and
do not easily scale to multivariate signals with many observations. Our
proposed approach consists of several key innovations. First, we develop a
flexible and scalable joint model based upon sparse multiple-output Gaussian
processes. Unlike state-of-the-art joint models, the proposed model can explain
highly challenging structure including non-Gaussian noise while scaling to
large data. Second, we derive an optimal policy for predicting events using the
distribution of the event occurrence estimated by the joint model. The derived
policy trades-off the cost of a delayed detection versus incorrect assessments
and abstains from making decisions when the estimated event probability does
not satisfy the derived confidence criteria. Experiments on a large dataset
show that the proposed framework significantly outperforms state-of-the-art
techniques in event prediction.
",1,0,0,1,0,0
16800,Variational Inference for Gaussian Process Models with Linear Complexity,"  Large-scale Gaussian process inference has long faced practical challenges
due to time and space complexity that is superlinear in dataset size. While
sparse variational Gaussian process models are capable of learning from
large-scale data, standard strategies for sparsifying the model can prevent the
approximation of complex functions. In this work, we propose a novel
variational Gaussian process model that decouples the representation of mean
and covariance functions in reproducing kernel Hilbert space. We show that this
new parametrization generalizes previous models. Furthermore, it yields a
variational inference problem that can be solved by stochastic gradient ascent
with time and space complexity that is only linear in the number of mean
function parameters, regardless of the choice of kernels, likelihoods, and
inducing points. This strategy makes the adoption of large-scale expressive
Gaussian process models possible. We run several experiments on regression
tasks and show that this decoupled approach greatly outperforms previous sparse
variational Gaussian process inference procedures.
",1,0,0,1,0,0
14097,Lie Transform Based Polynomial Neural Networks for Dynamical Systems Simulation and Identification,"  In the article, we discuss the architecture of the polynomial neural network
that corresponds to the matrix representation of Lie transform. The matrix form
of Lie transform is an approximation of general solution for the nonlinear
system of ordinary differential equations. Thus, it can be used for simulation
and modeling task. On the other hand, one can identify dynamical system from
time series data simply by optimization of the coefficient matrices of the Lie
transform. Representation of the approach by polynomial neural networks
integrates the strength of both neural networks and traditional model-based
methods for dynamical systems investigation. We provide a theoretical
explanation of learning dynamical systems from time series for the proposed
method, as well as demonstrate it in several applications. Namely, we show
results of modeling and identification for both well-known systems like
Lotka-Volterra equation and more complicated examples from retail,
biochemistry, and accelerator physics.
",1,0,0,0,0,0
16890,Generating Sentence Planning Variations for Story Telling,"  There has been a recent explosion in applications for dialogue interaction
ranging from direction-giving and tourist information to interactive story
systems. Yet the natural language generation (NLG) component for many of these
systems remains largely handcrafted. This limitation greatly restricts the
range of applications; it also means that it is impossible to take advantage of
recent work in expressive and statistical language generation that can
dynamically and automatically produce a large number of variations of given
content. We propose that a solution to this problem lies in new methods for
developing language generation resources. We describe the ES-Translator, a
computational language generator that has previously been applied only to
fables, and quantitatively evaluate the domain independence of the EST by
applying it to personal narratives from weblogs. We then take advantage of
recent work on language generation to create a parameterized sentence planner
for story generation that provides aggregation operations, variations in
discourse and in point of view. Finally, we present a user evaluation of
different personal narrative retellings.
",1,0,0,0,0,0
4661,Gee-Haw Whammy Diddle,"  Gee-Haw Whammy Diddle is a seemingly simple mechanical toy consisting of a
wooden stick and a second stick that is made up of a series of notches with a
propeller at its end. When the wooden stick is pulled over the notches, the
propeller starts to rotate. In spite of its simplicity, physical principles
governing the motion of the stick and the propeller are rather complicated and
interesting. Here we provide a thorough analysis of the system and parameters
influencing the motion. We show that contrary to the results published on this
topic so far, neither elliptic motion of the stick nor frequency
synchronization is needed for starting the motion of the propeller.
",0,1,0,0,0,0
2254,Truncation-free Hybrid Inference for DPMM,"  Dirichlet process mixture models (DPMM) are a cornerstone of Bayesian
non-parametrics. While these models free from choosing the number of components
a-priori, computationally attractive variational inference often reintroduces
the need to do so, via a truncation on the variational distribution. In this
paper we present a truncation-free hybrid inference for DPMM, combining the
advantages of sampling-based MCMC and variational methods. The proposed
hybridization enables more efficient variational updates, while increasing
model complexity only if needed. We evaluate the properties of the hybrid
updates and their empirical performance in single- as well as mixed-membership
models. Our method is easy to implement and performs favorably compared to
existing schemas.
",1,0,0,1,0,0
7057,On Decidability of the Ordered Structures of Numbers,"  The ordered structures of natural, integer, rational and real numbers are
studied here. It is known that the theories of these numbers in the language of
order are decidable and finitely axiomatizable. Also, their theories in the
language of order and addition are decidable and infinitely axiomatizable. For
the language of order and multiplication, it is known that the theories of
$\mathbb{N}$ and $\mathbb{Z}$ are not decidable (and so not axiomatizable by
any computably enumerable set of sentences). By Tarski's theorem, the
multiplicative ordered structure of $\mathbb{R}$ is decidable also; here we
prove this result directly and present an axiomatization. The structure of
$\mathbb{Q}$ in the language of order and multiplication seems to be missing in
the literature; here we show the decidability of its theory by the technique of
quantifier elimination and after presenting an infinite axiomatization for this
structure we prove that it is not finitely axiomatizable.
",1,0,1,0,0,0
16362,Deep Architectures for Neural Machine Translation,"  It has been shown that increasing model depth improves the quality of neural
machine translation. However, different architectural variants to increase
model depth have been proposed, and so far, there has been no thorough
comparative study.
In this work, we describe and evaluate several existing approaches to
introduce depth in neural machine translation. Additionally, we explore novel
architectural variants, including deep transition RNNs, and we vary how
attention is used in the deep decoder. We introduce a novel ""BiDeep"" RNN
architecture that combines deep transition RNNs and stacked RNNs.
Our evaluation is carried out on the English to German WMT news translation
dataset, using a single-GPU machine for both training and inference. We find
that several of our proposed architectures improve upon existing approaches in
terms of speed and translation quality. We obtain best improvements with a
BiDeep RNN of combined depth 8, obtaining an average improvement of 1.5 BLEU
over a strong shallow baseline.
We release our code for ease of adoption.
",1,0,0,0,0,0
7061,A combinatorial proof of Bass's determinant formula for the zeta function of regular graphs,"  We give an elementary combinatorial proof of Bass's determinant formula for
the zeta function of a finite regular graph. This is done by expressing the
number of non-backtracking cycles of a given length in terms of Chebychev
polynomials in the eigenvalues of the adjacency operator of the graph.
",1,0,0,0,0,0
13705,Structured Matrix Estimation and Completion,"  We study the problem of matrix estimation and matrix completion under a
general framework. This framework includes several important models as special
cases such as the gaussian mixture model, mixed membership model, bi-clustering
model and dictionary learning. We consider the optimal convergence rates in a
minimax sense for estimation of the signal matrix under the Frobenius norm and
under the spectral norm. As a consequence of our general result we obtain
minimax optimal rates of convergence for various special models.
",0,0,1,1,0,0
10067,Quantum Stress Tensor Fluctuations and Primordial Gravity Waves,"  We examine the effect of the stress tensor of a quantum matter field, such as
the electromagnetic field, on the spectrum of primordial gravity waves expected
in inflationary cosmology. We find that the net effect is a small reduction in
the power spectrum, especially at higher frequencies, but which has a different
form from that described by the usual spectral index. Thus this effect has a
characteristic signature, and is in principle observable. The net effect is a
sum of two contributions, one of which is due to quantum fluctuations of the
matter field stress tensor. The other is a quantum correction to the graviton
field due to coupling to the expectation value of this stress tensor. Both
contributions are sensitive to initial conditions in the very early universe,
so this effect has the potential to act as a probe of these initial conditions.
",0,1,0,0,0,0
6451,The Compressed Model of Residual CNDS,"  Convolutional neural networks have achieved a great success in the recent
years. Although, the way to maximize the performance of the convolutional
neural networks still in the beginning. Furthermore, the optimization of the
size and the time that need to train the convolutional neural networks is very
far away from reaching the researcher's ambition. In this paper, we proposed a
new convolutional neural network that combined several techniques to boost the
optimization of the convolutional neural network in the aspects of speed and
size. As we used our previous model Residual-CNDS (ResCNDS), which solved the
problems of slower convergence, overfitting, and degradation, and compressed
it. The outcome model called Residual-Squeeze-CNDS (ResSquCNDS), which we
demonstrated on our sold technique to add residual learning and our model of
compressing the convolutional neural networks. Our model of compressing adapted
from the SQUEEZENET model, but our model is more generalizable, which can be
applied almost to any neural network model, and fully integrated into the
residual learning, which addresses the problem of the degradation very
successfully. Our proposed model trained on very large-scale MIT
Places365-Standard scene datasets, which backing our hypothesis that the new
compressed model inherited the best of the previous ResCNDS8 model, and almost
get the same accuracy in the validation Top-1 and Top-5 with 87.64% smaller in
size and 13.33% faster in the training time.
",1,0,0,0,0,0
5446,Gradient-enhanced kriging for high-dimensional problems,"  Surrogate models provide a low computational cost alternative to evaluating
expensive functions. The construction of accurate surrogate models with large
numbers of independent variables is currently prohibitive because it requires a
large number of function evaluations. Gradient-enhanced kriging has the
potential to reduce the number of function evaluations for the desired accuracy
when efficient gradient computation, such as an adjoint method, is available.
However, current gradient-enhanced kriging methods do not scale well with the
number of sampling points due to the rapid growth in the size of the
correlation matrix where new information is added for each sampling point in
each direction of the design space. They do not scale well with the number of
independent variables either due to the increase in the number of
hyperparameters that needs to be estimated. To address this issue, we develop a
new gradient-enhanced surrogate model approach that drastically reduced the
number of hyperparameters through the use of the partial-least squares method
that maintains accuracy. In addition, this method is able to control the size
of the correlation matrix by adding only relevant points defined through the
information provided by the partial-least squares method. To validate our
method, we compare the global accuracy of the proposed method with conventional
kriging surrogate models on two analytic functions with up to 100 dimensions,
as well as engineering problems of varied complexity with up to 15 dimensions.
We show that the proposed method requires fewer sampling points than
conventional methods to obtain the desired accuracy, or provides more accuracy
for a fixed budget of sampling points. In some cases, we get over 3 times more
accurate models than a bench of surrogate models from the literature, and also
over 3200 times faster than standard gradient-enhanced kriging models.
",1,0,0,1,0,0
3093,Efficient Benchmarking of Algorithm Configuration Procedures via Model-Based Surrogates,"  The optimization of algorithm (hyper-)parameters is crucial for achieving
peak performance across a wide range of domains, ranging from deep neural
networks to solvers for hard combinatorial problems. The resulting algorithm
configuration (AC) problem has attracted much attention from the machine
learning community. However, the proper evaluation of new AC procedures is
hindered by two key hurdles. First, AC benchmarks are hard to set up. Second
and even more significantly, they are computationally expensive: a single run
of an AC procedure involves many costly runs of the target algorithm whose
performance is to be optimized in a given AC benchmark scenario. One common
workaround is to optimize cheap-to-evaluate artificial benchmark functions
(e.g., Branin) instead of actual algorithms; however, these have different
properties than realistic AC problems. Here, we propose an alternative
benchmarking approach that is similarly cheap to evaluate but much closer to
the original AC problem: replacing expensive benchmarks by surrogate benchmarks
constructed from AC benchmarks. These surrogate benchmarks approximate the
response surface corresponding to true target algorithm performance using a
regression model, and the original and surrogate benchmark share the same
(hyper-)parameter space. In our experiments, we construct and evaluate
surrogate benchmarks for hyperparameter optimization as well as for AC problems
that involve performance optimization of solvers for hard combinatorial
problems, drawing training data from the runs of existing AC procedures. We
show that our surrogate benchmarks capture overall important characteristics of
the AC scenarios, such as high- and low-performing regions, from which they
were derived, while being much easier to use and orders of magnitude cheaper to
evaluate.
",1,0,0,1,0,0
17746,Symbol Invariant of Partition and the Construction,"  The symbol is used to describe the Springer correspondence for the classical
groups. We propose equivalent definitions of symbols for rigid partitions in
the $B_n$, $C_n$, and $D_n$ theories uniformly. Analysing the new definition of
symbol in detail, we give rules to construct symbol of a partition, which are
easy to remember and to operate on. We introduce formal operations of a
partition, which reduce the difficulties in the proof of the construction
rules. According these rules, we give a closed formula of symbols for different
theories uniformly. As applications, previous results can be illustrated more
clearly by the construction rules of symbol.
",0,0,1,0,0,0
12400,Orbital degeneracy loci and applications,"  Degeneracy loci of morphisms between vector bundles have been used in a wide
variety of situations. We introduce a vast generalization of this notion, based
on orbit closures of algebraic groups in their linear representations. A
preferred class of our orbital degeneracy loci is characterized by a certain
crepancy condition on the orbit closure, that allows to get some control on the
canonical sheaf. This condition is fulfilled for Richardson nilpotent orbits,
and also for partially decomposable skew-symmetric three-forms in six
variables. In order to illustrate the efficiency and flexibility of our
methods, we construct in both situations many Calabi--Yau manifolds of
dimension three and four, as well as a few Fano varieties, including some new
Fano fourfolds.
",0,0,1,0,0,0
12159,An Adversarial Regularisation for Semi-Supervised Training of Structured Output Neural Networks,"  We propose a method for semi-supervised training of structured-output neural
networks. Inspired by the framework of Generative Adversarial Networks (GAN),
we train a discriminator network to capture the notion of a quality of network
output. To this end, we leverage the qualitative difference between outputs
obtained on the labelled training data and unannotated data. We then use the
discriminator as a source of error signal for unlabelled data. This effectively
boosts the performance of a network on a held out test set. Initial experiments
in image segmentation demonstrate that the proposed framework enables achieving
the same network performance as in a fully supervised scenario, while using two
times less annotations.
",1,0,0,0,0,0
7088,A unified method for maximal truncated Calderón-Zygmund operators in general function spaces by sparse domination,"  In this note we give simple proofs of several results involving maximal
truncated Caldeón-Zygmund operators in the general setting of rearrangement
invariant quasi-Banach function spaces by sparse domination. Our techniques
allow us to track the dependence of the constants in weighted norm
inequalities; additionally, our results hold in $\mathbb{R}^n$ as well as in
many spaces of homogeneous type.
",0,0,1,0,0,0
12640,Solitons and geometrical structures in a perfect fluid spacetime,"  Geometrical aspects of a perfect fluid spacetime are described in terms of
different curvature tensors and $\eta$-Ricci and $\eta$-Einstein solitons in a
perfect fluid spacetime are determined. Conditions for the Ricci soliton to be
steady, expanding or shrinking are also given. In a particular case when the
potential vector field $\xi$ of the soliton is of gradient type,
$\xi:=grad(f)$, we derive from the soliton equation a Laplacian equation
satisfied by $f$.
",0,0,1,0,0,0
19572,The role of complex analysis in modeling economic growth,"  Development and growth are complex and tumultuous processes. Modern economic
growth theories identify some key determinants of economic growth. However, the
relative importance of the determinants remains unknown, and additional
variables may help clarify the directions and dimensions of the interactions.
The novel stream of literature on economic complexity goes beyond aggregate
measures of productive inputs, and considers instead a more granular and
structural view of the productive possibilities of countries, i.e. their
capabilities. Different endowments of capabilities are crucial ingredients in
explaining differences in economic performances. In this paper we employ
economic fitness, a measure of productive capabilities obtained through complex
network techniques. Focusing on the combined roles of fitness and some more
traditional drivers of growth, we build a bridge between economic growth
theories and the economic complexity literature. Our findings, in agreement
with other recent empirical studies, show that fitness plays a crucial role in
fostering economic growth and, when it is included in the analysis, can be
either complementary to traditional drivers of growth or can completely
overshadow them.
",0,0,0,0,0,1
3255,Distributed Convolutional Dictionary Learning (DiCoDiLe): Pattern Discovery in Large Images and Signals,"  Convolutional dictionary learning (CDL) estimates shift invariant basis
adapted to multidimensional data. CDL has proven useful for image denoising or
inpainting, as well as for pattern discovery on multivariate signals. As
estimated patterns can be positioned anywhere in signals or images,
optimization techniques face the difficulty of working in extremely high
dimensions with millions of pixels or time samples, contrarily to standard
patch-based dictionary learning. To address this optimization problem, this
work proposes a distributed and asynchronous algorithm, employing locally
greedy coordinate descent and an asynchronous locking mechanism that does not
require a central server. This algorithm can be used to distribute the
computation on a number of workers which scales linearly with the encoded
signal's size. Experiments confirm the scaling properties which allows us to
learn patterns on large scales images from the Hubble Space Telescope.
",1,0,0,1,0,0
12889,Just ASK: Building an Architecture for Extensible Self-Service Spoken Language Understanding,"  This paper presents the design of the machine learning architecture that
underlies the Alexa Skills Kit (ASK) a large scale Spoken Language
Understanding (SLU) Software Development Kit (SDK) that enables developers to
extend the capabilities of Amazon's virtual assistant, Alexa. At Amazon, the
infrastructure powers over 25,000 skills deployed through the ASK, as well as
AWS's Amazon Lex SLU Service. The ASK emphasizes flexibility, predictability
and a rapid iteration cycle for third party developers. It imposes inductive
biases that allow it to learn robust SLU models from extremely small and sparse
datasets and, in doing so, removes significant barriers to entry for software
developers and dialogue systems researchers.
",1,0,0,0,0,0
6436,pyRecLab: A Software Library for Quick Prototyping of Recommender Systems,"  This paper introduces pyRecLab, a software library written in C++ with Python
bindings which allows to quickly train, test and develop recommender systems.
Although there are several software libraries for this purpose, only a few let
developers to get quickly started with the most traditional methods, permitting
them to try different parameters and approach several tasks without a
significant loss of performance. Among the few libraries that have all these
features, they are available in languages such as Java, Scala or C#, what is a
disadvantage for less experienced programmers more used to the popular Python
programming language. In this article we introduce details of pyRecLab, showing
as well performance analysis in terms of error metrics (MAE and RMSE) and
train/test time. We benchmark it against the popular Java-based library LibRec,
showing similar results. We expect programmers with little experience and
people interested in quickly prototyping recommender systems to be benefited
from pyRecLab.
",1,0,0,0,0,0
9424,Self-shielding of hydrogen in the IGM during the epoch of reionization,"  We investigate self-shielding of intergalactic hydrogen against ionizing
radiation in radiative transfer simulations of cosmic reionization carefully
calibrated with Lyman alpha forest data. While self-shielded regions manifest
as Lyman-limit systems in the post-reionization Universe, here we focus on
their evolution during reionization (redshifts z=6-10). At these redshifts, the
spatial distribution of hydrogen-ionizing radiation is highly inhomogeneous,
and some regions of the Universe are still neutral. After masking the neutral
regions and ionizing sources in the simulation, we find that the hydrogen
photoionization rate depends on the local hydrogen density in a manner very
similar to that in the post-reionization Universe. The characteristic physical
hydrogen density above which self-shielding becomes important at these
redshifts is about $\mathrm{n_H \sim 3 \times 10^{-3} cm^{-3}}$, or $\sim$ 20
times the mean hydrogen density, reflecting the fact that during reionization
photoionization rates are typically low enough that the filaments in the cosmic
web are often self-shielded. The value of the typical self-shielding density
decreases by a factor of 3 between redshifts z=3 and 10, and follows the
evolution of the average photoionization rate in ionized regions in a simple
fashion. We provide a simple parameterization of the photoionization rate as a
function of density in self-shielded regions during the epoch of reionization.
",0,1,0,0,0,0
9948,Strong interaction between graphene layer and Fano resonance in terahertz metamaterials,"  Graphene has emerged as a promising building block in the modern optics and
optoelectronics due to its novel optical and electrical properties. In the
mid-infrared and terahertz (THz) regime, graphene behaves like metals and
supports surface plasmon resonances (SPRs). Moreover, the continuously tunable
conductivity of graphene enables active SPRs and gives rise to a range of
active applications. However, the interaction between graphene and metal-based
resonant metamaterials has not been fully understood. In this work, a
simulation investigation on the interaction between the graphene layer and THz
resonances supported by the two-gap split ring metamaterials is systematically
conducted. The simulation results show that the graphene layer can
substantially reduce the Fano resonance and even switch it off, while leave the
dipole resonance nearly unaffected, which phenomenon is well explained with the
high conductivity of graphene. With the manipulation of graphene conductivity
via altering its Fermi energy or layer number, the amplitude of the Fano
resonance can be modulated. The tunable Fano resonance here together with the
underlying physical mechanism can be strategically important in designing
active metal-graphene hybrid metamaterials. In addition, the ""sensitivity"" to
the graphene layer of the Fano resonance is also highly appreciated in the
field of ultrasensitive sensing, where the novel physical mechanism can be
employed in sensing other graphene-like two-dimensional (2D) materials or
biomolecules with the high conductivity.
",0,1,0,0,0,0
469,Second-Order Analysis and Numerical Approximation for Bang-Bang Bilinear Control Problems,"  We consider bilinear optimal control problems, whose objective functionals do
not depend on the controls. Hence, bang-bang solutions will appear. We
investigate sufficient second-order conditions for bang-bang controls, which
guarantee local quadratic growth of the objective functional in $L^1$. In
addition, we prove that for controls that are not bang-bang, no such growth can
be expected. Finally, we study the finite-element discretization, and prove
error estimates of bang-bang controls in $L^1$-norms.
",0,0,1,0,0,0
6198,Signal-based Bayesian Seismic Monitoring,"  Detecting weak seismic events from noisy sensors is a difficult perceptual
task. We formulate this task as Bayesian inference and propose a generative
model of seismic events and signals across a network of spatially distributed
stations. Our system, SIGVISA, is the first to directly model seismic
waveforms, allowing it to incorporate a rich representation of the physics
underlying the signal generation process. We use Gaussian processes over
wavelet parameters to predict detailed waveform fluctuations based on
historical events, while degrading smoothly to simple parametric envelopes in
regions with no historical seismicity. Evaluating on data from the western US,
we recover three times as many events as previous work, and reduce mean
location errors by a factor of four while greatly increasing sensitivity to
low-magnitude events.
",1,1,0,0,0,0
14384,Fixed points of morphisms among binary generalized pseudostandard words,"  We introduce a class of fixed points of primitive morphisms among aperiodic
binary generalized pseudostandard words. We conjecture that this class contains
all fixed points of primitive morphisms among aperiodic binary generalized
pseudostandard words that are not standard Sturmian words.
",0,0,1,0,0,0
20482,Synkhronos: a Multi-GPU Theano Extension for Data Parallelism,"  We present Synkhronos, an extension to Theano for multi-GPU computations
leveraging data parallelism. Our framework provides automated execution and
synchronization across devices, allowing users to continue to write serial
programs without risk of race conditions. The NVIDIA Collective Communication
Library is used for high-bandwidth inter-GPU communication. Further
enhancements to the Theano function interface include input slicing (with
aggregation) and input indexing, which perform common data-parallel computation
patterns efficiently. One example use case is synchronous SGD, which has
recently been shown to scale well for a growing set of deep learning problems.
When training ResNet-50, we achieve a near-linear speedup of 7.5x on an NVIDIA
DGX-1 using 8 GPUs, relative to Theano-only code running a single GPU in
isolation. Yet Synkhronos remains general to any data-parallel computation
programmable in Theano. By implementing parallelism at the level of individual
Theano functions, our framework uniquely addresses a niche between manual
multi-device programming and prescribed multi-GPU training routines.
",1,0,0,0,0,0
13771,Whole-Body Nonlinear Model Predictive Control Through Contacts for Quadrupeds,"  In this work we present a whole-body Nonlinear Model Predictive Control
approach for Rigid Body Systems subject to contacts. We use a full dynamic
system model which also includes explicit contact dynamics. Therefore, contact
locations, sequences and timings are not prespecified but optimized by the
solver. Yet, thorough numerical and software engineering allows for running the
nonlinear Optimal Control solver at rates up to 190 Hz on a quadruped for a
time horizon of half a second. This outperforms the state of the art by at
least one order of magnitude. Hardware experiments in form of periodic and
non-periodic tasks are applied to two quadrupeds with different actuation
systems. The obtained results underline the performance, transferability and
robustness of the approach.
",1,0,0,0,0,0
12100,A KiDS weak lensing analysis of assembly bias in GAMA galaxy groups,"  We investigate possible signatures of halo assembly bias for
spectroscopically selected galaxy groups from the GAMA survey using weak
lensing measurements from the spatially overlapping regions of the deeper,
high-imaging-quality photometric KiDS survey. We use GAMA groups with an
apparent richness larger than 4 to identify samples with comparable mean host
halo masses but with a different radial distribution of satellite galaxies,
which is a proxy for the formation time of the haloes. We measure the weak
lensing signal for groups with a steeper than average and with a shallower than
average satellite distribution and find no sign of halo assembly bias, with the
bias ratio of $0.85^{+0.37}_{-0.25}$, which is consistent with the $\Lambda$CDM
prediction. Our galaxy groups have typical masses of $10^{13} M_{\odot}/h$,
naturally complementing previous studies of halo assembly bias on galaxy
cluster scales.
",0,1,0,0,0,0
18658,Discretization-free Knowledge Gradient Methods for Bayesian Optimization,"  This paper studies Bayesian ranking and selection (R&S) problems with
correlated prior beliefs and continuous domains, i.e. Bayesian optimization
(BO). Knowledge gradient methods [Frazier et al., 2008, 2009] have been widely
studied for discrete R&S problems, which sample the one-step Bayes-optimal
point. When used over continuous domains, previous work on the knowledge
gradient [Scott et al., 2011, Wu and Frazier, 2016, Wu et al., 2017] often rely
on a discretized finite approximation. However, the discretization introduces
error and scales poorly as the dimension of domain grows. In this paper, we
develop a fast discretization-free knowledge gradient method for Bayesian
optimization. Our method is not restricted to the fully sequential setting, but
useful in all settings where knowledge gradient can be used over continuous
domains. We show how our method can be generalized to handle (i) batch of
points suggestion (parallel knowledge gradient); (ii) the setting where
derivative information is available in the optimization process
(derivative-enabled knowledge gradient). In numerical experiments, we
demonstrate that the discretization-free knowledge gradient method finds global
optima significantly faster than previous Bayesian optimization algorithms on
both synthetic test functions and real-world applications, especially when
function evaluations are noisy; and derivative-enabled knowledge gradient can
further improve the performances, even outperforming the gradient-based
optimizer such as BFGS when derivative information is available.
",1,0,1,1,0,0
4241,Time-dynamic inference for non-Markov transition probabilities under independent right-censoring,"  In this article, weak convergence of the general non-Markov state transition
probability estimator by Titman (2015) is established which, up to now, has not
been verified yet for other general non-Markov estimators. A similar theorem is
shown for the bootstrap, yielding resampling-based inference methods for
statistical functionals. Formulas of the involved covariance functions are
presented in detail. Particular applications include the conditional expected
length of stay in a specific state, given occupation of another state in the
past, as well as the construction of time-simultaneous confidence bands for the
transition probabilities. The expected lengths of stay in the two-sample liver
cirrhosis data-set by Andersen et al. (1993) are compared and confidence
intervals for their difference are constructed. With borderline significance
and in comparison to the placebo group, the treatment group has an elevated
expected length of stay in the healthy state given an earlier disease state
occupation. In contrast, the Aalen-Johansen estimator-based confidence
interval, which relies on a Markov assumption, leads to a drastically different
conclusion. Also, graphical illustrations of confidence bands for the
transition probabilities demonstrate the biasedness of the Aalen-Johansen
estimator in this data example. The reliability of these results is assessed in
a simulation study.
",0,0,1,1,0,0
20515,Solar wind turbulent cascade from MHD to sub-ion scales: large-size 3D hybrid particle-in-cell simulations,"  Spectral properties of the turbulent cascade from fluid to kinetic scales in
collisionless plasmas are investigated by means of large-size three-dimensional
(3D) hybrid (fluid electrons, kinetic protons) particle-in-cell simulations.
Initially isotropic Alfvènic fluctuations rapidly develop a strongly
anisotropic turbulent cascade, mainly in the direction perpendicular to the
ambient magnetic field. The omnidirectional magnetic field spectrum shows a
double power-law behavior over almost two decades in wavenumber, with a
Kolmogorov-like index at large scales, a spectral break around ion scales, and
a steepening at sub-ion scales. Power laws are also observed in the spectra of
the ion bulk velocity, density, and electric field, both at magnetohydrodynamic
(MHD) and at kinetic scales. Despite the complex structure, the omnidirectional
spectra of all fields at ion and sub-ion scales are in remarkable quantitative
agreement with those of a two-dimensional (2D) simulation with similar physical
parameters. This provides a partial, a-posteriori validation of the 2D
approximation at kinetic scales. Conversely, at MHD scales, the spectra of the
density and of the velocity (and, consequently, of the electric field) exhibit
differences between the 2D and 3D cases. Although they can be partly ascribed
to the lower spatial resolution, the main reason is likely the larger
importance of compressible effects in a full geometry. Our findings are also in
remarkable quantitative agreement with solar wind observations.
",0,1,0,0,0,0
6554,Resonant Scattering Characteristics of Homogeneous Dielectric Sphere,"  In the present article the classical problem of electromagnetic scattering by
a single homogeneous sphere is revisited. Main focus is the study of the
scattering behavior as a function of the material contrast and the size
parameters for all electric and magnetic resonances of a dielectric sphere.
Specifically, the Padé approximants are introduced and utilized as an
alternative system expansion of the Mie coefficients. Low order Padé
approximants can give compact and physically insightful expressions for the
scattering system and the enabled dynamic mechanisms. Higher order approximants
are used for predicting accurately the resonant pole spectrum. These results
are summarized into general pole formulae, covering up to fifth order magnetic
and forth order electric resonances of a small dielectric sphere. Additionally,
the connection between the radiative damping process and the resonant linewidth
is investigated. The results obtained reveal the fundamental connection of the
radiative damping mechanism with the maximum width occurring for each
resonance. Finally, the suggested system ansatz is used for studying the
resonant absorption maximum through a circuit-inspired perspective.
",0,1,0,0,0,0
1752,Suppression of the superconductivity in ultrathin amorphous Mo$_{78}$Ge$_{22}$ thin films observed by STM,"  In contact with a superconductor, a normal metal modifies its properties due
to Andreev reflection. In the current work, the local density of states (LDOS)
of superconductor - normal metal Mo$_{78}$Ge$_{22}$ - Au bilayers are studied
by means of STM applied from the Au side. Three bilayers have been prepared on
silicate glass substrate consisting of 100, 10 and 5 nm MoGe thin films covered
always by 5 nm Au layer. The tunneling spectra were measured at temperatures
from 0.5 K to 7 K. The two-dimensional cross-correlation between topography and
normalized zero-bias conductance (ZBC) indicates a proximity effect between 100
and 10 nm MoGe thin films and Au layer where a superconducting gap slightly
smaller than that of bulk MoGe is observed. The effect of the thinnest 5 nm
MoGe layer on Au leads to much smaller gap moreover the LDOS reveals almost
completely suppressed coherence peaks. This is attributed to a strong
pair-breaking effect of spin-flip processes at the interface between MoGe films
and the substrate.
",0,1,0,0,0,0
16659,DeepSource: Point Source Detection using Deep Learning,"  Point source detection at low signal-to-noise is challenging for astronomical
surveys, particularly in radio interferometry images where the noise is
correlated. Machine learning is a promising solution, allowing the development
of algorithms tailored to specific telescope arrays and science cases. We
present DeepSource - a deep learning solution - that uses convolutional neural
networks to achieve these goals. DeepSource enhances the Signal-to-Noise Ratio
(SNR) of the original map and then uses dynamic blob detection to detect
sources. Trained and tested on two sets of 500 simulated 1 deg x 1 deg MeerKAT
images with a total of 300,000 sources, DeepSource is essentially perfect in
both purity and completeness down to SNR = 4 and outperforms PyBDSF in all
metrics. For uniformly-weighted images it achieves a Purity x Completeness (PC)
score at SNR = 3 of 0.73, compared to 0.31 for the best PyBDSF model. For
natural-weighting we find a smaller improvement of ~40% in the PC score at SNR
= 3. If instead we ask where either of the purity or completeness first drop to
90%, we find that DeepSource reaches this value at SNR = 3.6 compared to the
4.3 of PyBDSF (natural-weighting). A key advantage of DeepSource is that it can
learn to optimally trade off purity and completeness for any science case under
consideration. Our results show that deep learning is a promising approach to
point source detection in astronomical images.
",0,0,0,1,0,0
4551,Identification of a space varying coefficient of a linear viscoelastic string of Maxwell-Boltzman type,"  In this paper we solve the problem of the identification of a coefficient
which appears in the model of a distributed system with persistent memory
encountered in linear viscoelasticity (and in diffusion processes with memory).
The additional data used in the identification are subsumed in the input output
map from the deformation to the traction on the boundary. We extend a dynamical
approach to identification introduced by Belishev in the case of purely elastic
(memoryless) bodies and based on a special equation due to Blagoveshchenskii.
So, in particular, we extend Blagoveshchenskii equation to our class of systems
with persistent memory.
",1,0,1,0,0,0
10101,Teaching the Doppler Effect in Astrophysics,"  The Doppler effect is a shift in the frequency of waves emitted from an
object moving relative to the observer. By observing and analysing the Doppler
shift in electromagnetic waves from astronomical objects, astronomers gain
greater insight into the structure and operation of our universe. In this
paper, a simple technique is described for teaching the basics of the Doppler
effect to undergraduate astrophysics students using acoustic waves. An
advantage of the technique is that it produces a visual representation of the
acoustic Doppler shift. The equipment comprises a 40 kHz acoustic transmitter
and a microphone. The sound is bounced off a computer fan and the signal
collected by a DrDAQ ADC and processed by a spectrum analyser. Widening of the
spectrum is observed as the fan power supply potential is increased from 4 to
12 V.
",0,1,0,0,0,0
2441,Classification of grasping tasks based on EEG-EMG coherence,"  This work presents an innovative application of the well-known concept of
cortico-muscular coherence for the classification of various motor tasks, i.e.,
grasps of different kinds of objects. Our approach can classify objects with
different weights (motor-related features) and different surface frictions
(haptics-related features) with high accuracy (over 0:8). The outcomes
presented here provide information about the synchronization existing between
the brain and the muscles during specific activities; thus, this may represent
a new effective way to perform activity recognition.
",0,0,0,0,1,0
4166,High dimensional deformed rectangular matrices with applications in matrix denoising,"  We consider the recovery of a low rank $M \times N$ matrix $S$ from its noisy
observation $\tilde{S}$ in two different regimes. Under the assumption that $M$
is comparable to $N$, we propose two consistent estimators for $S$. Our
analysis relies on the local behavior of the large dimensional rectangular
matrices with finite rank perturbation. We also derive the convergent limits
and rates for the singular values and vectors of such matrices.
",0,0,1,1,0,0
6154,Musical Instrument Recognition Using Their Distinctive Characteristics in Artificial Neural Networks,"  In this study an Artificial Neural Network was trained to classify musical
instruments, using audio samples transformed to the frequency domain. Different
features of the sound, in both time and frequency domain, were analyzed and
compared in relation to how much information that could be derived from that
limited data. The study concluded that in comparison with the base experiment,
that had an accuracy of 93.5%, using the attack only resulted in 80.2% and the
initial 100 Hz in 64.2%.
",1,0,0,1,0,0
2337,Facets on the convex hull of $d$-dimensional Brownian and Lévy motion,"  For stationary, homogeneous Markov processes (viz., Lévy processes,
including Brownian motion) in dimension $d\geq 3$, we establish an exact
formula for the average number of $(d-1)$-dimensional facets that can be
defined by $d$ points on the process's path. This formula defines a
universality class in that it is independent of the increments' distribution,
and it admits a closed form when $d=3$, a case which is of particular interest
for applications in biophysics, chemistry and polymer science.
We also show that the asymptotical average number of facets behaves as
$\langle \mathcal{F}_T^{(d)}\rangle \sim 2\left[\ln \left( T/\Delta
t\right)\right]^{d-1}$, where $T$ is the total duration of the motion and
$\Delta t$ is the minimum time lapse separating points that define a facet.
",0,1,1,0,0,0
1750,On Chern number inequality in dimension 3,"  We prove that if $X---> X^+$ is a threefold terminal flip, then
$c_1(X).c_2(X)\leq c_1(X^+).c_2(X^+)$ where $c_1(X)$ and $c_2(X)$ denote the
Chern classes. This gives the affirmative answer to a Question by Xie
\cite{Xie2}. We obtain the similar but weaker result in the case of divisorial
contraction to curves.
",0,0,1,0,0,0
7473,Fractional quiver W-algebras,"  We introduce quiver gauge theory associated with the non-simply-laced type
fractional quiver, and define fractional quiver W-algebras by using
construction of arXiv:1512.08533 and arXiv:1608.04651 with representation of
fractional quivers.
",0,0,1,0,0,0
3138,On reproduction of On the regularization of Wasserstein GANs,"  This report has several purposes. First, our report is written to investigate
the reproducibility of the submitted paper On the regularization of Wasserstein
GANs (2018). Second, among the experiments performed in the submitted paper,
five aspects were emphasized and reproduced: learning speed, stability,
robustness against hyperparameter, estimating the Wasserstein distance, and
various sampling method. Finally, we identify which parts of the contribution
can be reproduced, and at what cost in terms of resources. All source code for
reproduction is open to the public.
",1,0,0,1,0,0
5001,Mathematical and numerical validation of the simplified spherical harmonics approach for time-dependent anisotropic-scattering transport problems in homogeneous media,"  In this work, we extend the solid harmonics derivation, which was used by
Ackroyd et al to derive the steady-state SP$_N$ equations, to transient
problems. The derivation expands the angular flux in ordinary surface harmonics
but uses harmonic polynomials to generate additional surface spherical harmonic
terms to be used in Galerkin projection. The derivation shows the equivalence
between the SP$_N$ and the P$_N$ approximation. Also, we use the line source
problem and McClarren's ""box"" problem to demonstrate such equivalence
numerically. Both problems were initially proposed for isotropic scattering,
but here we add higher-order scattering moments to them. Results show that the
difference between the SP$_N$ and P$_N$ scalar flux solution is at the roundoff
level.
",0,1,0,0,0,0
20583,k-server via multiscale entropic regularization,"  We present an $O((\log k)^2)$-competitive randomized algorithm for the
$k$-server problem on hierarchically separated trees (HSTs). This is the first
$o(k)$-competitive randomized algorithm for which the competitive ratio is
independent of the size of the underlying HST. Our algorithm is designed in the
framework of online mirror descent where the mirror map is a multiscale
entropy. When combined with Bartal's static HST embedding reduction, this leads
to an $O((\log k)^2 \log n)$-competitive algorithm on any $n$-point metric
space. We give a new dynamic HST embedding that yields an $O((\log k)^3 \log
\Delta)$-competitive algorithm on any metric space where the ratio of the
largest to smallest non-zero distance is at most $\Delta$.
",1,0,1,0,0,0
3334,Pretest and Stein-Type Estimations in Quantile Regression Model,"  In this study, we consider preliminary test and shrinkage estimation
strategies for quantile regression models. In classical Least Squares
Estimation (LSE) method, the relationship between the explanatory and explained
variables in the coordinate plane is estimated with a mean regression line. In
order to use LSE, there are three main assumptions on the error terms showing
white noise process of the regression model, also known as Gauss-Markov
Assumptions, must be met: (1) The error terms have zero mean, (2) The variance
of the error terms is constant and (3) The covariance between the errors is
zero i.e., there is no autocorrelation. However, data in many areas, including
econometrics, survival analysis and ecology, etc. does not provide these
assumptions. First introduced by Koenker, quantile regression has been used to
complement this deficiency of classical regression analysis and to improve the
least square estimation. The aim of this study is to improve the performance of
quantile regression estimators by using pre-test and shrinkage strategies. A
Monte Carlo simulation study including a comparison with quantile $L_1$--type
estimators such as Lasso, Ridge and Elastic Net are designed to evaluate the
performances of the estimators. Two real data examples are given for
illustrative purposes. Finally, we obtain the asymptotic results of suggested
estimators
",0,0,1,1,0,0
8920,Model order reduction for random nonlinear dynamical systems and low-dimensional representations for their quantities of interest,"  We examine nonlinear dynamical systems of ordinary differential equations or
differential algebraic equations. In an uncertainty quantification, physical
parameters are replaced by random variables. The inner variables as well as a
quantity of interest are expanded into series with orthogonal basis functions
like the polynomial chaos expansions, for example. On the one hand, the
stochastic Galerkin method yields a large coupled dynamical system. On the
other hand, a stochastic collocation method, which uses a quadrature rule or a
sampling scheme, can be written in the form of a large weakly coupled dynamical
system. We apply projection-based methods of nonlinear model order reduction to
the large systems. A reduced-order model implies a low-dimensional
representation of the quantity of interest. We focus on model order reduction
by proper orthogonal decomposition. The error of a best approximation located
in a low-dimensional subspace is analysed. We illustrate results of numerical
computations for test examples.
",0,0,1,0,0,0
7700,On the local and global comparison of generalized Bajraktarević means,"  Given two continuous functions $f,g:I\to\mathbb{R}$ such that $g$ is positive
and $f/g$ is strictly monotone, a measurable space $(T,A)$, a measurable family
of $d$-variable means $m: I^d\times T\to I$, and a probability measure $\mu$ on
the measurable sets $A$, the $d$-variable mean $M_{f,g,m;\mu}:I^d\to I$ is
defined by $$
M_{f,g,m;\mu}(\pmb{x})
:=\left(\frac{f}{g}\right)^{-1}\left(
\frac{\int_T f\big(m(x_1,\dots,x_d,t)\big) d\mu(t)}
{\int_T g\big(m(x_1,\dots,x_d,t)\big) d\mu(t)}\right)
\qquad(\pmb{x}=(x_1,\dots,x_d)\in I^d). $$ The aim of this paper is to study
the local and global comparison problem of these means, i.e., to find
conditions for the generating functions $(f,g)$ and $(h,k)$, for the families
of means $m$ and $n$, and for the measures $\mu,\nu$ such that the comparison
inequality $$
M_{f,g,m;\mu}(\pmb{x})\leq M_{h,k,n;\nu}(\pmb{x}) \qquad(\pmb{x}\in I^d) $$
be satisfied.
",0,0,1,0,0,0
15648,Finite-size effects in a stochastic Kuramoto model,"  We present a collective coordinate approach to study the collective behaviour
of a finite ensemble of $N$ stochastic Kuramoto oscillators using two degrees
of freedom; one describing the shape dynamics of the oscillators and one
describing their mean phase. Contrary to the thermodynamic limit $N\to\infty$
in which the mean phase of the cluster of globally synchronized oscillators is
constant in time, the mean phase of a finite-size cluster experiences Brownian
diffusion with a variance proportional to $1/N$. This finite-size effect is
quantitatively well captured by our collective coordinate approach.
",0,1,0,0,0,0
19882,Conceptual Frameworks for Building Online Citizen Science Projects,"  In recent years, citizen science has grown in popularity due to a number of
reasons, including the emphasis on informal learning and creativity potential
associated with these initiatives. Citizen science projects address research
questions from various domains, ranging from Ecology to Astronomy. Due to the
advancement of communication technologies, which makes outreach and engagement
of wider communities easier, scientists are keen to turn their own research
into citizen science projects. However, the development, deployment and
management of these projects remains challenging. One of the most important
challenges is building the project itself. There is no single tool or
framework, which guides the step-by-step development of the project, since
every project has specific characteristics, such as geographical constraints or
volunteers' mode of participation. Therefore, in this article, we present a
series of conceptual frameworks for categorisation, decision and deployment,
which guide a citizen science project creator in every step of creating a new
project starting from the research question to project deployment. The
frameworks are designed with consideration to the properties of already
existing citizen science projects and could be easily extended to include other
dimensions, which are not currently perceived.
",1,0,0,0,0,0
3540,Active Galactic Nuclei: what's in a name?,"  Active Galactic Nuclei (AGN) are energetic astrophysical sources powered by
accretion onto supermassive black holes in galaxies, and present unique
observational signatures that cover the full electromagnetic spectrum over more
than twenty orders of magnitude in frequency. The rich phenomenology of AGN has
resulted in a large number of different ""flavours"" in the literature that now
comprise a complex and confusing AGN ""zoo"". It is increasingly clear that these
classifications are only partially related to intrinsic differences between
AGN, and primarily reflect variations in a relatively small number of
astrophysical parameters as well the method by which each class of AGN is
selected. Taken together, observations in different electromagnetic bands as
well as variations over time provide complementary windows on the physics of
different sub-structures in the AGN. In this review, we present an overview of
AGN multi-wavelength properties with the aim of painting their ""big picture""
through observations in each electromagnetic band from radio to gamma-rays as
well as AGN variability. We address what we can learn from each observational
method, the impact of selection effects, the physics behind the emission at
each wavelength, and the potential for future studies. To conclude we use these
observations to piece together the basic architecture of AGN, discuss our
current understanding of unification models, and highlight some open questions
that present opportunities for future observational and theoretical progress.
",0,1,0,0,0,0
9885,Assessment Formats and Student Learning Performance: What is the Relation?,"  Although compelling assessments have been examined in recent years, more
studies are required to yield a better understanding of the several methods
where assessment techniques significantly affect student learning process. Most
of the educational research in this area does not consider demographics data,
differing methodologies, and notable sample size. To address these drawbacks,
the objective of our study is to analyse student learning outcomes of multiple
assessment formats for a web-facilitated in-class section with an asynchronous
online class of a core data communications course in the Undergraduate IT
program of the Information Sciences and Technology (IST) Department at George
Mason University (GMU). In this study, students were evaluated based on course
assessments such as home and lab assignments, skill-based assessments, and
traditional midterm and final exams across all four sections of the course. All
sections have equivalent content, assessments, and teaching methodologies.
Student demographics such as exam type and location preferences are considered
in our study to determine whether they have any impact on their learning
approach. Large amount of data from the learning management system (LMS),
Blackboard (BB) Learn, had to be examined to compare the results of several
assessment outcomes for all students within their respective section and
amongst students of other sections. To investigate the effect of dissimilar
assessment formats on student performance, we had to correlate individual
question formats with the overall course grade. The results show that
collective assessment formats allow students to be effective in demonstrating
their knowledge.
",1,0,0,1,0,0
12063,A perturbation theory for water with an associating reference fluid,"  The theoretical description of the thermodynamics of water is challenged by
the structural transition towards tetrahedral symmetry at ambient conditions.
As perturbation theories typically assume a spherically symmetric reference
fluid, they are incapable of accurately describing the liquid properties of
water at ambient conditions. In this paper we solve this problem, by
introducing the concept of an associated reference perturbation theory (APT).
In APT we treat the reference fluid as an associating hard sphere fluid which
transitions to tetrahedral symmetry in the fully hydrogen bonded limit. We
calculate this transition in a theoretically self-consistent manner without
appealing to molecular simulations. This associated reference provides the
reference fluid for a second order Barker-Hendersen perturbative treatment of
the long-range attractions. We demonstrate that this new approach gives a
significantly improved description of water as compared to standard
perturbation theories.
",0,1,0,0,0,0
15721,Universal experimental test for the role of free charge carriers in thermal Casimir effect within a micrometer separation range,"  We propose a universal experiment to measure the differential Casimir force
between a Au-coated sphere and two halves of a structured plate covered with a
P-doped Si overlayer. The concentration of free charge carriers in the
overlayer is chosen slightly below the critical one, f or which the phase
transition from dielectric to metal occurs. One ha f of the structured plate is
insulating, while its second half is made of gold. For the former we consider
two different structures, one consisting of bulk high-resistivity Si and the
other of a layer of silica followed by bulk high-resistivity Si. The
differential Casimir force is computed within the Lifshitz theory using four
approaches that have been proposed in the literature to account for the role of
free charge carriers in metallic and dielectric materials interacting with
quantum fluctuations. According to these approaches, Au at low frequencies is
described by either the Drude or the plasma model, whereas the free charge
carriers in dielectric materials at room temperature are either taken into
account or disregarded. It is shown that the values of differential Casimir
forces, computed in the micrometer separation range using these four
approaches, are widely distinct from each other and can be easily discriminated
experimentally. It is shown that for all approaches the thermal component of
the differential Casimir force is sufficiently large for direct observation.
The possible errors and uncertainties in the proposed experiment are estimated
and its importance for the theory of quantum fluctuations is discussed.
",0,1,0,0,0,0
3600,On the Robustness of the CVPR 2018 White-Box Adversarial Example Defenses,"  Neural networks are known to be vulnerable to adversarial examples. In this
note, we evaluate the two white-box defenses that appeared at CVPR 2018 and
find they are ineffective: when applying existing techniques, we can reduce the
accuracy of the defended models to 0%.
",0,0,0,1,0,0
20742,Theoretical Evaluation of Li et al.'s Approach for Improving a Binary Watermark-Based Scheme in Remote Sensing Data Communications,"  This letter is about a principal weakness of the published article by Li et
al. in 2014. It seems that the mentioned work has a terrible conceptual mistake
while presenting its theoretical approach. In fact, the work has tried to
design a new attack and its effective solution for a basic watermarking
algorithm by Zhu et al. published in 2013, however in practice, we show the Li
et al.'s approach is not correct to obtain the aim. For disproof of the
incorrect approach, we only apply a numerical example as the counterexample of
the Li et al.'s approach.
",1,0,0,0,0,0
3252,Clipped Matrix Completion: A Remedy for Ceiling Effects,"  We consider the problem of recovering a low-rank matrix from its clipped
observations. Clipping is conceivable in many scientific areas that obstructs
statistical analyses. On the other hand, matrix completion (MC) methods can
recover a low-rank matrix from various information deficits by using the
principle of low-rank completion. However, the current theoretical guarantees
for low-rank MC do not apply to clipped matrices, as the deficit depends on the
underlying values. Therefore, the feasibility of clipped matrix completion
(CMC) is not trivial. In this paper, we first provide a theoretical guarantee
for the exact recovery of CMC by using a trace-norm minimization algorithm.
Furthermore, we propose practical CMC algorithms by extending ordinary MC
methods. Our extension is to use the squared hinge loss in place of the squared
loss for reducing the penalty of over-estimation on clipped entries. We also
propose a novel regularization term tailored for CMC. It is a combination of
two trace-norm terms, and we theoretically bound the recovery error under the
regularization. We demonstrate the effectiveness of the proposed methods
through experiments using both synthetic and benchmark data for recommendation
systems.
",0,0,0,1,0,0
10450,On the reducibility of induced representations for classical p-adic groups and related affine Hecke algebras,"  Let $\pi $ be an irreducible smooth complex representation of a general
linear $p$-adic group and let $\sigma $ be an irreducible complex supercuspidal
representation of a classical $p$-adic group of a given type, so that
$\pi\otimes\sigma $ is a representation of a standard Levi subgroup of a
$p$-adic classical group of higher rank. We show that the reducibility of the
representation of the appropriate $p$-adic classical group obtained by
(normalized) parabolic induction from $\pi\otimes\sigma $ does not depend on
$\sigma $, if $\sigma $ is ""separated"" from the supercuspidal support of $\pi
$. (Here, ""separated"" means that, for each factor $\rho $ of a representation
in the supercuspidal support of $\pi $, the representation parabolically
induced from $\rho\otimes\sigma $ is irreducible.) This was conjectured by E.
Lapid and M. Tadić. (In addition, they proved, using results of C. Jantzen,
that this induced representation is always reducible if the supercuspidal
support is not separated.)
More generally, we study, for a given set $I$ of inertial orbits of
supercuspidal representations of $p$-adic general linear groups, the category
$\CC _{I,\sigma}$ of smooth complex finitely generated representations of
classical $p$-adic groups of fixed type, but arbitrary rank, and supercuspidal
support given by $\sigma $ and $I$, show that this category is equivalent to a
category of finitely generated right modules over a direct sum of tensor
products of extended affine Hecke algebras of type $A$, $B$ and $D$ and
establish functoriality properties, relating categories with disjoint $I$'s. In
this way, we extend results of C. Jantzen who proved a bijection between
irreducible representations corresponding to these categories. The proof of the
above reducibility result is then based on Hecke algebra arguments, using
Kato's exotic geometry.
",0,0,1,0,0,0
13081,Influence Networks in International Relations,"  Measuring influence and determining what drives it are persistent questions
in political science and in network analysis more generally. Herein we focus on
the domain of international relations. Our major substantive question is: How
can we determine what characteristics make an actor influential? To address the
topic of influence, we build on a multilinear tensor regression framework
(MLTR) that captures influence relationships using a tensor generalization of a
vector autoregression model. Influence relationships in that approach are
captured in a pair of n x n matrices and provide measurements of how the
network actions of one actor may influence the future actions of another. A
limitation of the MLTR and earlier latent space approaches is that there are no
direct mechanisms through which to explain why a certain actor is more or less
influential than others. Our new framework, social influence regression,
provides a way to statistically model the influence of one actor on another as
a function of characteristics of the actors. Thus we can move beyond just
estimating that an actor influences another to understanding why. To highlight
the utility of this approach, we apply it to studying monthly-level conflictual
events between countries as measured through the Integrated Crisis Early
Warning System (ICEWS) event data project.
",0,1,0,1,0,0
3251,Deep Learning for micro-Electrocorticographic (μECoG) Data,"  Machine learning can extract information from neural recordings, e.g.,
surface EEG, ECoG and {\mu}ECoG, and therefore plays an important role in many
research and clinical applications. Deep learning with artificial neural
networks has recently seen increasing attention as a new approach in brain
signal decoding. Here, we apply a deep learning approach using convolutional
neural networks to {\mu}ECoG data obtained with a wireless, chronically
implanted system in an ovine animal model. Regularized linear discriminant
analysis (rLDA), a filter bank component spatial pattern (FBCSP) algorithm and
convolutional neural networks (ConvNets) were applied to auditory evoked
responses captured by {\mu}ECoG. We show that compared with rLDA and FBCSP,
significantly higher decoding accuracy can be obtained by ConvNets trained in
an end-to-end manner, i.e., without any predefined signal features. Deep
learning thus proves a promising technique for {\mu}ECoG-based brain-machine
interfacing applications.
",0,0,0,0,1,0
2845,Bayesian uncertainty quantification in linear models for diffusion MRI,"  Diffusion MRI (dMRI) is a valuable tool in the assessment of tissue
microstructure. By fitting a model to the dMRI signal it is possible to derive
various quantitative features. Several of the most popular dMRI signal models
are expansions in an appropriately chosen basis, where the coefficients are
determined using some variation of least-squares. However, such approaches lack
any notion of uncertainty, which could be valuable in e.g. group analyses. In
this work, we use a probabilistic interpretation of linear least-squares
methods to recast popular dMRI models as Bayesian ones. This makes it possible
to quantify the uncertainty of any derived quantity. In particular, for
quantities that are affine functions of the coefficients, the posterior
distribution can be expressed in closed-form. We simulated measurements from
single- and double-tensor models where the correct values of several quantities
are known, to validate that the theoretically derived quantiles agree with
those observed empirically. We included results from residual bootstrap for
comparison and found good agreement. The validation employed several different
models: Diffusion Tensor Imaging (DTI), Mean Apparent Propagator MRI (MAP-MRI)
and Constrained Spherical Deconvolution (CSD). We also used in vivo data to
visualize maps of quantitative features and corresponding uncertainties, and to
show how our approach can be used in a group analysis to downweight subjects
with high uncertainty. In summary, we convert successful linear models for dMRI
signal estimation to probabilistic models, capable of accurate uncertainty
quantification.
",0,1,0,1,0,0
12352,Effects of a Price limit Change on Market Stability at the Intraday Horizon in the Korean Stock Market,"  This paper investigates the effects of a price limit change on the volatility
of the Korean stock market's (KRX) intraday stock price process. Based on the
most recent transaction data from the KRX, which experienced a change in the
price limit on June 15, 2015, we examine the change in realized variance after
the price limit change to investigate the overall effects of the change on the
intraday market volatility. We then analyze the effects in more detail by
applying the discrete Fourier transform (DFT) to the data set. We find evidence
that the market becomes more volatile in the intraday horizon because of the
increase in the amplitudes of the low-frequency components of the price
processes after the price limit change. Therefore, liquidity providers are in a
worse situation than they were prior to the change.
",0,0,0,0,0,1
19276,Automatic smoothness detection of the resolvent Krylov subspace method for the approximation of $C_0$-semigroups,"  The resolvent Krylov subspace method builds approximations to operator
functions $f(A)$ times a vector $v$. For the semigroup and related operator
functions, this method is proved to possess the favorable property that the
convergence is automatically faster when the vector $v$ is smoother. The user
of the method does not need to know the presented theory and alterations of the
method are not necessary in order to adapt to the (possibly unknown) smoothness
of $v$. The findings are illustrated by numerical experiments.
",0,0,1,0,0,0
14094,A further generalization of the Emden-Fowler equation,"  A generalization of the Emden-Fowler equation is presented and its solutions
are investigated. This paper is devoted to asymptotic behavior of its
solutions. The procedure is entirely based on a previous paper by the author.
",0,0,1,0,0,0
14447,Bayesian Semi-supervised Learning with Graph Gaussian Processes,"  We propose a data-efficient Gaussian process-based Bayesian approach to the
semi-supervised learning problem on graphs. The proposed model shows extremely
competitive performance when compared to the state-of-the-art graph neural
networks on semi-supervised learning benchmark experiments, and outperforms the
neural networks in active learning experiments where labels are scarce.
Furthermore, the model does not require a validation data set for early
stopping to control over-fitting. Our model can be viewed as an instance of
empirical distribution regression weighted locally by network connectivity. We
further motivate the intuitive construction of the model with a Bayesian linear
model interpretation where the node features are filtered by an operator
related to the graph Laplacian. The method can be easily implemented by
adapting off-the-shelf scalable variational inference algorithms for Gaussian
processes.
",1,0,0,1,0,0
8332,Grassmanians and Pseudosphere Arrangements,"  We extend vector configurations to more general objects that have nicer
combinatorial and topological properties, called weighted pseudosphere
arrangements. These are defined as a weighted variant of arrangements of
pseudospheres, as in the Topological Representation Theorem for oriented
matroids. We show that in rank 3, the real Stiefel manifold, Grassmanian, and
oriented Grassmanian are homotopy equivalent to the analagously defined spaces
of weighted pseudosphere arrangements. We also show for all rank 3 oriented
matroids, that the space of realizations by weighted pseudosphere arrangements
is contractible. This is a sharp contrast with vector configurations, where the
space of realizations can have the homotopy type of any primary semialgebraic
set.
",0,0,1,0,0,0
8989,Chiral magnetic textures in Ir/Fe/Co/Pt multilayers: Evolution and topological Hall signature,"  Skyrmions are topologically protected, two-dimensional, localized hedgehogs
and whorls of spin. Originally invented as a concept in field theory for
nuclear interactions, skyrmions are central to a wide range of phenomena in
condensed matter. Their realization at room temperature (RT) in magnetic
multilayers has generated considerable interest, fueled by technological
prospects and the access granted to fundamental questions. The interaction of
skyrmions with charge carriers gives rise to exotic electrodynamics, such as
the topological Hall effect (THE), the Hall response to an emergent magnetic
field, a manifestation of the skyrmion Berry-phase. The proposal that THE can
be used to detect skyrmions needs to be tested quantitatively. For that it is
imperative to develop comprehensive understanding of skyrmions and other chiral
textures, and their electrical fingerprint. Here, using Hall transport and
magnetic imaging, we track the evolution of magnetic textures and their THE
signature in a technologically viable multilayer film as a function of
temperature ($T$) and out-of-plane applied magnetic field ($H$). We show that
topological Hall resistivity ($\rho_\mathrm{TH}$) scales with the density of
isolated skyrmions ($n_\mathrm{sk}$) over a wide range of $T$, confirming the
impact of the skyrmion Berry-phase on electronic transport. We find that at
higher $n_\mathrm{sk}$ skyrmions cluster into worms which carry considerable
topological charge, unlike topologically-trivial spin spirals. While we
establish a qualitative agreement between $\rho_\mathrm{TH}(H,T)$ and areal
density of topological charge $n_\mathrm{T}(H,T)$, our detailed quantitative
analysis shows a much larger $\rho_\mathrm{TH}$ than the prevailing theory
predicts for observed $n_\mathrm{T}$.
",0,1,0,0,0,0
20017,Multi-Sensor Data Pattern Recognition for Multi-Target Localization: A Machine Learning Approach,"  Data-target pairing is an important step towards multi-target localization
for the intelligent operation of unmanned systems. Target localization plays a
crucial role in numerous applications, such as search, and rescue missions,
traffic management and surveillance. The objective of this paper is to present
an innovative target location learning approach, where numerous machine
learning approaches, including K-means clustering and supported vector machines
(SVM), are used to learn the data pattern across a list of spatially
distributed sensors. To enable the accurate data association from different
sensors for accurate target localization, appropriate data pre-processing is
essential, which is then followed by the application of different machine
learning algorithms to appropriately group data from different sensors for the
accurate localization of multiple targets. Through simulation examples, the
performance of these machine learning algorithms is quantified and compared.
",1,0,0,1,0,0
16478,A National Research Agenda for Intelligent Infrastructure,"  Our infrastructure touches the day-to-day life of each of our fellow
citizens, and its capabilities, integrity and sustainability are crucial to the
overall competitiveness and prosperity of our country. Unfortunately, the
current state of U.S. infrastructure is not good: the American Society of Civil
Engineers' latest report on America's infrastructure ranked it at a D+ -- in
need of $3.9 trillion in new investments. This dire situation constrains the
growth of our economy, threatens our quality of life, and puts our global
leadership at risk. The ASCE report called out three actions that need to be
taken to address our infrastructure problem: 1) investment and planning in the
system; 2) bold leadership by elected officials at the local and federal state;
and 3) planning sustainability and resiliency in our infrastructure.
While our immediate infrastructure needs are critical, it would be
shortsighted to simply replicate more of what we have today. By doing so, we
miss the opportunity to create Intelligent Infrastructure that will provide the
foundation for increased safety and resilience, improved efficiencies and civic
services, and broader economic opportunities and job growth. Indeed, our
challenge is to proactively engage the declining, incumbent national
infrastructure system and not merely repair it, but to enhance it; to create an
internationally competitive cyber-physical system that provides an immediate
opportunity for better services for citizens and that acts as a platform for a
21st century, high-tech economy and beyond.
",1,0,0,0,0,0
7285,Online Learning Rate Adaptation with Hypergradient Descent,"  We introduce a general method for improving the convergence rate of
gradient-based optimizers that is easy to implement and works well in practice.
We demonstrate the effectiveness of the method in a range of optimization
problems by applying it to stochastic gradient descent, stochastic gradient
descent with Nesterov momentum, and Adam, showing that it significantly reduces
the need for the manual tuning of the initial learning rate for these commonly
used algorithms. Our method works by dynamically updating the learning rate
during optimization using the gradient with respect to the learning rate of the
update rule itself. Computing this ""hypergradient"" needs little additional
computation, requires only one extra copy of the original gradient to be stored
in memory, and relies upon nothing more than what is provided by reverse-mode
automatic differentiation.
",1,0,0,1,0,0
18838,Parameterization of Sequence of MFCCs for DNN-based voice disorder detection,"  In this article a DNN-based system for detection of three common voice
disorders (vocal nodules, polyps and cysts; laryngeal neoplasm; unilateral
vocal paralysis) is presented. The input to the algorithm is (at least 3-second
long) audio recording of sustained vowel sound /a:/. The algorithm was
developed as part of the ""2018 FEMH Voice Data Challenge"" organized by Far
Eastern Memorial Hospital and obtained score value (defined in the challenge
specification) of 77.44. This was the second best result before final
submission. Final challenge results are not yet known during writing of this
document. The document also reports changes that were made for the final
submission which improved the score value in cross-validation by 0.6% points.
",1,0,0,0,0,0
6645,Obstacle Avoidance Using Stereo Camera,"  In this paper we present a novel method for obstacle avoidance using the
stereo camera. The conventional obstacle avoidance methods and their
limitations are discussed. A new algorithm is developed for the real-time
obstacle avoidance which responds faster to unexpected obstacles. In this
approach the depth map is divided into optimized number of regions and the
minimum depth at each section is assigned as the depth of that region. A fuzzy
controller is designed to create the drive commands for the robot/quadcopter.
The system was tested on multiple paths with different obstacles and the
results demonstrated the high accuracy of the developed system.
",1,0,0,0,0,0
2001,Autonomy in the interactive music system VIVO,"  Interactive Music Systems (IMS) have introduced a new world of music-making
modalities. But can we really say that they create music, as in true autonomous
creation? Here we discuss Video Interactive VST Orchestra (VIVO), an IMS that
considers extra-musical information by adopting a simple salience based model
of user-system interaction when simulating intentionality in automatic music
generation. Key features of the theoretical framework, a brief overview of
pilot research, and a case study providing validation of the model are
presented. This research demonstrates that a meaningful user/system interplay
is established in what we define as reflexive multidominance.
",1,0,0,0,0,0
7255,TLR: Transfer Latent Representation for Unsupervised Domain Adaptation,"  Domain adaptation refers to the process of learning prediction models in a
target domain by making use of data from a source domain. Many classic methods
solve the domain adaptation problem by establishing a common latent space,
which may cause the loss of many important properties across both domains. In
this manuscript, we develop a novel method, transfer latent representation
(TLR), to learn a better latent space. Specifically, we design an objective
function based on a simple linear autoencoder to derive the latent
representations of both domains. The encoder in the autoencoder aims to project
the data of both domains into a robust latent space. Besides, the decoder
imposes an additional constraint to reconstruct the original data, which can
preserve the common properties of both domains and reduce the noise that causes
domain shift. Experiments on cross-domain tasks demonstrate the advantages of
TLR over competing methods.
",0,0,0,1,0,0
14003,Efficient Regret Minimization in Non-Convex Games,"  We consider regret minimization in repeated games with non-convex loss
functions. Minimizing the standard notion of regret is computationally
intractable. Thus, we define a natural notion of regret which permits efficient
optimization and generalizes offline guarantees for convergence to an
approximate local optimum. We give gradient-based methods that achieve optimal
regret, which in turn guarantee convergence to equilibrium in this framework.
",1,0,0,1,0,0
5796,Richardson's solutions in the real- and complex-energy spectrum,"  The constant pairing Hamiltonian holds exact solutions worked out by
Richardson in the early Sixties. This exact solution of the pairing Hamiltonian
regained interest at the end of the Nineties. The discret complex-energy states
had been included in the Richardson's solutions by Hasegawa et al. [1]. In this
contribution we reformulate the problem of determining the exact eigenenergies
of the pairing Hamiltonian when the continuum is included through the single
particle level density. The solutions with discret complex-energy states is
recovered by analytic continuation of the equations to the complex energy
plane. This formulation may be applied to loosely bound system where the
correlations with the continuum-spectrum of energy is really important. Some
details are given to show how the many-body eigenenergy emerges as sum of the
pair-energies.
",0,1,0,0,0,0
11299,Inverse mean curvature flow in quaternionic hyperbolic space,"  In this paper we complete the study started in [Pi2] of evolution by inverse
mean curvature flow of star-shaped hypersurface in non-compact rank one
symmetric spaces. We consider the evolution by inverse mean curvature flow of a
closed, mean convex and star-shaped hypersurface in the quaternionic hyperbolic
space. We prove that the flow is defined for any positive time, the evolving
hypersurface stays star-shaped and mean convex. Moreover the induced metric
converges, after rescaling, to a conformal multiple of the standard
sub-Riemannian metric on the sphere defined on a codimension 3 distribution.
Finally we show that there exists a family of examples such that the qc-scalar
curvature of this sub-Riemannian limit is not constant.
",0,0,1,0,0,0
12206,Fracton Models on General Three-Dimensional Manifolds,"  Fracton models, a collection of exotic gapped lattice Hamiltonians recently
discovered in three spatial dimensions, contain some 'topological' features:
they support fractional bulk excitations (dubbed fractons), and a ground state
degeneracy that is robust to local perturbations. However, because previous
fracton models have only been defined and analyzed on a cubic lattice with
periodic boundary conditions, it is unclear to what extent a notion of topology
is applicable. In this paper, we demonstrate that the X-cube model, a
prototypical type-I fracton model, can be defined on general three-dimensional
manifolds. Our construction revolves around the notion of a singular compact
total foliation of the spatial manifold, which constructs a lattice from
intersecting stacks of parallel surfaces called leaves. We find that the ground
state degeneracy depends on the topology of the leaves and the pattern of leaf
intersections. We further show that such a dependence can be understood from a
renormalization group transformation for the X-cube model, wherein the system
size can be changed by adding or removing 2D layers of topological states. Our
results lead to an improved definition of fracton phase and bring to the fore
the topological nature of fracton orders.
",0,1,0,0,0,0
4160,Fraction of the X-ray selected AGNs with optical emission lines in galaxy groups,"  Compared with numerous X-ray dominant active galactic nuclei (AGNs) without
emission-line signatures in their optical spectra, the X-ray selected AGNs with
optical emission lines are probably still in the high-accretion phase of black
hole growth. This paper presents an investigation on the fraction of these
X-ray detected AGNs with optical emission-line spectra in 198 galaxy groups at
$z<1$ in a rest frame 0.1-2.4 keV luminosity range 41.3 <log(L_X/erg s-1) <
44.1 within the COSMOS field, as well as its variations with redshift and group
richness. For various selection criteria of member galaxies, the numbers of
galaxies and the AGNs with optical emission lines in each galaxy group are
obtained. It is found that, in total 198 X-ray groups, there are 27 AGNs
detected in 26 groups. AGN fraction is on everage less than $4.6 (\pm 1.2)\%$
for individual groups hosting at least one AGN. The corrected overall AGN
fraction for whole group sample is less than $0.98 (\pm 0.11) \%$. The
normalized locations of group AGNs show that 15 AGNs are found to be located in
group centers, including all 6 low-luminosity group AGNs. A week rising
tendency with $z$ are found: overall AGN fraction is 0.30-0.43% for the groups
at $z<0.5$, and 0.55-0.64% at 0.5 < z < 1.0. For the X-ray groups at $z>0.5$,
most member AGNs are X-ray bright, optically dull, which results in a lower AGN
fractions at higher redshifts. The AGN fraction in isolated fields also
exhibits a rising trend with redshift, and the slope is consistent with that in
groups. The environment of galaxy groups seems to make no difference in
detection probability of the AGNs with emission lines. Additionally, a larger
AGN fractions are found in poorer groups, which implies that the AGNs in poorer
groups might still be in the high-accretion phase, whereas the AGN population
in rich clusters is mostly in the low-accretion, X-ray dominant phase.
",0,1,0,0,0,0
2380,The efficiency of community detection by most similar node pairs,"  Community analysis is an important way to ascertain whether or not a complex
system consists of sub-structures with different properties. In this paper, we
give a two level community structure analysis for the SSCI journal system by
most similar co-citation pattern. Five different strategies for the selection
of most similar node (journal) pairs are introduced. The efficiency is checked
by the normalized mutual information technique. Statistical properties and
comparisons of the community results show that both of the two level detection
could give instructional information for the community structure of complex
systems. Further comparisons of the five strategies indicates that, the most
efficient strategy is to assign nodes with maximum similarity into the same
community whether the similarity information is complete or not, while random
selection generates small world local community with no inside order. These
results give valuable indication for efficient community detection by most
similar node pairs.
",1,0,0,0,0,0
20333,On infinite order differential operators in fractional viscoelasticity,"  In this paper we discuss some general properties of viscoelastic models
defined in terms of constitutive equations involving infinitely many
derivatives (of integer and fractional order). In particular, we consider as a
working example the recently developed Bessel models of linear viscoelasticiy
that, for short times, behave like fractional Maxwell bodies of order $1/2$.
",0,1,1,0,0,0
3755,Testing isomorphism of lattices over CM-orders,"  A CM-order is a reduced order equipped with an involution that mimics complex
conjugation. The Witt-Picard group of such an order is a certain group of ideal
classes that is closely related to the ""minus part"" of the class group. We
present a deterministic polynomial-time algorithm for the following problem,
which may be viewed as a special case of the principal ideal testing problem:
given a CM-order, decide whether two given elements of its Witt-Picard group
are equal. In order to prevent coefficient blow-up, the algorithm operates with
lattices rather than with ideals. An important ingredient is a technique
introduced by Gentry and Szydlo in a cryptographic context. Our application of
it to lattices over CM-orders hinges upon a novel existence theorem for
auxiliary ideals, which we deduce from a result of Konyagin and Pomerance in
elementary number theory.
",1,0,1,0,0,0
18258,Difficulties of Timestamping Archived Web Pages,"  We show that state-of-the-art services for creating trusted timestamps in
blockchain-based networks do not adequately allow for timestamping of web
pages. They accept data by value (e.g., images and text), but not by reference
(e.g., URIs of web pages). Also, we discuss difficulties in repeatedly
generating the same cryptographic hash value of an archived web page. We then
introduce several requirements to be fulfilled in order to produce repeatable
hash values for archived web pages.
",1,0,0,0,0,0
16751,Misconceptions about Calorimetry,"  In the past 50 years, calorimeters have become the most important detectors
in many particle physics experiments, especially experiments in colliding-beam
accelerators at the energy frontier. In this paper, we describe and discuss a
number of common misconceptions about these detectors, as well as the
consequences of these misconceptions. We hope that it may serve as a useful
source of information for young colleagues who want to familiarize themselves
with these tricky instruments.
",0,1,0,0,0,0
5374,Model-Based Clustering of Time-Evolving Networks through Temporal Exponential-Family Random Graph Models,"  Dynamic networks are a general language for describing time-evolving complex
systems, and discrete time network models provide an emerging statistical
technique for various applications. It is a fundamental research question to
detect the community structure in time-evolving networks. However, due to
significant computational challenges and difficulties in modeling communities
of time-evolving networks, there is little progress in the current literature
to effectively find communities in time-evolving networks. In this work, we
propose a novel model-based clustering framework for time-evolving networks
based on discrete time exponential-family random graph models. To choose the
number of communities, we use conditional likelihood to construct an effective
model selection criterion. Furthermore, we propose an efficient variational
expectation-maximization (EM) algorithm to find approximate maximum likelihood
estimates of network parameters and mixing proportions. By using variational
methods and minorization-maximization (MM) techniques, our method has appealing
scalability for large-scale time-evolving networks. The power of our method is
demonstrated in simulation studies and empirical applications to international
trade networks and the collaboration networks of a large American research
university.
",0,0,0,1,0,0
3330,Analysis of Annual Cyclone Frequencies over Bay of Bengal: Effect of 2004 Indian Ocean Tsunami,"  This paper discusses the time series trend and variability of the cyclone
frequencies over Bay of Bengal, particularly in order to conclude if there is
any significant difference in the pattern visible before and after the
disastrous 2004 Indian ocean tsunami based on the observed annual cyclone
frequency data obtained by India Meteorological Department over the years
1891-2015. Three different categories of cyclones- depression (<34 knots),
cyclonic storm (34-47 knots) and severe cyclonic storm (>47 knots) have been
analyzed separately using a non-homogeneous Poisson process approach. The
estimated intensity functions of the Poisson processes along with their first
two derivatives are discussed and all three categories show decreasing trend of
the intensity functions after the tsunami. Using an exact change-point
analysis, we show that the drops in mean intensity functions are significant
for all three categories. As of author's knowledge, no study so far have
discussed the relation between cyclones and tsunamis. Bay of Bengal is
surrounded by one of the most densely populated areas of the world and any kind
of significant change in tropical cyclone pattern has a large impact in various
ways, for example, disaster management planning and our study is immensely
important from that perspective.
",0,0,0,1,0,0
16753,Automatic Detection of Knee Joints and Quantification of Knee Osteoarthritis Severity using Convolutional Neural Networks,"  This paper introduces a new approach to automatically quantify the severity
of knee OA using X-ray images. Automatically quantifying knee OA severity
involves two steps: first, automatically localizing the knee joints; next,
classifying the localized knee joint images. We introduce a new approach to
automatically detect the knee joints using a fully convolutional neural network
(FCN). We train convolutional neural networks (CNN) from scratch to
automatically quantify the knee OA severity optimizing a weighted ratio of two
loss functions: categorical cross-entropy and mean-squared loss. This joint
training further improves the overall quantification of knee OA severity, with
the added benefit of naturally producing simultaneous multi-class
classification and regression outputs. Two public datasets are used to evaluate
our approach, the Osteoarthritis Initiative (OAI) and the Multicenter
Osteoarthritis Study (MOST), with extremely promising results that outperform
existing approaches.
",1,0,0,0,0,0
20286,Quantitative Photoacoustic Imaging in the Acoustic Regime using SPIM,"  While in standard photoacoustic imaging the propagation of sound waves is
modeled by the standard wave equation, our approach is based on a generalized
wave equation with variable sound speed and material density, respectively. In
this paper we present an approach for photoacoustic imaging, which in addition
to recovering of the absorption density parameter, the imaging parameter of
standard photoacoustics, also allows to reconstruct the spatially varying sound
speed and density, respectively, of the medium. We provide analytical
reconstruction formulas for all three parameters based in a linearized model
based on single plane illumination microscopy (SPIM) techniques.
",0,0,1,0,0,0
7695,Tangent: Automatic differentiation using source-code transformation for dynamically typed array programming,"  The need to efficiently calculate first- and higher-order derivatives of
increasingly complex models expressed in Python has stressed or exceeded the
capabilities of available tools. In this work, we explore techniques from the
field of automatic differentiation (AD) that can give researchers expressive
power, performance and strong usability. These include source-code
transformation (SCT), flexible gradient surgery, efficient in-place array
operations, higher-order derivatives as well as mixing of forward and reverse
mode AD. We implement and demonstrate these ideas in the Tangent software
library for Python, the first AD framework for a dynamic language that uses
SCT.
",1,0,0,0,0,0
18136,"Targeted and Imaging-guided In Vivo Photodynamic Therapy of Tumors Using Dual-functional, Aggregation-induced Emission Nanoparticles","  Dual-functional nanoparticles, with the property of aggregation-induced
emission and the capability of reactive oxygen species, were used to achieve
passive/active targeting of tumor. Good contrast in in vivo imaging and obvious
therapeutic efficiency were realized with a low dose of AIE nanoparticles as
well as a low power density of light, resulting in negligible side effects.
",0,1,0,0,0,0
11952,The asymptotic behavior of automorphism groups of function fields over finite fields,"  The purpose of this paper is to investigate the asymptotic behavior of
automorphism groups of function fields when genus tends to infinity.
Motivated by applications in coding and cryptography, we consider the maximum
size of abelian subgroups of the automorphism group
$\mbox{Aut}(F/\mathbb{F}_q)$ in terms of genus ${g_F}$ for a function field $F$
over a finite field $\mathbb{F}_q$. Although the whole group
$\mbox{Aut}(F/\mathbb{F}_q)$ could have size $\Omega({g_F}^4)$, the maximum
size $m_F$ of abelian subgroups of the automorphism group
$\mbox{Aut}(F/\mathbb{F}_q)$ is upper bounded by $4g_F+4$ for $g_F\ge 2$. In
the present paper, we study the asymptotic behavior of $m_F$ by defining
$M_q=\limsup_{{g_F}\rightarrow\infty}\frac{m_F \cdot \log_q m_F}{g_F}$, where
$F$ runs through all function fields over $\mathbb{F}_q$. We show that $M_q$
lies between $2$ and $3$ (or $4$) for odd characteristic (or for even
characteristic, respectively). This means that $m_F$ grows much more slowly
than genus does asymptotically.
The second part of this paper is to study the maximum size $b_F$ of subgroups
of $\mbox{Aut}(F/\mathbb{F}_q)$ whose order is coprime to $q$. The Hurwitz
bound gives an upper bound $b_F\le 84(g_F-1)$ for every function field
$F/\mathbb{F}_q$ of genus $g_F\ge 2$. We investigate the asymptotic behavior of
$b_F$ by defining ${B_q}=\limsup_{{g_F}\rightarrow\infty}\frac{b_F}{g_F}$,
where $F$ runs through all function fields over $\mathbb{F}_q$. Although the
Hurwitz bound shows ${B_q}\le 84$, there are no lower bounds on $B_q$ in
literature. One does not even know if ${B_q}=0$. For the first time, we show
that ${B_q}\ge 2/3$ by explicitly constructing some towers of function fields
in this paper.
",0,0,1,0,0,0
1801,Composing Differential Privacy and Secure Computation: A case study on scaling private record linkage,"  Private record linkage (PRL) is the problem of identifying pairs of records
that are similar as per an input matching rule from databases held by two
parties that do not trust one another. We identify three key desiderata that a
PRL solution must ensure: 1) perfect precision and high recall of matching
pairs, 2) a proof of end-to-end privacy, and 3) communication and computational
costs that scale subquadratically in the number of input records. We show that
all of the existing solutions for PRL - including secure 2-party computation
(S2PC), and their variants that use non-private or differentially private (DP)
blocking to ensure subquadratic cost - violate at least one of the three
desiderata. In particular, S2PC techniques guarantee end-to-end privacy but
have either low recall or quadratic cost. In contrast, no end-to-end privacy
guarantee has been formalized for solutions that achieve subquadratic cost.
This is true even for solutions that compose DP and S2PC: DP does not permit
the release of any exact information about the databases, while S2PC algorithms
for PRL allow the release of matching records.
In light of this deficiency, we propose a novel privacy model, called output
constrained differential privacy, that shares the strong privacy protection of
DP, but allows for the truthful release of the output of a certain function
applied to the data. We apply this to PRL, and show that protocols satisfying
this privacy model permit the disclosure of the true matching records, but
their execution is insensitive to the presence or absence of a single
non-matching record. We find that prior work that combine DP and S2PC
techniques even fail to satisfy this end-to-end privacy model. Hence, we
develop novel protocols that provably achieve this end-to-end privacy
guarantee, together with the other two desiderata of PRL.
",1,0,0,0,0,0
6757,Smoothing of transport plans with fixed marginals and rigorous semiclassical limit of the Hohenberg-Kohn functional,"  We prove rigorously that the exact N-electron Hohenberg-Kohn density
functional converges in the strongly interacting limit to the strictly
correlated electrons (SCE) functional, and that the absolute value squared of
the associated constrained-search wavefunction tends weakly in the sense of
probability measures to a minimizer of the multi-marginal optimal transport
problem with Coulomb cost associated to the SCE functional. This extends our
previous work for N=2 [CFK11]. The correct limit problem has been derived in
the physics literature by Seidl [Se99] and Seidl, Gori-Giorgi and Savin
[SGS07]; in these papers the lack of a rigorous proof was pointed out.
We also give a mathematical counterexample to this type of result, by
replacing the constraint of given one-body density -- an infinite-dimensional
quadratic expression in the wavefunction -- by an infinite-dimensional
quadratic expression in the wavefunction and its gradient. Connections with the
Lawrentiev phenomenon in the calculus of variations are indicated.
",0,0,1,0,0,0
15277,The Conditional Analogy GAN: Swapping Fashion Articles on People Images,"  We present a novel method to solve image analogy problems : it allows to
learn the relation between paired images present in training data, and then
generalize and generate images that correspond to the relation, but were never
seen in the training set. Therefore, we call the method Conditional Analogy
Generative Adversarial Network (CAGAN), as it is based on adversarial training
and employs deep convolutional neural networks. An especially interesting
application of that technique is automatic swapping of clothing on fashion
model photos. Our work has the following contributions. First, the definition
of the end-to-end trainable CAGAN architecture, which implicitly learns
segmentation masks without expensive supervised labeling data. Second,
experimental results show plausible segmentation masks and often convincing
swapped images, given the target article. Finally, we discuss the next steps
for that technique: neural network architecture improvements and more advanced
applications.
",1,0,0,1,0,0
5281,Improving the Expected Improvement Algorithm,"  The expected improvement (EI) algorithm is a popular strategy for information
collection in optimization under uncertainty. The algorithm is widely known to
be too greedy, but nevertheless enjoys wide use due to its simplicity and
ability to handle uncertainty and noise in a coherent decision theoretic
framework. To provide rigorous insight into EI, we study its properties in a
simple setting of Bayesian optimization where the domain consists of a finite
grid of points. This is the so-called best-arm identification problem, where
the goal is to allocate measurement effort wisely to confidently identify the
best arm using a small number of measurements. In this framework, one can show
formally that EI is far from optimal. To overcome this shortcoming, we
introduce a simple modification of the expected improvement algorithm.
Surprisingly, this simple change results in an algorithm that is asymptotically
optimal for Gaussian best-arm identification problems, and provably outperforms
standard EI by an order of magnitude.
",1,0,0,1,0,0
10493,Some divisibility properties of binomial coefficients,"  In this paper, we gave some properties of binomial coefficient.
",0,0,1,0,0,0
7365,Experimental and Theoretical Study of Magnetohydrodynamic Ship Models,"  Magnetohydrodynamic (MHD) ships represent a clear demonstration of the
Lorentz force in fluids, which explains the number of students practicals or
exercises described on the web. However, the related literature is rather
specific and no complete comparison between theory and typical small scale
experiments is currently available. This work provides, in a self-consistent
framework, a detailed presentation of the relevant theoretical equations for
small MHD ships and experimental measurements for future benchmarks.
Theoretical results of the literature are adapted to these simple
battery/magnets powered ships moving on salt water. Comparison between theory
and experiments are performed to validate each theoretical step such as the
Tafel and the Kohlrausch laws, or the predicted ship speed. A successful
agreement is obtained without any adjustable parameter. Finally, based on these
results, an optimal design is then deduced from the theory. Therefore this work
provides a solid theoretical and experimental ground for small scale MHD ships,
by presenting in detail several approximations and how they affect the boat
efficiency. Moreover, the theory is general enough to be adapted to other
contexts, such as large scale ships or industrial flow measurement techniques.
",0,1,0,0,0,0
15200,Proper quadrics in the Euclidean $n$-space,"  In this paper we investigate the metric properties of quadrics and cones of
the $n$-dimensional Euclidean space. As applications of our formulas we give a
more detailed description of the construction of Chasles and the wire model of
Staude, respectively.
",0,0,1,0,0,0
1407,Training deep learning based denoisers without ground truth data,"  Recent deep learning based denoisers often outperform state-of-the-art
conventional denoisers such as BM3D. They are typically trained to minimize the
mean squared error (MSE) between the output of a deep neural network and the
ground truth image. In deep learning based denoisers, it is important to use
high quality noiseless ground truth for high performance, but it is often
challenging or even infeasible to obtain such a clean image in application
areas such as hyperspectral remote sensing and medical imaging. We propose a
Stein's Unbiased Risk Estimator (SURE) based method for training deep neural
network denoisers only with noisy images. We demonstrated that our SURE based
method without ground truth was able to train deep neural network denoisers to
yield performance close to deep learning denoisers trained with ground truth
and to outperform state-of-the-art BM3D. Further improvements were achieved by
including noisy test images for training denoiser networks using our proposed
SURE based method.
",0,0,0,1,0,0
703,Is It Safe to Uplift This Patch? An Empirical Study on Mozilla Firefox,"  In rapid release development processes, patches that fix critical issues, or
implement high-value features are often promoted directly from the development
channel to a stabilization channel, potentially skipping one or more
stabilization channels. This practice is called patch uplift. Patch uplift is
risky, because patches that are rushed through the stabilization phase can end
up introducing regressions in the code. This paper examines patch uplift
operations at Mozilla, with the aim to identify the characteristics of uplifted
patches that introduce regressions. Through statistical and manual analyses, we
quantitatively and qualitatively investigate the reasons behind patch uplift
decisions and the characteristics of uplifted patches that introduced
regressions. Additionally, we interviewed three Mozilla release managers to
understand organizational factors that affect patch uplift decisions and
outcomes. Results show that most patches are uplifted because of a wrong
functionality or a crash. Uplifted patches that lead to faults tend to have
larger patch size, and most of the faults are due to semantic or memory errors
in the patches. Also, release managers are more inclined to accept patch uplift
requests that concern certain specific components, and-or that are submitted by
certain specific developers.
",1,0,0,0,0,0
8803,Weakly supervised training of deep convolutional neural networks for overhead pedestrian localization in depth fields,"  Overhead depth map measurements capture sufficient amount of information to
enable human experts to track pedestrians accurately. However, fully automating
this process using image analysis algorithms can be challenging. Even though
hand-crafted image analysis algorithms are successful in many common cases,
they fail frequently when there are complex interactions of multiple objects in
the image. Many of the assumptions underpinning the hand-crafted solutions do
not hold in these cases and the multitude of exceptions are hard to model
precisely. Deep Learning (DL) algorithms, on the other hand, do not require
hand crafted solutions and are the current state-of-the-art in object
localization in images. However, they require exceeding amount of annotations
to produce successful models. In the case of object localization these
annotations are difficult and time consuming to produce. In this work we
present an approach for developing pedestrian localization models using DL
algorithms with efficient weak supervision from an expert. We circumvent the
need for annotation of large corpus of data by annotating only small amount of
patches and relying on synthetic data augmentation as a vehicle for injecting
expert knowledge in the model training. This approach of weak supervision
through expert selection of representative patches, suitable transformations
and synthetic data augmentations enables us to successfully develop DL models
for pedestrian localization efficiently.
",1,1,0,0,0,0
10747,Stochastic Gradient Descent on Highly-Parallel Architectures,"  There is an increased interest in building data analytics frameworks with
advanced algebraic capabilities both in industry and academia. Many of these
frameworks, e.g., TensorFlow and BIDMach, implement their compute-intensive
primitives in two flavors---as multi-thread routines for multi-core CPUs and as
highly-parallel kernels executed on GPU. Stochastic gradient descent (SGD) is
the most popular optimization method for model training implemented extensively
on modern data analytics platforms. While the data-intensive properties of SGD
are well-known, there is an intense debate on which of the many SGD variants is
better in practice. In this paper, we perform a comprehensive study of parallel
SGD for training generalized linear models. We consider the impact of three
factors -- computing architecture (multi-core CPU or GPU), synchronous or
asynchronous model updates, and data sparsity -- on three measures---hardware
efficiency, statistical efficiency, and time to convergence. In the process, we
design an optimized asynchronous SGD algorithm for GPU that leverages warp
shuffling and cache coalescing for data and model access. We draw several
interesting findings from our extensive experiments with logistic regression
(LR) and support vector machines (SVM) on five real datasets. For synchronous
SGD, GPU always outperforms parallel CPU---they both outperform a sequential
CPU solution by more than 400X. For asynchronous SGD, parallel CPU is the
safest choice while GPU with data replication is better in certain situations.
The choice between synchronous GPU and asynchronous CPU depends on the task and
the characteristics of the data. As a reference, our best implementation
outperforms TensorFlow and BIDMach consistently. We hope that our insights
provide a useful guide for applying parallel SGD to generalized linear models.
",1,0,0,0,0,0
11823,Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy,"  Deep learning networks have achieved state-of-the-art accuracies on computer
vision workloads like image classification and object detection. The performant
systems, however, typically involve big models with numerous parameters. Once
trained, a challenging aspect for such top performing models is deployment on
resource constrained inference systems - the models (often deep networks or
wide networks or both) are compute and memory intensive. Low-precision numerics
and model compression using knowledge distillation are popular techniques to
lower both the compute requirements and memory footprint of these deployed
models. In this paper, we study the combination of these two techniques and
show that the performance of low-precision networks can be significantly
improved by using knowledge distillation techniques. Our approach, Apprentice,
achieves state-of-the-art accuracies using ternary precision and 4-bit
precision for variants of ResNet architecture on ImageNet dataset. We present
three schemes using which one can apply knowledge distillation techniques to
various stages of the train-and-deploy pipeline.
",1,0,0,0,0,0
12457,Stochastic variance reduced multiplicative update for nonnegative matrix factorization,"  Nonnegative matrix factorization (NMF), a dimensionality reduction and factor
analysis method, is a special case in which factor matrices have low-rank
nonnegative constraints. Considering the stochastic learning in NMF, we
specifically address the multiplicative update (MU) rule, which is the most
popular, but which has slow convergence property. This present paper introduces
on the stochastic MU rule a variance-reduced technique of stochastic gradient.
Numerical comparisons suggest that our proposed algorithms robustly outperform
state-of-the-art algorithms across different synthetic and real-world datasets.
",1,0,0,1,0,0
1550,Minimal solutions to generalized Lambda-semiflows and gradient flows in metric spaces,"  Generalized Lambda-semiflows are an abstraction of semiflows with
non-periodic solutions, for which there may be more than one solution
corresponding to given initial data. A select class of solutions to generalized
Lambda-semiflows is introduced. It is proved that such minimal solutions are
unique corresponding to given ranges and generate all other solutions by time
reparametrization. Special qualities of minimal solutions are shown. The
concept of minimal solutions is applied to gradient flows in metric spaces and
generalized semiflows. Generalized semiflows have been introduced by Ball.
",0,0,1,0,0,0
15894,"Hyperfunctions, the Duistermaat-Heckman theorem, and Loop Groups","  In this article we investigate the Duistermaat-Heckman theorem using the
theory of hyperfunctions. In applications involving Hamiltonian torus actions
on infinite dimensional manifolds, this more general theory seems to be
necessary in order to accomodate the existence of the infinite order
differential operators which arise from the isotropy representations on the
tangent spaces to fixed points. We will quickly review of the theory of
hyperfunctions and their Fourier transforms. We will then apply this theory to
construct a hyperfunction analogue of the Duistermaat-Heckman distribution. Our
main goal will be to study the Duistermaat-Heckman hyperfunction of $\Omega
SU(2)$, but in getting to this goal we will also characterize the singular
locus of the moment map for the Hamiltonian action of $T\times S^1$ on $\Omega
G$. The main goal of this paper is to present a Duistermaat-Heckman
hyperfunction arising from a Hamiltonian action on an infinite dimensional
manifold.
",0,0,1,0,0,0
12932,Most Complex Non-Returning Regular Languages,"  A regular language $L$ is non-returning if in the minimal deterministic
finite automaton accepting it there are no transitions into the initial state.
Eom, Han and Jirásková derived upper bounds on the state complexity of
boolean operations and Kleene star, and proved that these bounds are tight
using two different binary witnesses. They derived upper bounds for
concatenation and reversal using three different ternary witnesses. These five
witnesses use a total of six different transformations. We show that for each
$n\ge 4$ there exists a ternary witness of state complexity $n$ that meets the
bound for reversal and that at least three letters are needed to meet this
bound. Moreover, the restrictions of this witness to binary alphabets meet the
bounds for product, star, and boolean operations. We also derive tight upper
bounds on the state complexity of binary operations that take arguments with
different alphabets. We prove that the maximal syntactic semigroup of a
non-returning language has $(n-1)^n$ elements and requires at least
$\binom{n}{2}$ generators. We find the maximal state complexities of atoms of
non-returning languages. Finally, we show that there exists a most complex
non-returning language that meets the bounds for all these complexity measures.
",1,0,0,0,0,0
20254,Parameter estimation for fractional Ornstein-Uhlenbeck processes of general Hurst parameter,"  This paper provides several statistical estimators for the drift and
volatility parameters of an Ornstein-Uhlenbeck process driven by fractional
Brownian motion, whose observations can be made either continuously or at
discrete time instants. First and higher order power variations are used to
estimate the volatility parameter. The almost sure convergence of the
estimators and the corresponding central limit theorems are obtained for all
the Hurst parameter range $H\in (0, 1)$. The least squares estimator is used
for the drift parameter. A central limit theorem is proved when the Hurst
parameter $H \in (0, 1/2)$ and a noncentral limit theorem is proved for
$H\in[3/4, 1)$. Thus, the open problem left in the paper by Hu and Nualart
(2010) is completely solved, where a central limit theorem for least squares
estimator is proved for $H\in [1/2, 3/4)$.
",0,0,1,1,0,0
20737,Enabling Visual Design Verification Analytics - From Prototype Visualizations to an Analytics Tool using the Unity Game Engine,"  The ever-increasing architectural complexity in contemporary ASIC projects
turns Design Verification (DV) into a highly advanced endeavor. Pressing needs
for short time-to-market has made automation a key solution in DV. However,
recurring execution of large regression suites inevitably leads to challenging
amounts of test results. Following the design science paradigm, we present an
action research study to introduce visual analytics in a commercial ASIC
project. We develop a cityscape visualization tool using the game engine Unity.
Initial evaluations are promising, suggesting that the tool offers a novel
approach to identify error-prone parts of the design, as well as coverage
holes.
",1,0,0,0,0,0
12536,Changing users' security behaviour towards security questions: A game based learning approach,"  Fallback authentication is used to retrieve forgotten passwords. Security
questions are one of the main techniques used to conduct fallback
authentication. In this paper, we propose a serious game design that uses
system-generated security questions with the aim of improving the usability of
fallback authentication. For this purpose, we adopted the popular picture-based
""4 Pics 1 word"" mobile game. This game was selected because of its use of
pictures and cues, which previous psychology research found to be crucial to
aid memorability. This game asks users to pick the word that relates to the
given pictures. We then customized this game by adding features which help
maximize the following memory retrieval skills: (a) verbal cues - by providing
hints with verbal descriptions, (b) spatial cues - by maintaining the same
order of pictures, (c) graphical cues - by showing 4 images for each challenge,
(d) interactivity/engaging nature of the game.
",1,0,0,0,0,0
20830,Strong submeasures and several applications,"  A strong submeasure on a compact metric space X is a sub-linear and bounded
operator on the space of continuous functions on X. A strong submeasure is
positive if it is non-decreasing. By Hahn-Banach theorem, a positive strong
submeasure is the supremum of a non-empty collection of measures whose masses
are uniformly bounded from above.
We give several applications of strong submeasures in various diverse topics,
thus illustrate the usefulness of this classical but largely overlooked notion.
The applications include:
- Pullback and pushforward of all measures by meromorphic selfmaps of compact
complex varieties.
- The existence of invariant positive strong submeasures for meromorphic maps
between compact complex varieties, a notion of entropy for such submeasures
(which coincide with the classical ones in good cases) and a version of the
Variation Principle.
- Intersection of every positive closed (1,1) currents on compact Kähler
manifolds. Explicit calculations are given for self-intersection of the current
of integration of some curves $C$ in a compact Kähler surface where the
self-intersection in cohomology is negative.
All of these points are new and have not been previously given in work by
other authors. In addition, we will apply the same ideas to entropy of
transcendental maps of $\mathbb{C}$ and $\mathbb{C}^2$.
",0,0,1,0,0,0
20432,Uncertainty quantification for radio interferometric imaging: II. MAP estimation,"  Uncertainty quantification is a critical missing component in radio
interferometric imaging that will only become increasingly important as the
big-data era of radio interferometry emerges. Statistical sampling approaches
to perform Bayesian inference, like Markov Chain Monte Carlo (MCMC) sampling,
can in principle recover the full posterior distribution of the image, from
which uncertainties can then be quantified. However, for massive data sizes,
like those anticipated from the Square Kilometre Array (SKA), it will be
difficult if not impossible to apply any MCMC technique due to its inherent
computational cost. We formulate Bayesian inference problems with
sparsity-promoting priors (motivated by compressive sensing), for which we
recover maximum a posteriori (MAP) point estimators of radio interferometric
images by convex optimisation. Exploiting recent developments in the theory of
probability concentration, we quantify uncertainties by post-processing the
recovered MAP estimate. Three strategies to quantify uncertainties are
developed: (i) highest posterior density credible regions; (ii) local credible
intervals (cf. error bars) for individual pixels and superpixels; and (iii)
hypothesis testing of image structure. These forms of uncertainty
quantification provide rich information for analysing radio interferometric
observations in a statistically robust manner. Our MAP-based methods are
approximately $10^5$ times faster computationally than state-of-the-art MCMC
methods and, in addition, support highly distributed and parallelised
algorithmic structures. For the first time, our MAP-based techniques provide a
means of quantifying uncertainties for radio interferometric imaging for
realistic data volumes and practical use, and scale to the emerging big-data
era of radio astronomy.
",0,1,0,1,0,0
7156,An Application of Multi-band Forced Photometry to One Square Degree of SERVS: Accurate Photometric Redshifts and Implications for Future Science,"  We apply The Tractor image modeling code to improve upon existing multi-band
photometry for the Spitzer Extragalactic Representative Volume Survey (SERVS).
SERVS consists of post-cryogenic Spitzer observations at 3.6 and 4.5 micron
over five well-studied deep fields spanning 18 square degrees. In concert with
data from ground-based near-infrared (NIR) and optical surveys, SERVS aims to
provide a census of the properties of massive galaxies out to z ~ 5. To
accomplish this, we are using The Tractor to perform ""forced photometry."" This
technique employs prior measurements of source positions and surface brightness
profiles from a high-resolution fiducial band from the VISTA Deep Extragalactic
Observations (VIDEO) survey to model and fit the fluxes at lower-resolution
bands. We discuss our implementation of The Tractor over a square degree test
region within the XMM-LSS field with deep imaging in 12 NIR/optical bands. Our
new multi-band source catalogs offer a number of advantages over traditional
position-matched catalogs, including 1) consistent source cross-identification
between bands, 2) de-blending of sources that are clearly resolved in the
fiducial band but blended in the lower-resolution SERVS data, 3) a higher
source detection fraction in each band, 4) a larger number of candidate
galaxies in the redshift range 5 < z < 6, and 5) a statistically significant
improvement in the photometric redshift accuracy as evidenced by the
significant decrease in the fraction of outliers compared to spectroscopic
redshifts. Thus, forced photometry using The Tractor offers a means of
improving the accuracy of multi-band extragalactic surveys designed for galaxy
evolution studies. We will extend our application of this technique to the full
SERVS footprint in the future.
",0,1,0,0,0,0
10213,Spin liquid and infinitesimal-disorder-driven cluster spin glass in the kagome lattice,"  The interplay between geometric frustration (GF) and bond disorder is studied
in the Ising kagome lattice within a cluster approach. The model considers
antiferromagnetic (AF) short-range couplings and long-range intercluster
disordered interactions. The replica formalism is used to obtain an effective
single cluster model from where the thermodynamics is analyzed by exact
diagonalization. We found that the presence of GF can introduce cluster
freezing at very low levels of disorder. The system exhibits an entropy plateau
followed by a large entropy drop close to the freezing temperature. In this
scenario, a spin-liquid (SL) behavior prevents conventional long-range order,
but an infinitesimal disorder picks out uncompensated cluster states from the
multi degenerate SL regime, potentializing the intercluster disordered coupling
and bringing the cluster spin-glass state. To summarize, our results suggest
that the SL state combined with low levels of disorder can activate small
clusters, providing hypersensitivity to the freezing process in geometrically
frustrated materials and playing a key role in the glassy stabilization. We
propose that this physical mechanism could be present in several geometrically
frustrated materials. In particular, we discuss our results in connection to
the recent experimental investigations of the Ising kagome compound
Co$_3$Mg(OH)$_6$Cl$_2$.
",0,1,0,0,0,0
7913,Evolutionary sequences for hydrogen-deficient white dwarfs,"  We present a set of full evolutionary sequences for white dwarfs with
hydrogen-deficient atmospheres. We take into account the evolutionary history
of the progenitor stars, all the relevant energy sources involved in the
cooling, element diffusion in the very outer layers, and outer boundary
conditions provided by new and detailed non-gray white dwarf model atmospheres
for pure helium composition. These model atmospheres are based on the most
up-to-date physical inputs. Our calculations extend down to very low effective
temperatures, of $\sim 2\,500$~K, provide a homogeneous set of evolutionary
cooling tracks that are appropriate for mass and age determinations of old
hydrogen-deficient white dwarfs, and represent a clear improvement over
previous efforts, which were computed using gray atmospheres.
",0,1,0,0,0,0
17887,Learning Local Feature Aggregation Functions with Backpropagation,"  This paper introduces a family of local feature aggregation functions and a
novel method to estimate their parameters, such that they generate optimal
representations for classification (or any task that can be expressed as a cost
function minimization problem). To achieve that, we compose the local feature
aggregation function with the classifier cost function and we backpropagate the
gradient of this cost function in order to update the local feature aggregation
function parameters. Experiments on synthetic datasets indicate that our method
discovers parameters that model the class-relevant information in addition to
the local feature space. Further experiments on a variety of motion and visual
descriptors, both on image and video datasets, show that our method outperforms
other state-of-the-art local feature aggregation functions, such as Bag of
Words, Fisher Vectors and VLAD, by a large margin.
",1,0,0,1,0,0
3714,A Bootstrap Method for Error Estimation in Randomized Matrix Multiplication,"  In recent years, randomized methods for numerical linear algebra have
received growing interest as a general approach to large-scale problems.
Typically, the essential ingredient of these methods is some form of randomized
dimension reduction, which accelerates computations, but also creates random
approximation error. In this way, the dimension reduction step encodes a
tradeoff between cost and accuracy. However, the exact numerical relationship
between cost and accuracy is typically unknown, and consequently, it may be
difficult for the user to precisely know (1) how accurate a given solution is,
or (2) how much computation is needed to achieve a given level of accuracy. In
the current paper, we study randomized matrix multiplication (sketching) as a
prototype setting for addressing these general problems. As a solution, we
develop a bootstrap method for {directly estimating} the accuracy as a function
of the reduced dimension (as opposed to deriving worst-case bounds on the
accuracy in terms of the reduced dimension). From a computational standpoint,
the proposed method does not substantially increase the cost of standard
sketching methods, and this is made possible by an ""extrapolation"" technique.
In addition, we provide both theoretical and empirical results to demonstrate
the effectiveness of the proposed method.
",1,0,0,1,0,0
17849,POSEYDON - Converting the DAFNE Collider into a double Positron Facility: a High Duty-Cycle pulse stretcher and a storage ring,"  This project proposes to reuse the DAFNE accelerator complex for producing a
high intensity (up to 10^10), high-quality beam of high-energy (up to 500 MeV)
positrons for HEP experiments, mainly - but not only - motivated by light dark
particles searches. Such a facility would provide a unique source of
ultra-relativistic, narrow-band and low-emittance positrons, with a high duty
factor, without employing a cold technology, that would be an ideal facility
for exploring the existence of light dark matter particles, produced in
positron-on-target annihilations into a photon+missing mass, and using the
bump-hunt technique. The PADME experiment, that will use the extracted beam
from the DAFNE BTF, is indeed limited by the low duty-factor (10^-5=200 ns/20
ms). The idea is to use a variant of the third of integer resonant extraction,
with the aim of getting a <10^-6 m rad emittance and, at the same time,
tailoring the scheme to the peculiar optics of the DAFNE machine. In
alternative, the possibility of kicking the positrons by means of channelling
effects in crystals can be evaluated. This would not only increase the
extraction efficiency but also improve the beam quality, thanks to the high
collimation of channelled particles.
",0,1,0,0,0,0
3841,Deep Domain Adaptation Based Video Smoke Detection using Synthetic Smoke Images,"  In this paper, a deep domain adaptation based method for video smoke
detection is proposed to extract a powerful feature representation of smoke.
Due to the smoke image samples limited in scale and diversity for deep CNN
training, we systematically produced adequate synthetic smoke images with a
wide variation in the smoke shape, background and lighting conditions.
Considering that the appearance gap (dataset bias) between synthetic and real
smoke images degrades significantly the performance of the trained model on the
test set composed fully of real images, we build deep architectures based on
domain adaptation to confuse the distributions of features extracted from
synthetic and real smoke images. This approach expands the domain-invariant
feature space for smoke image samples. With their approximate feature
distribution off non-smoke images, the recognition rate of the trained model is
improved significantly compared to the model trained directly on mixed dataset
of synthetic and real images. Experimentally, several deep architectures with
different design choices are applied to the smoke detector. The ultimate
framework can get a satisfactory result on the test set. We believe that our
approach is a start in the direction of utilizing deep neural networks enhanced
with synthetic smoke images for video smoke detection.
",1,0,0,0,0,0
18039,"Fast, Robust, and Versatile Event Detection through HMM Belief State Gradient Measures","  Event detection is a critical feature in data-driven systems as it assists
with the identification of nominal and anomalous behavior. Event detection is
increasingly relevant in robotics as robots operate with greater autonomy in
increasingly unstructured environments. In this work, we present an accurate,
robust, fast, and versatile measure for skill and anomaly identification. A
theoretical proof establishes the link between the derivative of the
log-likelihood of the HMM filtered belief state and the latest emission
probabilities. The key insight is the inverse relationship in which gradient
analysis is used for skill and anomaly identification. Our measure showed
better performance across all metrics than related state-of-the art works. The
result is broadly applicable to domains that use HMMs for event detection.
",1,0,0,0,0,0
7824,Discrete flow posteriors for variational inference in discrete dynamical systems,"  Each training step for a variational autoencoder (VAE) requires us to sample
from the approximate posterior, so we usually choose simple (e.g. factorised)
approximate posteriors in which sampling is an efficient computation that fully
exploits GPU parallelism. However, such simple approximate posteriors are often
insufficient, as they eliminate statistical dependencies in the posterior.
While it is possible to use normalizing flow approximate posteriors for
continuous latents, some problems have discrete latents and strong statistical
dependencies. The most natural approach to model these dependencies is an
autoregressive distribution, but sampling from such distributions is inherently
sequential and thus slow. We develop a fast, parallel sampling procedure for
autoregressive distributions based on fixed-point iterations which enables
efficient and accurate variational inference in discrete state-space latent
variable dynamical systems. To optimize the variational bound, we considered
two ways to evaluate probabilities: inserting the relaxed samples directly into
the pmf for the discrete distribution, or converting to continuous logistic
latent variables and interpreting the K-step fixed-point iterations as a
normalizing flow. We found that converting to continuous latent variables gave
considerable additional scope for mismatch between the true and approximate
posteriors, which resulted in biased inferences, we thus used the former
approach. Using our fast sampling procedure, we were able to realize the
benefits of correlated posteriors, including accurate uncertainty estimates for
one cell, and accurate connectivity estimates for multiple cells, in an order
of magnitude less time.
",0,0,0,1,1,0
9796,Urban Analytics: Multiplexed and Dynamic Community Networks,"  In the past decade, cities have experienced rapid growth, expansion, and
changes in their community structure. Many aspects of critical urban
infrastructure are closely coupled with the human communities that they serve.
Urban communities are composed of a multiplex of overlapping factors which can
be distinguished into cultural, religious, social-economic, political, and
geographical layers. In this paper, we review how increasingly available
heterogeneous mobile big data sets can be leveraged to detect the community
interaction structure using natural language processing and machine learning
techniques. A number of community layer and interaction detection algorithms
are then reviewed, with a particular focus on robustness, stability, and
causality of evolving communities. The better understanding of the structural
dynamics and multiplexed relationships can provide useful information to inform
both urban planning policies and shape the design of socially coupled urban
infrastructure systems.
",1,1,0,0,0,0
1703,Controlling Sources of Inaccuracy in Stochastic Kriging,"  Scientists and engineers commonly use simulation models to study real systems
for which actual experimentation is costly, difficult, or impossible. Many
simulations are stochastic in the sense that repeated runs with the same input
configuration will result in different outputs. For expensive or time-consuming
simulations, stochastic kriging \citep{ankenman} is commonly used to generate
predictions for simulation model outputs subject to uncertainty due to both
function approximation and stochastic variation. Here, we develop and justify a
few guidelines for experimental design, which ensure accuracy of stochastic
kriging emulators. We decompose error in stochastic kriging predictions into
nominal, numeric, parameter estimation and parameter estimation numeric
components and provide means to control each in terms of properties of the
underlying experimental design. The design properties implied for each source
of error are weakly conflicting and broad principles are proposed. In brief,
space-filling properties ""small fill distance"" and ""large separation distance""
should balance with replication at distinct input configurations, with number
of replications depending on the relative magnitudes of stochastic and process
variability. Non-stationarity implies higher input density in more active
regions, while regression functions imply a balance with traditional design
properties. A few examples are presented to illustrate the results.
",0,0,1,1,0,0
16621,Crime Prediction by Data-Driven Green's Function method,"  We develop an algorithm that forecasts cascading events, by employing a
Green's function scheme on the basis of the self-exciting point process model.
This method is applied to open data of 10 types of crimes happened in Chicago.
It shows a good prediction accuracy superior to or comparable to the standard
methods which are the expectation-maximization method and prospective hotspot
maps method. We find a cascade influence of the crimes that has a long-time,
logarithmic tail; this result is consistent with an earlier study on
burglaries. This long-tail feature cannot be reproduced by the other standard
methods. In addition, a merit of the Green's function method is the low
computational cost in the case of high density of events and/or large amount of
the training data.
",1,1,0,1,0,0
11580,Introduction of Improved Repairing Locality into Secret Sharing Schemes with Perfect Security,"  Repairing locality is an appreciated feature for distributed storage, in
which a damaged or lost data share can be repaired by accessing a subset of
other shares much smaller than is required for decoding the complete data.
However for Secret Sharing (SS) schemes, it has been proven theoretically that
local repairing can not be achieved with perfect security for the majority of
threshold SS schemes, where all the shares are equally regarded in both secret
recovering and share repairing. In this paper we make an attempt on decoupling
the two processes to make secure local repairing possible. Dedicated repairing
redundancies only for the repairing process are generated, which are random
numbers to the original secret. Through this manner a threshold SS scheme with
improved repairing locality is achieved on the condition that security of
repairing redundancies is ensured, or else our scheme degenerates into a
perfect access structure that is equivalent to the best existing schemes can
do. To maximize security of the repairing redundancies, a random placement
mechanism is also proposed.
",1,0,0,0,0,0
20671,Parallelizing Over Artificial Neural Network Training Runs with Multigrid,"  Artificial neural networks are a popular and effective machine learning
technique. Great progress has been made parallelizing the expensive training
phase of an individual network, leading to highly specialized pieces of
hardware, many based on GPU-type architectures, and more concurrent algorithms
such as synthetic gradients. However, the training phase continues to be a
bottleneck, where the training data must be processed serially over thousands
of individual training runs. This work considers a multigrid reduction in time
(MGRIT) algorithm that is able to parallelize over the thousands of training
runs and converge to the exact same solution as traditional training would
provide. MGRIT was originally developed to provide parallelism for time
evolution problems that serially step through a finite number of time-steps.
This work recasts the training of a neural network similarly, treating neural
network training as an evolution equation that evolves the network weights from
one step to the next. Thus, this work concerns distributed computing approaches
for neural networks, but is distinct from other approaches which seek to
parallelize only over individual training runs. The work concludes with
supporting numerical results for two model problems.
",1,0,0,0,0,0
19873,Two-domain and three-domain limit cycles in a typical aeroelastic system with freeplay in pitch,"  Freeplay is a significant source of nonlinearity in aeroelastic systems and
is strictly regulated by airworthiness authorities. It splits the phase plane
of such systems into three piecewise linear subdomains. Depending on the
location of the freeplay, limit cycle oscillations can result that span either
two or three of these subdomains. The purpose of this work is to demonstrate
the existence of two-domain cycles both theoretically and experimentally. A
simple aeroelastic system with pitch, plunge and control deflection degrees of
freedom is investigated in the presence of freeplay in pitch. It is shown that
two-domain and three-domain cycles can result from a grazing bifurcation and
propagate in the decreasing airspeed direction. Close to the bifurcation, the
two limit cycle branches interact with each other and aperiodic oscillations
ensue. Equivalent linearization is used to derive the conditions of existence
of each type of limit cycle and to predict their amplitudes and frequencies.
Comparisons with measurements from wind tunnel experiments demonstrate that the
theory describes these phenomena with accuracy.
",0,1,0,0,0,0
402,Efficient Online Bandit Multiclass Learning with $\tilde{O}(\sqrt{T})$ Regret,"  We present an efficient second-order algorithm with
$\tilde{O}(\frac{1}{\eta}\sqrt{T})$ regret for the bandit online multiclass
problem. The regret bound holds simultaneously with respect to a family of loss
functions parameterized by $\eta$, for a range of $\eta$ restricted by the norm
of the competitor. The family of loss functions ranges from hinge loss
($\eta=0$) to squared hinge loss ($\eta=1$). This provides a solution to the
open problem of (J. Abernethy and A. Rakhlin. An efficient bandit algorithm for
$\sqrt{T}$-regret in online multiclass prediction? In COLT, 2009). We test our
algorithm experimentally, showing that it also performs favorably against
earlier algorithms.
",0,0,0,1,0,0
10534,A contour for the entanglement entropies in harmonic lattices,"  We construct a contour function for the entanglement entropies in generic
harmonic lattices. In one spatial dimension, numerical analysis are performed
by considering harmonic chains with either periodic or Dirichlet boundary
conditions. In the massless regime and for some configurations where the
subsystem is a single interval, the numerical results for the contour function
are compared to the inverse of the local weight function which multiplies the
energy-momentum tensor in the corresponding entanglement hamiltonian, found
through conformal field theory methods, and a good agreement is observed. A
numerical analysis of the contour function for the entanglement entropy is
performed also in a massless harmonic chain for a subsystem made by two
disjoint intervals.
",0,1,0,0,0,0
17802,Image-domain multi-material decomposition for dual-energy CT based on correlation and sparsity of material images,"  Dual energy CT (DECT) enhances tissue characterization because it can produce
images of basis materials such as soft-tissue and bone. DECT is of great
interest in applications to medical imaging, security inspection and
nondestructive testing. Theoretically, two materials with different linear
attenuation coefficients can be accurately reconstructed using DECT technique.
However, the ability to reconstruct three or more basis materials is clinically
and industrially important. Under the assumption that there are at most three
materials in each pixel, there are a few methods that estimate multiple
material images from DECT measurements by enforcing sum-to-one and a box
constraint ([0 1]) derived from both the volume and mass conservation
assumption. The recently proposed image-domain multi-material decomposition
(MMD) method introduces edge-preserving regularization for each material image
which neglects the relations among material images, and enforced the assumption
that there are at most three materials in each pixel using a time-consuming
loop over all possible material-triplet in each iteration of optimizing its
cost function. We propose a new image-domain MMD method for DECT that considers
the prior information that different material images have common edges and
encourages sparsity of material composition in each pixel using regularization.
",0,1,0,0,0,0
6355,Direct observation of domain wall surface tension by deflating or inflating a magnetic bubble,"  The surface energy of a magnetic Domain Wall (DW) strongly affects its static
and dynamic behaviours. However, this effect was seldom directly observed and
many related phenomena have not been well understood. Moreover, a reliable
method to quantify the DW surface energy is still missing. Here, we report a
series of experiments in which the DW surface energy becomes a dominant
parameter. We observed that a semicircular magnetic domain bubble could
spontaneously collapse under the Laplace pressure induced by DW surface energy.
We further demonstrated that the surface energy could lead to a geometrically
induced pinning when the DW propagates in a Hall cross or from a nanowire into
a nucleation pad. Based on these observations, we developed two methods to
quantify the DW surface energy, which could be very helpful to estimate
intrinsic parameters such as Dzyaloshinskii-Moriya Interactions (DMI) or
exchange stiffness in magnetic ultra-thin films.
",0,1,0,0,0,0
8709,DeepMoTIon: Learning to Navigate Like Humans,"  We present a novel human-aware navigation approach, where the robot learns to
mimic humans to navigate safely in crowds. The presented model referred to as
DeepMoTIon, is trained with pedestrian surveillance data to predict human
velocity. The robot processes LiDAR scans via the trained network to navigate
to the target location. We conduct extensive experiments to assess the
different components of our network and prove the necessity of each to imitate
humans. Our experiments show that DeepMoTIon outperforms state-of-the-art in
terms of human imitation and reaches the target on 100% of the test cases
without breaching humans' safe distance.
",1,0,0,1,0,0
11287,Using polarimetry to retrieve the cloud coverage of Earth-like exoplanets,"  Context. Clouds have already been detected in exoplanetary atmospheres. They
play crucial roles in a planet's atmosphere and climate and can also create
ambiguities in the determination of atmospheric parameters such as trace gas
mixing ratios. Knowledge of cloud properties is required when assessing the
habitability of a planet. Aims. We aim to show that various types of cloud
cover such as polar cusps, subsolar clouds, and patchy clouds on Earth-like
exoplanets can be distinguished from each other using the polarization and flux
of light that is reflected by the planet. Methods. We have computed the flux
and polarization of reflected starlight for different types of (liquid water)
cloud covers on Earth-like model planets using the adding-doubling method, that
fully includes multiple scattering and polarization. Variations in cloud-top
altitudes and planet-wide cloud cover percentages were taken into account.
Results. We find that the different types of cloud cover (polar cusps, subsolar
clouds, and patchy clouds) can be distinguished from each other and that the
percentage of cloud cover can be estimated within 10%. Conclusions. Using our
proposed observational strategy, one should be able to determine basic orbital
parameters of a planet such as orbital inclination and estimate cloud coverage
with reduced ambiguities from the planet's polarization signals along its
orbit.
",0,1,0,0,0,0
16642,A Model that Predicts the Material Recognition Performance of Thermal Tactile Sensing,"  Tactile sensing can enable a robot to infer properties of its surroundings,
such as the material of an object. Heat transfer based sensing can be used for
material recognition due to differences in the thermal properties of materials.
While data-driven methods have shown promise for this recognition problem, many
factors can influence performance, including sensor noise, the initial
temperatures of the sensor and the object, the thermal effusivities of the
materials, and the duration of contact. We present a physics-based mathematical
model that predicts material recognition performance given these factors. Our
model uses semi-infinite solids and a statistical method to calculate an F1
score for the binary material recognition. We evaluated our method using
simulated contact with 69 materials and data collected by a real robot with 12
materials. Our model predicted the material recognition performance of support
vector machine (SVM) with 96% accuracy for the simulated data, with 92%
accuracy for real-world data with constant initial sensor temperatures, and
with 91% accuracy for real-world data with varied initial sensor temperatures.
Using our model, we also provide insight into the roles of various factors on
recognition performance, such as the temperature difference between the sensor
and the object. Overall, our results suggest that our model could be used to
help design better thermal sensors for robots and enable robots to use them
more effectively.
",1,0,0,0,0,0
4858,Analysing Magnetism Using Scanning SQUID Microscopy,"  Scanning superconducting quantum interference device microscopy (SSM) is a
scanning probe technique that images local magnetic flux, which allows for
mapping of magnetic fields with high field and spatial accuracy. Many studies
involving SSM have been published in the last decades, using SSM to make
qualitative statements about magnetism. However, quantitative analysis using
SSM has received less attention. In this work, we discuss several aspects of
interpreting SSM images and methods to improve quantitative analysis. First, we
analyse the spatial resolution and how it depends on several factors. Second,
we discuss the analysis of SSM scans and the information obtained from the SSM
data. Using simulations, we show how signals evolve as a function of changing
scan height, SQUID loop size, magnetization strength and orientation. We also
investigated 2-dimensional autocorrelation analysis to extract information
about the size, shape and symmetry of magnetic features. Finally, we provide an
outlook on possible future applications and improvements.
",0,1,0,0,0,0
4122,SGDLibrary: A MATLAB library for stochastic gradient descent algorithms,"  We consider the problem of finding the minimizer of a function $f:
\mathbb{R}^d \rightarrow \mathbb{R}$ of the finite-sum form $\min f(w) =
1/n\sum_{i}^n f_i(w)$. This problem has been studied intensively in recent
years in the field of machine learning (ML). One promising approach for
large-scale data is to use a stochastic optimization algorithm to solve the
problem. SGDLibrary is a readable, flexible and extensible pure-MATLAB library
of a collection of stochastic optimization algorithms. The purpose of the
library is to provide researchers and implementers a comprehensive evaluation
environment for the use of these algorithms on various ML problems.
",1,0,0,1,0,0
13270,Mechanics of disordered auxetic metamaterials,"  Auxetic materials are of great engineering interest not only because of their
fascinating negative Poisson's ratio, but also due to their increased toughness
and indentation resistance. These materials are typically synthesized polyester
foams with a very heterogeneous structure, but the role of disorder in auxetic
behavior is not fully understood. Here, we provide a systematic theoretical and
experimental investigation in to the effect of disorder on the mechanical
properties of a paradigmatic auxetic lattice with a re-entrant hexagonal
geometry. We show that disorder has a marginal effect on the Poisson's ratio
unless the lattice topology is altered, and in all cases examined the disorder
preserves the auxetic characteristics. Depending on the direction of loading
applied to these disordered auxetic lattices, either brittle or ductile failure
is observed. It is found that brittle failure is associated with a
disorder-dependent tensile strength, whereas in ductile failure disorder does
not affect strength. Our work thus provides general guidelines to optimize
elasticity and strength of disordered auxetic metamaterials.
",0,1,0,0,0,0
12196,Navigating through the R packages for movement,"  The advent of miniaturized biologging devices has provided ecologists with
unparalleled opportunities to record animal movement across scales, and led to
the collection of ever-increasing quantities of tracking data. In parallel,
sophisticated tools to process, visualize and analyze tracking data have been
developed in abundance. Within the R software alone, we listed 57 focused on
these tasks, called here tracking packages. Here, we reviewed these tracking
packages, as an introduction to this set of packages for researchers, and to
provide feedback and recommendations to package developers, from a user
perspective. We described each package based on a workflow centered around
tracking data (i.e. (x,y,t)), broken down in three stages: pre-processing,
post-processing, and analysis (data visualization, track description, path
reconstruction, behavioral pattern identification, space use characterization,
trajectory simulation and others).
Supporting documentation is key to the accessibility of a package for users.
Based on a user survey, we reviewed the quality of packages' documentation, and
identified $12$ packages with good or excellent documentation. Links between
packages were assessed through a network graph analysis. Although a large group
of packages shows some degree of connectivity (either depending on functions or
suggesting the use of another tracking package), a third of tracking packages
work on isolation, reflecting a fragmentation in the R Movement-Ecology
programming community.
Finally, we provide recommendations for users to choose packages, and for
developers to maximize usefulness of their contribution and strengthen the
links between the programming community.
",0,0,0,0,1,0
10773,Analog Experiments on Tensile Strength of Dusty and Cometary Matter,"  The tensile strength of small dusty bodies in the solar system is determined
by the interaction between the composing grains. In the transition regime
between small and sticky dust ($\rm \mu m$) and non cohesive large grains (mm),
particles still stick to each other but are easily separated. In laboratory
experiments we find that thermal creep gas flow at low ambient pressure
generates an overpressure sufficient to overcome the tensile strength. For the
first time it allows a direct measurement of the tensile strength of
individual, very small (sub)-mm aggregates which consist of only tens of grains
in the (sub)-mm size range. We traced the disintegration of aggregates by
optical imaging in ground based as well as microgravity experiments and present
first results for basalt, palagonite and vitreous carbon samples with up to a
few hundred Pa. These measurements show that low tensile strength can be the
result of building loose aggregates with compact (sub)-mm units. This is in
favour of a combined cometary formation scenario by aggregation to compact
aggreates and gravitational instability of these units.
",0,1,0,0,0,0
9073,Adaptive Sampling Strategies for Stochastic Optimization,"  In this paper, we propose a stochastic optimization method that adaptively
controls the sample size used in the computation of gradient approximations.
Unlike other variance reduction techniques that either require additional
storage or the regular computation of full gradients, the proposed method
reduces variance by increasing the sample size as needed. The decision to
increase the sample size is governed by an inner product test that ensures that
search directions are descent directions with high probability. We show that
the inner product test improves upon the well known norm test, and can be used
as a basis for an algorithm that is globally convergent on nonconvex functions
and enjoys a global linear rate of convergence on strongly convex functions.
Numerical experiments on logistic regression problems illustrate the
performance of the algorithm.
",0,0,0,1,0,0
2760,Equations of state for real gases on the nuclear scale,"  The formalism to augment the classical models of equation of state for real
gases with the quantum statistical effects is presented. It allows an arbitrary
excluded volume procedure to model repulsive interactions, and an arbitrary
density-dependent mean field to model attractive interactions. Variations on
the excluded volume mechanism include van der Waals (VDW) and Carnahan-Starling
models, while the mean fields are based on VDW, Redlich-Kwong-Soave,
Peng-Robinson, and Clausius equations of state. The VDW parameters of the
nucleon-nucleon interaction are fitted in each model to the properties of the
ground state of nuclear matter, and the following range of values is obtained:
$a = 330 - 430$ MeV fm$^3$ and $b = 2.5 - 4.4$ fm$^3$. In the context of the
excluded-volume approach, the fits to the nuclear ground state disfavor the
values of the effective hard-core radius of a nucleon significantly smaller
than $0.5$ fm, at least for the nuclear matter region of the phase diagram.
Modifications to the standard VDW repulsion and attraction terms allow to
improve significantly the value of the nuclear incompressibility factor $K_0$,
bringing it closer to empirical estimates. The generalization to include the
baryon-baryon interactions into the hadron resonance gas model is performed.
The behavior of the baryon-related lattice QCD observables at zero chemical
potential is shown to be strongly correlated to the nuclear matter properties:
an improved description of the nuclear incompressibility also yields an
improved description of the lattice data at $\mu = 0$.
",0,1,0,0,0,0
19301,Diversified essential properties in halogenated graphenes,"  The significant halogenation effects on the essential properties of graphene
are investigated by the first-principles method. The geometric structures,
electronic properties, and magnetic configurations are greatly diversified
under the various halogen adsorptions. Fluorination, with the strong
multi-orbital chemical bondings, can create the buckled graphene structure,
while the other halogenations do not change the planar {\sigma} bonding in the
presence of single-orbital hybridization. Electronic structures consist of the
carbon-, adatom- and (carbon, adatom)-dominated energy bands. All halogenated
graphenes belong to hole-doped metals except that fluorinated systems are
middle-gap semiconductors at sufficiently high concentration. Moreover, the
metallic ferromagnetism is revealed in certain adatom distributions. The
unusual hybridization-induced features are clearly evidenced in many van Hove
singularities of the density of states. The structure- and adatom-enriched
essential properties are compared with the measured results, and potential
applications are also discussed.
",0,1,0,0,0,0
822,Gorenstein homological properties of tensor rings,"  Let $R$ be a two-sided noetherian ring and $M$ be a nilpotent $R$-bimodule,
which is finitely generated on both sides. We study Gorenstein homological
properties of the tensor ring $T_R(M)$. Under certain conditions, the ring $R$
is Gorenstein if and only if so is $T_R(M)$. We characterize Gorenstein
projective $T_R(M)$-modules in terms of $R$-modules.
",0,0,1,0,0,0
11782,Total energy of radial mappings,"  The main aim of this paper is to extend one of the main results of Iwaniec
and Onninen (Arch. Ration. Mech. Anal., 194: 927-986, 2009). We prove that, the
so called total energy functional defined on the class of radial streachings
between annuli attains its minimum on a total energy diffeomorphism between
annuli. This involves a subtle analysis of some special ODE.
",0,0,1,0,0,0
4474,Solvability of curves on surfaces,"  In this article, we study subloci of solvable curves in $\mathcal{M}_g$ which
are contained in either a K3-surface or a quadric or a cubic surface. We give a
bound on the dimension of such subloci. In the case of complete intersection
genus $g$ curves in a cubic surface, we show that a general such curve is
solvable.
",0,0,1,0,0,0
4587,Lose The Views: Limited Angle CT Reconstruction via Implicit Sinogram Completion,"  Computed Tomography (CT) reconstruction is a fundamental component to a wide
variety of applications ranging from security, to healthcare. The classical
techniques require measuring projections, called sinograms, from a full
180$^\circ$ view of the object. This is impractical in a limited angle
scenario, when the viewing angle is less than 180$^\circ$, which can occur due
to different factors including restrictions on scanning time, limited
flexibility of scanner rotation, etc. The sinograms obtained as a result, cause
existing techniques to produce highly artifact-laden reconstructions. In this
paper, we propose to address this problem through implicit sinogram completion,
on a challenging real world dataset containing scans of common checked-in
luggage. We propose a system, consisting of 1D and 2D convolutional neural
networks, that operates on a limited angle sinogram to directly produce the
best estimate of a reconstruction. Next, we use the x-ray transform on this
reconstruction to obtain a ""completed"" sinogram, as if it came from a full
180$^\circ$ measurement. We feed this to standard analytical and iterative
reconstruction techniques to obtain the final reconstruction. We show with
extensive experimentation that this combined strategy outperforms many
competitive baselines. We also propose a measure of confidence for the
reconstruction that enables a practitioner to gauge the reliability of a
prediction made by our network. We show that this measure is a strong indicator
of quality as measured by the PSNR, while not requiring ground truth at test
time. Finally, using a segmentation experiment, we show that our reconstruction
preserves the 3D structure of objects effectively.
",0,0,0,1,0,0
18832,Higher order mobile coverage control with application to localization,"  Most current results on coverage control using mobile sensors require that
one partitioned cell is associated with precisely one sensor. In this paper, we
consider a class of coverage control problems involving higher order Voronoi
partitions, motivated by applications where more than one sensor is required to
monitor and cover one cell. Such applications are frequent in scenarios
requiring the sensors to localize targets. We introduce a framework depending
on a coverage performance function incorporating higher order Voronoi cells and
then design a gradient-based controller which allows the multi-sensor system to
achieve a local equilibrium in a distributed manner. The convergence properties
are studied and related to Lloyd algorithm. We study also the extension to
coverage of a discrete set of points. In addition, we provide a number of real
world scenarios where our framework can be applied. Simulation results are also
provided to show the controller performance.
",1,0,0,0,0,0
5216,Dimensional reduction and the equivariant Chern character,"  We propose a dimensional reduction procedure in the Stolz--Teichner framework
of supersymmetric Euclidean field theories (EFTs) that is well-suited in the
presence of a finite gauge group or, more generally, for field theories over an
orbifold. As an illustration, we give a geometric interpretation of the Chern
character for manifolds with an action by a finite group.
",0,0,1,0,0,0
4702,802.11 Wireless Simulation and Anomaly Detection using HMM and UBM,"  Despite the growing popularity of 802.11 wireless networks, users often
suffer from connectivity problems and performance issues due to unstable radio
conditions and dynamic user behavior among other reasons. Anomaly detection and
distinction are in the thick of major challenges that network managers
encounter. Complication of monitoring the broaden and complex WLANs, that often
requires heavy instrumentation of the user devices, makes the anomaly detection
analysis even harder. In this paper we exploit 802.11 access point usage data
and propose an anomaly detection technique based on Hidden Markov Model (HMM)
and Universal Background Model (UBM) on data that is inexpensive to obtain. We
then generate a number of network anomalous scenarios in OMNeT++/INET network
simulator and compare the detection outcomes with those in baseline approaches
(RawData and PCA). The experimental results show the superiority of HMM and
HMM-UBM models in detection precision and sensitivity.
",1,0,0,0,0,0
18246,Inhomogeneous hard-core bosonic mixture with checkerboard supersolid phase: Quantum and thermal phase diagram,"  We introduce an inhomogeneous bosonic mixture composed of two kinds of
hard-core and semi-hard-core bosons with different nilpotency conditions and
demonstrate that in contrast with the standard hard-core Bose-Hubbard model,
our bosonic mixture with nearest- and next-nearest-neighbor interactions on a
square lattice develops the checkerboard supersolid phase characterized by the
simultaneous superfluid and checkerboard solid orders. Our bosonic mixture is
created from a two-orbital Bose-Hubbard model including two kinds of bosons: a
single-orbital boson and a two-orbital boson. By mapping the bosonic mixture to
an anisotropic inhomogeneous spin model in the presence of a magnetic field, we
study the ground-state phase diagram of the model by means of cluster mean
field theory and linear spin-wave theory and show that various phases such as
solid, superfluid, supersolid, and Mott insulator appear in the phase diagram
of the mixture. Competition between the interactions and magnetic field causes
the mixture to undergo different kinds of first- and second-order phase
transitions. By studying the behavior of the spin-wave excitations, we find the
reasons of all first- and second-order phase transitions. We also obtain the
temperature phase diagram of the system using cluster mean field theory. We
show that the checkerboard supersolid phase persists at finite temperature
comparable with the interaction energies of bosons.
",0,1,0,0,0,0
14821,Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration,"  Computing optimal transport distances such as the earth mover's distance is a
fundamental problem in machine learning, statistics, and computer vision.
Despite the recent introduction of several algorithms with good empirical
performance, it is unknown whether general optimal transport distances can be
approximated in near-linear time. This paper demonstrates that this ambitious
goal is in fact achieved by Cuturi's Sinkhorn Distances. This result relies on
a new analysis of Sinkhorn iteration, which also directly suggests a new greedy
coordinate descent algorithm, Greenkhorn, with the same theoretical guarantees.
Numerical simulations illustrate that Greenkhorn significantly outperforms the
classical Sinkhorn algorithm in practice.
",1,0,0,1,0,0
14280,MIMIC-CXR: A large publicly available database of labeled chest radiographs,"  Chest radiography is an extremely powerful imaging modality, allowing for a
detailed inspection of a patient's thorax, but requiring specialized training
for proper interpretation. With the advent of high performance general purpose
computer vision algorithms, the accurate automated analysis of chest
radiographs is becoming increasingly of interest to researchers. However, a key
challenge in the development of these techniques is the lack of sufficient
data. Here we describe MIMIC-CXR, a large dataset of 371,920 chest x-rays
associated with 227,943 imaging studies sourced from the Beth Israel Deaconess
Medical Center between 2011 - 2016. Each imaging study can pertain to one or
more images, but most often are associated with two images: a frontal view and
a lateral view. Images are provided with 14 labels derived from a natural
language processing tool applied to the corresponding free-text radiology
reports. All images have been de-identified to protect patient privacy. The
dataset is made freely available to facilitate and encourage a wide range of
research in medical computer vision.
",1,0,0,0,0,0
10670,Computing Stable Models of Normal Logic Programs Without Grounding,"  We present a method for computing stable models of normal logic programs,
i.e., logic programs extended with negation, in the presence of predicates with
arbitrary terms. Such programs need not have a finite grounding, so traditional
methods do not apply. Our method relies on the use of a non-Herbrand universe,
as well as coinduction, constructive negation and a number of other novel
techniques. Using our method, a normal logic program with predicates can be
executed directly under the stable model semantics without requiring it to be
grounded either before or during execution and without requiring that its
variables range over a finite domain. As a result, our method is quite general
and supports the use of terms as arguments, including lists and complex data
structures. A prototype implementation and non-trivial applications have been
developed to demonstrate the feasibility of our method.
",1,0,0,0,0,0
7743,A Hilbert Space of Stationary Ergodic Processes,"  Identifying meaningful signal buried in noise is a problem of interest
arising in diverse scenarios of data-driven modeling. We present here a
theoretical framework for exploiting intrinsic geometry in data that resists
noise corruption, and might be identifiable under severe obfuscation. Our
approach is based on uncovering a valid complete inner product on the space of
ergodic stationary finite valued processes, providing the latter with the
structure of a Hilbert space on the real field. This rigorous construction,
based on non-standard generalizations of the notions of sum and scalar
multiplication of finite dimensional probability vectors, allows us to
meaningfully talk about ""angles"" between data streams and data sources, and,
make precise the notion of orthogonal stochastic processes. In particular, the
relative angles appear to be preserved, and identifiable, under severe noise,
and will be developed in future as the underlying principle for robust
classification, clustering and unsupervised featurization algorithms.
",0,0,0,1,0,1
16894,Computation of ground-state properties in molecular systems: back-propagation with auxiliary-field quantum Monte Carlo,"  We address the computation of ground-state properties of chemical systems and
realistic materials within the auxiliary-field quantum Monte Carlo method. The
phase constraint to control the fermion phase problem requires the random walks
in Slater determinant space to be open-ended with branching. This in turn makes
it necessary to use back-propagation (BP) to compute averages and correlation
functions of operators that do not commute with the Hamiltonian. Several BP
schemes are investigated and their optimization with respect to the phaseless
constraint is considered. We propose a modified BP method for the computation
of observables in electronic systems, discuss its numerical stability and
computational complexity, and assess its performance by computing ground-state
properties for several substances, including constituents of the primordial
terrestrial atmosphere and small organic molecules.
",0,1,0,0,0,0
7201,Spectral Method and Regularized MLE Are Both Optimal for Top-$K$ Ranking,"  This paper is concerned with the problem of top-$K$ ranking from pairwise
comparisons. Given a collection of $n$ items and a few pairwise comparisons
across them, one wishes to identify the set of $K$ items that receive the
highest ranks. To tackle this problem, we adopt the logistic parametric model
--- the Bradley-Terry-Luce model, where each item is assigned a latent
preference score, and where the outcome of each pairwise comparison depends
solely on the relative scores of the two items involved. Recent works have made
significant progress towards characterizing the performance (e.g. the mean
square error for estimating the scores) of several classical methods, including
the spectral method and the maximum likelihood estimator (MLE). However, where
they stand regarding top-$K$ ranking remains unsettled.
We demonstrate that under a natural random sampling model, the spectral
method alone, or the regularized MLE alone, is minimax optimal in terms of the
sample complexity --- the number of paired comparisons needed to ensure exact
top-$K$ identification, for the fixed dynamic range regime. This is
accomplished via optimal control of the entrywise error of the score estimates.
We complement our theoretical studies by numerical experiments, confirming that
both methods yield low entrywise errors for estimating the underlying scores.
Our theory is established via a novel leave-one-out trick, which proves
effective for analyzing both iterative and non-iterative procedures. Along the
way, we derive an elementary eigenvector perturbation bound for probability
transition matrices, which parallels the Davis-Kahan $\sin\Theta$ theorem for
symmetric matrices. This also allows us to close the gap between the $\ell_2$
error upper bound for the spectral method and the minimax lower limit.
",1,0,1,1,0,0
5489,Toward Incorporation of Relevant Documents in word2vec,"  Recent advances in neural word embedding provide significant benefit to
various information retrieval tasks. However as shown by recent studies,
adapting the embedding models for the needs of IR tasks can bring considerable
further improvements. The embedding models in general define the term
relatedness by exploiting the terms' co-occurrences in short-window contexts.
An alternative (and well-studied) approach in IR for related terms to a query
is using local information i.e. a set of top-retrieved documents. In view of
these two methods of term relatedness, in this work, we report our study on
incorporating the local information of the query in the word embeddings. One
main challenge in this direction is that the dense vectors of word embeddings
and their estimation of term-to-term relatedness remain difficult to interpret
and hard to analyze. As an alternative, explicit word representations propose
vectors whose dimensions are easily interpretable, and recent methods show
competitive performance to the dense vectors. We introduce a neural-based
explicit representation, rooted in the conceptual ideas of the word2vec
Skip-Gram model. The method provides interpretable explicit vectors while
keeping the effectiveness of the Skip-Gram model. The evaluation of various
explicit representations on word association collections shows that the newly
proposed method out- performs the state-of-the-art explicit representations
when tasked with ranking highly similar terms. Based on the introduced ex-
plicit representation, we discuss our approaches on integrating local documents
in globally-trained embedding models and discuss the preliminary results.
",1,0,0,0,0,0
19385,Robust Online Multi-Task Learning with Correlative and Personalized Structures,"  Multi-Task Learning (MTL) can enhance a classifier's generalization
performance by learning multiple related tasks simultaneously. Conventional MTL
works under the offline or batch setting, and suffers from expensive training
cost and poor scalability. To address such inefficiency issues, online learning
techniques have been applied to solve MTL problems. However, most existing
algorithms of online MTL constrain task relatedness into a presumed structure
via a single weight matrix, which is a strict restriction that does not always
hold in practice. In this paper, we propose a robust online MTL framework that
overcomes this restriction by decomposing the weight matrix into two
components: the first one captures the low-rank common structure among tasks
via a nuclear norm and the second one identifies the personalized patterns of
outlier tasks via a group lasso. Theoretical analysis shows the proposed
algorithm can achieve a sub-linear regret with respect to the best linear model
in hindsight. Even though the above framework achieves good performance, the
nuclear norm that simply adds all nonzero singular values together may not be a
good low-rank approximation. To improve the results, we use a log-determinant
function as a non-convex rank approximation. The gradient scheme is applied to
optimize log-determinant function and can obtain a closed-form solution for
this refined problem. Experimental results on a number of real-world
applications verify the efficacy of our method.
",1,0,0,1,0,0
18121,The Digital Flynn Effect: Complexity of Posts on Social Media Increases over Time,"  Parents and teachers often express concern about the extensive use of social
media by youngsters. Some of them see emoticons, undecipherable initialisms and
loose grammar typical for social media as evidence of language degradation. In
this paper, we use a simple measure of text complexity to investigate how the
complexity of public posts on a popular social networking site changes over
time. We analyze a unique dataset that contains texts posted by 942, 336 users
from a large European city across nine years. We show that the chosen
complexity measure is correlated with the academic performance of users: users
from high-performing schools produce more complex texts than users from
low-performing schools. We also find that complexity of posts increases with
age. Finally, we demonstrate that overall language complexity of posts on the
social networking site is constantly increasing. We call this phenomenon the
digital Flynn effect. Our results may suggest that the worries about language
degradation are not warranted.
",1,0,0,0,0,0
16146,Combating Fake News: A Survey on Identification and Mitigation Techniques,"  The proliferation of fake news on social media has opened up new directions
of research for timely identification and containment of fake news, and
mitigation of its widespread impact on public opinion. While much of the
earlier research was focused on identification of fake news based on its
contents or by exploiting users' engagements with the news on social media,
there has been a rising interest in proactive intervention strategies to
counter the spread of misinformation and its impact on society. In this survey,
we describe the modern-day problem of fake news and, in particular, highlight
the technical challenges associated with it. We discuss existing methods and
techniques applicable to both identification and mitigation, with a focus on
the significant advances in each method and their advantages and limitations.
In addition, research has often been limited by the quality of existing
datasets and their specific application contexts. To alleviate this problem, we
comprehensively compile and summarize characteristic features of available
datasets. Furthermore, we outline new directions of research to facilitate
future development of effective and interdisciplinary solutions.
",1,0,0,1,0,0
7385,Magneto-thermopower in the Weak Ferromagnetic Oxide CaRu0.8Sc0.2O3: An Experimental Test for the Kelvin Formula in a Magnetic Material,"  We have measured the resistivity, the thermopower, and the specific heat of
the weak ferromagnetic oxide CaRu0.8Sc0.2O3 in external magnetic fields up to
140 kOe below 80 K. We have observed that the thermopower Q is significantly
suppressed by magnetic fields at around the ferromagnetic transition
temperature of 30 K, and have further found that the magneto-thermopower
{\Delta}Q(H, T) = Q(H, T) - Q(0, T) is roughly proportional to the
magneto-entropy {\Delta}S(H, T) = S(H, T)-S(0, T).We discuss this relationship
between the two quantities in terms of the Kelvin formula, and find that the
observed {\Delta}Q is quantitatively consistent with the values expected from
the Kelvin formula, a possible physical meaning of which is discussed.
",0,1,0,0,0,0
13637,Machine Learning of Linear Differential Equations using Gaussian Processes,"  This work leverages recent advances in probabilistic machine learning to
discover conservation laws expressed by parametric linear equations. Such
equations involve, but are not limited to, ordinary and partial differential,
integro-differential, and fractional order operators. Here, Gaussian process
priors are modified according to the particular form of such operators and are
employed to infer parameters of the linear equations from scarce and possibly
noisy observations. Such observations may come from experiments or ""black-box""
computer simulations.
",1,0,1,1,0,0
8454,Bifurcation to locked fronts in two component reaction-diffusion systems,"  We study invasion fronts and spreading speeds in two component
reaction-diffusion systems. Using a variation of Lin's method, we construct
traveling front solutions and show the existence of a bifurcation to locked
fronts where both components invade at the same speed. Expansions of the wave
speed as a function of the diffusion constant of one species are obtained. The
bifurcation can be sub or super-critical depending on whether the locked fronts
exist for parameter values above or below the bifurcation value. Interestingly,
in the sub-critical case numerical simulations reveal that the spreading speed
of the PDE system does not depend continuously on the coefficient of diffusion.
",0,1,1,0,0,0
15711,$J_1$-$J_2$ square lattice antiferromagnetism in the orbitally quenched insulator MoOPO$_4$,"  We report magnetic and thermodynamic properties of a $4d^1$ (Mo$^{5+}$)
magnetic insulator MoOPO$_4$ single crystal, which realizes a $J_1$-$J_2$
Heisenberg spin-$1/2$ model on a stacked square lattice. The specific-heat
measurements show a magnetic transition at 16 K which is also confirmed by
magnetic susceptibility, ESR, and neutron diffraction measurements. Magnetic
entropy deduced from the specific heat corresponds to a two-level degree of
freedom per Mo$^{5+}$ ion, and the effective moment from the susceptibility
corresponds to the spin-only value. Using {\it ab initio} quantum chemistry
calculations we demonstrate that the Mo$^{5+}$ ion hosts a purely spin-$1/2$
magnetic moment, indicating negligible effects of spin-orbit interaction. The
quenched orbital moments originate from the large displacement of Mo ions
inside the MoO$_6$ octahedra along the apical direction. The ground state is
shown by neutron diffraction to support a collinear Néel-type magnetic order,
and a spin-flop transition is observed around an applied magnetic field of 3.5
T. The magnetic phase diagram is reproduced by a mean-field calculation
assuming a small easy-axis anisotropy in the exchange interactions. Our results
suggest $4d$ molybdates as an alternative playground to search for model
quantum magnets.
",0,1,0,0,0,0
9568,Fault Tolerance of Random Graphs with respect to Connectivity: Phase Transition in Logarithmic Average Degree,"  The fault tolerance of random graphs with unbounded degrees with respect to
connectivity is investigated. It is related to the reliability of wireless
sensor networks with unreliable relay nodes. The model evaluates the network
breakdown probability that a graph is disconnected after stochastic node
removal. To establish a mean-field approximation for the model, the cavity
method for finite systems is proposed. Then the asymptotic analysis is applied.
As a result, the former enables us to obtain an approximation formula for any
number of nodes and an arbitrary and degree distribution. In addition, the
latter reveals that the phase transition occurs on random graphs with
logarithmic average degrees. Those results, which are supported by numerical
simulations, coincide with the mathematical results, indicating successful
predictions by mean-field approximation for unbounded but not dense random
graphs.
",0,1,0,0,0,0
18939,Temporal Justification Logic,"  Justification logics are modal-like logics with the additional capability of
recording the reason, or justification, for modalities in syntactic structures,
called justification terms. Justification logics can be seen as explicit
counterparts to modal logics. The behavior and interaction of agents in
distributed system is often modeled using logics of knowledge and time. In this
paper, we sketch some preliminary ideas on how the modal knowledge part of such
logics of knowledge and time could be replaced with an appropriate
justification logic.
",1,0,0,0,0,0
14382,N-GCN: Multi-scale Graph Convolution for Semi-supervised Node Classification,"  Graph Convolutional Networks (GCNs) have shown significant improvements in
semi-supervised learning on graph-structured data. Concurrently, unsupervised
learning of graph embeddings has benefited from the information contained in
random walks. In this paper, we propose a model: Network of GCNs (N-GCN), which
marries these two lines of work. At its core, N-GCN trains multiple instances
of GCNs over node pairs discovered at different distances in random walks, and
learns a combination of the instance outputs which optimizes the classification
objective. Our experiments show that our proposed N-GCN model improves
state-of-the-art baselines on all of the challenging node classification tasks
we consider: Cora, Citeseer, Pubmed, and PPI. In addition, our proposed method
has other desirable properties, including generalization to recently proposed
semi-supervised learning methods such as GraphSAGE, allowing us to propose
N-SAGE, and resilience to adversarial input perturbations.
",1,0,0,1,0,0
13119,On Bivariate Discrete Weibull Distribution,"  Recently, Lee and Cha (2015, `On two generalized classes of discrete
bivariate distributions', {\it American Statistician}, 221 - 230) proposed two
general classes of discrete bivariate distributions. They have discussed some
general properties and some specific cases of their proposed distributions. In
this paper we have considered one model, namely bivariate discrete Weibull
distribution, which has not been considered in the literature yet. The proposed
bivariate discrete Weibull distribution is a discrete analogue of the
Marshall-Olkin bivariate Weibull distribution. We study various properties of
the proposed distribution and discuss its interesting physical interpretations.
The proposed model has four parameters, and because of that it is a very
flexible distribution. The maximum likelihood estimators of the parameters
cannot be obtained in closed forms, and we have proposed a very efficient
nested EM algorithm which works quite well for discrete data. We have also
proposed augmented Gibbs sampling procedure to compute Bayes estimates of the
unknown parameters based on a very flexible set of priors. Two data sets have
been analyzed to show how the proposed model and the method work in practice.
We will see that the performances are quite satisfactory. Finally, we conclude
the paper.
",0,0,0,1,0,0
1165,System Description: Russell - A Logical Framework for Deductive Systems,"  Russell is a logical framework for the specification and implementation of
deductive systems. It is a high-level language with respect to Metamath
language, so inherently it uses a Metamath foundations, i.e. it doesn't rely on
any particular formal calculus, but rather is a pure logical framework. The
main difference with Metamath is in the proof language and approach to syntax:
the proofs have a declarative form, i.e. consist of actual expressions, which
are used in proofs, while syntactic grammar rules are separated from the
meaningful rules of inference.
Russell is implemented in c++14 and is distributed under GPL v3 license. The
repository contains translators from Metamath to Russell and back. Original
Metamath theorem base (almost 30 000 theorems) can be translated to Russell,
verified, translated back to Metamath and verified with the original Metamath
verifier. Russell can be downloaded from the repository
this https URL
",1,0,1,0,0,0
20547,Interaction energy between vortices of vector fields on Riemannian surfaces,"  We study a variational Ginzburg-Landau type model depending on a small
parameter $\epsilon>0$ for (tangent) vector fields on a $2$-dimensional
Riemannian surface. As $\epsilon\to 0$, the vector fields tend to be of unit
length and will have singular points of a (non-zero) index, called vortices.
Our main result determines the interaction energy between these vortices as a
$\Gamma$-limit (at the second order) as $\epsilon\to 0$.
",0,0,1,0,0,0
18202,Characterizing information importance and the effect on the spread in various graph topologies,"  In this paper we present a thorough analysis of the nature of news in
different mediums across the ages, introducing a unique mathematical model to
fit the characteristics of information spread. This model enhances the
information diffusion model to account for conflicting information and the
topical distribution of news in terms of popularity for a given era. We
translate this information to a separate graphical node model to determine the
spread of a news item given a certain category and relevance factor. The two
models are used as a base for a simulation of information dissemination for
varying graph topoligies. The simulation is stress-tested and compared against
real-world data to prove its relevancy. We are then able to use these
simulations to deduce some conclusive statements about the optimization of
information spread.
",1,1,0,0,0,0
13303,Visualized Insights into the Optimization Landscape of Fully Convolutional Networks,"  Many image processing tasks involve image-to-image mapping, which can be
addressed well by fully convolutional networks (FCN) without any heavy
preprocessing. Although empirically designing and training FCNs can achieve
satisfactory results, reasons for the improvement in performance are slightly
ambiguous. Our study is to make progress in understanding their generalization
abilities through visualizing the optimization landscapes. The visualization of
objective functions is obtained by choosing a solution and projecting its
vicinity onto a 3D space. We compare three FCN-based networks (two existing
models and a new proposed in this paper for comparison) on multiple datasets.
It has been observed in practice that the connections from the pre-pooled
feature maps to the post-upsampled can achieve better results. We investigate
the cause and provide experiments to shows that the skip-layer connections in
FCN can promote flat optimization landscape, which is well known to generalize
better. Additionally, we explore the relationship between the models
generalization ability and loss surface under different batch sizes. Results
show that large-batch training makes the model converge to sharp minimizers
with chaotic vicinities while small-batch method leads the model to flat
minimizers with smooth and nearly convex regions. Our work may contribute to
insights and analysis for designing and training FCNs.
",1,0,0,1,0,0
17483,On universal operators and universal pairs,"  We study some basic properties of the class of universal operators on Hilbert
space, and provide new examples of universal operators and universal pairs.
",0,0,1,0,0,0
1856,High-performance parallel computing in the classroom using the public goods game as an example,"  The use of computers in statistical physics is common because the sheer
number of equations that describe the behavior of an entire system particle by
particle often makes it impossible to solve them exactly. Monte Carlo methods
form a particularly important class of numerical methods for solving problems
in statistical physics. Although these methods are simple in principle, their
proper use requires a good command of statistical mechanics, as well as
considerable computational resources. The aim of this paper is to demonstrate
how the usage of widely accessible graphics cards on personal computers can
elevate the computing power in Monte Carlo simulations by orders of magnitude,
thus allowing live classroom demonstration of phenomena that would otherwise be
out of reach. As an example, we use the public goods game on a square lattice
where two strategies compete for common resources in a social dilemma
situation. We show that the second-order phase transition to an absorbing phase
in the system belongs to the directed percolation universality class, and we
compare the time needed to arrive at this result by means of the main processor
and by means of a suitable graphics card. Parallel computing on graphics
processing units has been developed actively during the last decade, to the
point where today the learning curve for entry is anything but steep for those
familiar with programming. The subject is thus ripe for inclusion in graduate
and advanced undergraduate curricula, and we hope that this paper will
facilitate this process in the realm of physics education. To that end, we
provide a documented source code for an easy reproduction of presented results
and for further development of Monte Carlo simulations of similar systems.
",0,1,0,0,0,0
15561,In-Place Initializable Arrays,"  Initializing all elements of an array to a specified value is a basic
operation that frequently appears in numerous algorithms and programs.
Initializable arrays are abstract arrays that support initialization as well as
reading and writing of any element of the array in less than linear time
proportional to the length of the array. On the word RAM model with $w$ bits
word size, we propose an in-place algorithm using only 1 extra bit which
implements an initializable array of length $N$ each of whose elements can
store $\ell \in O(w)$ bits value, and supports all operations in constant worst
case time. We also show that our algorithm is not only time optimal but also
space optimal. Our algorithm significantly improves upon the previous best
algorithm [Navarro, CSUR 2014] using $N + \ell + o(N)$ extra bits supporting
all operations in constant worst case time.
Moreover, for a special cast that $\ell \ge 2 \lceil \log N \rceil$ and $\ell
\in O(w)$, we also propose an algorithm so that each element of initializable
array can store $2^\ell$ normal states and a one optional state, which uses
$\ell + \lceil \log N \rceil + 1$ extra bits and supports all operations in
constant worst case time.
",1,0,0,0,0,0
14998,The magnetic and electronic properties of Oxyselenides - influence of transition metal ions and lanthanides,"  Magnetic oxyselenides have been the topic of research for several decades
being first of interest in the context of photoconductivity and
thermoelectricity owing to their intrinsic semiconducting properties and
ability to tune the energy gap through metal ion substitution. More recently,
interest in the oxyselenides has experienced a resurgence owing to the possible
relation to strongly correlated phenomena given the fact that many oxyslenides
share a similar structure to unconventional superconducting pnictides and
chalcogenides. The two dimensional nature of many oxyselenide systems also
draws an analogy to cuprate physics where a strong interplay between
unconventional electronic phases and localised magnetism has been studied for
several decades. It is therefore timely to review the physics of the
oxyselenides in the context of the broader field of strongly correlated
magnetism and electronic phenomena. Here we review the current status and
progress in this area of research with the focus on the influence of
lanthanides and transition metal ions on the intertwined magnetic and
electronic properties of oxyselenides. The emphasis of the review is on the
magnetic properties and comparisons are made with iron based pnictide and
chalcogenide systems.
",0,1,0,0,0,0
111,Fermi-edge singularity and the functional renormalization group,"  We study the Fermi-edge singularity, describing the response of a degenerate
electron system to optical excitation, in the framework of the functional
renormalization group (fRG). Results for the (interband) particle-hole
susceptibility from various implementations of fRG (one- and two-
particle-irreducible, multi-channel Hubbard-Stratonovich, flowing
susceptibility) are compared to the summation of all leading logarithmic (log)
diagrams, achieved by a (first-order) solution of the parquet equations. For
the (zero-dimensional) special case of the X-ray-edge singularity, we show that
the leading log formula can be analytically reproduced in a consistent way from
a truncated, one-loop fRG flow. However, reviewing the underlying diagrammatic
structure, we show that this derivation relies on fortuitous partial
cancellations special to the form of and accuracy applied to the X-ray-edge
singularity and does not generalize.
",0,1,0,0,0,0
20751,Controllability of impulse controlled systems of heat equations coupled by constant matrices,"  This paper studies the approximate and null controllability for impulse
controlled systems of heat equations coupled by a pair (A,B) of constant
matrices. We present a necessary and sufficient condition for the approximate
controllability, which is exactly Kalman's controllability rank condition of
(A,B). We prove that when such a system is approximately controllable, the
approximate controllability over an interval [0,T] can be realized by adding
controls at arbitrary n different control instants
0<\tau_1<\tau_2<\cdots<\tau_n<T, provided that \tau_n-\tau_1<d_A, where
d_A=\min\{\pi/|Im \lambda| : \lambda\in \sigma(A)\}. We also show that in
general, such systems are not null controllable.
",0,0,1,0,0,0
14026,Numerical assessment of the percolation threshold using complement networks,"  Models of percolation processes on networks currently assume locally
tree-like structures at low densities, and are derived exactly only in the
thermodynamic limit. Finite size effects and the presence of short loops in
real systems however cause a deviation between the empirical percolation
threshold $p_c$ and its model-predicted value $\pi_c$. Here we show the
existence of an empirical linear relation between $p_c$ and $\pi_c$ across a
large number of real and model networks. Such a putatively universal relation
can then be used to correct the estimated value of $\pi_c$. We further show how
to obtain a more precise relation using the concept of the complement graph, by
investigating on the connection between the percolation threshold of a network,
$p_c$, and that of its complement, $\bar{p}_c$.
",1,0,0,0,0,0
11656,Fourier Multipliers on the Heisenberg groups revisited,"  In this paper, we give explicit expressions of differential-difference
operators appeared in the hypothesis of the general Fourier multiplier theorem
associated to the Heisenberg groups proved by Mauceri and De Micheal for one
dimension and C. Lin for higher dimension. We also give a much shorter proof of
the above-mentioned theorem. Then we obtain a sharp weighted estimate for
Fourier multipliers on the Heisenberg groups.
",0,0,1,0,0,0
10525,Impurity band conduction in group-IV ferromagnetic semiconductor Ge1-xFex with nanoscale fluctuations in Fe concentration,"  We study the carrier transport and magnetic properties of group-IV-based
ferromagnetic semiconductor Ge1-xFex thin films (Fe concentration x = 2.3 - 14
%) with and without boron (B) doping, by measuring their transport
characteristics; the temperature dependence of resistivity, hole concentration,
mobility, and the relation between the anomalous Hall conductivity versus
conductivity. At relatively low x (= 2.3 %), the transport in the undoped
Ge1-xFex film is dominated by hole hopping between Fe-rich hopping sites in the
Fe impurity band, whereas that in the B-doped Ge1-xFex film is dominated by the
holes in the valence band in the degenerated Fe-poor regions. As x increases (x
= 2.3 - 14 %), the transport in the both undoped and B-doped Ge1-xFex films is
dominated by hole hopping between the Fe-rich hopping sites of the impurity
band. The magnetic properties of the Ge1-xFex films are studied by various
methods including magnetic circular dichroism, magnetization and anomalous Hall
resistance, and are not influenced by B-doping. We show band profile models of
both undoped and B-doped Ge1-xFex films, which can explain the transport and
the magnetic properties of the Ge1-xFex films.
",0,1,0,0,0,0
3450,Robust Regulation of Infinite-Dimensional Port-Hamiltonian Systems,"  We will give general sufficient conditions under which a controller achieves
robust regulation for a boundary control and observation system. Utilizing
these conditions we construct a minimal order robust controller for an
arbitrary order impedance passive linear port-Hamiltonian system. The
theoretical results are illustrated with a numerical example where we implement
a controller for a one-dimensional Euler-Bernoulli beam with boundary controls
and boundary observations.
",0,0,1,0,0,0
6472,Verifiable Light-Weight Monitoring for Certificate Transparency Logs,"  Trust in publicly verifiable Certificate Transparency (CT) logs is reduced
through cryptography, gossip, auditing, and monitoring. The role of a monitor
is to observe each and every log entry, looking for suspicious certificates
that interest the entity running the monitor. While anyone can run a monitor,
it requires continuous operation and copies of the logs to be inspected. This
has lead to the emergence of monitoring-as-a-service: a trusted party runs the
monitor and provides registered subjects with selective certificate
notifications, e.g., ""notify me of all foo.com certificates"". We present a
CT/bis extension for verifiable light-weight monitoring that enables subjects
to verify the correctness of such notifications, reducing the trust that is
placed in these monitors. Our extension supports verifiable monitoring of
wild-card domains and piggybacks on CT's existing gossip-audit security model.
",1,0,0,0,0,0
18123,Emergent universal critical behavior of the 2D $N$-color Ashkin-Teller model in the presence of correlated disorder,"  We study the critical behavior of the 2D $N$-color Ashkin-Teller model in the
presence of random bond disorder whose correlations decays with the distance
$r$ as a power-law $r^{-a}$. We consider the case when the spins of different
colors sitting at the same site are coupled by the same bond and map this
problem onto the 2D system of $N/2$ flavors of interacting Dirac fermions in
the presence of correlated disorder. Using renormalization group we show that
for $N=2$, a ""weakly universal"" scaling behavior at the continuous transition
becomes universal with new critical exponents. For $N>2$, the first-order phase
transition is rounded by the correlated disorder and turns into a continuous
one.
",0,1,0,0,0,0
14535,Strong convergence rates of modified truncated EM method for stochastic differential equations,"  Motivated by truncated EM method introduced by Mao (2015), a new explicit
numerical method named modified truncated Euler-Maruyama method is developed in
this paper. Strong convergence rates of the given numerical scheme to the exact
solutions to stochastic differential equations are investigated under given
conditions in this paper. Compared with truncated EM method, the given
numerical simulation strongly converges to the exact solution at fixed time $T$
and over a time interval $[0,T]$ under weaker sufficient conditions. Meanwhile,
the convergence rates are also obtained for both cases. Two examples are
provided to support our conclusions.
",0,0,1,0,0,0
18196,The homotopy theory of coalgebras over simplicial comonads,"  We apply the Acyclicity Theorem of Hess, Kerdziorek, Riehl, and Shipley
(recently corrected by Garner, Kedziorek, and Riehl) to establishing the
existence of model category structure on categories of coalgebras over comonads
arising from simplicial adjunctions, under mild conditions on the adjunction
and the associated comonad. We study three concrete examples of such
adjunctions where the left adjoint is comonadic and show that in each case the
component of the derived counit of the comparison adjunction at any fibrant
object is an isomorphism, while the component of the derived unit at any
1-connected object is a weak equivalence. To prove this last result, we explain
how to construct explicit fibrant replacements for 1-connected coalgebras in
the image of the canonical comparison functor from the Postnikov decompositions
of their underlying simplicial sets. We also show in one case that the derived
unit is precisely the Bousfield-Kan completion map.
",0,0,1,0,0,0
2909,The influence of contrarians in the dynamics of opinion formation,"  In this work we consider the presence of contrarian agents in discrete
3-state kinetic exchange opinion models. The contrarians are individuals that
adopt the choice opposite to the prevailing choice of their contacts, whatever
this choice is. We consider binary as well as three-agent interactions, with
stochastic parameters, in a fully-connected population. Our numerical results
suggest that the presence of contrarians destroys the absorbing state of the
original model, changing the transition to the para-ferromagnetic type. In this
case, the consequence for the society is that the three opinions coexist in the
population, in both phases (ordered and disordered). Furthermore, the
order-disorder transition is suppressed for a sufficient large fraction of
contrarians. In some cases the transition is discontinuous, and it changes to
continuous before it is suppressed. Some of our results are complemented by
analytical calculations based on the master equation.
",0,1,0,0,0,0
7212,Maps on statistical manifolds exactly reduced from the Perron-Frobenius equations for solvable chaotic maps,"  Maps on a parameter space for expressing distribution functions are exactly
derived from the Perron-Frobenius equations for a generalized Boole transform
family. Here the generalized Boole transform family is a one-parameter family
of maps where it is defined on a subset of the real line and its probability
distribution function is the Cauchy distribution with some parameters. With
this reduction, some relations between the statistical picture and the orbital
one are shown. From the viewpoint of information geometry, the parameter space
can be identified with a statistical manifold, and then it is shown that the
derived maps can be characterized. Also, with an induced symplectic structure
from a statistical structure, symplectic and information geometric aspects of
the derived maps are discussed.
",0,1,0,0,0,0
15483,Diclofenac sodium ion exchange resin complex loaded melt cast films for sustained release ocular delivery,"  The goal of the present study is to develop polymeric matrix films loaded
with a combination of free diclofenac sodium (DFSfree) and DFS:Ion exchange
resin complexes (DFS:IR) for immediate and sustained release profiles,
respectively. Effect of ratio of DFS and IR on the DFS:IR complexation
efficiency was studied using batch processing. DFS:IR complex, DFSfree, or a
combination of DFSfree+DFS:IR loaded matrix films were prepared by melt-cast
technology. DFS content was 20% w/w in these matrix films. In vitro
transcorneal permeability from the film formulations were compared against DFS
solution, using a side-by-side diffusion apparatus, over a 6 h period. Ocular
disposition of DFS from the solution, films and corresponding suspensions were
evaluated in conscious New Zealand albino rabbits, 4 h and 8 h post-topical
administration. All in vivo studies were carried out as per the University of
Mississippi IACUC approved protocol. Complexation efficiency of DFS:IR was
found to be 99% with a 1:1 ratio of DFS:IR. DFS release from DFS:IR suspension
and the film were best-fit to a Higuchi model. In vitro transcorneal flux with
the DFSfree+DFS:IR(1:1)(1 + 1) was twice that of only DFS:IR(1:1) film. In
vivo, DFS solution and DFS:IR(1:1) suspension formulations were not able to
maintain therapeutic DFS levels in the aqueous humor (AH). Both DFSfree and
DFSfree+DFS:IR(1:1)(3 + 1) loaded matrix films were able to achieve and
maintain high DFS concentrations in the AH, but elimination of DFS from the
ocular tissues was much faster with the DFSfree formulation. DFSfree+DFS:IR
combination loaded matrix films were able to deliver and maintain therapeutic
DFS concentrations in the anterior ocular chamber for up to 8 h. Thus, free
drug/IR complex loaded matrix films could be a potential topical ocular
delivery platform for achieving immediate and sustained release
characteristics.
",0,1,0,0,0,0
3111,Robustifying Independent Component Analysis by Adjusting for Group-Wise Stationary Noise,"  We introduce coroICA, confounding-robust independent component analysis, a
novel ICA algorithm which decomposes linearly mixed multivariate observations
into independent components that are corrupted (and rendered dependent) by
hidden group-wise stationary confounding. It extends the ordinary ICA model in
a theoretically sound and explicit way to incorporate group-wise (or
environment-wise) confounding. We show that our general noise model allows to
perform ICA in settings where other noisy ICA procedures fail. Additionally, it
can be used for applications with grouped data by adjusting for different
stationary noise within each group. We show that the noise model has a natural
relation to causality and explain how it can be applied in the context of
causal inference. In addition to our theoretical framework, we provide an
efficient estimation procedure and prove identifiability of the unmixing matrix
under mild assumptions. Finally, we illustrate the performance and robustness
of our method on simulated data, provide audible and visual examples, and
demonstrate the applicability to real-world scenarios by experiments on
publicly available Antarctic ice core data as well as two EEG data sets. We
provide a scikit-learn compatible pip-installable Python package coroICA as
well as R and Matlab implementations accompanied by a documentation at
this https URL.
",0,0,0,1,1,0
10583,Core-powered mass loss and the radius distribution of small exoplanets,"  Recent observations identify a valley in the radius distribution of small
exoplanets, with planets in the range $1.5-2.0\,{\rm R}_{\oplus}$ significantly
less common than somewhat smaller or larger planets. This valley may suggest a
bimodal population of rocky planets that are either engulfed by massive gas
envelopes that significantly enlarge their radius, or do not have detectable
atmospheres at all. One explanation of such a bimodal distribution is
atmospheric erosion by high-energy stellar photons. We investigate an
alternative mechanism: the luminosity of the cooling rocky core, which can
completely erode light envelopes while preserving heavy ones, produces a
deficit of intermediate sized planets. We evolve planetary populations that are
derived from observations using a simple analytical prescription, accounting
self-consistently for envelope accretion, cooling and mass loss, and
demonstrate that core-powered mass loss naturally reproduces the observed
radius distribution, regardless of the high-energy incident flux. Observations
of planets around different stellar types may distinguish between
photoevaporation, which is powered by the high-energy tail of the stellar
radiation, and core-powered mass loss, which depends on the bolometric flux
through the planet's equilibrium temperature that sets both its cooling and
mass-loss rates.
",0,1,0,0,0,0
7765,Automated text summarisation and evidence-based medicine: A survey of two domains,"  The practice of evidence-based medicine (EBM) urges medical practitioners to
utilise the latest research evidence when making clinical decisions. Because of
the massive and growing volume of published research on various medical topics,
practitioners often find themselves overloaded with information. As such,
natural language processing research has recently commenced exploring
techniques for performing medical domain-specific automated text summarisation
(ATS) techniques-- targeted towards the task of condensing large medical texts.
However, the development of effective summarisation techniques for this task
requires cross-domain knowledge. We present a survey of EBM, the
domain-specific needs for EBM, automated summarisation techniques, and how they
have been applied hitherto. We envision that this survey will serve as a first
resource for the development of future operational text summarisation
techniques for EBM.
",1,0,0,0,0,0
5011,Buckling in Armored Droplets,"  The issue of the buckling mechanism in droplets stabilized by solid particles
(armored droplets) is tackled at a mesoscopic level using dissipative particle
dynamics simulations. We consider spherical water droplet in a decane solvent
coated with nanoparticle monolayers of two different types: Janus and
homogeneous. The chosen particles yield comparable initial three-phase contact
angles, chosen to maximize the adsorption energy at the interface. We study the
interplay between the evolution of droplet shape, layering of the particles,
and their distribution at the interface when the volume of the droplets is
reduced. We show that Janus particles affect strongly the shape of the droplet
with the formation of a crater-like depression. This evolution is actively
controlled by a close-packed particle monolayer at the curved interface. On the
contrary, homogeneous particles follow passively the volume reduction of the
droplet, whose shape does not deviate too much from spherical, even when a
nanoparticle monolayer/bilayer transition is detected at the interface. We
discuss how these buckled armored droplets might be of relevance in various
applications including potential drug delivery systems and biomimetic design of
functional surfaces.
",0,1,0,0,0,0
15563,Modeling Hormesis Using a Non-Monotonic Copula Method,"  This paper presents a probabilistic method for capturing non-monotonic
behavior under the biphasic dose-response regime observed in many biological
systems experiencing different types of stress. The proposed method is based on
the rolling-pin method introduced earlier to estimate highly nonlinear and
non-monotonic joint probability distributions from continuous domain data. We
show that the proposed method outperforms the conventional parametric methods
in terms of the error (namely RMSE) and it needs fewer parameters to be
estimated a priori, while offering high flexibility. The application and
performance of the proposed method are shown through an example.
",0,0,0,1,0,0
5293,Formalizing Timing Diagram Requirements in Discrete Duration Calulus,"  Several temporal logics have been proposed to formalise timing diagram
requirements over hardware and embedded controllers. These include LTL,
discrete time MTL and the recent industry standard PSL. However, succintness
and visual structure of a timing diagram are not adequately captured by their
formulae. Interval temporal logic QDDC is a highly succint and visual notation
for specifying patterns of behaviours.
In this paper, we propose a practically useful notation called SeCeCntnl
which enhances negation free fragment of QDDC with features of nominals and
limited liveness. We show that timing diagrams can be naturally
(compositionally) and succintly formalized in SeCeCntnl as compared with PSL
and MTL. We give a linear time translation from timing diagrams to SeCeCntnl.
As our second main result, we propose a linear time translation of SeCeCntnl
into QDDC. This allows QDDC tools such as DCVALID and DCSynth to be used for
checking consistency of timing diagram requirements as well as for automatic
synthesis of property monitors and controllers. We give examples of a minepump
controller and a bus arbiter to illustrate our tools. Giving a theoretical
analysis, we show that for the proposed SeCeCntnl, the satisfiability and model
checking have elementary complexity as compared to the non-elementary
complexity for the full logic QDDC.
",1,0,0,0,0,0
20416,Experimental Tests of Spirituality,"  We currently harness technologies that could shed new light on old
philosophical questions, such as whether our mind entails anything beyond our
body or whether our moral values reflect universal truth.
",0,0,0,0,1,0
14634,Dust-trapping vortices and a potentially planet-triggered spiral wake in the pre-transitional disk of V1247 Orionis,"  The radial drift problem constitutes one of the most fundamental problems in
planet formation theory, as it predicts particles to drift into the star before
they are able to grow to planetesimal size. Dust-trapping vortices have been
proposed as a possible solution to this problem, as they might be able to trap
particles over millions of years, allowing them to grow beyond the radial drift
barrier. Here, we present ALMA 0.04""-resolution imaging of the pre-transitional
disk of V1247 Orionis that reveals an asymmetric ring as well as a
sharply-confined crescent structure, resembling morphologies seen in
theoretical models of vortex formation. The asymmetric ring (at 0.17""=54 au
separation from the star) and the crescent (at 0.38""=120 au) seem smoothly
connected through a one-armed spiral arm structure that has been found
previously in scattered light. We propose a physical scenario with a planet
orbiting at $\sim0.3$""$\approx$100 au, where the one-armed spiral arm detected
in polarised light traces the accretion stream feeding the protoplanet. The
dynamical influence of the planet clears the gap between the ring and the
crescent and triggers two vortices that trap mm-sized particles, namely the
crescent and the bright asymmetry seen in the ring. We conducted dedicated
hydrodynamics simulations of a disk with an embedded planet, which results in
similar spiral-arm morphologies as seen in our scattered light images. At the
position of the spiral wake and the crescent we also observe $^{12}$CO (3-2)
and H$^{12}$CO$^{+}$ (4-3) excess line emission, likely tracing the increased
scale-height in these disk regions.
",0,1,0,0,0,0
4044,An effective formalism for testing extensions to General Relativity with gravitational waves,"  The recent direct observation of gravitational waves (GW) from merging black
holes opens up the possibility of exploring the theory of gravity in the strong
regime at an unprecedented level. It is therefore interesting to explore which
extensions to General Relativity (GR) could be detected. We construct an
Effective Field Theory (EFT) satisfying the following requirements. It is
testable with GW observations; it is consistent with other experiments,
including short distance tests of GR; it agrees with widely accepted principles
of physics, such as locality, causality and unitarity; and it does not involve
new light degrees of freedom. The most general theory satisfying these
requirements corresponds to adding to the GR Lagrangian operators constructed
out of powers of the Riemann tensor, suppressed by a scale comparable to the
curvature of the observed merging binaries. The presence of these operators
modifies the gravitational potential between the compact objects, as well as
their effective mass and current quadrupoles, ultimately correcting the
waveform of the emitted GW.
",0,1,0,0,0,0
7869,Finite Sample Analysis of Two-Timescale Stochastic Approximation with Applications to Reinforcement Learning,"  Two-timescale Stochastic Approximation (SA) algorithms are widely used in
Reinforcement Learning (RL). Their iterates have two parts that are updated
using distinct stepsizes. In this work, we develop a novel recipe for their
finite sample analysis. Using this, we provide a concentration bound, which is
the first such result for a two-timescale SA. The type of bound we obtain is
known as `lock-in probability'. We also introduce a new projection scheme, in
which the time between successive projections increases exponentially. This
scheme allows one to elegantly transform a lock-in probability into a
convergence rate result for projected two-timescale SA. From this latter
result, we then extract key insights on stepsize selection. As an application,
we finally obtain convergence rates for the projected two-timescale RL
algorithms GTD(0), GTD2, and TDC.
",1,0,0,0,0,0
1209,Integrable structure of products of finite complex Ginibre random matrices,"  We consider the squared singular values of the product of $M$ standard
complex Gaussian matrices. Since the squared singular values form a
determinantal point process with a particular Meijer G-function kernel, the gap
probabilities are given by a Fredholm determinant based on this kernel. It was
shown by Strahov \cite{St14} that a hard edge scaling limit of the gap
probabilities is described by Hamiltonian differential equations which can be
formulated as an isomonodromic deformation system similar to the theory of the
Kyoto school. We generalize this result to the case of finite matrices by first
finding a representation of the finite kernel in integrable form. As a result
we obtain the Hamiltonian structure for a finite size matrices and formulate it
in terms of a $(M+1) \times (M+1)$ matrix Schlesinger system. The case $M=1$
reproduces the Tracy and Widom theory which results in the Painlevé V
equation for the $(0,s)$ gap probability. Some integrals of motion for $M = 2$
are identified, and a coupled system of differential equations in two unknowns
is presented which uniquely determines the corresponding $(0,s)$ gap
probability.
",0,1,1,0,0,0
7271,Improved Training of Wasserstein GANs,"  Generative Adversarial Networks (GANs) are powerful generative models, but
suffer from training instability. The recently proposed Wasserstein GAN (WGAN)
makes progress toward stable training of GANs, but sometimes can still generate
only low-quality samples or fail to converge. We find that these problems are
often due to the use of weight clipping in WGAN to enforce a Lipschitz
constraint on the critic, which can lead to undesired behavior. We propose an
alternative to clipping weights: penalize the norm of gradient of the critic
with respect to its input. Our proposed method performs better than standard
WGAN and enables stable training of a wide variety of GAN architectures with
almost no hyperparameter tuning, including 101-layer ResNets and language
models over discrete data. We also achieve high quality generations on CIFAR-10
and LSUN bedrooms.
",1,0,0,1,0,0
20821,Learning to Predict Indoor Illumination from a Single Image,"  We propose an automatic method to infer high dynamic range illumination from
a single, limited field-of-view, low dynamic range photograph of an indoor
scene. In contrast to previous work that relies on specialized image capture,
user input, and/or simple scene models, we train an end-to-end deep neural
network that directly regresses a limited field-of-view photo to HDR
illumination, without strong assumptions on scene geometry, material
properties, or lighting. We show that this can be accomplished in a three step
process: 1) we train a robust lighting classifier to automatically annotate the
location of light sources in a large dataset of LDR environment maps, 2) we use
these annotations to train a deep neural network that predicts the location of
lights in a scene from a single limited field-of-view photo, and 3) we
fine-tune this network using a small dataset of HDR environment maps to predict
light intensities. This allows us to automatically recover high-quality HDR
illumination estimates that significantly outperform previous state-of-the-art
methods. Consequently, using our illumination estimates for applications like
3D object insertion, we can achieve results that are photo-realistic, which is
validated via a perceptual user study.
",1,0,0,1,0,0
18226,A Two-Level Graph Partitioning Problem Arising in Mobile Wireless Communications,"  In the k-partition problem (k-PP), one is given an edge-weighted undirected
graph, and one must partition the node set into at most k subsets, in order to
minimise (or maximise) the total weight of the edges that have their end-nodes
in the same cluster. Various hierarchical variants of this problem have been
studied in the context of data mining. We consider a 'two-level' variant that
arises in mobile wireless communications. We show that an exact algorithm based
on intelligent preprocessing, cutting planes and symmetry-breaking is capable
of solving small- and medium-size instances to proven optimality, and providing
strong lower bounds for larger instances.
",1,0,1,0,0,0
7260,Sensivity of the Hermite rank,"  The Hermite rank appears in limit theorems involving long memory. We show
that an Hermite rank higher than one is unstable when the data is slightly
perturbed by transformations such as shift and scaling. We carry out a ""near
higher order rank analysis"" to illustrate how the limit theorems are affected
by a shift perturbation that is decreasing in size. As a byproduct of our
analysis, we also prove the coincidence of the Hermite rank and the power rank
in the Gaussian context. The paper is a technical companion of
\citet{bai:taqqu:2017:instability} which discusses the instability of the
Hermite rank in the statistical context. (Older title ""Some properties of the
Hermite rank"">)
",0,0,1,1,0,0
4352,A Capillary Surface with No Radial Limits,"  In 1996, Kirk Lancaster and David Siegel investigated the existence and
behavior of radial limits at a corner of the boundary of the domain of
solutions of capillary and other prescribed mean curvature problems with
contact angle boundary data. In Theorem 3, they provide an example of a
capillary surface in a unit disk $D$ which has no radial limits at
$(0,0)\in\partial D.$ In their example, the contact angle ($\gamma$) cannot be
bounded away from zero and $\pi.$
Here we consider a domain $\Omega$ with a convex corner at $(0,0)$ and find a
capillary surface $z=f(x,y)$ in $\Omega\times\mathbb{R}$ which has no radial
limits at $(0,0)\in\partial\Omega$ such that $\gamma$ is bounded away from $0$
and $\pi.$
",0,0,1,0,0,0
9899,An estimator for the tail-index of graphex processes,"  Sparse exchangeable graphs resolve some pathologies in traditional random
graph models, notably, providing models that are both projective and allow
sparsity. In a recent paper, Caron and Rousseau (2017) show that for a large
class of sparse exchangeable models, the sparsity behaviour is governed by a
single parameter: the tail-index of the function (the graphon) that
parameterizes the model. We propose an estimator for this parameter and
quantify its risk. Our estimator is a simple, explicit function of the degrees
of the observed graph. In many situations of practical interest, the risk
decays polynomially in the size of the observed graph. Accordingly, the
estimator is practically useful for estimation of sparse exchangeable models.
We also derive the analogous results for the bipartite sparse exchangeable
case.
",0,0,1,1,0,0
4137,Deep Reinforcement Learning based Optimal Control of Hot Water Systems,"  Energy consumption for hot water production is a major draw in high
efficiency buildings. Optimizing this has typically been approached from a
thermodynamics perspective, decoupled from occupant influence. Furthermore,
optimization usually presupposes existence of a detailed dynamics model for the
hot water system. These assumptions lead to suboptimal energy efficiency in the
real world. In this paper, we present a novel reinforcement learning based
methodology which optimizes hot water production. The proposed methodology is
completely generalizable, and does not require an offline step or human domain
knowledge to build a model for the hot water vessel or the heating element.
Occupant preferences too are learnt on the fly. The proposed system is applied
to a set of 32 houses in the Netherlands where it reduces energy consumption
for hot water production by roughly 20% with no loss of occupant comfort.
Extrapolating, this translates to absolute savings of roughly 200 kWh for a
single household on an annual basis. This performance can be replicated to any
domestic hot water system and optimization objective, given that the fairly
minimal requirements on sensor data are met. With millions of hot water systems
operational worldwide, the proposed framework has the potential to reduce
energy consumption in existing and new systems on a multi Gigawatt-hour scale
in the years to come.
",0,0,0,1,0,0
10658,Emergence of magnetic long-range order in kagome quantum antiferromagnets,"  The existence of a spin-liquid ground state of the $s=1/2$ Heisenberg kagome
antiferromagnet (KAFM) is well established. Meanwhile, also for the $s=1$
Heisenberg KAFM evidence for the absence of magnetic long-range order (LRO) was
found. Magnetic LRO in Heisenberg KAFMs can emerge by increasing the spin
quantum number $s$ to $s>1$ and for $s=1$ by an easy-plane anisotropy. In the
present paper we discuss the route to magnetic order in $s=1/2$ KAFMs by
including an isotropic interlayer coupling (ILC) $J_\perp$ as well as an
easy-plane anisotropy in the kagome layers by using the coupled-cluster method
to high orders of approximation. We consider ferro- as well as
antiferromagnetic $J_\perp$. To discuss the general question for the crossover
from a purely two-dimensional (2D) to a quasi-2D and finally to a
three-dimensional system we consider the simplest model of stacked (unshifted)
kagome layers. Although the ILC of real kagome compounds is often more
sophisticated, such a geometry of the ILC can be relevant for barlowite. We
find that the spin-liquid ground state present for the strictly 2D $s=1/2$
$XXZ$ KAFM survives a finite ILC, where the spin-liquid region shrinks
monotonously with increasing anisotropy. If the ILC becomes large enough (about
15\% of intralayer coupling for the isotropic Heisenberg case and about 4\% for
the $XY$ limit) magnetic LRO can be established, where the $q=0$ symmetry is
favorable if $J_\perp$ is of moderate strength. If the strength of the ILC
further increases, $\sqrt{3}\times \sqrt{3}$ LRO can become favorable against
$q=0$ LRO.
",0,1,0,0,0,0
15097,Machine learning application in the life time of materials,"  Materials design and development typically takes several decades from the
initial discovery to commercialization with the traditional trial and error
development approach. With the accumulation of data from both experimental and
computational results, data based machine learning becomes an emerging field in
materials discovery, design and property prediction. This manuscript reviews
the history of materials science as a disciplinary the most common machine
learning method used in materials science, and specifically how they are used
in materials discovery, design, synthesis and even failure detection and
analysis after materials are deployed in real application. Finally, the
limitations of machine learning for application in materials science and
challenges in this emerging field is discussed.
",1,1,0,0,0,0
13486,From parabolic-trough to metasurface-concentrator,"  Metasurfaces are promising tools towards novel designs for flat optics
applications. As such their quality and tolerance to fabrication imperfections
need to be evaluated with specific tools. However, most such tools rely on the
geometrical optics approximation and are not straightforwardly applicable to
metasurfaces. In this Letter, we introduce and evaluate, for metasurfaces,
parameters such as the intercept factor and the slope error usually defined for
solar concentrators in the realm of ray-optics. After proposing definitions
valid in physical optics, we put forward an approach to calculate them. As
examples, we design three different concentrators based on three specific unit
cells and assess them numerically. The concept allows for the comparison of the
efficiency of the metasurfaces, their sensitivities to fabrication
imperfections and will be critical for practical systems.
",0,1,0,0,0,0
9169,Diagrammatic Approach to Multiphoton Scattering,"  We present a method to systematically study multi-photon transmission in one
dimensional systems comprised of correlated quantum emitters coupled to input
and output waveguides. Within the Green's function approach of the scattering
matrix (S-matrix), we develop a diagrammatic technique to analytically obtain
the system's scattering amplitudes while at the same time visualise all the
possible absorption and emission processes. Our method helps to reduce the
significant effort in finding the general response of a many-body bosonic
system, particularly the nonlinear response embedded in the Green's functions.
We demonstrate our proposal through physically relevant examples involving
scattering of multi-photon states from two-level emitters as well as from
arrays of correlated Kerr nonlinear resonators in the Bose-Hubbard model.
",0,1,0,0,0,0
10259,Block Motion Changes in Japan Triggered by the 2011 Great Tohoku Earthquake,"  Plate motions are governed by equilibrium between basal and edge forces.
Great earthquakes may induce differential static stress changes across tectonic
plates, enabling a new equilibrium state. Here we consider the torque balance
for idealized circular plates and find a simple scalar relationship for changes
in relative plate speed as a function of its size, upper mantle viscosity, and
coseismic stress changes. Applied to Japan, the 2011
$\mathrm{M}_{\mathrm{W}}=9.0$ Tohoku earthquake generated coseismic stresses of
$10^2-10^5$~Pa that could have induced changes in motion of small (radius
$\sim100$~km) crustal blocks within Honshu. Analysis of time-dependent GPS
velocities, with corrections for earthquake cycle effects, reveals that plate
speeds may have changed by up to $\sim3$ mm/yr between $\sim3.75$-year epochs
bracketing this earthquake, consistent with an upper mantle viscosity of $\sim
5\times10^{18}$Pa$\cdot$s, suggesting that great earthquakes may modulate
motions of proximal crustal blocks at frequencies as high as $10^-8$~Hz.
",0,1,0,0,0,0
4977,Shot noise in ultrathin superconducting wires,"  Quantum phase slips (QPS) may produce non-equilibrium voltage fluctuations in
current-biased superconducting nanowires. Making use of the Keldysh technique
and employing the phase-charge duality arguments we investigate such
fluctuations within the four-point measurement scheme and demonstrate that shot
noise of the voltage detected in such nanowires may essentially depend on the
particular measurement setup. In long wires the shot noise power decreases with
increasing frequency $\Omega$ and vanishes beyond a threshold value of $\Omega$
at $T \to 0$
",0,1,0,0,0,0
20478,Universal equilibrium scaling functions at short times after a quench,"  By analyzing spin-spin correlation functions at relatively short distances,
we show that equilibrium near-critical properties can be extracted at short
times after quenches into the vicinity of a quantum critical point. The time
scales after which equilibrium properties can be extracted are sufficiently
short so that the proposed scheme should be viable for quantum simulators of
spin models based on ultracold atoms or trapped ions. Our results, analytic as
well as numeric, are for one-dimensional spin models, either integrable or
nonintegrable, but we expect our conclusions to be valid in higher dimensions
as well.
",0,1,0,0,0,0
11515,Rationality proofs by curve counting,"  We propose an approach for showing rationality of an algebraic variety $X$.
We try to cover $X$ by rational curves of certain type and count how many
curves pass through a generic point. If the answer is $1$, then we can
sometimes reduce the question of rationality of $X$ to the question of
rationality of a closed subvariety of $X$. This approach is applied to the case
of the so-called Ueno-Campana manifolds. Our experiments indicate that the
previously open cases $X_{4,6}$ and $X_{5,6}$ are both rational. However, this
result is not rigorously justified and depends on a heuristic argument and a
Monte Carlo type computer simulation. In an unexpected twist, existence of
lattices $D_6$, $E_8$ and $\Lambda_{10}$ turns out to be crucial.
",0,0,1,0,0,0
13429,ProSLAM: Graph SLAM from a Programmer's Perspective,"  In this paper we present ProSLAM, a lightweight stereo visual SLAM system
designed with simplicity in mind. Our work stems from the experience gathered
by the authors while teaching SLAM to students and aims at providing a highly
modular system that can be easily implemented and understood. Rather than
focusing on the well known mathematical aspects of Stereo Visual SLAM, in this
work we highlight the data structures and the algorithmic aspects that one
needs to tackle during the design of such a system. We implemented ProSLAM
using the C++ programming language in combination with a minimal set of well
known used external libraries. In addition to an open source implementation, we
provide several code snippets that address the core aspects of our approach
directly in this paper. The results of a thorough validation performed on
standard benchmark datasets show that our approach achieves accuracy comparable
to state of the art methods, while requiring substantially less computational
resources.
",1,0,0,0,0,0
814,New type integral inequalities for convex functions with applications II,"  We have recently established some integral inequalities for convex functions
via the Hermite-Hadamard's inequalities. In continuation here, we also
establish some interesting new integral inequalities for convex functions via
the Hermite--Hadamard's inequalities and Jensen's integral inequality. Useful
applications involving special means are also included.
",0,0,1,0,0,0
2143,Meta-Learning for Contextual Bandit Exploration,"  We describe MELEE, a meta-learning algorithm for learning a good exploration
policy in the interactive contextual bandit setting. Here, an algorithm must
take actions based on contexts, and learn based only on a reward signal from
the action taken, thereby generating an exploration/exploitation trade-off.
MELEE addresses this trade-off by learning a good exploration strategy for
offline tasks based on synthetic data, on which it can simulate the contextual
bandit setting. Based on these simulations, MELEE uses an imitation learning
strategy to learn a good exploration policy that can then be applied to true
contextual bandit tasks at test time. We compare MELEE to seven strong baseline
contextual bandit algorithms on a set of three hundred real-world datasets, on
which it outperforms alternatives in most settings, especially when differences
in rewards are large. Finally, we demonstrate the importance of having a rich
feature representation for learning how to explore.
",1,0,0,1,0,0
277,Vocabulary-informed Extreme Value Learning,"  The novel unseen classes can be formulated as the extreme values of known
classes. This inspired the recent works on open-set recognition
\cite{Scheirer_2013_TPAMI,Scheirer_2014_TPAMIb,EVM}, which however can have no
way of naming the novel unseen classes. To solve this problem, we propose the
Extreme Value Learning (EVL) formulation to learn the mapping from visual
feature to semantic space. To model the margin and coverage distributions of
each class, the Vocabulary-informed Learning (ViL) is adopted by using vast
open vocabulary in the semantic space. Essentially, by incorporating the EVL
and ViL, we for the first time propose a novel semantic embedding paradigm --
Vocabulary-informed Extreme Value Learning (ViEVL), which embeds the visual
features into semantic space in a probabilistic way. The learned embedding can
be directly used to solve supervised learning, zero-shot and open set
recognition simultaneously. Experiments on two benchmark datasets demonstrate
the effectiveness of proposed frameworks.
",1,0,1,1,0,0
733,Homology theory formulas for generalized Riemann-Hurwitz and generalized monoidal transformations,"  In the context of orientable circuits and subcomplexes of these as
representing certain singular spaces, we consider characteristic class formulas
generalizing those classical results as seen for the Riemann-Hurwitz formula
for regulating the topology of branched covering maps and that for monoidal
transformations which include the standard blowing-up process. Here the results
are presented as cap product pairings, which will be elements of a suitable
homology theory, rather than characteristic numbers as would be the case when
taking Kronecker products once Poincaré duality is defined. We further
consider possible applications and examples including branched covering maps,
singular varieties involving virtual tangent bundles, the
Chern-Schwartz-MacPherson class, the homology L-class, generalized signature,
and the cohomology signature class.
",0,0,1,0,0,0
8905,Half-quadratic transportation problems,"  We present a primal--dual memory efficient algorithm for solving a relaxed
version of the general transportation problem. Our approach approximates the
original cost function with a differentiable one that is solved as a sequence
of weighted quadratic transportation problems. The new formulation allows us to
solve differentiable, non-- convex transportation problems.
",1,0,1,0,0,0
16182,The Minimum Euclidean-Norm Point on a Convex Polytope: Wolfe's Combinatorial Algorithm is Exponential,"  The complexity of Philip Wolfe's method for the minimum Euclidean-norm point
problem over a convex polytope has remained unknown since he proposed the
method in 1974. The method is important because it is used as a subroutine for
one of the most practical algorithms for submodular function minimization. We
present the first example that Wolfe's method takes exponential time.
Additionally, we improve previous results to show that linear programming
reduces in strongly-polynomial time to the minimum norm point problem over a
simplex.
",1,0,1,0,0,0
7959,Angular and Temporal Correlation of V2X Channels Across Sub-6 GHz and mmWave Bands,"  5G millimeter wave (mmWave) technology is envisioned to be an integral part
of next-generation vehicle-to-everything (V2X) networks and autonomous vehicles
due to its broad bandwidth, wide field of view sensing, and precise
localization capabilities. The reliability of mmWave links may be compromised
due to difficulties in beam alignment for mobile channels and due to blocking
effects between a mmWave transmitter and a receiver. To address such
challenges, out-of-band information from sub-6 GHz channels can be utilized for
predicting the temporal and angular channel characteristics in mmWave bands,
which necessitates a good understanding of how propagation characteristics are
coupled across different bands. In this paper, we use ray tracing simulations
to characterize the angular and temporal correlation across a wide range of
propagation frequencies for V2X channels ranging from 900 MHz up to 73 GHz, for
a vehicle maintaining line-of-sight (LOS) and non-LOS (NLOS) beams with a
transmitter in an urban environment. Our results shed light on increasing
sparsity behavior of propagation channels with increasing frequency and
highlight the strong temporal/angular correlation among 5.9 GHz and 28 GHz
bands especially for LOS channels.
",1,0,0,0,0,0
7442,Feature Enhancement in Visually Impaired Images,"  One of the major open problems in computer vision is detection of features in
visually impaired images. In this paper, we describe a potential solution using
Phase Stretch Transform, a new computational approach for image analysis, edge
detection and resolution enhancement that is inspired by the physics of the
photonic time stretch technique. We mathematically derive the intrinsic
nonlinear transfer function and demonstrate how it leads to (1) superior
performance at low contrast levels and (2) a reconfigurable operator for
hyper-dimensional classification. We prove that the Phase Stretch Transform
equalizes the input image brightness across the range of intensities resulting
in a high dynamic range in visually impaired images. We also show further
improvement in the dynamic range by combining our method with the conventional
techniques. Finally, our results show a method for computation of mathematical
derivatives via group delay dispersion operations.
",1,1,0,0,0,0
13462,ORBIT: Ordering Based Information Transfer Across Space and Time for Global Surface Water Monitoring,"  Many earth science applications require data at both high spatial and
temporal resolution for effective monitoring of various ecosystem resources.
Due to practical limitations in sensor design, there is often a trade-off in
different resolutions of spatio-temporal datasets and hence a single sensor
alone cannot provide the required information. Various data fusion methods have
been proposed in the literature that mainly rely on individual timesteps when
both datasets are available to learn a mapping between features values at
different resolutions using local relationships between pixels. Earth
observation data is often plagued with spatially and temporally correlated
noise, outliers and missing data due to atmospheric disturbances which pose a
challenge in learning the mapping from a local neighborhood at individual
timesteps. In this paper, we aim to exploit time-independent global
relationships between pixels for robust transfer of information across
different scales. Specifically, we propose a new framework, ORBIT (Ordering
Based Information Transfer) that uses relative ordering constraint among pixels
to transfer information across both time and scales. The effectiveness of the
framework is demonstrated for global surface water monitoring using both
synthetic and real-world datasets.
",1,0,0,0,0,0
4871,Quantum Klein Space and Superspace,"  We give an algebraic quantization, in the sense of quantum groups, of the
complex Minkowski space, and we examine the real forms corresponding to the
signatures $(3,1)$, $(2,2)$, $(4,0)$, constructing the corresponding quantum
metrics and providing an explicit presentation of the quantized coordinate
algebras. In particular, we focus on the Kleinian signature $(2,2)$. The
quantizations of the complex and real spaces come together with a coaction of
the quantizations of the respective symmetry groups. We also extend such
quantizations to the $\mathcal{N}=1$ supersetting.
",0,0,1,0,0,0
528,Ensemble Sampling,"  Thompson sampling has emerged as an effective heuristic for a broad range of
online decision problems. In its basic form, the algorithm requires computing
and sampling from a posterior distribution over models, which is tractable only
for simple special cases. This paper develops ensemble sampling, which aims to
approximate Thompson sampling while maintaining tractability even in the face
of complex models such as neural networks. Ensemble sampling dramatically
expands on the range of applications for which Thompson sampling is viable. We
establish a theoretical basis that supports the approach and present
computational results that offer further insight.
",1,0,0,1,0,0
18943,Four revolutions in physics and the second quantum revolution -- a unification of force and matter by quantum information,"  Newton's mechanical revolution unifies the motion of planets in the sky and
falling of apple on earth. Maxwell's electromagnetic revolution unifies
electricity, magnetism, and light. Einstein's relativity revolution unifies
space with time, and gravity with space-time distortion. The quantum revolution
unifies particle with waves, and energy with frequency. Each of those
revolution changes our world view. In this article, we will describe a
revolution that is happening now: the second quantum revolution which unifies
matter/space with information. In other words, the new world view suggests that
elementary particles (the bosonic force particles and fermionic matter
particles) all originated from quantum information (qubits): they are
collective excitations of an entangled qubit ocean that corresponds to our
space. The beautiful geometric Yang-Mills gauge theory and the strange Fermi
statistics of matter particles now have a common algebraic quantum
informational origin.
",0,1,0,0,0,0
20284,"Thermal transitions, pseudogap behavior and BCS-BEC crossover in Fermi-Fermi mixtures","  We study the mass imbalanced Fermi-Fermi mixture within the framework of a
two-dimensional lattice fermion model. Based on the thermodynamic and species
dependent quasiparticle behavior we map out the finite temperature phase
diagram of this system and show that unlike the balanced Fermi superfluid there
are now two different pseudogap regimes as PG-I and PG-II. While within the
PG-I regime both the fermionic species are pseudogapped, PG-II corresponds to
the regime where pseudogap feature survives only in the light species. We
believe that the single particle spectral features that we discuss in this
paper are observable through the species resolved radio frequency spectroscopy
and momentum resolved photo emission spectroscopy measurements on systems such
as, 6$_{Li}$-40$_{K}$ mixture. We further investigate the interplay between the
population and mass imbalances and report that at a fixed population imbalance
the BCS-BEC crossover in a Fermi-Fermi mixture would require a critical
interaction (U$_{c}$), for the realization of the uniform superfluid state. The
effect of imbalance in mass on the exotic Fulde-Ferrell-Larkin-Ovchinnikov
(FFLO) superfluid phase has been probed in detail in terms of the thermodynamic
and quasiparticle behavior of this phase. It has been observed that in spite of
the s-wave symmetry of the pairing field a nodal superfluid gap is realized in
the LO regime. Our results on the various thermal scales and regimes are
expected to serve as benchmarks for the experimental observations on
6$_{Li}$-40$_{K}$ mixture.
",0,1,0,0,0,0
3809,"LDPC Code Design for Distributed Storage: Balancing Repair Bandwidth, Reliability and Storage Overhead","  Distributed storage systems suffer from significant repair traffic generated
due to frequent storage node failures. This paper shows that properly designed
low-density parity-check (LDPC) codes can substantially reduce the amount of
required block downloads for repair thanks to the sparse nature of their factor
graph representation. In particular, with a careful construction of the factor
graph, both low repair-bandwidth and high reliability can be achieved for a
given code rate. First, a formula for the average repair bandwidth of LDPC
codes is developed. This formula is then used to establish that the minimum
repair bandwidth can be achieved by forcing a regular check node degree in the
factor graph. Moreover, it is shown that given a fixed code rate, the variable
node degree should also be regular to yield minimum repair bandwidth, under
some reasonable minimum variable node degree constraint. It is also shown that
for a given repair-bandwidth requirement, LDPC codes can yield substantially
higher reliability than currently utilized Reed-Solomon (RS) codes. Our
reliability analysis is based on a formulation of the general equation for the
mean-time-to-data-loss (MTTDL) associated with LDPC codes. The formulation
reveals that the stopping number is closely related to the MTTDL. It is further
shown that LDPC codes can be designed such that a small loss of
repair-bandwidth optimality may be traded for a large improvement in
erasure-correction capability and thus the MTTDL.
",1,0,0,0,0,0
2901,Emission line galaxies behind the planetary nebula IC 5148: Potential for a serendipity survey with archival data,"  During the start of a survey program using FORS2 long slit spectroscopy on
planetary nebulae (PN) and their haloes, we serendipitously discovered six
background emission line galaxies (ELG) with redshifts of z = 0.2057, 0.3137,
0.37281, 0.4939, 0.7424 and 0.8668. Thus they clearly do not belong to a common
cluster structure. We derived the major physical properties of the targets.
Since the used long slit covers a sky area of only 570 arcsec^2, we discuss
further potential of serendipitous discoveries in archival data, beside the
deep systematic work of the ongoing and upcoming big surveys. We conclude that
archival data provide a decent potential for extending the overall data on ELGs
without any selection bias.
",0,1,0,0,0,0
2816,Single Molecule Studies Under Constant Force Using Model Based Robust Control Design,"  Optical tweezers have enabled important insights into intracellular transport
through the investigation of motor proteins, with their ability to manipulate
particles at the microscale, affording femto Newton force resolution. Its use
to realize a constant force clamp has enabled vital insights into the behavior
of motor proteins under different load conditions. However, the varying nature
of disturbances and the effect of thermal noise pose key challenges to force
regulation. Furthermore, often the main aim of many studies is to determine the
motion of the motor and the statistics related to the motion, which can be at
odds with the force regulation objective. In this article, we propose a mixed
objective H2-Hinfinity optimization framework using a model-based design, that
achieves the dual goals of force regulation and real time motion estimation
with quantifiable guarantees. Here, we minimize the Hinfinity norm for the
force regulation and error in step estimation while maintaining the H2 norm of
the noise on step estimate within user specified bounds. We demonstrate the
efficacy of the framework through extensive simulations and an experimental
implementation using an optical tweezer setup with live samples of the motor
protein kinesin; where regulation of forces below 1 pico Newton with errors
below 10 percent is obtained while simultaneously providing real time estimates
of motor motion.
",0,1,1,0,0,0
1367,Learning to Acquire Information,"  We consider the problem of diagnosis where a set of simple observations are
used to infer a potentially complex hidden hypothesis. Finding the optimal
subset of observations is intractable in general, thus we focus on the problem
of active diagnosis, where the agent selects the next most-informative
observation based on the results of previous observations. We show that under
the assumption of uniform observation entropy, one can build an implication
model which directly predicts the outcome of the potential next observation
conditioned on the results of past observations, and selects the observation
with the maximum entropy. This approach enjoys reduced computation complexity
by bypassing the complicated hypothesis space, and can be trained on
observation data alone, learning how to query without knowledge of the hidden
hypothesis.
",1,0,0,1,0,0
18547,Double-slit Fraunhofer pattern as the signature of the Josephson effect between Berezinskii superconductors through the ferromagnetic vortex,"  I apply the recently developed formalism of generalized quasiclassical theory
to show that using hybrid superconducting systems with non-collinear strong
ferromagnets one can realize the Josephson junction between Berezinskii-type
superconductors. The reported calculation reproduces main features observed in
the recent experiment, namely the the slightly asymmetric double-slit
Fraunhofer interference pattern of the Josephson current through the
ferromagnetic vortex. The double-slit structure results from the spatially
inhomogeneous Berezinskii state with the amplitude controlled by the local
angle between magnetic moments in two ferromagnetic layers. The critical
current asymmetry by the sign of magnetic field can signal the presence of
spontaneous supercurrents generated by the non-coplanar magnetic texture near
the core of the ferromagnetic vortex core. I demonstrate that ferromagnetic
vortex can induce spontaneous vorticity in the odd-frequency order parameter
manifesting the possibility of the emergent magnetic field to create
topological defects.
",0,1,0,0,0,0
14764,A unified treatment of multiple testing with prior knowledge using the p-filter,"  There is a significant literature on methods for incorporating knowledge into
multiple testing procedures so as to improve their power and precision. Some
common forms of prior knowledge include (a) beliefs about which hypotheses are
null, modeled by non-uniform prior weights; (b) differing importances of
hypotheses, modeled by differing penalties for false discoveries; (c) multiple
arbitrary partitions of the hypotheses into (possibly overlapping) groups; and
(d) knowledge of independence, positive or arbitrary dependence between
hypotheses or groups, suggesting the use of more aggressive or conservative
procedures. We present a unified algorithmic framework called p-filter for
global null testing and false discovery rate (FDR) control that allows the
scientist to incorporate all four types of prior knowledge (a)-(d)
simultaneously, recovering a variety of known algorithms as special cases.
",0,0,1,1,0,0
20191,Generic Dynamical Phase Transition in One-Dimensional Bulk-Driven Lattice Gases with Exclusion,"  Dynamical phase transitions are crucial features of the fluctuations of
statistical systems, corresponding to boundaries between qualitatively
different mechanisms of maintaining unlikely values of dynamical observables
over long periods of time. They manifest themselves in the form of
non-analyticities in the large deviation function of those observables. In this
paper, we look at bulk-driven exclusion processes with open boundaries. It is
known that the standard asymmetric simple exclusion process exhibits a
dynamical phase transition in the large deviations of the current of particles
flowing through it. That phase transition has been described thanks to specific
calculation methods relying on the model being exactly solvable, but more
general methods have also been used to describe the extreme large deviations of
that current, far from the phase transition. We extend those methods to a large
class of models based on the ASEP, where we add arbitrary spatial
inhomogeneities in the rates and short-range potentials between the particles.
We show that, as for the regular ASEP, the large deviation function of the
current scales differently with the size of the system if one considers very
high or very low currents, pointing to the existence of a dynamical phase
transition between those two regimes: high current large deviations are
extensive in the system size, and the typical states associated to them are
Coulomb gases, which are correlated ; low current large deviations do not
depend on the system size, and the typical states associated to them are
anti-shocks, consistently with a hydrodynamic behaviour. Finally, we illustrate
our results numerically on a simple example, and we interpret the transition in
terms of the current pushing beyond its maximal hydrodynamic value, as well as
relate it to the appearance of Tracy-Widom distributions in the relaxation
statistics of such models.
",0,1,0,0,0,0
19831,3D printable multimaterial cellular auxetics with tunable stiffness,"  Auxetic materials are a novel class of mechanical metamaterials which exhibit
an interesting property of negative Poisson ratio by virtue of their
architecture rather than composition. It has been well established that a wide
range of negative Poisson ratio can be obtained by varying the geometry and
architecture of the cellular materials. However, the limited range of stiffness
values obtained from a given geometry restricts their applications. Research
trials have revealed that multi-material cellular designs have the capability
to generate range of stiffness values as per the requirement of application.
With the advancements in 3D printing, multi-material cellular designs can be
realized in practice. In this work, multi-material cellular designs are
investigated using finite element method. It was observed that introduction of
material gradient/distribution in the cell provides a means to tune cellular
stiffness as per the specific requirement. These results will aid in the design
of wearable auxetic impact protection devices which rely on stiffness gradients
and variable auxeticity.
",0,1,0,0,0,0
8562,DeepDownscale: a Deep Learning Strategy for High-Resolution Weather Forecast,"  Running high-resolution physical models is computationally expensive and
essential for many disciplines. Agriculture, transportation, and energy are
sectors that depend on high-resolution weather models, which typically consume
many hours of large High Performance Computing (HPC) systems to deliver timely
results. Many users cannot afford to run the desired resolution and are forced
to use low resolution output. One simple solution is to interpolate results for
visualization. It is also possible to combine an ensemble of low resolution
models to obtain a better prediction. However, these approaches fail to capture
the redundant information and patterns in the low-resolution input that could
help improve the quality of prediction. In this paper, we propose and evaluate
a strategy based on a deep neural network to learn a high-resolution
representation from low-resolution predictions using weather forecast as a
practical use case. We take a supervised learning approach, since obtaining
labeled data can be done automatically. Our results show significant
improvement when compared with standard practices and the strategy is still
lightweight enough to run on modest computer systems.
",0,0,0,1,0,0
16658,The VLA-COSMOS 3 GHz Large Project: Continuum data and source catalog release,"  We present the VLA-COSMOS 3 GHz Large Project based on 384 hours of
observations with the Karl G. Jansky Very Large Array (VLA) at 3 GHz (10 cm)
toward the two square degree Cosmic Evolution Survey (COSMOS) field. The final
mosaic reaches a median rms of 2.3 uJy/beam over the two square degrees at an
angular resolution of 0.75"". To fully account for the spectral shape and
resolution variations across the broad (2 GHz) band, we image all data with a
multiscale, multifrequency synthesis algorithm. We present a catalog of 10,830
radio sources down to 5 sigma, out of which 67 are combined from multiple
components. Comparing the positions of our 3 GHz sources with those from the
Very Long Baseline Array (VLBA)-COSMOS survey, we estimate that the astrometry
is accurate to 0.01"" at the bright end (signal-to-noise ratio, S/N_3GHz > 20).
Survival analysis on our data combined with the VLA-COSMOS 1.4~GHz Joint
Project catalog yields an expected median radio spectral index of alpha=-0.7.
We compute completeness corrections via Monte Carlo simulations to derive the
corrected 3 GHz source counts. Our counts are in agreement with previously
derived 3 GHz counts based on single-pointing (0.087 square degrees) VLA data.
In summary, the VLA-COSMOS 3 GHz Large Project simultaneously provides the
largest and deepest radio continuum survey at high (0.75"") angular resolution
to date, bridging the gap between last-generation and next-generation surveys.
",0,1,0,0,0,0
5752,Novel paradigms for advanced distribution grid energy management,"  The electricity distribution grid was not designed to cope with load dynamics
imposed by high penetration of electric vehicles, neither to deal with the
increasing deployment of distributed Renewable Energy Sources. Distribution
System Operators (DSO) will increasingly rely on flexible Distributed Energy
Resources (flexible loads, controllable generation and storage) to keep the
grid stable and to ensure quality of supply. In order to properly integrate
demand-side flexibility, DSOs need new energy management architectures, capable
of fostering collaboration with wholesale market actors and pro-sumers. We
propose the creation of Virtual Distribution Grids (VDG) over a common physical
infrastructure , to cope with heterogeneity of resources and actors, and with
the increasing complexity of distribution grid management and related resources
allocation problems. Focusing on residential VDG, we propose an agent-based
hierarchical architecture for providing Demand-Side Management services through
a market-based approach, where households transact their surplus/lack of energy
and their flexibility with neighbours, aggregators, utilities and DSOs. For
implementing the overall solution, we consider fine-grained control of smart
homes based on Inter-net of Things technology. Homes seamlessly transact
self-enforcing smart contracts over a blockchain-based generic platform.
Finally, we extend the architecture to solve existing problems on smart home
control, beyond energy management.
",1,0,0,0,0,0
15090,"Supporting Crowd-Powered Science in Economics: FRACTI, a Conceptual Framework for Large-Scale Collaboration and Transparent Investigation in Financial Markets","  Modern investigation in economics and in other sciences requires the ability
to store, share, and replicate results and methods of experiments that are
often multidisciplinary and yield a massive amount of data. Given the
increasing complexity and growing interaction across diverse bodies of
knowledge it is becoming imperative to define a platform to properly support
collaborative research and track origin, accuracy and use of data. This paper
starts by defining a set of methods leveraging scientific principles and
advocating the importance of those methods in multidisciplinary, computer
intensive fields like computational finance. The next part of this paper
defines a class of systems called scientific support systems, vis-a-vis usages
in other research fields such as bioinformatics, physics and engineering. We
outline a basic set of fundamental concepts, and list our goals and motivation
for leveraging such systems to enable large-scale investigation, ""crowd powered
science"", in economics. The core of this paper provides an outline of FRACTI in
five steps. First we present definitions related to scientific support systems
intrinsic to finance and describe common characteristics of financial use
cases. The second step concentrates on what can be exchanged through the
definition of shareable entities called contributions. The third step is the
description of a classification system for building blocks of the conceptual
framework, called facets. The fourth step introduces the meta-model that will
enable provenance tracking and representation of data fragments and simulation.
Finally we describe intended cases of use to highlight main strengths of
FRACTI: application of the scientific method for investigation in computational
finance, large-scale collaboration and simulation.
",0,0,0,0,0,1
12606,Principal Component Analysis for Functional Data on Riemannian Manifolds and Spheres,"  Functional data analysis on nonlinear manifolds has drawn recent interest.
Sphere-valued functional data, which are encountered for example as movement
trajectories on the surface of the earth, are an important special case. We
consider an intrinsic principal component analysis for smooth Riemannian
manifold-valued functional data and study its asymptotic properties. Riemannian
functional principal component analysis (RFPCA) is carried out by first mapping
the manifold-valued data through Riemannian logarithm maps to tangent spaces
around the time-varying Fréchet mean function, and then performing a
classical multivariate functional principal component analysis on the linear
tangent spaces. Representations of the Riemannian manifold-valued functions and
the eigenfunctions on the original manifold are then obtained with exponential
maps. The tangent-space approximation through functional principal component
analysis is shown to be well-behaved in terms of controlling the residual
variation if the Riemannian manifold has nonnegative curvature. Specifically,
we derive a central limit theorem for the mean function, as well as root-$n$
uniform convergence rates for other model components, including the covariance
function, eigenfunctions, and functional principal component scores. Our
applications include a novel framework for the analysis of longitudinal
compositional data, achieved by mapping longitudinal compositional data to
trajectories on the sphere, illustrated with longitudinal fruit fly behavior
patterns. RFPCA is shown to be superior in terms of trajectory recovery in
comparison to an unrestricted functional principal component analysis in
applications and simulations and is also found to produce principal component
scores that are better predictors for classification compared to traditional
functional functional principal component scores.
",0,0,1,1,0,0
13840,"AdaGrad stepsizes: Sharp convergence over nonconvex landscapes, from any initialization","  Adaptive gradient methods such as AdaGrad and its variants update the
stepsize in stochastic gradient descent on the fly according to the gradients
received along the way; such methods have gained widespread use in large-scale
optimization for their ability to converge robustly, without the need to fine
tune parameters such as the stepsize schedule. Yet, the theoretical guarantees
to date for AdaGrad are for online and convex optimization, which is quite
different from the offline and nonconvex setting where adaptive gradient
methods shine in practice. We bridge this gap by providing strong theoretical
guarantees in batch and stochastic setting, for the convergence of AdaGrad over
smooth, nonconvex landscapes, from any initialization of the stepsize, without
knowledge of Lipschitz constant of the gradient. We show in the stochastic
setting that AdaGrad converges to a stationary point at the optimal
$O(1/\sqrt{N})$ rate (up to a $\log(N)$ factor), and in the batch setting, at
the optimal $O(1/N)$ rate. Moreover, in both settings, the constant in the rate
matches the constant obtained as if the variance of the gradient noise and
Lipschitz constant of the gradient were known in advance and used to tune the
stepsize, up to a logarithmic factor of the mismatch between the optimal
stepsize and the stepsize used to initialize AdaGrad. In particular, our
results imply that AdaGrad is robust to both the unknown Lipschitz constant and
level of stochastic noise on the gradient, in a near-optimal sense. When there
is noise, AdaGrad converges at the rate of $O(1/\sqrt{N})$ with well-tuned
stepsize, and when there is not noise, the same algorithm converges at the rate
of $O(1/N)$ like well-tuned batch gradient descent.
",0,0,0,1,0,0
9010,Interpretable Neural Networks for Predicting Mortality Risk using Multi-modal Electronic Health Records,"  We present an interpretable neural network for predicting an important
clinical outcome (1-year mortality) from multi-modal Electronic Health Record
(EHR) data. Our approach builds on prior multi-modal machine learning models by
now enabling visualization of how individual factors contribute to the overall
outcome risk, assuming other factors remain constant, which was previously
impossible.
We demonstrate the value of this approach using a large multi-modal clinical
dataset including both EHR data and 31,278 echocardiographic videos of the
heart from 26,793 patients. We generated separate models for (i) clinical data
only (CD) (e.g. age, sex, diagnoses and laboratory values), (ii) numeric
variables derived from the videos, which we call echocardiography-derived
measures (EDM), and (iii) CD+EDM+raw videos (pixel data). The interpretable
multi-modal model maintained performance compared to non-interpretable models
(Random Forest, XGBoost), and also performed significantly better than a model
using a single modality (average AUC=0.82). Clinically relevant insights and
multi-modal variable importance rankings were also facilitated by the new
model, which have previously been impossible.
",1,0,0,1,0,0
11026,An advanced active quenching circuit for ultra-fast quantum cryptography,"  Commercial photon-counting modules based on actively quenched solid-state
avalanche photodiode sensors are used in a wide variety of applications.
Manufacturers characterize their detectors by specifying a small set of
parameters, such as detection efficiency, dead time, dark counts rate,
afterpulsing probability and single-photon arrival-time resolution (jitter).
However, they usually do not specify the range of conditions over which these
parameters are constant or present a sufficient description of the
characterization process. In this work, we perform a few novel tests on two
commercial detectors and identify an additional set of imperfections that must
be specified to sufficiently characterize their behavior. These include
rate-dependence of the dead time and jitter, detection delay shift, and
""twilighting."" We find that these additional non-ideal behaviors can lead to
unexpected effects or strong deterioration of the performance of a system using
these devices. We explain their origin by an in-depth analysis of the active
quenching process. To mitigate the effects of these imperfections, a
custom-built detection system is designed using a novel active quenching
circuit. Its performance is compared against two commercial detectors in a fast
quantum key distribution system with hyper-entangled photons and a random
number generator.
",1,1,0,0,0,0
5223,Generative Temporal Models with Memory,"  We consider the general problem of modeling temporal data with long-range
dependencies, wherein new observations are fully or partially predictable based
on temporally-distant, past observations. A sufficiently powerful temporal
model should separate predictable elements of the sequence from unpredictable
elements, express uncertainty about those unpredictable elements, and rapidly
identify novel elements that may help to predict the future. To create such
models, we introduce Generative Temporal Models augmented with external memory
systems. They are developed within the variational inference framework, which
provides both a practical training methodology and methods to gain insight into
the models' operation. We show, on a range of problems with sparse, long-term
temporal dependencies, that these models store information from early in a
sequence, and reuse this stored information efficiently. This allows them to
perform substantially better than existing models based on well-known recurrent
neural networks, like LSTMs.
",1,0,0,1,0,0
7148,Steganographic Generative Adversarial Networks,"  Steganography is collection of methods to hide secret information (""payload"")
within non-secret information (""container""). Its counterpart, Steganalysis, is
the practice of determining if a message contains a hidden payload, and
recovering it if possible. Presence of hidden payloads is typically detected by
a binary classifier. In the present study, we propose a new model for
generating image-like containers based on Deep Convolutional Generative
Adversarial Networks (DCGAN). This approach allows to generate more
setganalysis-secure message embedding using standard steganography algorithms.
Experiment results demonstrate that the new model successfully deceives the
steganography analyzer, and for this reason, can be used in steganographic
applications.
",1,0,0,1,0,0
773,SLAM-Assisted Coverage Path Planning for Indoor LiDAR Mapping Systems,"  Applications involving autonomous navigation and planning of mobile agents
can benefit greatly by employing online Simultaneous Localization and Mapping
(SLAM) techniques, however, their proper implementation still warrants an
efficient amalgamation with any offline path planning method that may be used
for the particular application. In this paper, such a case of amalgamation is
considered for a LiDAR-based indoor mapping system which presents itself as a
2D coverage path planning problem implemented along with online SLAM. This
paper shows how classic offline Coverage Path Planning (CPP) can be altered for
use with online SLAM by proposing two modifications: (i) performing convex
decomposition of the polygonal coverage area to allow for an arbitrary choice
of an initial point while still tracing the shortest coverage path and (ii)
using a new approach to stitch together the different cells within the
polygonal area to form a continuous coverage path. Furthermore, an alteration
to the SLAM operation to suit the coverage path planning strategy is also made
that evaluates navigation errors in terms of an area coverage cost function.
The implementation results show how the combination of the two modified offline
and online planning strategies allow for an improvement in the total area
coverage by the mapping system - the modification thus presents an approach for
modifying offline and online navigation strategies for robust operation.
",1,0,0,0,0,0
15535,Long-Term Evolution of Genetic Programming Populations,"  We evolve binary mux-6 trees for up to 100000 generations evolving some
programs with more than a hundred million nodes. Our unbounded Long-Term
Evolution Experiment LTEE GP appears not to evolve building blocks but does
suggests a limit to bloat. We do see periods of tens even hundreds of
generations where the population is 100 percent functionally converged. The
distribution of tree sizes is not as predicted by theory.
",1,0,0,0,0,0
12150,A Note on Exponential Inequalities in Hilbert Spaces for Spatial Processes with Applications to the Functional Kernel Regression Model,"  In this manuscript we present exponential inequalities for spatial lattice
processes which take values in a separable Hilbert space and satisfy certain
dependence conditions. We consider two types of dependence: spatial data under
$\alpha$-mixing conditions and spatial data which satisfies a weak dependence
condition introduced by Dedecker and Prieur [2005]. We demonstrate their
usefulness in the functional kernel regression model of Ferraty and Vieu [2004]
where we study uniform consistency properties of the estimated regression
operator on increasing subsets of the underlying function space.
",0,0,1,1,0,0
505,Checklists to Support Test Charter Design in Exploratory Testing,"  During exploratory testing sessions the tester simultaneously learns, designs
and executes tests. The activity is iterative and utilizes the skills of the
tester and provides flexibility and creativity.Test charters are used as a
vehicle to support the testers during the testing. The aim of this study is to
support practitioners in the design of test charters through checklists. We
aimed to identify factors allowing practitioners to critically reflect on their
designs and contents of test charters to support practitioners in making
informed decisions of what to include in test charters. The factors and
contents have been elicited through interviews. Overall, 30 factors and 35
content elements have been elicited.
",1,0,0,0,0,0
6817,Stack Overflow: A Code Laundering Platform?,"  Developers use Question and Answer (Q&A) websites to exchange knowledge and
expertise. Stack Overflow is a popular Q&A website where developers discuss
coding problems and share code examples. Although all Stack Overflow posts are
free to access, code examples on Stack Overflow are governed by the Creative
Commons Attribute-ShareAlike 3.0 Unported license that developers should obey
when reusing code from Stack Overflow or posting code to Stack Overflow. In
this paper, we conduct a case study with 399 Android apps, to investigate
whether developers respect license terms when reusing code from Stack Overflow
posts (and the other way around). We found 232 code snippets in 62 Android apps
from our dataset that were potentially reused from Stack Overflow, and 1,226
Stack Overflow posts containing code examples that are clones of code released
in 68 Android apps, suggesting that developers may have copied the code of
these apps to answer Stack Overflow questions. We investigated the licenses of
these pieces of code and observed 1,279 cases of potential license violations
(related to code posting to Stack overflow or code reuse from Stack overflow).
This paper aims to raise the awareness of the software engineering community
about potential unethical code reuse activities taking place on Q&A websites
like Stack Overflow.
",1,0,0,0,0,0
1908,Multi-Erasure Locally Recoverable Codes Over Small Fields For Flash Memory Array,"  Erasure codes play an important role in storage systems to prevent data loss.
In this work, we study a class of erasure codes called Multi-Erasure Locally
Recoverable Codes (ME-LRCs) for flash memory array. Compared to previous
related works, we focus on the construction of ME-LRCs over small fields. We
first develop upper and lower bounds on the minimum distance of ME-LRCs. These
bounds explicitly take the field size into account. Our main contribution is to
propose a general construction of ME-LRCs based on generalized tensor product
codes, and study their erasure-correcting property. A decoding algorithm
tailored for erasure recovery is given. We then prove that our construction
yields optimal ME-LRCs with a wide range of code parameters. Finally, we
present several families of ME-LRCs over different fields.
",1,0,0,0,0,0
400,On vector measures and extensions of transfunctions,"  We are interested in extending operators defined on positive measures, called
here transfunctions, to signed measures and vector measures. Our methods use a
somewhat nonstandard approach to measures and vector measures. The necessary
background, including proofs of some auxiliary results, is included.
",0,0,1,0,0,0
4028,Nichols Algebras and Quantum Principal Bundles,"  A general procedure for constructing Yetter-Drinfeld modules from quantum
principal bundles is introduced. As an application a Yetter-Drinfeld structure
is put on the cotangent space of the Heckenberger-Kolb calculi of the quantum
Grassmannians. For the special case of quantum projective space the associated
braiding is shown to be non-diagonal and of Hecke type. Moreover, its Nichols
algebra is shown to be finite-dimensional and equal to the anti-holomorphic
part of the total differential calculus.
",0,0,1,0,0,0
8964,Lower Bounds on the Complexity of Solving Two Classes of Non-cooperative Games,"  This paper studies the complexity of solving two classes of non-cooperative
games in a distributed manner in which the players communicate with a set of
system nodes over noisy communication channels. The complexity of solving each
game class is defined as the minimum number of iterations required to find a
Nash equilibrium (NE) of any game in that class with $\epsilon$ accuracy.
First, we consider the class $\mathcal{G}$ of all $N$-player non-cooperative
games with a continuous action space that admit at least one NE. Using
information-theoretic inequalities, we derive a lower bound on the complexity
of solving $\mathcal{G}$ that depends on the Kolmogorov $2\epsilon$-capacity of
the constraint set and the total capacity of the communication channels. We
also derive a lower bound on the complexity of solving games in $\mathcal{G}$
which depends on the volume and surface area of the constraint set. We next
consider the class of all $N$-player non-cooperative games with at least one NE
such that the players' utility functions satisfy a certain (differential)
constraint. We derive lower bounds on the complexity of solving this game class
under both Gaussian and non-Gaussian noise models. Our result in the
non-Gaussian case is derived by establishing a connection between the
Kullback-Leibler distance and Fisher information.
",1,0,0,0,0,0
13224,Median statistics estimates of Hubble and Newton's Constant,"  Robustness of any statistics depends upon the number of assumptions it makes
about the measured data. We point out the advantages of median statistics using
toy numerical experiments and demonstrate its robustness, when the number of
assumptions we can make about the data are limited. We then apply the median
statistics technique to obtain estimates of two constants of nature, Hubble
Constant ($H_0$) and Newton's Gravitational Constant($G$), both of which show
significant differences between different measurements. For $H_0$, we update
the analysis done by Chen and Ratra (2011) and Gott et al. (2001) using $576$
measurements. We find after grouping the different results according to their
primary type of measurement, the median estimates are given by
$H_0=72.5^{+2.5}_{-8}$ km/sec/Mpc with errors corresponding to 95% c.l.
(2$\sigma$) and $G=6.674702^{+0.0014}_{-0.0009} \times 10^{-11} \mathrm{N
m^{2}kg^{-2}}$ corresponding to 68% c.l. (1$\sigma$).
",0,1,0,0,0,0
14742,Radon Transform for Sheaves,"  We define the Radon transform functor for sheaves and prove that it is an
equivalence after suitable microlocal localizations. As a result, the sheaf
category associated to a Legendrian is invariant under the Radon transform. We
also manage to place the Radon transform and other transforms in microlocal
sheaf theory altogether in a diagram.
",0,0,1,0,0,0
3894,Learning Policies for Markov Decision Processes from Data,"  We consider the problem of learning a policy for a Markov decision process
consistent with data captured on the state-actions pairs followed by the
policy. We assume that the policy belongs to a class of parameterized policies
which are defined using features associated with the state-action pairs. The
features are known a priori, however, only an unknown subset of them could be
relevant. The policy parameters that correspond to an observed target policy
are recovered using $\ell_1$-regularized logistic regression that best fits the
observed state-action samples. We establish bounds on the difference between
the average reward of the estimated and the original policy (regret) in terms
of the generalization error and the ergodic coefficient of the underlying
Markov chain. To that end, we combine sample complexity theory and sensitivity
analysis of the stationary distribution of Markov chains. Our analysis suggests
that to achieve regret within order $O(\sqrt{\epsilon})$, it suffices to use
training sample size on the order of $\Omega(\log n \cdot poly(1/\epsilon))$,
where $n$ is the number of the features. We demonstrate the effectiveness of
our method on a synthetic robot navigation example.
",1,0,1,1,0,0
8727,The Importance of System-Level Information in Multiagent Systems Design: Cardinality and Covering Problems,"  A fundamental challenge in multiagent systems is to design local control
algorithms to ensure a desirable collective behaviour. The information
available to the agents, gathered either through communication or sensing,
naturally restricts the achievable performance. Hence, it is fundamental to
identify what piece of information is valuable and can be exploited to design
control laws with enhanced performance guarantees. This paper studies the case
when such information is uncertain or inaccessible for a class of submodular
resource allocation problems termed covering problems. In the first part of
this work we pinpoint a fundamental risk-reward tradeoff faced by the system
operator when conditioning the control design on a valuable but uncertain piece
of information, which we refer to as the cardinality, that represents the
maximum number of agents that can simultaneously select any given resource.
Building on this analysis, we propose a distributed algorithm that allows
agents to learn the cardinality while adjusting their behaviour over time. This
algorithm is proved to perform on par or better to the optimal design obtained
when the exact cardinality is known a priori.
",1,0,0,0,0,0
3017,Derived Picard groups of preprojective algebras of Dynkin type,"  In this paper, we study two-sided tilting complexes of preprojective algebras
of Dynkin type. We construct the most fundamental class of two-sided tilting
complexes, which has a group structure by derived tensor products and induces a
group of auto-equivalences of the derived category. We show that the group
structure of the two-sided tilting complexes is isomorphic to the braid group
of the corresponding folded graph. Moreover we show that these two-sided
tilting complexes induce tilting mutation and any tilting complex is given as
the derived tensor products of them. Using these results, we determine the
derived Picard group of preprojective algebras for type $A$ and $D$.
",0,0,1,0,0,0
19174,Development of non-modal shear induced instabilities in atmospheric tornadoes,"  In this paper we consider the role of nonmodal instabilities in the dynamics
of atmospheric tornadoes. For this purpose we consider the Euler equation,
continuity equation and the equation of state and linearise them. As an example
we study several different velocity profiles: the so-called Rankine vortex
model; the Burgers-Rott vortex model; Sullivan and modified Sullivan vortex
models. It has been shown that in the two dimensional Rankine vortex model no
instability appears in the inner region of a tornado. On the contrary, outside
this area the physical system undergoes strong exponential instability. We have
found that initially perturbed velocity components lead to amplified sound wave
excitations. The similar results have been shown in Burgers-Rott vortex model
as well. As it was numerically estimated, in this case, the unstable wave
increases its energy by a factor of $400$ only in $\sim 0.5$min. According to
the numerical study, in Sullivan and modified Sullivan models, the instability
does not differ much by the growth. Despite the fact that in the inner area the
exponential instability does not appear in a purely two dimensional case, we
have found that in the modified Sullivan vortex even a small contribution from
vertical velocities can drive unstable nonmodal waves.
",0,1,0,0,0,0
1102,Measuring the polarization of electromagnetic fields using Rabi-rate measurements with spatial resolution: experiment and theory,"  When internal states of atoms are manipulated using coherent optical or
radio-frequency (RF) radiation, it is essential to know the polarization of the
radiation with respect to the quantization axis of the atom. We first present a
measurement of the two-dimensional spatial distribution of the electric-field
amplitude of a linearly-polarized pulsed RF electric field at $\sim 25.6\,$GHz
and its angle with respect to a static electric field. The measurements exploit
coherent population transfer between the $35$s and $35$p Rydberg states of
helium atoms in a pulsed supersonic beam. Based on this experimental result, we
develop a general framework in the form of a set of equations relating the five
independent polarization parameters of a coherently oscillating field in a
fixed laboratory frame to Rabi rates of transitions between a ground and three
excited states of an atom with arbitrary quantization axis. We then explain how
these equations can be used to fully characterize the polarization in a minimum
of five Rabi rate measurements by rotation of an external bias-field, or,
knowing the polarization of the driving field, to determine the orientation of
the static field using two measurements. The presented technique is not limited
to Rydberg atoms and RF fields but can also be applied to characterize optical
fields. The technique has potential for sensing the spatiotemporal properties
of electromagnetic fields, e.g., in metrology devices or in hybrid experiments
involving atoms close to surfaces.
",0,1,0,0,0,0
19661,Efficient Modelling & Forecasting with range based volatility models and application,"  This paper considers an alternative method for fitting CARR models using
combined estimating functions (CEF) by showing its usefulness in applications
in economics and quantitative finance. The associated information matrix for
corresponding new estimates is derived to calculate the standard errors. A
simulation study is carried out to demonstrate its superiority relative to
other two competitors: linear estimating functions (LEF) and the maximum
likelihood (ML). Results show that CEF estimates are more efficient than LEF
and ML estimates when the error distribution is mis-specified. Taking a real
data set from financial economics, we illustrate the usefulness and
applicability of the CEF method in practice and report reliable forecast values
to minimize the risk in the decision making process.
",0,0,1,1,0,0
3724,Using Social Network Information in Bayesian Truth Discovery,"  We investigate the problem of truth discovery based on opinions from multiple
agents who may be unreliable or biased. We consider the case where agents'
reliabilities or biases are correlated if they belong to the same community,
which defines a group of agents with similar opinions regarding a particular
event. An agent can belong to different communities for different events, and
these communities are unknown a priori. We incorporate knowledge of the agents'
social network in our truth discovery framework and develop Laplace variational
inference methods to estimate agents' reliabilities, communities, and the event
states. We also develop a stochastic variational inference method to scale our
model to large social networks. Simulations and experiments on real data
suggest that when observations are sparse, our proposed methods perform better
than several other inference methods, including majority voting, TruthFinder,
AccuSim, the Confidence-Aware Truth Discovery method, the Bayesian Classifier
Combination (BCC) method, and the Community BCC method.
",1,0,0,1,0,0
11596,Computing Human-Understandable Strategies,"  Algorithms for equilibrium computation generally make no attempt to ensure
that the computed strategies are understandable by humans. For instance the
strategies for the strongest poker agents are represented as massive binary
files. In many situations, we would like to compute strategies that can
actually be implemented by humans, who may have computational limitations and
may only be able to remember a small number of features or components of the
strategies that have been computed. We study poker games where private
information distributions can be arbitrary. We create a large training set of
game instances and solutions, by randomly selecting the information
probabilities, and present algorithms that learn from the training instances in
order to perform well in games with unseen information distributions. We are
able to conclude several new fundamental rules about poker strategy that can be
easily implemented by humans.
",0,0,0,1,0,0
8023,Quasi-Frobenius-splitting and lifting of Calabi-Yau varieties in characteristic $p$,"  Extending the notion of Frobenius-splitting, we prove that every finite
height Calabi-Yau variety defined over an algebraically closed field of
positive characteristic can be lifted to the ring of Witt vectors of length
two.
",0,0,1,0,0,0
20841,Wavelength Does Not Equal Pressure: Vertical Contribution Functions and their Implications for Mapping Hot Jupiters,"  Multi-band phase variations in principle allow us to infer the longitudinal
temperature distributions of planets as a function of height in their
atmospheres. For example, 3.6 micron emission originates from deeper layers of
the atmosphere than 4.5 micron due to greater water vapor absorption at the
longer wavelength. Since heat transport efficiency increases with pressure, we
expect thermal phase curves at 3.6 micron to exhibit smaller amplitudes and
greater phase offsets than at 4.5 micron; this trend is not observed. Of the
seven hot Jupiters with full-orbit phase curves at 3.6 and 4.5 micron, all have
greater phase amplitude at 3.6 micron than at 4.5 micron, while four of seven
exhibit a greater phase offset at 3.6 micron. We use a 3D
radiative-hydrodynamic model to calculate theoretical phase curves of HD
189733b, assuming thermo-chemical equilibrium. The model exhibits temperature,
pressure, and wavelength dependent opacity, primarily driven by carbon
chemistry: CO is energetically favored on the dayside, while CH4 is favored on
the cooler nightside. Infrared opacity therefore changes by orders of magnitude
between day and night, producing dramatic vertical shifts in the
wavelength-specific photospheres, which would complicate eclipse or phase
mapping with spectral data. The model predicts greater relative phase amplitude
and greater phase offset at 3.6 micron than at 4.5 micron, in agreement with
the data. Our model qualitatively explains the observed phase curves, but is in
tension with current thermo-chemical kinetics models that predict zonally
uniform atmospheric composition due to transport of CO from the hot regions of
the atmosphere.
",0,1,0,0,0,0
7000,Action-depedent Control Variates for Policy Optimization via Stein's Identity,"  Policy gradient methods have achieved remarkable successes in solving
challenging reinforcement learning problems. However, it still often suffers
from the large variance issue on policy gradient estimation, which leads to
poor sample efficiency during training. In this work, we propose a control
variate method to effectively reduce variance for policy gradient methods.
Motivated by the Stein's identity, our method extends the previous control
variate methods used in REINFORCE and advantage actor-critic by introducing
more general action-dependent baseline functions. Empirical studies show that
our method significantly improves the sample efficiency of the state-of-the-art
policy gradient approaches.
",1,0,0,1,0,0
16331,Tail sums of Wishart and GUE eigenvalues beyond the bulk edge,"  Consider the classical Gaussian unitary ensemble of size $N$ and the real
Wishart ensemble $W_N(n,I)$. In the limits as $N \to \infty$ and $N/n \to
\gamma > 0$, the expected number of eigenvalues that exit the upper bulk edge
is less than one, 0.031 and 0.170 respectively, the latter number being
independent of $\gamma$. These statements are consequences of quantitative
bounds on tail sums of eigenvalues outside the bulk which are established here
for applications in high dimensional covariance matrix estimation.
",0,0,1,1,0,0
9441,Variational integrators for anelastic and pseudo-incompressible flows,"  The anelastic and pseudo-incompressible equations are two well-known
soundproof approximations of compressible flows useful for both theoretical and
numerical analysis in meteorology, atmospheric science, and ocean studies. In
this paper, we derive and test structure-preserving numerical schemes for these
two systems. The derivations are based on a discrete version of the
Euler-Poincaré variational method. This approach relies on a finite
dimensional approximation of the (Lie) group of diffeomorphisms that preserve
weighted-volume forms. These weights describe the background stratification of
the fluid and correspond to the weighed velocity fields for anelastic and
pseudo-incompressible approximations. In particular, we identify to these
discrete Lie group configurations the associated Lie algebras such that
elements of the latter correspond to weighted velocity fields that satisfy the
divergence-free conditions for both systems. Defining discrete Lagrangians in
terms of these Lie algebras, the discrete equations follow by means of
variational principles. Descending from variational principles, the schemes
exhibit further a discrete version of Kelvin circulation theorem, are
applicable to irregular meshes, and show excellent long term energy behavior.
We illustrate the properties of the schemes by performing preliminary test
cases.
",0,1,1,0,0,0
886,Controlling Chiral Domain Walls in Antiferromagnets Using Spin-Wave Helicity,"  In antiferromagnets, the Dzyaloshinskii-Moriya interaction lifts the
degeneracy of left- and right-circularly polarized spin waves. This
relativistic coupling increases the efficiency of spin-wave-induced domain wall
motion and leads to higher drift velocities. We show that in biaxial
antiferromagnets, the spin-wave helicity controls both the direction and
magnitude of the magnonic force on chiral domain walls. By contrast, in
uniaxial antiferromagnets, the magnonic force is propulsive with a helicity
dependent strength.
",0,1,0,0,0,0
552,Learning Neural Models for End-to-End Clustering,"  We propose a novel end-to-end neural network architecture that, once trained,
directly outputs a probabilistic clustering of a batch of input examples in one
pass. It estimates a distribution over the number of clusters $k$, and for each
$1 \leq k \leq k_\mathrm{max}$, a distribution over the individual cluster
assignment for each data point. The network is trained in advance in a
supervised fashion on separate data to learn grouping by any perceptual
similarity criterion based on pairwise labels (same/different group). It can
then be applied to different data containing different groups. We demonstrate
promising performance on high-dimensional data like images (COIL-100) and
speech (TIMIT). We call this ``learning to cluster'' and show its conceptual
difference to deep metric learning, semi-supervise clustering and other related
approaches while having the advantage of performing learnable clustering fully
end-to-end.
",0,0,0,1,0,0
9612,Adaptive Information Gathering via Imitation Learning,"  In the adaptive information gathering problem, a policy is required to select
an informative sensing location using the history of measurements acquired thus
far. While there is an extensive amount of prior work investigating effective
practical approximations using variants of Shannon's entropy, the efficacy of
such policies heavily depends on the geometric distribution of objects in the
world. On the other hand, the principled approach of employing online POMDP
solvers is rendered impractical by the need to explicitly sample online from a
posterior distribution of world maps.
We present a novel data-driven imitation learning framework to efficiently
train information gathering policies. The policy imitates a clairvoyant oracle
- an oracle that at train time has full knowledge about the world map and can
compute maximally informative sensing locations. We analyze the learnt policy
by showing that offline imitation of a clairvoyant oracle is implicitly
equivalent to online oracle execution in conjunction with posterior sampling.
This observation allows us to obtain powerful near-optimality guarantees for
information gathering problems possessing an adaptive sub-modularity property.
As demonstrated on a spectrum of 2D and 3D exploration problems, the trained
policies enjoy the best of both worlds - they adapt to different world map
distributions while being computationally inexpensive to evaluate.
",1,0,0,0,0,0
4394,Application of Self-Play Reinforcement Learning to a Four-Player Game of Imperfect Information,"  We introduce a new virtual environment for simulating a card game known as
""Big 2"". This is a four-player game of imperfect information with a relatively
complicated action space (being allowed to play 1,2,3,4 or 5 card combinations
from an initial starting hand of 13 cards). As such it poses a challenge for
many current reinforcement learning methods. We then use the recently proposed
""Proximal Policy Optimization"" algorithm to train a deep neural network to play
the game, purely learning via self-play, and find that it is able to reach a
level which outperforms amateur human players after only a relatively short
amount of training time and without needing to search a tree of future game
states.
",0,0,0,1,0,0
16526,Synthesis versus analysis in patch-based image priors,"  In global models/priors (for example, using wavelet frames), there is a well
known analysis vs synthesis dichotomy in the way signal/image priors are
formulated. In patch-based image models/priors, this dichotomy is also present
in the choice of how each patch is modeled. This paper shows that there is
another analysis vs synthesis dichotomy, in terms of how the whole image is
related to the patches, and that all existing patch-based formulations that
provide a global image prior belong to the analysis category. We then propose a
synthesis formulation, where the image is explicitly modeled as being
synthesized by additively combining a collection of independent patches. We
formally establish that these analysis and synthesis formulations are not
equivalent in general and that both formulations are compatible with analysis
and synthesis formulations at the patch level. Finally, we present an instance
of the alternating direction method of multipliers (ADMM) that can be used to
perform image denoising under the proposed synthesis formulation, showing its
computational feasibility. Rather than showing the superiority of the synthesis
or analysis formulations, the contributions of this paper is to establish the
existence of both alternatives, thus closing the corresponding gap in the field
of patch-based image processing.
",1,0,0,0,0,0
16260,Investigation and Automating Extraction of Thumbnails Produced by Image viewers,"  Today, in digital forensics, images normally provide important information
within an investigation. However, not all images may still be available within
a forensic digital investigation as they were all deleted for example. Data
carving can be used in this case to retrieve deleted images but the carving
time is normally significant and these images can be moreover overwritten by
other data. One of the solutions is to look at thumbnails of images that are no
longer available. These thumbnails can often be found within databases created
by either operating systems or image viewers. In literature, most research and
practical focus on the extraction of thumbnails from databases created by the
operating system. There is a little research working on the thumbnails created
by the image reviewers as these thumbnails are application-driven in terms of
pre-defined sizes, adjustments and storage location. Eventually, thumbnail
databases from image viewers are significant forensic artefacts for
investigators as these programs deal with large amounts of images. However,
investigating these databases so far is still manual or semi-automatic task
that leads to the huge amount of forensic time. Therefore, in this paper we
propose a new approach of automating extraction of thumbnails produced by image
viewers. We also test our approach with popular image viewers in different
storage structures and locations to show its robustness.
",1,0,0,0,0,0
17251,Highrisk Prediction from Electronic Medical Records via Deep Attention Networks,"  Predicting highrisk vascular diseases is a significant issue in the medical
domain. Most predicting methods predict the prognosis of patients from
pathological and radiological measurements, which are expensive and require
much time to be analyzed. Here we propose deep attention models that predict
the onset of the high risky vascular disease from symbolic medical histories
sequence of hypertension patients such as ICD-10 and pharmacy codes only,
Medical History-based Prediction using Attention Network (MeHPAN). We
demonstrate two types of attention models based on 1) bidirectional gated
recurrent unit (R-MeHPAN) and 2) 1D convolutional multilayer model (C-MeHPAN).
Two MeHPAN models are evaluated on approximately 50,000 hypertension patients
with respect to precision, recall, f1-measure and area under the curve (AUC).
Experimental results show that our MeHPAN methods outperform standard
classification models. Comparing two MeHPANs, R-MeHPAN provides more better
discriminative capability with respect to all metrics while C-MeHPAN presents
much shorter training time with competitive accuracy.
",1,0,0,1,0,0
14285,MIDI-VAE: Modeling Dynamics and Instrumentation of Music with Applications to Style Transfer,"  We introduce MIDI-VAE, a neural network model based on Variational
Autoencoders that is capable of handling polyphonic music with multiple
instrument tracks, as well as modeling the dynamics of music by incorporating
note durations and velocities. We show that MIDI-VAE can perform style transfer
on symbolic music by automatically changing pitches, dynamics and instruments
of a music piece from, e.g., a Classical to a Jazz style. We evaluate the
efficacy of the style transfer by training separate style validation
classifiers. Our model can also interpolate between short pieces of music,
produce medleys and create mixtures of entire songs. The interpolations
smoothly change pitches, dynamics and instrumentation to create a harmonic
bridge between two music pieces. To the best of our knowledge, this work
represents the first successful attempt at applying neural style transfer to
complete musical compositions.
",1,0,0,0,0,0
5727,Applications of Trajectory Data from the Perspective of a Road Transportation Agency: Literature Review and Maryland Case Study,"  Transportation agencies have an opportunity to leverage
increasingly-available trajectory datasets to improve their analyses and
decision-making processes. However, this data is typically purchased from
vendors, which means agencies must understand its potential benefits beforehand
in order to properly assess its value relative to the cost of acquisition.
While the literature concerned with trajectory data is rich, it is naturally
fragmented and focused on technical contributions in niche areas, which makes
it difficult for government agencies to assess its value across different
transportation domains. To overcome this issue, the current paper explores
trajectory data from the perspective of a road transportation agency interested
in acquiring trajectories to enhance its analyses. The paper provides a
literature review illustrating applications of trajectory data in six areas of
road transportation systems analysis: demand estimation, modeling human
behavior, designing public transit, traffic performance measurement and
prediction, environment and safety. In addition, it visually explores 20
million GPS traces in Maryland, illustrating existing and suggesting new
applications of trajectory data.
",1,0,0,1,0,0
1171,Computing the projected reachable set of switched affine systems: an application to systems biology,"  A fundamental question in systems biology is what combinations of mean and
variance of the species present in a stochastic biochemical reaction network
are attainable by perturbing the system with an external signal. To address
this question, we show that the moments evolution in any generic network can be
either approximated or, under suitable assumptions, computed exactly as the
solution of a switched affine system. Motivated by this application, we propose
a new method to approximate the reachable set of switched affine systems. A
remarkable feature of our approach is that it allows one to easily compute
projections of the reachable set for pairs of moments of interest, without
requiring the computation of the full reachable set, which can be prohibitive
for large networks. As a second contribution, we also show how to select the
external signal in order to maximize the probability of reaching a target set.
To illustrate the method we study a renown model of controlled gene expression
and we derive estimates of the reachable set, for the protein mean and
variance, that are more accurate than those available in the literature and
consistent with experimental data.
",1,0,1,0,0,0
11122,On compact Hermitian manifolds with flat Gauduchon connections,"  Given a Hermitian manifold $(M^n,g)$, the Gauduchon connections are the one
parameter family of Hermitian connections joining the Chern connection and the
Bismut connection. We will call $\nabla^s = (1-\frac{s}{2})\nabla^c +
\frac{s}{2}\nabla^b$ the $s$-Gauduchon connection of $M$, where $\nabla^c$ and
$\nabla^b$ are respectively the Chern and Bismut connections. It is natural to
ask when a compact Hermitian manifold could admit a flat $s$-Gauduchon
connection. This is related to a question asked by Yau \cite{Yau}. The cases
with $s=0$ (a flat Chern connection) or $s=2$ (a flat Bismut connection) are
classified respectively by Boothby \cite{Boothby} in the 1950s or by Q. Wang
and the authors recently \cite{WYZ}. In this article, we observe that if either
$s\geq 4+2\sqrt{3} \approx 7.46$ or $s\leq 4-2\sqrt{3}\approx 0.54$ and $s\neq
0$, then $g$ is Kähler. We also show that, when $n=2$, $g$ is always Kähler
unless $s=2$. Note that non-Kähler compact Bismut flat surfaces are exactly
those isosceles Hopf surfaces by \cite{WYZ}.
",0,0,1,0,0,0
3663,A New Sparse and Robust Adaptive Lasso Estimator for the Independent Contamination Model,"  Many problems in signal processing require finding sparse solutions to
under-determined, or ill-conditioned, linear systems of equations. When dealing
with real-world data, the presence of outliers and impulsive noise must also be
accounted for. In past decades, the vast majority of robust linear regression
estimators has focused on robustness against rowwise contamination. Even so
called `high breakdown' estimators rely on the assumption that a majority of
rows of the regression matrix is not affected by outliers. Only very recently,
the first cellwise robust regression estimation methods have been developed. In
this paper, we define robust oracle properties, which an estimator must have in
order to perform robust model selection for under-determined, or
ill-conditioned linear regression models that are contaminated by cellwise
outliers in the regression matrix. We propose and analyze a robustly weighted
and adaptive Lasso type regularization term which takes into account cellwise
outliers for model selection. The proposed regularization term is integrated
into the objective function of the MM-estimator, which yields the proposed
MM-Robust Weighted Adaptive Lasso (MM-RWAL), for which we prove that at least
the weak robust oracle properties hold. A performance comparison to existing
robust Lasso estimators is provided using Monte Carlo experiments. Further, the
MM-RWAL is applied to determine the temporal releases of the European Tracer
Experiment (ETEX) at the source location. This ill-conditioned linear inverse
problem contains cellwise and rowwise outliers and is sparse both in the
regression matrix and the parameter vector. The proposed RWAL penalty is not
limited to the MM-estimator but can easily be integrated into the objective
function of other robust estimators.
",0,0,1,1,0,0
19199,Diagrammatic Monte-Carlo for weak-coupling expansion of non-Abelian lattice field theories: large-N U(N)xU(N) principal chiral model,"  We develop numerical tools for Diagrammatic Monte-Carlo simulations of
non-Abelian lattice field theories in the t'Hooft large-N limit based on the
weak-coupling expansion. First we note that the path integral measure of such
theories contributes a bare mass term in the effective action which is
proportional to the bare coupling constant. This mass term renders the
perturbative expansion infrared-finite and allows to study it directly in the
large-N and infinite-volume limits using the Diagrammatic Monte-Carlo approach.
On the exactly solvable example of a large-N O(N) sigma model in D=2 dimensions
we show that this infrared-finite weak-coupling expansion contains, in addition
to powers of bare coupling, also powers of its logarithm, reminiscent of
re-summed perturbation theory in thermal field theory and resurgent
trans-series without exponential terms. We numerically demonstrate the
convergence of these double series to the manifestly non-perturbative dynamical
mass gap. We then develop a Diagrammatic Monte-Carlo algorithm for sampling
planar diagrams in the large-N matrix field theory, and apply it to study this
infrared-finite weak-coupling expansion for large-N U(N)xU(N) nonlinear sigma
model (principal chiral model) in D=2. We sample up to 12 leading orders of the
weak-coupling expansion, which is the practical limit set by the increasingly
strong sign problem at high orders. Comparing Diagrammatic Monte-Carlo with
conventional Monte-Carlo simulations extrapolated to infinite N, we find a good
agreement for the energy density as well as for the critical temperature of the
""deconfinement"" transition. Finally, we comment on the applicability of our
approach to planar QCD at zero and finite density.
",0,1,0,0,0,0
12527,Decentralized Tube-based Model Predictive Control of Uncertain Nonlinear Multi-Agent Systems,"  This paper addresses the problem of decentralized tube-based nonlinear Model
Predictive Control (NMPC) for a class of uncertain nonlinear continuous-time
multi-agent systems with additive and bounded disturbance. In particular, the
problem of robust navigation of a multi-agent system to predefined states of
the workspace while using only local information is addressed, under certain
distance and control input constraints. We propose a decentralized feedback
control protocol that consists of two terms: a nominal control input, which is
computed online and is the outcome of a Decentralized Finite Horizon Optimal
Control Problem (DFHOCP) that each agent solves at every sampling time, for its
nominal system dynamics; and an additive state feedback law which is computed
offline and guarantees that the real trajectories of each agent will belong to
a hyper-tube centered along the nominal trajectory, for all times. The volume
of the hyper-tube depends on the upper bound of the disturbances as well as the
bounds of the derivatives of the dynamics. In addition, by introducing certain
distance constraints, the proposed scheme guarantees that the initially
connected agents remain connected for all times. Under standard assumptions
that arise in nominal NMPC schemes, controllability assumptions as well as
communication capabilities between the agents, we guarantee that the
multi-agent system is ISS (Input to State Stable) with respect to the
disturbances, for all initial conditions satisfying the state constraints.
Simulation results verify the correctness of the proposed framework.
",1,0,0,0,0,0
459,Criterion of positivity for semilinear problems with applications in biology,"  The goal of this article is to provide an useful criterion of positivity and
well-posedness for a wide range of infinite dimensional semilinear abstract
Cauchy problems. This criterion is based on some weak assumptions on the
non-linear part of the semilinear problem and on the existence of a strongly
continuous semigroup generated by the differential operator. To illustrate a
large variety of applications, we exhibit the feasibility of this criterion
through three examples in mathematical biology: epidemiology, predator-prey
interactions and oncology.
",0,0,1,0,0,0
543,An explicit determination of the $K$-theoretic structure constants of the affine Grassmannian associated to $SL_2$,"  Let $G:=\widehat{SL_2}$ denote the affine Kac-Moody group associated to
$SL_2$ and $\bar{\mathcal{X}}$ the associated affine Grassmannian. We determine
an inductive formula for the Schubert basis structure constants in the
torus-equivariant Grothendieck group of $\bar{\mathcal{X}}$. In the case of
ordinary (non-equivariant) $K$-theory we find an explicit closed form for the
structure constants. We also determine an inductive formula for the structure
constants in the torus-equivariant cohomology ring, and use this formula to
find closed forms for some of the structure constants.
",0,0,1,0,0,0
20915,3D Modeling of Electric Fields in the LUX Detector,"  This work details the development of a three-dimensional (3D) electric field
model for the LUX detector. The detector took data during two periods of
searching for weakly interacting massive particle (WIMP) searches. After the
first period completed, a time-varying non-uniform negative charge developed in
the polytetrafluoroethylene (PTFE) panels that define the radial boundary of
the detector's active volume. This caused electric field variations in the
detector in time, depth and azimuth, generating an electrostatic
radially-inward force on electrons on their way upward to the liquid surface.
To map this behavior, 3D electric field maps of the detector's active volume
were built on a monthly basis. This was done by fitting a model built in COMSOL
Multiphysics to the uniformly distributed calibration data that were collected
on a regular basis. The modeled average PTFE charge density increased over the
course of the exposure from -3.6 to $-5.5~\mu$C/m$^2$. From our studies, we
deduce that the electric field magnitude varied while the mean value of the
field of $\sim200$~V/cm remained constant throughout the exposure. As a result
of this work the varying electric fields and their impact on event
reconstruction and discrimination were successfully modeled.
",0,1,0,0,0,0
5332,Secure Search on the Cloud via Coresets and Sketches,"  \emph{Secure Search} is the problem of retrieving from a database table (or
any unsorted array) the records matching specified attributes, as in SQL SELECT
queries, but where the database and the query are encrypted. Secure search has
been the leading example for practical applications of Fully Homomorphic
Encryption (FHE) starting in Gentry's seminal work; however, to the best of our
knowledge all state-of-the-art secure search algorithms to date are realized by
a polynomial of degree $\Omega(m)$ for $m$ the number of records, which is
typically too slow in practice even for moderate size $m$.
In this work we present the first algorithm for secure search that is
realized by a polynomial of degree polynomial in $\log m$. We implemented our
algorithm in an open source library based on HELib implementation for the
Brakerski-Gentry-Vaikuntanthan's FHE scheme, and ran experiments on Amazon's
EC2 cloud. Our experiments show that we can retrieve the first match in a
database of millions of entries in less than an hour using a single machine;
the time reduced almost linearly with the number of machines.
Our result utilizes a new paradigm of employing coresets and sketches, which
are modern data summarization techniques common in computational geometry and
machine learning, for efficiency enhancement for homomorphic encryption. As a
central tool we design a novel sketch that returns the first positive entry in
a (not necessarily sparse) array; this sketch may be of independent interest.
",1,0,0,0,0,0
9897,The trouble with tensor ring decompositions,"  The tensor train decomposition decomposes a tensor into a ""train"" of 3-way
tensors that are interconnected through the summation of auxiliary indices. The
decomposition is stable, has a well-defined notion of rank and enables the user
to perform various linear algebra operations on vectors and matrices of
exponential size in a computationally efficient manner. The tensor ring
decomposition replaces the train by a ring through the introduction of one
additional auxiliary variable. This article discusses a major issue with the
tensor ring decomposition: its inability to compute an exact minimal-rank
decomposition from a decomposition with sub-optimal ranks. Both the contraction
operation and Hadamard product are motivated from applications and it is shown
through simple examples how the tensor ring-rounding procedure fails to
retrieve minimal-rank decompositions with these operations. These observations,
together with the already known issue of not being able to find a best low-rank
tensor ring approximation to a given tensor indicate that the applicability of
tensor rings is severely limited.
",1,0,0,0,0,0
997,A graph model of message passing processes,"  In the paper we consider a graph model of message passing processes and
present a method verification of message passing processes. The method is
illustrated by an example of a verification of sliding window protocol.
",1,0,0,0,0,0
3193,The Double Galaxy Cluster Abell 2465 III. X-ray and Weak-lensing Observations,"  We report Chandra X-ray observations and optical weak-lensing measurements
from Subaru/Suprime-Cam images of the double galaxy cluster Abell 2465
(z=0.245). The X-ray brightness data are fit to a beta-model to obtain the
radial gas density profiles of the northeast (NE) and southwest (SW)
sub-components, which are seen to differ in structure. We determine core radii,
central temperatures, the gas masses within $r_{500c}$, and the total masses
for the broader NE and sharper SW components assuming hydrostatic equilibrium.
The central entropy of the NE clump is about two times higher than the SW.
Along with its structural properties, this suggests that it has undergone
merging on its own. The weak-lensing analysis gives virial masses for each
substructure, which compare well with earlier dynamical results. The derived
outer mass contours of the SW sub-component from weak lensing are more
irregular and extended than those of the NE. Although there is a weak
enhancement and small offsets between X-ray gas and mass centers from weak
lensing, the lack of large amounts of gas between the two sub-clusters
indicates that Abell 2465 is in a pre-merger state. A dynamical model that is
consistent with the observed cluster data, based on the FLASH program and the
radial infall model, is constructed, where the subclusters currently separated
by ~1.2Mpc are approaching each other at ~2000km/s and will meet in ~0.4Gyr.
",0,1,0,0,0,0
1947,Provably Accurate Double-Sparse Coding,"  Sparse coding is a crucial subroutine in algorithms for various signal
processing, deep learning, and other machine learning applications. The central
goal is to learn an overcomplete dictionary that can sparsely represent a given
input dataset. However, a key challenge is that storage, transmission, and
processing of the learned dictionary can be untenably high if the data
dimension is high. In this paper, we consider the double-sparsity model
introduced by Rubinstein et al. (2010b) where the dictionary itself is the
product of a fixed, known basis and a data-adaptive sparse component. First, we
introduce a simple algorithm for double-sparse coding that can be amenable to
efficient implementation via neural architectures. Second, we theoretically
analyze its performance and demonstrate asymptotic sample complexity and
running time benefits over existing (provable) approaches for sparse coding. To
our knowledge, our work introduces the first computationally efficient
algorithm for double-sparse coding that enjoys rigorous statistical guarantees.
Finally, we support our analysis via several numerical experiments on simulated
data, confirming that our method can indeed be useful in problem sizes
encountered in practical applications.
",1,0,0,1,0,0
19622,Active Learning with Gaussian Processes for High Throughput Phenotyping,"  A looming question that must be solved before robotic plant phenotyping
capabilities can have significant impact to crop improvement programs is
scalability. High Throughput Phenotyping (HTP) uses robotic technologies to
analyze crops in order to determine species with favorable traits, however, the
current practices rely on exhaustive coverage and data collection from the
entire crop field being monitored under the breeding experiment. This works
well in relatively small agricultural fields but can not be scaled to the
larger ones, thus limiting the progress of genetics research. In this work, we
propose an active learning algorithm to enable an autonomous system to collect
the most informative samples in order to accurately learn the distribution of
phenotypes in the field with the help of a Gaussian Process model. We
demonstrate the superior performance of our proposed algorithm compared to the
current practices on sorghum phenotype data collection.
",1,0,0,1,0,0
17567,"Decoupled molecules with binding polynomials of bidegree (n,2)","  We present a result on the number of decoupled molecules for systems binding
two different types of ligands. In the case of $n$ and $2$ binding sites
respectively, we show that, generically, there are $2(n!)^{2}$ decoupled
molecules with the same binding polynomial. For molecules with more binding
sites for the second ligand, we provide computational results.
",1,1,0,0,0,0
2960,A note on Weyl groups and crystallographic root lattices,"  We follow the dual approach to Coxeter systems and show for Weyl groups a
criterium which decides whether a set of reflections is generating the group
depending on the root and the coroot lattice. Further we study special
generating sets involving a parabolic subgroup and show that they are very
tame.
",0,0,1,0,0,0
18001,Separation of the charge density wave and superconducting states by an intermediate semimetal phase in pressurized TaTe2,"  In layered transition metal dichalcogenides (LTMDCs) that display both charge
density waves (CDWs) and superconductivity, the superconducting state generally
emerges directly on suppression of the CDW state. Here, however, we report a
different observation for pressurized TaTe2, a non-superconducting CDW-bearing
LTMDC at ambient pressure. We find that a superconducting state does not occur
in TaTe2 after the full suppression of its CDW state, which we observe at about
3 GPa, but, rather, a non-superconducting semimetal state is observed. At a
higher pressure, ~21 GPa, where both the semimetal state and the corresponding
positive magnetoresistance effect are destroyed, superconductivity finally
emerges and remains present up to ~50 GPa, the high pressure limit of our
measurements. Our pressure-temperature phase diagram for TaTe2 demonstrates
that the CDW and the superconducting phases in TaTe2 do not directly transform
one to the other, but rather are separated by a semimetal state, - the first
experimental case where the CDW and superconducting states are separated by an
intermediate phase in LTMDC systems.
",0,1,0,0,0,0
15526,Information-entropic analysis of Korteweg--de Vries solitons in the quark-gluon plasma,"  Solitary waves propagation of baryonic density perturbations, ruled by the
Korteweg--de Vries equation in a mean-field quark-gluon plasma model, are
investigated from the point of view of the theory of information. A recently
proposed continuous logarithmic measure of information, called configurational
entropy, is used to derive the soliton width, defining the pulse, for which the
informational content of the soliton spatial profile is more compressed, in the
Shannon's sense.
",0,1,0,0,0,0
13183,One-way quantum computing in superconducting circuits,"  We propose a method for the implementation of one-way quantum computing in
superconducting circuits. Measurement-based quantum computing is a universal
quantum computation paradigm in which an initial cluster-state provides the
quantum resource, while the iteration of sequential measurements and local
rotations encodes the quantum algorithm. Up to now, technical constraints have
limited a scalable approach to this quantum computing alternative. The initial
cluster state can be generated with available controlled-phase gates, while the
quantum algorithm makes use of high-fidelity readout and coherent feedforward.
With current technology, we estimate that quantum algorithms with above 20
qubits may be implemented in the path towards quantum supremacy. Moreover, we
propose an alternative initial state with properties of maximal persistence and
maximal connectedness, reducing the required resources of one-way quantum
computing protocols.
",0,1,0,0,0,0
1410,"Attention Solves Your TSP, Approximately","  The development of efficient (heuristic) algorithms for practical
combinatorial optimization problems is costly, so we want to automatically
learn them instead. We show the feasibility of this approach on the important
Travelling Salesman Problem (TSP). We learn a heuristic algorithm that uses a
Neural Network policy to construct a tour. As an alternative to the Pointer
Network, our model is based entirely on (graph) attention layers and is
invariant to the input order of the nodes. We train the model efficiently using
REINFORCE with a simple and robust baseline based on a deterministic (greedy)
rollout of the best policy so far. We significantly improve over results from
previous works that consider learned heuristics for the TSP, reducing the
optimality gap for a single tour construction from 1.51% to 0.32% for instances
with 20 nodes, from 4.59% to 1.71% for 50 nodes and from 6.89% to 4.43% for 100
nodes. Additionally, we improve over a recent Reinforcement Learning framework
for two variants of the Vehicle Routing Problem (VRP).
",0,0,0,1,0,0
20715,Online Factorization and Partition of Complex Networks From Random Walks,"  Finding the reduced-dimensional structure is critical to understanding
complex networks. Existing approaches such as spectral clustering are
applicable only when the full network is explicitly observed. In this paper, we
focus on the online factorization and partition of implicit large-scale
networks based on observations from an associated random walk. We formulate
this into a nonconvex stochastic factorization problem and propose an efficient
and scalable stochastic generalized Hebbian algorithm. The algorithm is able to
process dependent state-transition data dynamically generated by the underlying
network and learn a low-dimensional representation for each vertex. By applying
a diffusion approximation analysis, we show that the continuous-time limiting
process of the stochastic algorithm converges globally to the ""principal
components"" of the Markov chain and achieves a nearly optimal sample
complexity. Once given the learned low-dimensional representations, we further
apply clustering techniques to recover the network partition. We show that when
the associated Markov process is lumpable, one can recover the partition
exactly with high probability. We apply the proposed approach to model the
traffic flow of Manhattan as city-wide random walks. By using our algorithm to
analyze the taxi trip data, we discover a latent partition of the Manhattan
city that closely matches the traffic dynamics.
",1,0,1,1,0,0
1250,Football and Beer - a Social Media Analysis on Twitter in Context of the FIFA Football World Cup 2018,"  In many societies alcohol is a legal and common recreational substance and
socially accepted. Alcohol consumption often comes along with social events as
it helps people to increase their sociability and to overcome their
inhibitions. On the other hand we know that increased alcohol consumption can
lead to serious health issues, such as cancer, cardiovascular diseases and
diseases of the digestive system, to mention a few. This work examines alcohol
consumption during the FIFA Football World Cup 2018, particularly the usage of
alcohol related information on Twitter. For this we analyse the tweeting
behaviour and show that the tournament strongly increases the interest in beer.
Furthermore we show that countries who had to leave the tournament at early
stage might have done something good to their fans as the interest in beer
decreased again.
",1,0,0,0,0,0
11167,Least models of second-order set theories,"  The main theorems of this paper are (1) there is no least transitive model of
Kelley--Morse set theory $\mathsf{KM}$ and (2) there is a least
$\beta$-model---that is, a transitive model which is correct about which of its
classes are well-founded---of Gödel--Bernays set theory $\mathsf{GBC}$ +
Elementary Transfinite Recursion. Along the way I characterize when a countable
model of $\mathsf{ZFC}$ has a least $\mathsf{GBC}$-realization and show that no
countable model of $\mathsf{ZFC}$ has a least $\mathsf{KM}$-realization. I also
show that fragments of Elementary Transfinite Recursion have least
$\beta$-models and, for sufficiently weak fragments, least transitive models.
These fragments can be separated from each other and from the full principle of
Elementary Transfinite Recursion by consistency strength. The main question
left unanswered by this article is whether there is a least transitive model of
$\mathsf{GBC}$ + Elementary Transfinite Recursion.
",0,0,1,0,0,0
11003,Low-energy electron-positron collider to search and study (μ^+μ^-) bound state,"  We discuss a low energy $e^+e^-$ collider for production of the not yet
observed ($\mu^+\mu^-$) bound system (dimuonium). Collider with large crossing
angle for $e^+e^-$ beams intersection produces dimuonium with non-zero
momentum, therefore, its decay point is shifted from the beam collision area
providing effective suppression of the elastic $e^+e^-$ scattering background.
The experimental constraints define subsequent collider specifications. We show
preliminary layout of the accelerator and obtained main parameters. High
luminosity in chosen beam energy range allows to study $\pi^\pm$ and $\eta$
-mesons.
",0,1,0,0,0,0
3730,First-Principles Many-Body Investigation of Correlated Oxide Heterostructures: Few-Layer-Doped SmTiO$_3$,"  Correlated oxide heterostructures pose a challenging problem in condensed
matter research due to their structural complexity interweaved with demanding
electron states beyond the effective single-particle picture. By exploring the
correlated electronic structure of SmTiO$_3$ doped with few layers of SrO, we
provide an insight into the complexity of such systems. Furthermore, it is
shown how the advanced combination of band theory on the level of Kohn-Sham
density functional theory with explicit many-body theory on the level of
dynamical mean-field theory provides an adequate tool to cope with the problem.
Coexistence of band-insulating, metallic and Mott-critical electronic regions
is revealed in individual heterostructures with multi-orbital manifolds.
Intriguing orbital polarizations, that qualitatively vary between the metallic
and the Mott layers are also encountered.
",0,1,0,0,0,0
1972,A Higher Structure Identity Principle,"  We prove a Structure Identity Principle for theories defined on types of
$h$-level 3 by defining a general notion of saturation for a large class of
structures definable in the Univalent Foundations.
",1,0,1,0,0,0
3815,Two provably consistent divide and conquer clustering algorithms for large networks,"  In this article, we advance divide-and-conquer strategies for solving the
community detection problem in networks. We propose two algorithms which
perform clustering on a number of small subgraphs and finally patches the
results into a single clustering. The main advantage of these algorithms is
that they bring down significantly the computational cost of traditional
algorithms, including spectral clustering, semi-definite programs, modularity
based methods, likelihood based methods etc., without losing on accuracy and
even improving accuracy at times. These algorithms are also, by nature,
parallelizable. Thus, exploiting the facts that most traditional algorithms are
accurate and the corresponding optimization problems are much simpler in small
problems, our divide-and-conquer methods provide an omnibus recipe for scaling
traditional algorithms up to large networks. We prove consistency of these
algorithms under various subgraph selection procedures and perform extensive
simulations and real-data analysis to understand the advantages of the
divide-and-conquer approach in various settings.
",0,0,1,1,0,0
11992,Magnetic Correlations in the Two-dimensional Repulsive Fermi Hubbard Model,"  The repulsive Fermi Hubbard model on the square lattice has a rich phase
diagram near half-filling (corresponding to the particle density per lattice
site $n=1$): for $n=1$ the ground state is an antiferromagnetic insulator, at
$0.6 < n \lesssim 0.8$, it is a $d_{x^2-y^2}$-wave superfluid (at least for
moderately strong interactions $U \lesssim 4t$ in terms of the hopping $t$),
and the region $1-n \ll 1$ is most likely subject to phase separation. Much of
this physics is preempted at finite temperatures and to an extent driven by
strong magnetic fluctuations, their quantitative characteristics and how they
change with the doping level being much less understood. Experiments on
ultra-cold atoms have recently gained access to this interesting fluctuation
regime, which is now under extensive investigation. In this work we employ a
self-consistent skeleton diagrammatic approach to quantify the characteristic
temperature scale $T_{M}(n)$ for the onset of magnetic fluctuations with a
large correlation length and identify their nature. Our results suggest that
the strongest fluctuations---and hence highest $T_{M}$ and easiest experimental
access to this regime---are observed at $U/t \approx 4-6$.
",0,1,0,0,0,0
5277,Employee turnover prediction and retention policies design: a case study,"  This paper illustrates the similarities between the problems of customer
churn and employee turnover. An example of employee turnover prediction model
leveraging classical machine learning techniques is developed. Model outputs
are then discussed to design \& test employee retention policies. This type of
retention discussion is, to our knowledge, innovative and constitutes the main
value of this paper.
",1,0,0,1,0,0
6559,An Iterative Scheme for Leverage-based Approximate Aggregation,"  The current data explosion poses great challenges to the approximate
aggregation with an efficiency and accuracy. To address this problem, we
propose a novel approach to calculate the aggregation answers with a high
accuracy using only a small portion of the data. We introduce leverages to
reflect individual differences in the samples from a statistical perspective.
Two kinds of estimators, the leverage-based estimator, and the sketch estimator
(a ""rough picture"" of the aggregation answer), are in constraint relations and
iteratively improved according to the actual conditions until their difference
is below a threshold. Due to the iteration mechanism and the leverages, our
approach achieves a high accuracy. Moreover, some features, such as not
requiring recording the sampled data and easy to extend to various execution
modes (e.g., the online mode), make our approach well suited to deal with big
data. Experiments show that our approach has an extraordinary performance, and
when compared with the uniform sampling, our approach can achieve high-quality
answers with only 1/3 of the same sample size.
",1,0,0,0,0,0
5324,Stable Unitary Integrators for the Numerical Implementation of Continuous Unitary Transformations,"  The technique of continuous unitary transformations has recently been used to
provide physical insight into a diverse array of quantum mechanical systems.
However, the question of how to best numerically implement the flow equations
has received little attention. The most immediately apparent approach, using
standard Runge-Kutta numerical integration algorithms, suffers from both severe
inefficiency due to stiffness and the loss of unitarity. After reviewing the
formalism of continuous unitary transformations and Wegner's original choice
for the infinitesimal generator of the flow, we present a number of approaches
to resolving these issues including a choice of generator which induces what we
call the ""uniform tangent decay flow"" and three numerical integrators
specifically designed to perform continuous unitary transformations efficiently
while preserving the unitarity of flow. We conclude by applying one of the flow
algorithms to a simple calculation that visually demonstrates the many-body
localization transition.
",1,1,0,0,0,0
17201,Mitigating Evasion Attacks to Deep Neural Networks via Region-based Classification,"  Deep neural networks (DNNs) have transformed several artificial intelligence
research areas including computer vision, speech recognition, and natural
language processing. However, recent studies demonstrated that DNNs are
vulnerable to adversarial manipulations at testing time. Specifically, suppose
we have a testing example, whose label can be correctly predicted by a DNN
classifier. An attacker can add a small carefully crafted noise to the testing
example such that the DNN classifier predicts an incorrect label, where the
crafted testing example is called adversarial example. Such attacks are called
evasion attacks. Evasion attacks are one of the biggest challenges for
deploying DNNs in safety and security critical applications such as
self-driving cars. In this work, we develop new methods to defend against
evasion attacks. Our key observation is that adversarial examples are close to
the classification boundary. Therefore, we propose region-based classification
to be robust to adversarial examples. For a benign/adversarial testing example,
we ensemble information in a hypercube centered at the example to predict its
label. In contrast, traditional classifiers are point-based classification,
i.e., given a testing example, the classifier predicts its label based on the
testing example alone. Our evaluation results on MNIST and CIFAR-10 datasets
demonstrate that our region-based classification can significantly mitigate
evasion attacks without sacrificing classification accuracy on benign examples.
Specifically, our region-based classification achieves the same classification
accuracy on testing benign examples as point-based classification, but our
region-based classification is significantly more robust than point-based
classification to various evasion attacks.
",1,0,0,1,0,0
757,A Recorded Debating Dataset,"  This paper describes an English audio and textual dataset of debating
speeches, a unique resource for the growing research field of computational
argumentation and debating technologies. We detail the process of speech
recording by professional debaters, the transcription of the speeches with an
Automatic Speech Recognition (ASR) system, their consequent automatic
processing to produce a text that is more ""NLP-friendly"", and in parallel --
the manual transcription of the speeches in order to produce gold-standard
""reference"" transcripts. We release 60 speeches on various controversial
topics, each in five formats corresponding to the different stages in the
production of the data. The intention is to allow utilizing this resource for
multiple research purposes, be it the addition of in-domain training data for a
debate-specific ASR system, or applying argumentation mining on either noisy or
clean debate transcripts. We intend to make further releases of this data in
the future.
",1,0,0,0,0,0
17150,Introduction to Formal Concept Analysis and Its Applications in Information Retrieval and Related Fields,"  This paper is a tutorial on Formal Concept Analysis (FCA) and its
applications. FCA is an applied branch of Lattice Theory, a mathematical
discipline which enables formalisation of concepts as basic units of human
thinking and analysing data in the object-attribute form. Originated in early
80s, during the last three decades, it became a popular human-centred tool for
knowledge representation and data analysis with numerous applications. Since
the tutorial was specially prepared for RuSSIR 2014, the covered FCA topics
include Information Retrieval with a focus on visualisation aspects, Machine
Learning, Data Mining and Knowledge Discovery, Text Mining and several others.
",1,0,0,1,0,0
10824,Force sensing with an optically levitated charged nanoparticle,"  Levitated optomechanics is showing potential for precise force measurements.
Here, we report a case study, to show experimentally the capacity of such a
force sensor. Using an electric field as a tool to detect a Coulomb force
applied onto a levitated nanosphere. We experimentally observe the spatial
displacement of up to 6.6 nm of the levitated nanosphere by imposing a DC
field. We further apply an AC field and demonstrate resonant enhancement of
force sensing when a driving frequency, $\omega_{AC}$, and the frequency of the
levitated mechanical oscillator, $\omega_0$, converge. We directly measure a
force of $3.0 \pm 1.5 \times 10^{-20}$ N with 10 second integration time, at a
centre of mass temperature of 3 K and at a pressure of $1.6 \times 10^{-5}$
mbar.
",0,1,0,0,0,0
20853,Recovering Nonuniform Planted Partitions via Iterated Projection,"  In the planted partition problem, the $n$ vertices of a random graph are
partitioned into $k$ ""clusters,"" and edges between vertices in the same cluster
and different clusters are included with constant probability $p$ and $q$,
respectively (where $0 \le q < p \le 1$). We give an efficient spectral
algorithm that recovers the clusters with high probability, provided that the
sizes of any two clusters are either very close or separated by $\geq
\Omega(\sqrt n)$. We also discuss a generalization of planted partition in
which the algorithm's input is not a random graph, but a random real symmetric
matrix with independent above-diagonal entries.
Our algorithm is an adaptation of a previous algorithm for the uniform case,
i.e., when all clusters are size $n / k \geq \Omega(\sqrt n)$. The original
algorithm recovers the clusters one by one via iterated projection: it
constructs the orthogonal projection operator onto the dominant $k$-dimensional
eigenspace of the random graph's adjacency matrix, uses it to recover one of
the clusters, then deletes it and recurses on the remaining vertices. We show
herein that a similar algorithm works in the nonuniform case.
",1,0,0,0,0,0
3329,Synthesizing SystemC Code from Delay Hybrid CSP,"  Delay is omnipresent in modern control systems, which can prompt oscillations
and may cause deterioration of control performance, invalidate both stability
and safety properties. This implies that safety or stability certificates
obtained on idealized, delay-free models of systems prone to delayed coupling
may be erratic, and further the incorrectness of the executable code generated
from these models. However, automated methods for system verification and code
generation that ought to address models of system dynamics reflecting delays
have not been paid enough attention yet in the computer science community. In
our previous work, on one hand, we investigated the verification of delay
dynamical and hybrid systems; on the other hand, we also addressed how to
synthesize SystemC code from a verified hybrid system modelled by Hybrid CSP
(HCSP) without delay. In this paper, we give a first attempt to synthesize
SystemC code from a verified delay hybrid system modelled by Delay HCSP
(dHCSP), which is an extension of HCSP by replacing ordinary differential
equations (ODEs) with delay differential equations (DDEs). We implement a tool
to support the automatic translation from dHCSP to SystemC.
",1,0,0,0,0,0
13503,Effects of global gas flows on type I migration,"  Magnetically-driven disk winds would alter the surface density slope of gas
in the inner region of a protoplanetary disk $(r \lesssim 1 {\rm au})$. This in
turn affects planet formation. Recently, the effect of disk wind torque has
been considered with the suggestion that it would carve out the surface density
of the disk from inside and would induce global gas flows (wind-driven
accretion). We aim to investigate effects of global gas flows on type I
migration and also examine planet formation. A simplified approach was taken to
address this issue, and N-body simulations with isolation-mass planets were
also performed. In previous studies, the effect of gas flow induced by
turbulence-driven accretion has been taken into account for its desaturation
effect of the corotation torque. If more rapid gas flows (e.g., wind-driven
accretion) are considered, the desaturation effect can be modified. In
MRI-inactive disks, in which the wind-driven accretion dominates the disk
evolution, the gas flow at the midplane plays an important role. If this flow
is fast, the corotation torque is efficiently desaturated. Then, the fact that
the surface density slope can be positive in the inner region due to the wind
torque can generate an outward migration region extended to super-Earth mass
planets. In this case, we observe that no planets fall onto the central star in
N-body simulations with migration forces imposed to reproduce such migration
pattern. We also see that super-Earth mass planets can undergo outward
migration. Relatively rapid gas flows affects type I migration and thus the
formation of close-in planets.
",0,1,0,0,0,0
10149,A Computational Study of Yttria-Stabilized Zirconia: I. Using Crystal Chemistry to Search for the Ground State on a Glassy Energy Landscape,"  Yttria-stabilized zirconia (YSZ), a ZrO2-Y2O3 solid solution that contains a
large population of oxygen vacancies, is widely used in energy and industrial
applications. Past computational studies correctly predicted the anion
diffusivity but not the cation diffusivity, which is important for material
processing and stability. One of the challenges lies in identifying a plausible
configuration akin to the ground state in a glassy landscape. This is unlikely
to come from random sampling of even a very large sample space, but the odds
are much improved by incorporating packing preferences revealed by a modest
sized configurational library established from empirical potential
calculations. Ab initio calculations corroborated these preferences, which
prove remarkably robust extending to the fifth cation-oxygen shell about 8
{\AA} away. Yet because of frustration there are still rampant violations of
packing preferences and charge neutrality in the ground state, and the approach
toward it bears a close analogy to glass relaxations. Fast relaxations proceed
by fast oxygen movement around cations, while slow relaxations require slow
cation diffusion. The latter is necessarily cooperative because of strong
coupling imposed by the long-range packing preferences.
",0,1,0,0,0,0
8968,Improved lower bounds for the Mahler measure of the Fekete polynomials,"  We show that there is an absolute constant $c > 1/2$ such that the Mahler
measure of the Fekete polynomials $f_p$ of the form $$f_p(z) :=
\sum_{k=1}^{p-1}{\left( \frac kp \right)z^k}\,,$$ (where the coefficients are
the usual Legendre symbols) is at least $c\sqrt{p}$ for all sufficiently large
primes $p$. This improves the lower bound $\left(\frac 12 -
\varepsilon\right)\sqrt{p}$ known before for the Mahler measure of the Fekete
polynomials $f_p$ for all sufficiently large primes $p \geq c_{\varepsilon}$.
Our approach is based on the study of the zeros of the Fekete polynomials on
the unit circle.
",0,0,1,0,0,0
17004,Significance of Side Information in the Graph Matching Problem,"  Percolation based graph matching algorithms rely on the availability of seed
vertex pairs as side information to efficiently match users across networks.
Although such algorithms work well in practice, there are other types of side
information available which are potentially useful to an attacker. In this
paper, we consider the problem of matching two correlated graphs when an
attacker has access to side information, either in the form of community labels
or an imperfect initial matching. In the former case, we propose a naive graph
matching algorithm by introducing the community degree vectors which harness
the information from community labels in an efficient manner. Furthermore, we
analyze a variant of the basic percolation algorithm proposed in literature for
graphs with community structure. In the latter case, we propose a novel
percolation algorithm with two thresholds which uses an imperfect matching as
input to match correlated graphs.
We evaluate the proposed algorithms on synthetic as well as real world
datasets using various experiments. The experimental results demonstrate the
importance of communities as side information especially when the number of
seeds is small and the networks are weakly correlated.
",1,1,0,0,0,0
13334,Querying Best Paths in Graph Databases,"  Querying graph databases has recently received much attention. We propose a
new approach to this problem, which balances competing goals of expressive
power, language clarity and computational complexity. A distinctive feature of
our approach is the ability to express properties of minimal (e.g. shortest)
and maximal (e.g. most valuable) paths satisfying given criteria. To express
complex properties in a modular way, we introduce labelling-generating
ontologies. The resulting formalism is computationally attractive -- queries
can be answered in non-deterministic logarithmic space in the size of the
database.
",1,0,0,0,0,0
13211,Quantum quench dynamics,"  Quench dynamics is an active area of study encompassing condensed matter
physics and quantum information, with applications to cold-atomic gases and
pump-probe spectroscopy of materials. Recent theoretical progress in studying
quantum quenches is reviewed. Quenches in interacting one dimensional systems
as well as systems in higher spatial dimensions are covered. The appearance of
non-trivial steady states following a quench in exactly solvable models is
discussed, and the stability of these states to perturbations is described.
Proper conserving approximations needed to capture the onset of thermalization
at long times are outlined. The appearance of universal scaling for quenches
near critical points, and the role of the renormalization group in capturing
the transient regime, are reviewed. Finally the effect of quenches near
critical points on the dynamics of entanglement entropy and entanglement
statistics is discussed. The extraction of critical exponents from the
entanglement statistics is outlined.
",0,1,0,0,0,0
8376,When Can Neural Networks Learn Connected Decision Regions?,"  Previous work has questioned the conditions under which the decision regions
of a neural network are connected and further showed the implications of the
corresponding theory to the problem of adversarial manipulation of classifiers.
It has been proven that for a class of activation functions including leaky
ReLU, neural networks having a pyramidal structure, that is no layer has more
hidden units than the input dimension, produce necessarily connected decision
regions. In this paper, we advance this important result by further developing
the sufficient and necessary conditions under which the decision regions of a
neural network are connected. We then apply our framework to overcome the
limits of existing work and further study the capacity to learn connected
regions of neural networks for a much wider class of activation functions
including those widely used, namely ReLU, sigmoid, tanh, softlus, and
exponential linear function.
",1,0,0,1,0,0
18232,Flux-Rope Twist in Eruptive Flares and CMEs: due to Zipper and Main-Phase Reconnection,"  The nature of three-dimensional reconnection when a twisted flux tube erupts
during an eruptive flare or coronal mass ejection is considered. The
reconnection has two phases: first of all, 3D ""zipper reconnection"" propagates
along the initial coronal arcade, parallel to the polarity inversion line
(PIL), then subsequent quasi-2D ""main phase reconnection"" in the low corona
around a flux rope during its eruption produces coronal loops and chromospheric
ribbons that propagate away from the PIL in a direction normal to it.
One scenario starts with a sheared arcade: the zipper reconnection creates a
twisted flux rope of roughly one turn ($2\pi$ radians of twist), and then main
phase reconnection builds up the bulk of the erupting flux rope with a
relatively uniform twist of a few turns. A second scenario starts with a
pre-existing flux rope under the arcade. Here the zipper phase can create a
core with many turns that depend on the ratio of the magnetic fluxes in the
newly formed flare ribbons and the new flux rope. Main phase reconnection then
adds a layer of roughly uniform twist to the twisted central core. Both phases
and scenarios are modeled in a simple way that assumes the initial magnetic
flux is fragmented along the PIL. The model uses conservation of magnetic
helicity and flux, together with equipartition of magnetic helicity, to deduce
the twist of the erupting flux rope in terms the geometry of the initial
configuration.
Interplanetary observations show some flux ropes have a fairly uniform twist,
which could be produced when the zipper phase and any pre-existing flux rope
possess small or moderate twist (up to one or two turns). Other interplanetary
flux ropes have highly twisted cores (up to five turns), which could be
produced when there is a pre-existing flux rope and an active zipper phase that
creates substantial extra twist.
",0,1,0,0,0,0
736,Mixing of odd- and even-frequency pairings in strongly correlated electron systems under magnetic field,"  Even- and odd-frequency superconductivity coexist due to broken time-reversal
symmetry under magnetic field. In order to describe this mixing, we extend the
linearized Eliashberg equation for the spin and charge fluctuation mechanism in
strongly correlated electron systems. We apply this extended Eliashberg
equation to the odd-frequency superconductivity on a quasi-one-dimensional
isosceles triangular lattice under in-plane magnetic field and examine the
effect of the even-frequency component.
",0,1,0,0,0,0
6438,Radiative effects during the assembly of direct collapse black holes,"  We perform a post-processing radiative feedback analysis on a 3D ab initio
cosmological simulation of an atomic cooling halo under the direct collapse
black hole (DCBH) scenario. We maintain the spatial resolution of the
simulation by incorporating native ray-tracing on unstructured mesh data,
including Monte Carlo Lyman-alpha (Ly{\alpha}) radiative transfer. DCBHs are
born in gas-rich, metal-poor environments with the possibility of Compton-thick
conditions, $N_H \gtrsim 10^{24} {\rm cm}^{-2}$. Therefore, the surrounding gas
is capable of experiencing the full impact of the bottled-up radiation
pressure. In particular, we find that multiple scattering of Ly{\alpha} photons
provides an important source of mechanical feedback after the gas in the
sub-parsec region becomes partially ionized, avoiding the bottleneck of
destruction via the two-photon emission mechanism. We provide detailed
discussion of the simulation environment, expansion of the ionization front,
emission and escape of Ly{\alpha} radiation, and Compton scattering. A sink
particle prescription allows us to extract approximate limits on the
post-formation evolution of the radiative feedback. Fully coupled Ly{\alpha}
radiation hydrodynamics will be crucial to consider in future DCBH simulations.
",0,1,0,0,0,0
17285,Finite model reasoning over existential rules,"  Ontology-based query answering (OBQA) asks whether a Boolean conjunctive
query is satisfied by all models of a logical theory consisting of a relational
database paired with an ontology. The introduction of existential rules (i.e.,
Datalog rules extended with existential quantifiers in rule-heads) as a means
to specify the ontology gave birth to Datalog+/-, a framework that has received
increasing attention in the last decade, with focus also on decidability and
finite controllability to support effective reasoning. Five basic decidable
fragments have been singled out: linear, weakly-acyclic, guarded, sticky, and
shy. Moreover, for all these fragments, except shy, the important property of
finite controllability has been proved, ensuring that a query is satisfied by
all models of the theory iff it is satisfied by all its finite models. In this
paper we complete the picture by demonstrating that finite controllability of
OBQA holds also for shy ontologies, and it therefore applies to all basic
decidable Datalog+/- classes. To make the demonstration, we devise a general
technique to facilitate the process of (dis)proving finite controllability of
an arbitrary ontological fragment. This paper is under consideration for
acceptance in TPLP.
",1,0,0,0,0,0
8093,Are crossing dependencies really scarce?,"  The syntactic structure of a sentence can be modelled as a tree, where
vertices correspond to words and edges indicate syntactic dependencies. It has
been claimed recurrently that the number of edge crossings in real sentences is
small. However, a baseline or null hypothesis has been lacking. Here we
quantify the amount of crossings of real sentences and compare it to the
predictions of a series of baselines. We conclude that crossings are really
scarce in real sentences. Their scarcity is unexpected by the hubiness of the
trees. Indeed, real sentences are close to linear trees, where the potential
number of crossings is maximized.
",1,1,0,0,0,0
17120,Exploring Features for Predicting Policy Citations,"  In this study we performed an initial investigation and evaluation of
altmetrics and their relationship with public policy citation of research
papers. We examined methods for using altmetrics and other data to predict
whether a research paper is cited in public policy and applied receiver
operating characteristic curve on various feature groups in order to evaluate
their potential usefulness. From the methods we tested, classifying based on
tweet count provided the best results, achieving an area under the ROC curve of
0.91.
",1,0,0,0,0,0
15722,Gauss-Bonnet for matrix conformally rescaled Dirac,"  We derive an explicit formula for the scalar curvature over a two-torus with
a Dirac operator conformally rescaled by a globally diagonalizable matrix. We
show that the Gauss-Bonnet theorem holds and extend the result to all Riemann
surfaces with Dirac operators modified in the same way.
",0,0,1,0,0,0
940,Bridging Semantic Gaps between Natural Languages and APIs with Word Embedding,"  Developers increasingly rely on text matching tools to analyze the relation
between natural language words and APIs. However, semantic gaps, namely textual
mismatches between words and APIs, negatively affect these tools. Previous
studies have transformed words or APIs into low-dimensional vectors for
matching; however, inaccurate results were obtained due to the failure of
modeling words and APIs simultaneously. To resolve this problem, two main
challenges are to be addressed: the acquisition of massive words and APIs for
mining and the alignment of words and APIs for modeling. Therefore, this study
proposes Word2API to effectively estimate relatedness of words and APIs.
Word2API collects millions of commonly used words and APIs from code
repositories to address the acquisition challenge. Then, a shuffling strategy
is used to transform related words and APIs into tuples to address the
alignment challenge. Using these tuples, Word2API models words and APIs
simultaneously. Word2API outperforms baselines by 10%-49.6% of relatedness
estimation in terms of precision and NDCG. Word2API is also effective on
solving typical software tasks, e.g., query expansion and API documents
linking. A simple system with Word2API-expanded queries recommends up to 21.4%
more related APIs for developers. Meanwhile, Word2API improves comparison
algorithms by 7.9%-17.4% in linking questions in Question&Answer communities to
API documents.
",1,0,0,0,0,0
19759,Thermographic measurements of the spin Peltier effect in metal/yttrium-iron-garnet junction systems,"  The spin Peltier effect (SPE), heat-current generation due to spin-current
injection, in various metal (Pt, W, and Au single layers and Pt/Cu
bilayer)/ferrimagnetic insulator (yttrium iron garnet: YIG) junction systems
has been investigated by means of a lock-in thermography (LIT) method. The SPE
is excited by a spin current across the metal/YIG interface, which is generated
by applying a charge current to the metallic layer via the spin Hall effect.
The LIT method enables the thermal imaging of the SPE free from the
Joule-heating contribution. Importantly, we observed spin-current-induced
temperature modulation not only in the Pt/YIG and W/YIG systems but also in the
Au/YIG and Pt/Cu/YIG systems, excluding the possible contamination by anomalous
Ettingshausen effects due to proximity-induced ferromagnetism near the
metal/YIG interface. As demonstrated in our previous study, the SPE signals are
confined only in the vicinity of the metal/YIG interface; we buttress this
conclusion by reducing a spatial blur due to thermal diffusion in an infrared
emission layer on the sample surface used for the LIT measurements. We also
found that the YIG-thickness dependence of the SPE is similar to that of the
spin Seebeck effect measured in the same Pt/YIG sample, implying the reciprocal
relation between them.
",0,1,0,0,0,0
2067,MobInsight: A Framework Using Semantic Neighborhood Features for Localized Interpretations of Urban Mobility,"  Collective urban mobility embodies the residents' local insights on the city.
Mobility practices of the residents are produced from their spatial choices,
which involve various considerations such as the atmosphere of destinations,
distance, past experiences, and preferences. The advances in mobile computing
and the rise of geo-social platforms have provided the means for capturing the
mobility practices; however, interpreting the residents' insights is
challenging due to the scale and complexity of an urban environment, and its
unique context. In this paper, we present MobInsight, a framework for making
localized interpretations of urban mobility that reflect various aspects of the
urbanism. MobInsight extracts a rich set of neighborhood features through
holistic semantic aggregation, and models the mobility between all-pairs of
neighborhoods. We evaluate MobInsight with the mobility data of Barcelona and
demonstrate diverse localized and semantically-rich interpretations.
",1,0,0,0,0,0
10519,A model for a Lindenmayer reconstruction algorithm,"  Given an input string s and a specific Lindenmayer system (the so-called
Fibonacci grammar), we define an automaton which is capable of (i) determining
whether s belongs to the set of strings that the Fibonacci grammar can generate
(in other words, if s corresponds to a generation of the grammar) and, if so,
(ii) reconstructing the previous generation.
",1,0,0,0,0,0
5988,Reviving and Improving Recurrent Back-Propagation,"  In this paper, we revisit the recurrent back-propagation (RBP) algorithm,
discuss the conditions under which it applies as well as how to satisfy them in
deep neural networks. We show that RBP can be unstable and propose two variants
based on conjugate gradient on the normal equations (CG-RBP) and Neumann series
(Neumann-RBP). We further investigate the relationship between Neumann-RBP and
back propagation through time (BPTT) and its truncated version (TBPTT). Our
Neumann-RBP has the same time complexity as TBPTT but only requires constant
memory, whereas TBPTT's memory cost scales linearly with the number of
truncation steps. We examine all RBP variants along with BPTT and TBPTT in
three different application domains: associative memory with continuous
Hopfield networks, document classification in citation networks using graph
neural networks and hyperparameter optimization for fully connected networks.
All experiments demonstrate that RBPs, especially the Neumann-RBP variant, are
efficient and effective for optimizing convergent recurrent neural networks.
",0,0,0,1,0,0
6285,Data-driven Job Search Engine Using Skills and Company Attribute Filters,"  According to a report online, more than 200 million unique users search for
jobs online every month. This incredibly large and fast growing demand has
enticed software giants such as Google and Facebook to enter this space, which
was previously dominated by companies such as LinkedIn, Indeed and
CareerBuilder. Recently, Google released their ""AI-powered Jobs Search Engine"",
""Google For Jobs"" while Facebook released ""Facebook Jobs"" within their
platform. These current job search engines and platforms allow users to search
for jobs based on general narrow filters such as job title, date posted,
experience level, company and salary. However, they have severely limited
filters relating to skill sets such as C++, Python, and Java and company
related attributes such as employee size, revenue, technographics and
micro-industries. These specialized filters can help applicants and companies
connect at a very personalized, relevant and deeper level. In this paper we
present a framework that provides an end-to-end ""Data-driven Jobs Search
Engine"". In addition, users can also receive potential contacts of recruiters
and senior positions for connection and networking opportunities. The high
level implementation of the framework is described as follows: 1) Collect job
postings data in the United States, 2) Extract meaningful tokens from the
postings data using ETL pipelines, 3) Normalize the data set to link company
names to their specific company websites, 4) Extract and ranking the skill
sets, 5) Link the company names and websites to their respective company level
attributes with the EVERSTRING Company API, 6) Run user-specific search queries
on the database to identify relevant job postings and 7) Rank the job search
results. This framework offers a highly customizable and highly targeted search
experience for end users.
",1,0,0,0,0,0
20370,On the Effects of Batch and Weight Normalization in Generative Adversarial Networks,"  Generative adversarial networks (GANs) are highly effective unsupervised
learning frameworks that can generate very sharp data, even for data such as
images with complex, highly multimodal distributions. However GANs are known to
be very hard to train, suffering from problems such as mode collapse and
disturbing visual artifacts. Batch normalization (BN) techniques have been
introduced to address the training. Though BN accelerates the training in the
beginning, our experiments show that the use of BN can be unstable and
negatively impact the quality of the trained model. The evaluation of BN and
numerous other recent schemes for improving GAN training is hindered by the
lack of an effective objective quality measure for GAN models. To address these
issues, we first introduce a weight normalization (WN) approach for GAN
training that significantly improves the stability, efficiency and the quality
of the generated samples. To allow a methodical evaluation, we introduce
squared Euclidean reconstruction error on a test set as a new objective
measure, to assess training performance in terms of speed, stability, and
quality of generated samples. Our experiments with a standard DCGAN
architecture on commonly used datasets (CelebA, LSUN bedroom, and CIFAR-10)
indicate that training using WN is generally superior to BN for GANs, achieving
10% lower mean squared loss for reconstruction and significantly better
qualitative results than BN. We further demonstrate the stability of WN on a
21-layer ResNet trained with the CelebA data set. The code for this paper is
available at this https URL
",1,0,0,1,0,0
115,Effect of Meltdown and Spectre Patches on the Performance of HPC Applications,"  In this work we examine how the updates addressing Meltdown and Spectre
vulnerabilities impact the performance of HPC applications. To study this we
use the application kernel module of XDMoD to test the performance before and
after the application of the vulnerability patches. We tested the performance
difference for multiple application and benchmarks including: NWChem, NAMD,
HPCC, IOR, MDTest and IMB. The results show that although some specific
functions can have performance decreased by as much as 74%, the majority of
individual metrics indicates little to no decrease in performance. The
real-world applications show a 2-3% decrease in performance for single node
jobs and a 5-11% decrease for parallel multi node jobs.
",1,0,0,0,0,0
14651,On the orbits that generate the X-shape in the Milky Way bulge,"  The Milky Way bulge shows a box/peanut or X-shaped bulge (hereafter BP/X)
when viewed in infrared or microwave bands. We examine orbits in an N-body
model of a barred disk galaxy that is scaled to match the kinematics of the
Milky Way (MW) bulge. We generate maps of projected stellar surface density,
unsharp masked images, 3D excess-mass distributions (showing mass outside
ellipsoids), line-of-sight number count distributions, and 2D line-of-sight
kinematics for the simulation as well as co-added orbit families, in order to
identify the orbits primarily responsible for the BP/X shape. We estimate that
between 19-23\% of the mass of the bar is associated with the BP/X shape and
that most bar orbits contribute to this shape which is clearly seen in
projected surface density maps and 3D excess mass for non-resonant box orbits,
""banana"" orbits, ""fish/pretzel"" orbits and ""brezel"" orbits. {We find that
nearly all bar orbit families contribute some mass to the 3D BP/X-shape. All
co-added orbit families show a bifurcation in stellar number count distribution
with heliocentric distance that resembles the bifurcation observed in red clump
stars in the MW. However, only the box orbit family shows an increasing
separation of peaks with increasing galactic latitude $|b|$, similar to that
observed.} Our analysis shows that no single orbit family fully explains all
the observed features associated with the MW's BP/X shaped bulge, but
collectively the non-resonant boxes and various resonant boxlet orbits
contribute at different distances from the center to produce this feature. We
propose that since box orbits have three incommensurable orbital fundamental
frequencies, their 3-dimensional shapes are highly flexible and, like Lissajous
figures, this family of orbits is most easily able to adapt to evolution in the
shape of the underlying potential.
",0,1,0,0,0,0
20481,Irreducible network backbones: unbiased graph filtering via maximum entropy,"  Networks provide an informative, yet non-redundant description of complex
systems only if links represent truly dyadic relationships that cannot be
directly traced back to node-specific properties such as size, importance, or
coordinates in some embedding space. In any real-world network, some links may
be reducible, and others irreducible, to such local properties. This dichotomy
persists despite the steady increase in data availability and resolution, which
actually determines an even stronger need for filtering techniques aimed at
discerning essential links from non-essential ones. Here we introduce a
rigorous method that, for any desired level of statistical significance,
outputs the network backbone that is irreducible to the local properties of
nodes, i.e. their degrees and strengths. Unlike previous approaches, our method
employs an exact maximum-entropy formulation guaranteeing that the filtered
network encodes only the links that cannot be inferred from local information.
Extensive empirical analysis confirms that this approach uncovers essential
backbones that are otherwise hidden amidst many redundant relationships and
inaccessible to other methods. For instance, we retrieve the hub-and-spoke
skeleton of the US airport network and many specialised patterns of
international trade. Being irreducible to local transportation and economic
constraints of supply and demand, these backbones single out genuinely
higher-order wiring principles.
",1,1,0,0,0,0
18703,Angiogenic Factors produced by Hypoxic Cells are a leading driver of Anastomoses in Sprouting Angiogenesis---a computational study,"  Angiogenesis - the growth of new blood vessels from a pre-existing
vasculature - is key in both physiological processes and on several
pathological scenarios such as cancer progression or diabetic retinopathy. For
the new vascular networks to be functional, it is required that the growing
sprouts merge either with an existing functional mature vessel or with another
growing sprout. This process is called anastomosis. We present a systematic 2D
and 3D computational study of vessel growth in a tissue to address the
capability of angiogenic factor gradients to drive anastomosis formation. We
consider that these growth factors are produced only by tissue cells in
hypoxia, i.e. until nearby vessels merge and become capable of carrying blood
and irrigating their vicinity. We demonstrate that this increased production of
angiogenic factors by hypoxic cells is able to promote vessel anastomoses
events in both 2D and 3D. The simulations also verify that the morphology of
these networks has an increased resilience toward variations in the endothelial
cell's proliferation and chemotactic response. The distribution of tissue
cell`s and the concentration of the growth factors they produce are the major
factors in determining the final morphology of the network.
",0,0,0,0,1,0
1959,Rate Optimal Binary Linear Locally Repairable Codes with Small Availability,"  A locally repairable code with availability has the property that every code
symbol can be recovered from multiple, disjoint subsets of other symbols of
small size. In particular, a code symbol is said to have $(r,t)$-availability
if it can be recovered from $t$ disjoint subsets, each of size at most $r$. A
code with availability is said to be 'rate-optimal', if its rate is maximum
among the class of codes with given locality, availability, and alphabet size.
This paper focuses on rate-optimal binary, linear codes with small
availability, and makes four contributions. First, it establishes tight upper
bounds on the rate of binary linear codes with $(r,2)$ and $(2,3)$
availability. Second, it establishes a uniqueness result for binary
rate-optimal codes, showing that for certain classes of binary linear codes
with $(r,2)$ and $(2,3)$-availability, any rate optimal code must be a direct
sum of shorter rate optimal codes. Third, it presents novel upper bounds on the
rates of binary linear codes with $(2,t)$ and $(r,3)$-availability. In
particular, the main contribution here is a new method for bounding the number
of cosets of the dual of a code with availability, using its covering
properties. Finally, it presents a class of locally repairable linear codes
associated with convex polyhedra, focusing on the codes associated with the
Platonic solids. It demonstrates that these codes are locally repairable with
$t = 2$, and that the codes associated with (geometric) dual polyhedra are
(coding theoretic) duals of each other.
",1,0,1,0,0,0
16892,Perfect Half Space Games,"  We introduce perfect half space games, in which the goal of Player 2 is to
make the sums of encountered multi-dimensional weights diverge in a direction
which is consistent with a chosen sequence of perfect half spaces (chosen
dynamically by Player 2). We establish that the bounding games of Jurdziński
et al. (ICALP 2015) can be reduced to perfect half space games, which in turn
can be translated to the lexicographic energy games of Colcombet and
Niwiński, and are positionally determined in a strong sense (Player 2 can
play without knowing the current perfect half space). We finally show how
perfect half space games and bounding games can be employed to solve
multi-dimensional energy parity games in pseudo-polynomial time when both the
numbers of energy dimensions and of priorities are fixed, regardless of whether
the initial credit is given as part of the input or existentially quantified.
This also yields an optimal 2-EXPTIME complexity with given initial credit,
where the best known upper bound was non-elementary.
",1,0,0,0,0,0
13844,Using Session Types for Reasoning About Boundedness in the Pi-Calculus,"  The classes of depth-bounded and name-bounded processes are fragments of the
pi-calculus for which some of the decision problems that are undecidable for
the full calculus become decidable. P is depth-bounded at level k if every
reduction sequence for P contains successor processes with at most k active
nested restrictions. P is name-bounded at level k if every reduction sequence
for P contains successor processes with at most k active bound names.
Membership of these classes of processes is undecidable. In this paper we use
binary session types to decise two type systems that give a sound
characterization of the properties: If a process is well-typed in our first
system, it is depth-bounded. If a process is well-typed in our second, more
restrictive type system, it will also be name-bounded.
",1,0,0,0,0,0
3641,Alternative derivation of exact law for compressible and isothermal magnetohydrodynamics turbulence,"  The exact law for fully developed homogeneous compressible
magnetohydrodynamics (CMHD) turbulence is derived. For an isothermal plasma,
without the assumption of isotropy, the exact law is expressed as a function of
the plasma velocity field, the compressible Alfvén velocity and the scalar
density, instead of the Elsässer variables used in previous works. The
theoretical results show four different types of terms that are involved in the
nonlinear cascade of the total energy in the inertial range. Each category is
examined in detail, in particular those that can be written either as source or
flux terms. Finally, the role of the background magnetic field $B_0$ is
highlighted and comparison with the incompressible MHD (IMHD) model is
discussed. This point is particularly important when testing the exact law on
numerical simulations and in situ observations in space plasmas.
",0,1,0,0,0,0
11657,Free Boundary Minimal Surfaces in the Unit Three-Ball via Desingularization of the Critical Catenoid and the Equatorial Disk,"  We construct a new family of high genus examples of free boundary minimal
surfaces in the Euclidean unit 3-ball by desingularizing the intersection of a
coaxial pair of a critical catenoid and an equatorial disk. The surfaces are
constructed by singular perturbation methods and have three boundary
components. They are the free boundary analogue of the Costa-Hoffman-Meeks
surfaces and the surfaces constructed by Kapouleas by desingularizing coaxial
catenoids and planes. It is plausible that the minimal surfaces we constructed
here are the same as the ones obtained recently by Ketover using the min-max
method.
",0,0,1,0,0,0
5604,Powerful numbers in $(1^{\ell}+q^{\ell})(2^{\ell}+q^{\ell})\cdots (n^{\ell}+q^{\ell})$,"  Let $q$ be a positive integer. Recently, Niu and Liu proved that if $n\ge
\max\{q,1198-q\}$, then the product $(1^3+q^3)(2^3+q^3)\cdots (n^3+q^3)$ is not
a powerful number. In this note, we prove that (i) for any odd prime power
$\ell$ and $n\ge \max\{q,11-q\}$, the product
$(1^{\ell}+q^{\ell})(2^{\ell}+q^{\ell})\cdots (n^{\ell}+q^{\ell})$ is not a
powerful number; (2) for any positive odd integer $\ell$, there exists an
integer $N_{q,\ell}$ such that for any positive integer $n\ge N_{q,\ell}$, the
product $(1^{\ell}+q^{\ell})(2^{\ell}+q^{\ell})\cdots (n^{\ell}+q^{\ell})$ is
not a powerful number.
",0,0,1,0,0,0
10223,Optimal Prediction for Additive Function-on-Function Regression,"  As with classic statistics, functional regression models are invaluable in
the analysis of functional data. While there are now extensive tools with
accompanying theory available for linear models, there is still a great deal of
work to be done concerning nonlinear models for functional data. In this work
we consider the Additive Function-on-Function Regression model, a type of
nonlinear model that uses an additive relationship between the functional
outcome and functional covariate. We present an estimation methodology built
upon Reproducing Kernel Hilbert Spaces, and establish optimal rates of
convergence for our estimates in terms of prediction error. We also discuss
computational challenges that arise with such complex models, developing a
representer theorem for our estimate as well as a more practical and
computationally efficient approximation. Simulations and an application to
Cumulative Intraday Returns around the 2008 financial crisis are also provided.
",0,0,1,1,0,0
8348,A PCA-based approach for subtracting thermal background emission in high-contrast imaging data,"  Ground-based observations at thermal infrared wavelengths suffer from large
background radiation due to the sky, telescope and warm surfaces in the
instrument. This significantly limits the sensitivity of ground-based
observations at wavelengths longer than 3 microns. We analyzed this background
emission in infrared high contrast imaging data, show how it can be modelled
and subtracted and demonstrate that it can improve the detection of faint
sources, such as exoplanets. We applied principal component analysis to model
and subtract the thermal background emission in three archival high contrast
angular differential imaging datasets in the M and L filter. We describe how
the algorithm works and explain how it can be applied. The results of the
background subtraction are compared to the results from a conventional mean
background subtraction scheme. Finally, both methods for background subtraction
are also compared by performing complete data reductions. We analyze the
results from the M dataset of HD100546 qualitatively. For the M band dataset of
beta Pic and the L band dataset of HD169142, which was obtained with an annular
groove phase mask vortex vector coronagraph, we also calculate and analyze the
achieved signal to noise (S/N). We show that applying PCA is an effective way
to remove spatially and temporarily varying thermal background emission down to
close to the background limit. The procedure also proves to be very successful
at reconstructing the background that is hidden behind the PSF. In the complete
data reductions, we find at least qualitative improvements for HD100546 and
HD169142, however, we fail to find a significant increase in S/N of beta Pic b.
We discuss these findings and argue that in particular datasets with strongly
varying observing conditions or infrequently sampled sky background will
benefit from the new approach.
",0,1,0,0,0,0
9858,Using lab notebooks to examine students' engagement in modeling in an upper-division electronics lab course,"  We demonstrate how students' use of modeling can be examined and assessed
using student notebooks collected from an upper-division electronics lab
course. The use of models is a ubiquitous practice in undergraduate physics
education, but the process of constructing, testing, and refining these models
is much less common. We focus our attention on a lab course that has been
transformed to engage students in this modeling process during lab activities.
The design of the lab activities was guided by a framework that captures the
different components of model-based reasoning, called the Modeling Framework
for Experimental Physics. We demonstrate how this framework can be used to
assess students' written work and to identify how students' model-based
reasoning differed from activity to activity. Broadly speaking, we were able to
identify the different steps of students' model-based reasoning and assess the
completeness of their reasoning. Varying degrees of scaffolding present across
the activities had an impact on how thoroughly students would engage in the
full modeling process, with more scaffolded activities resulting in more
thorough engagement with the process. Finally, we identified that the step in
the process with which students had the most difficulty was the comparison
between their interpreted data and their model prediction. Students did not use
sufficiently sophisticated criteria in evaluating such comparisons, which had
the effect of halting the modeling process. This may indicate that in order to
engage students further in using model-based reasoning during lab activities,
the instructor needs to provide further scaffolding for how students make these
types of experimental comparisons. This is an important design consideration
for other such courses attempting to incorporate modeling as a learning goal.
",0,1,0,0,0,0
13128,Simulation of Matrix Product State on a Quantum Computer,"  The study of tensor network theory is an important field and promises a wide
range of experimental and quantum information theoretical applications. Matrix
product state is the most well-known example of tensor network states, which
provides an effective and efficient representation of one-dimensional quantum
systems. Indeed, it lies at the heart of density matrix renormalization group
(DMRG), a most common method for simulation of one-dimensional strongly
correlated quantum systems. It has got attention from several areas varying
from solid-state systems to quantum computing and quantum simulators. We have
considered maximally entangled matrix product states (GHZ and W). Here, we
designed the quantum circuits for implementing the matrix product states. In
this paper, we simulated the matrix product states in customized IBM (2-qubit,
3-qubit and 4-qubit) quantum systems and determined the probability
distribution among the quantum states.
",1,0,0,0,0,0
12398,Multiview Learning of Weighted Majority Vote by Bregman Divergence Minimization,"  We tackle the issue of classifier combinations when observations have
multiple views. Our method jointly learns view-specific weighted majority vote
classifiers (i.e. for each view) over a set of base voters, and a second
weighted majority vote classifier over the set of these view-specific weighted
majority vote classifiers. We show that the empirical risk minimization of the
final majority vote given a multiview training set can be cast as the
minimization of Bregman divergences. This allows us to derive a parallel-update
optimization algorithm for learning our multiview model. We empirically study
our algorithm with a particular focus on the impact of the training set size on
the multiview learning results. The experiments show that our approach is able
to overcome the lack of labeled information.
",0,0,0,1,0,0
19305,Tackling Diversity and Heterogeneity by Vertical Memory Management,"  Existing memory management mechanisms used in commodity computing machines
typically adopt hardware based address interleaving and OS directed random
memory allocation to service generic application requests. These conventional
memory management mechanisms are challenged by contention at multiple memory
levels, a daunting variety of workload behaviors, and an increasingly
complicated memory hierarchy. Our ISCA-41 paper proposes vertical partitioning
to eliminate shared resource contention at multiple levels in the memory
hierarchy. Combined with horizontal memory management policies, our framework
supports a flexible policy space for tackling diverse application needs in
production environment and is suitable for future heterogeneous memory systems.
",1,0,0,0,0,0
11004,Følner functions and the generic Word Problem for finitely generated amenable groups,"  We introduce and investigate different definitions of effective amenability,
in terms of computability of F{\o}lner sets, Reiter functions, and F{\o}lner
functions. As a consequence, we prove that recursively presented amenable
groups have subrecursive F{\o}lner function, answering a question of Gromov,
for the same class of groups we prove that solvability of the Equality Problem
on a generic set (generic EP) is equivalent to solvability of the Word Problem
on the whole group (WP), thus providing the first examples of finitely
presented groups with unsolvable generic EP. In particular, we prove that for
finitely presented groups, solvability of generic WP doesn't imply solvability
of generic EP.
",0,0,1,0,0,0
9985,Quenching the Kitaev honeycomb model,"  I studied the non-equilibrium response of an initial Néel state under
time evolution with the Kitaev honeycomb model. This time evolution can be
computed using a random sampling over all relevant flux configurations. With
isotropic interactions the system quickly equilibrates into a steady state
valence bond solid. Anisotropy induces an exponentially long prethermal regime
whose dynamics are governed by an effective toric code. Signatures of topology
are absent, however, due to the high energy density nature of the initial
state.
",0,1,0,0,0,0
11453,Universal Scalable Robust Solvers from Computational Information Games and fast eigenspace adapted Multiresolution Analysis,"  We show how the discovery of robust scalable numerical solvers for arbitrary
bounded linear operators can be automated as a Game Theory problem by
reformulating the process of computing with partial information and limited
resources as that of playing underlying hierarchies of adversarial information
games. When the solution space is a Banach space $B$ endowed with a quadratic
norm $\|\cdot\|$, the optimal measure (mixed strategy) for such games (e.g. the
adversarial recovery of $u\in B$, given partial measurements $[\phi_i, u]$ with
$\phi_i\in B^*$, using relative error in $\|\cdot\|$-norm as a loss) is a
centered Gaussian field $\xi$ solely determined by the norm $\|\cdot\|$, whose
conditioning (on measurements) produces optimal bets. When measurements are
hierarchical, the process of conditioning this Gaussian field produces a
hierarchy of elementary bets (gamblets). These gamblets generalize the notion
of Wavelets and Wannier functions in the sense that they are adapted to the
norm $\|\cdot\|$ and induce a multi-resolution decomposition of $B$ that is
adapted to the eigensubspaces of the operator defining the norm $\|\cdot\|$.
When the operator is localized, we show that the resulting gamblets are
localized both in space and frequency and introduce the Fast Gamblet Transform
(FGT) with rigorous accuracy and (near-linear) complexity estimates. As the FFT
can be used to solve and diagonalize arbitrary PDEs with constant coefficients,
the FGT can be used to decompose a wide range of continuous linear operators
(including arbitrary continuous linear bijections from $H^s_0$ to $H^{-s}$ or
to $L^2$) into a sequence of independent linear systems with uniformly bounded
condition numbers and leads to $\mathcal{O}(N \operatorname{polylog} N)$
solvers and eigenspace adapted Multiresolution Analysis (resulting in near
linear complexity approximation of all eigensubspaces).
",0,0,1,1,0,0
18240,Pulsejet engine dynamics in vertical motion using momentum conservation,"  The momentum conservation law is applied to analyse the dynamics of pulsejet
engine in vertical motion in a uniform gravitational field in the absence of
friction. The model predicts existence of a terminal speed given frequency of
the short pulses. The conditions that the engine does not return to the
starting position are identified. The number of short periodic pulses after
which the engine returns to the starting position is found to be independent of
the exhaust velocity and gravitational field intensity for certain frequency of
the pulses. The pulsejet engine and turbojet engine aircraft models of dynamics
are compared. Also the octopus dynamics is modelled. The paper is addressed to
intermediate undergraduate students of classical mechanics and aerospace
engineering.
",0,1,0,0,0,0
4581,Intense automorphisms of finite groups,"  Let $G$ be a group. An automorphism of $G$ is called intense if it sends each
subgroup of $G$ to a conjugate; the collection of such automorphisms is denoted
by $\mathrm{Int}(G)$. In the special case in which $p$ is a prime number and
$G$ is a finite $p$-group, one can show that $\mathrm{Int}(G)$ is the
semidirect product of a normal $p$-Sylow and a cyclic subgroup of order
dividing $p-1$. In this thesis we classify the finite $p$-groups whose groups
of intense automorphisms are not themselves $p$-groups. It emerges from our
investigation that the structure of such groups is almost completely determined
by their nilpotency class: for $p>3$, they share a quotient, growing with their
class, with a uniquely determined infinite $2$-generated pro-$p$ group.
",0,0,1,0,0,0
16321,Cooperative Multi-Sender Index Coding,"  In this paper, we propose a new coding scheme and establish new bounds on the
capacity region for the multi-sender unicast index-coding problem. We revisit
existing partitioned Distributed Composite Coding (DCC) proposed by Sadeghi et
al. and identify its limitations in the implementation of multi-sender
composite coding and in the strategy of sender partitioning. We then propose
two new coding components to overcome these limitations and develop a
multi-sender Cooperative Composite Coding (CCC). We show that CCC can strictly
improve upon partitioned DCC, and is the key to achieve optimality for a number
of index-coding instances. The usefulness of CCC and its special cases is
illuminated via non-trivial examples, and the capacity region is established
for each example. Comparisons between CCC and other non-cooperative schemes in
recent works are also provided to further demonstrate the advantage of CCC.
",1,0,1,0,0,0
4789,The process of purely event-driven programs,"  Using process algebra, this paper describes the formalisation of the
process/semantics behind the purely event-driven programming language.
",1,0,0,0,0,0
522,Fully Bayesian Estimation Under Informative Sampling,"  Bayesian estimation is increasingly popular for performing model based
inference to support policymaking. These data are often collected from surveys
under informative sampling designs where subject inclusion probabilities are
designed to be correlated with the response variable of interest. Sampling
weights constructed from marginal inclusion probabilities are typically used to
form an exponentiated pseudo likelihood that adjusts the population likelihood
for estimation on the sample due to ease-of-estimation. We propose an
alternative adjustment based on a Bayes rule construction that simultaneously
performs weight smoothing and estimates the population model parameters in a
fully Bayesian construction. We formulate conditions on known marginal and
pairwise inclusion probabilities that define a class of sampling designs where
$L_{1}$ consistency of the joint posterior is guaranteed. We compare
performances between the two approaches on synthetic data, which reveals that
our fully Bayesian approach better estimates posterior uncertainty without a
requirement to calibrate the normalization of the sampling weights. We
demonstrate our method on an application concerning the National Health and
Nutrition Examination Survey exploring the relationship between caffeine
consumption and systolic blood pressure.
",0,0,1,1,0,0
17663,Boosting Variational Inference: an Optimization Perspective,"  Variational inference is a popular technique to approximate a possibly
intractable Bayesian posterior with a more tractable one. Recently, boosting
variational inference has been proposed as a new paradigm to approximate the
posterior by a mixture of densities by greedily adding components to the
mixture. However, as is the case with many other variational inference
algorithms, its theoretical properties have not been studied. In the present
work, we study the convergence properties of this approach from a modern
optimization viewpoint by establishing connections to the classic Frank-Wolfe
algorithm. Our analyses yields novel theoretical insights regarding the
sufficient conditions for convergence, explicit rates, and algorithmic
simplifications. Since a lot of focus in previous works for variational
inference has been on tractability, our work is especially important as a much
needed attempt to bridge the gap between probabilistic models and their
corresponding theoretical properties.
",1,0,0,1,0,0
14747,"Laplace equation for the Dirac, Euler and the harmonic oscillator","  In this article, we give the explicit solutions to the Laplace equations
associated to the Dirac operator, Euler operator and the harmonic oscillator in
R.
",0,0,1,0,0,0
20677,Weakly tripotent rings,"  We study the class of rings $R$ with the property that for $x\in R$ at least
one of the elements $x$ and $1+x$ are tripotent.
",0,0,1,0,0,0
18664,Dispersionless and multicomponent BKP hierarchies with quantum torus symmetries,"  In this article, we will construct the additional perturbative quantum torus
symmetry of the dispersionless BKP hierarchy basing on the $W_{\infty}$
infinite dimensional Lie symmetry. These results show that the complete quantum
torus symmetry is broken from the BKP hierarchy to its dispersionless
hierarchy. Further a series of additional flows of the multicomponent BKP
hierarchy will be defined and these flows constitute an $N$-folds direct
product of the positive half of the quantum torus symmetries.
",0,1,1,0,0,0
11211,Random problems with R,"  R (Version 3.5.1 patched) has an issue with its random sampling
functionality. R generates random integers between $1$ and $m$ by multiplying
random floats by $m$, taking the floor, and adding $1$ to the result.
Well-known quantization effects in this approach result in a non-uniform
distribution on $\{ 1, \ldots, m\}$. The difference, which depends on $m$, can
be substantial. Because the sample function in R relies on generating random
integers, random sampling in R is biased. There is an easy fix: construct
random integers directly from random bits, rather than multiplying a random
float by $m$. That is the strategy taken in Python's numpy.random.randint()
function, among others. Example source code in Python is available at
this https URL
(see functions getrandbits() and randbelow_from_randbits()).
",0,0,0,1,0,0
12163,Metachronal motion of artificial magnetic cilia,"  Organisms use hair-like cilia that beat in a metachronal fashion to actively
transport fluid and suspended particles. Metachronal motion emerges due to a
phase difference between beating cycles of neighboring cilia and appears as
traveling waves propagating along ciliary carpet. In this work, we demonstrate
biomimetic artificial cilia capable of metachronal motion. The cilia are
micromachined magnetic thin filaments attached at one end to a substrate and
actuated by a uniform rotating magnetic field. We show that the difference in
magnetic cilium length controls the phase of the beating motion. We use this
property to induce metachronal waves within a ciliary array and explore the
effect of operation parameters on the wave motion. The metachronal motion in
our artificial system is shown to depend on the magnetic and elastic properties
of the filaments, unlike natural cilia, where metachronal motion arises due to
fluid coupling. Our approach enables an easy integration of metachronal
magnetic cilia in lab-on-a-chip devices for enhanced fluid and particle
manipulations.
",0,0,0,0,1,0
12132,QCRI Machine Translation Systems for IWSLT 16,"  This paper describes QCRI's machine translation systems for the IWSLT 2016
evaluation campaign. We participated in the Arabic->English and English->Arabic
tracks. We built both Phrase-based and Neural machine translation models, in an
effort to probe whether the newly emerged NMT framework surpasses the
traditional phrase-based systems in Arabic-English language pairs. We trained a
very strong phrase-based system including, a big language model, the Operation
Sequence Model, Neural Network Joint Model and Class-based models along with
different domain adaptation techniques such as MML filtering, mixture modeling
and using fine tuning over NNJM model. However, a Neural MT system, trained by
stacking data from different genres through fine-tuning, and applying ensemble
over 8 models, beat our very strong phrase-based system by a significant 2 BLEU
points margin in Arabic->English direction. We did not obtain similar gains in
the other direction but were still able to outperform the phrase-based system.
We also applied system combination on phrase-based and NMT outputs.
",1,0,0,0,0,0
7094,Efficient Computation of the Stochastic Behavior of Partial Sum Processes,"  In this paper the computational aspects of probability calculations for
dynamical partial sum expressions are discussed. Such dynamical partial sum
expressions have many important applications, and examples are provided in the
fields of reliability, product quality assessment, and stochastic control.
While these probability calculations are ostensibly of a high dimension, and
consequently intractable in general, it is shown how a recursive integration
methodology can be implemented to obtain exact calculations as a series of
two-dimensional calculations. The computational aspects of the implementaion of
this methodology, with the adoption of Fast Fourier Transforms, are discussed.
",0,0,0,1,0,0
8666,Dissecting spin-phonon equilibration in ferrimagnetic insulators by ultrafast lattice excitation,"  To gain control over magnetic order on ultrafast time scales, a fundamental
understanding of the way electron spins interact with the surrounding crystal
lattice is required. However, measurement and analysis even of basic collective
processes such as spin-phonon equilibration have remained challenging. Here, we
directly probe the flow of energy and angular momentum in the model insulating
ferrimagnet yttrium iron garnet. Following ultrafast resonant lattice
excitation, we observe that magnetic order reduces on distinct time scales of 1
ps and 100 ns. Temperature-dependent measurements, a spin-coupling analysis and
simulations show that the two dynamics directly reflect two stages of
spin-lattice equilibration. On the 1-ps scale, spins and phonons reach
quasi-equilibrium in terms of energy through phonon-induced modulation of the
exchange interaction. This mechanism leads to identical demagnetization of the
ferrimagnet's two spin-sublattices and a novel ferrimagnetic state of increased
temperature yet unchanged total magnetization. Finally, on the much slower,
100-ns scale, the excess of spin angular momentum is released to the crystal
lattice, resulting in full equilibrium. Our findings are relevant for all
insulating ferrimagnets and indicate that spin manipulation by phonons,
including the spin Seebeck effect, can be extended to antiferromagnets and into
the terahertz frequency range.
",0,1,0,0,0,0
4599,The sequential loss of allelic diversity,"  This paper gives a new flavor of what Peter Jagers and his co-authors call
`the path to extinction'. In a neutral population with constant size $N$, we
assume that each individual at time $0$ carries a distinct type, or allele. We
consider the joint dynamics of these $N$ alleles, for example the dynamics of
their respective frequencies and more plainly the nonincreasing process
counting the number of alleles remaining by time $t$. We call this process the
extinction process. We show that in the Moran model, the extinction process is
distributed as the process counting (in backward time) the number of common
ancestors to the whole population, also known as the block counting process of
the $N$-Kingman coalescent. Stimulated by this result, we investigate: (1)
whether it extends to an identity between the frequencies of blocks in the
Kingman coalescent and the frequencies of alleles in the extinction process,
both evaluated at jump times; (2) whether it extends to the general case of
$\Lambda$-Fleming-Viot processes.
",0,0,0,0,1,0
6313,An approach to Griffiths conjecture,"  The Griffiths conjecture asserts that every ample vector bundle $E$ over a
compact complex manifold $S$ admits a hermitian metric with positive curvature
in the sense of Griffiths. In this article we give a sufficient condition for a
positive hermitian metric on $\mathcal{O}_{\mathbb{P}(E^*)}(1)$ to induce a
Griffiths positive $L^2$-metric on the vector bundle $E$. This result suggests
to study the relative Kähler-Ricci flow on $\mathcal{O}_{\mathbb{P}(E^*)}(1)$
for the fibration $\mathbb{P}(E^*)\to S$. We define a flow and give arguments
for the convergence.
",0,0,1,0,0,0
17961,Effective Blog Pages Extractor for Better UGC Accessing,"  Blog is becoming an increasingly popular media for information publishing.
Besides the main content, most of blog pages nowadays also contain noisy
information such as advertisements etc. Removing these unrelated elements can
improves user experience, but also can better adapt the content to various
devices such as mobile phones. Though template-based extractors are highly
accurate, they may incur expensive cost in that a large number of template need
to be developed and they will fail once the template is updated. To address
these issues, we present a novel template-independent content extractor for
blog pages. First, we convert a blog page into a DOM-Tree, where all elements
including the title and body blocks in a page correspond to subtrees. Then we
construct subtree candidate set for the title and the body blocks respectively,
and extract both spatial and content features for elements contained in the
subtree. SVM classifiers for the title and the body blocks are trained using
these features. Finally, the classifiers are used to extract the main content
from blog pages. We test our extractor on 2,250 blog pages crawled from nine
blog sites with obviously different styles and templates. Experimental results
verify the effectiveness of our extractor.
",1,0,0,0,0,0
3872,Automatic Analysis of EEGs Using Big Data and Hybrid Deep Learning Architectures,"  Objective: A clinical decision support tool that automatically interprets
EEGs can reduce time to diagnosis and enhance real-time applications such as
ICU monitoring. Clinicians have indicated that a sensitivity of 95% with a
specificity below 5% was the minimum requirement for clinical acceptance. We
propose a highperformance classification system based on principles of big data
and machine learning. Methods: A hybrid machine learning system that uses
hidden Markov models (HMM) for sequential decoding and deep learning networks
for postprocessing is proposed. These algorithms were trained and evaluated
using the TUH EEG Corpus, which is the world's largest publicly available
database of clinical EEG data. Results: Our approach delivers a sensitivity
above 90% while maintaining a specificity below 5%. This system detects three
events of clinical interest: (1) spike and/or sharp waves, (2) periodic
lateralized epileptiform discharges, (3) generalized periodic epileptiform
discharges. It also detects three events used to model background noise: (1)
artifacts, (2) eye movement (3) background. Conclusions: A hybrid HMM/deep
learning system can deliver a low false alarm rate on EEG event detection,
making automated analysis a viable option for clinicians. Significance: The TUH
EEG Corpus enables application of highly data consumptive machine learning
algorithms to EEG analysis. Performance is approaching clinical acceptance for
real-time applications.
",1,0,0,1,0,0
3292,Ensemble Classifier for Eye State Classification using EEG Signals,"  The growing importance and utilization of measuring brain waves (e.g. EEG
signals of eye state) in brain-computer interface (BCI) applications
highlighted the need for suitable classification methods. In this paper, a
comparison between three of well-known classification methods (i.e. support
vector machine (SVM), hidden Markov map (HMM), and radial basis function (RBF))
for EEG based eye state classification was achieved. Furthermore, a suggested
method that is based on ensemble model was tested. The suggested (ensemble
system) method based on a voting algorithm with two kernels: random forest (RF)
and Kstar classification methods. The performance was tested using three
measurement parameters: accuracy, mean absolute error (MAE), and confusion
matrix. Results showed that the proposed method outperforms the other tested
methods. For instance, the suggested method's performance was 97.27% accuracy
and 0.13 MAE.
",1,0,0,0,0,0
3303,Rapid Adaptation with Conditionally Shifted Neurons,"  We describe a mechanism by which artificial neural networks can learn rapid
adaptation - the ability to adapt on the fly, with little data, to new tasks -
that we call conditionally shifted neurons. We apply this mechanism in the
framework of metalearning, where the aim is to replicate some of the
flexibility of human learning in machines. Conditionally shifted neurons modify
their activation values with task-specific shifts retrieved from a memory
module, which is populated rapidly based on limited task experience. On
metalearning benchmarks from the vision and language domains, models augmented
with conditionally shifted neurons achieve state-of-the-art results.
",1,0,0,1,0,0
11116,A Privacy-preserving Community-based P2P OSNs Using Broadcast Encryption Supporting Recommendation Mechanism,"  Online Social Networks (OSNs) have become one of the most important
activities on the Internet, such as Facebook and Google+. However, security and
privacy have become major concerns in existing C/S based OSNs. In this paper,
we propose a novel scheme called a Privacy-preserving Community-based P2P OSNs
Using Broadcast Encryption Supporting Recommendation Mechanism (PCBE) that
supports cross-platform availability in stringent privacy requirements. For the
first time, we introduce recommendation mechanism into a privacy-preserving P2P
based OSNs, in which we firstly employ the Open Directory Project to generate
user interest model. We firstly introduce broadcast encryption into P2P
community-based social networks together with reputation mechanism to decrease
the system overhead. We formulate the security requirements and design goals
for privacy- preserving P2P based OSNs supporting recommendation mechanism. The
RESTful web-services help to ensure cross-platform availability and
transmission security. As a result, thorough security analysis and performance
evaluation on experiments demonstrate that the PCBE scheme indeed accords with
our proposed design goals.
",1,0,0,0,0,0
2201,"On the incorporation of interval-valued fuzzy sets into the Bousi-Prolog system: declarative semantics, implementation and applications","  In this paper we analyse the benefits of incorporating interval-valued fuzzy
sets into the Bousi-Prolog system. A syntax, declarative semantics and im-
plementation for this extension is presented and formalised. We show, by using
potential applications, that fuzzy logic programming frameworks enhanced with
them can correctly work together with lexical resources and ontologies in order
to improve their capabilities for knowledge representation and reasoning.
",1,0,0,0,0,0
14951,Exploring the Single-Particle Mobility Edge in a One-Dimensional Quasiperiodic Optical Lattice,"  A single-particle mobility edge (SPME) marks a critical energy separating
extended from localized states in a quantum system. In one-dimensional systems
with uncorrelated disorder, a SPME cannot exist, since all single-particle
states localize for arbitrarily weak disorder strengths. However, if
correlations are present in the disorder potential, the localization transition
can occur at a finite disorder strength and SPMEs become possible. In this
work, we find experimental evidence for the existence of such a SPME in a
one-dimensional quasi-periodic optical lattice. Specifically, we find a regime
where extended and localized single-particle states coexist, in good agreement
with theoretical simulations, which predict a SPME in this regime.
",0,1,0,0,0,0
9404,FPT-algorithms for The Shortest Lattice Vector and Integer Linear Programming Problems,"  In this paper, we present FPT-algorithms for special cases of the shortest
vector problem (SVP) and the integer linear programming problem (ILP), when
matrices included to the problems' formulations are near square. The main
parameter is the maximal absolute value of rank minors of matrices included to
the problem formulation. Additionally, we present FPT-algorithms with respect
to the same main parameter for the problems, when the matrices have no singular
rank sub-matrices.
",1,0,0,0,0,0
1258,Neural IR Meets Graph Embedding: A Ranking Model for Product Search,"  Recently, neural models for information retrieval are becoming increasingly
popular. They provide effective approaches for product search due to their
competitive advantages in semantic matching. However, it is challenging to use
graph-based features, though proved very useful in IR literature, in these
neural approaches. In this paper, we leverage the recent advances in graph
embedding techniques to enable neural retrieval models to exploit
graph-structured data for automatic feature extraction. The proposed approach
can not only help to overcome the long-tail problem of click-through data, but
also incorporate external heterogeneous information to improve search results.
Extensive experiments on a real-world e-commerce dataset demonstrate
significant improvement achieved by our proposed approach over multiple strong
baselines both as an individual retrieval model and as a feature used in
learning-to-rank frameworks.
",1,0,0,0,0,0
3373,A contract-based method to specify stimulus-response requirements,"  A number of formal methods exist for capturing stimulus-response requirements
in a declarative form. Someone yet needs to translate the resulting declarative
statements into imperative programs. The present article describes a method for
specification and verification of stimulus-response requirements in the form of
imperative program routines with conditionals and assertions. A program prover
then checks a candidate program directly against the stated requirements. The
article illustrates the approach by applying it to an ASM model of the Landing
Gear System, a widely used realistic example proposed for evaluating
specification and verification techniques.
",1,0,0,0,0,0
20556,How Usable are Rust Cryptography APIs?,"  Context: Poor usability of cryptographic APIs is a severe source of
vulnerabilities. Aim: We wanted to find out what kind of cryptographic
libraries are present in Rust and how usable they are. Method: We explored
Rust's cryptographic libraries through a systematic search, conducted an
exploratory study on the major libraries and a controlled experiment on two of
these libraries with 28 student participants. Results: Only half of the major
libraries explicitly focus on usability and misuse resistance, which is
reflected in their current APIs. We found that participants were more
successful using rust-crypto which we considered less usable than ring before
the experiment. Conclusion: We discuss API design insights and make
recommendations for the design of crypto libraries in Rust regarding the detail
and structure of the documentation, higher-level APIs as wrappers for the
existing low-level libraries, and selected, good-quality example code to
improve the emerging cryptographic libraries of Rust.
",1,0,0,0,0,0
15649,The normal distribution is freely selfdecomposable,"  The class of selfdecomposable distributions in free probability theory was
introduced by Barndorff-Nielsen and the third named author. It constitutes a
fairly large subclass of the freely infinitely divisible distributions, but so
far specific examples have been limited to Wigner's semicircle distributions,
the free stable distributions, two kinds of free gamma distributions and a few
other examples. In this paper, we prove that the (classical) normal
distributions are freely selfdecomposable. More generally it is established
that the Askey-Wimp-Kerov distribution $\mu_c$ is freely selfdecomposable for
any $c$ in $[-1,0]$. The main ingredient in the proof is a general
characterization of the freely selfdecomposable distributions in terms of the
derivative of their free cumulant transform.
",0,0,1,0,0,0
968,Usability of Humanly Computable Passwords,"  Reusing passwords across multiple websites is a common practice that
compromises security. Recently, Blum and Vempala have proposed password
strategies to help people calculate, in their heads, passwords for different
sites without dependence on third-party tools or external devices. Thus far,
the security and efficiency of these ""mental algorithms"" has been analyzed only
theoretically. But are such methods usable? We present the first usability
study of humanly computable password strategies, involving a learning phase (to
learn a password strategy), then a rehearsal phase (to login to a few
websites), and multiple follow-up tests. In our user study, with training,
participants were able to calculate a deterministic eight-character password
for an arbitrary new website in under 20 seconds.
",1,0,0,0,0,0
1186,Learning to attend in a brain-inspired deep neural network,"  Recent machine learning models have shown that including attention as a
component results in improved model accuracy and interpretability, despite the
concept of attention in these approaches only loosely approximating the brain's
attention mechanism. Here we extend this work by building a more brain-inspired
deep network model of the primate ATTention Network (ATTNet) that learns to
shift its attention so as to maximize the reward. Using deep reinforcement
learning, ATTNet learned to shift its attention to the visual features of a
target category in the context of a search task. ATTNet's dorsal layers also
learned to prioritize these shifts of attention so as to maximize success of
the ventral pathway classification and receive greater reward. Model behavior
was tested against the fixations made by subjects searching images for the same
cued category. Both subjects and ATTNet showed evidence for attention being
preferentially directed to target goals, behaviorally measured as oculomotor
guidance to targets. More fundamentally, ATTNet learned to shift its attention
to target like objects and spatially route its visual inputs to accomplish the
task. This work makes a step toward a better understanding of the role of
attention in the brain and other computational systems.
",0,0,0,0,1,0
15409,Transparency and Explanation in Deep Reinforcement Learning Neural Networks,"  Autonomous AI systems will be entering human society in the near future to
provide services and work alongside humans. For those systems to be accepted
and trusted, the users should be able to understand the reasoning process of
the system, i.e. the system should be transparent. System transparency enables
humans to form coherent explanations of the system's decisions and actions.
Transparency is important not only for user trust, but also for software
debugging and certification. In recent years, Deep Neural Networks have made
great advances in multiple application areas. However, deep neural networks are
opaque. In this paper, we report on work in transparency in Deep Reinforcement
Learning Networks (DRLN). Such networks have been extremely successful in
accurately learning action control in image input domains, such as Atari games.
In this paper, we propose a novel and general method that (a) incorporates
explicit object recognition processing into deep reinforcement learning models,
(b) forms the basis for the development of ""object saliency maps"", to provide
visualization of internal states of DRLNs, thus enabling the formation of
explanations and (c) can be incorporated in any existing deep reinforcement
learning framework. We present computational results and human experiments to
evaluate our approach.
",0,0,0,1,0,0
4923,From Quenched Disorder to Continuous Time Random Walk,"  This work focuses on quantitative representation of transport in systems with
quenched disorder. Explicit mapping of the quenched trap model to continuous
time random walk is presented. Linear temporal transformation: $t\to
t/\Lambda^{1/\alpha}$ for transient process on translationally invariant
lattice, in the sub-diffusive regime, is sufficient for asymptotic mapping.
Exact form of the constant $\Lambda^{1/\alpha}$ is established. Disorder
averaged position probability density function for quenched trap model is
obtained and analytic expressions for the diffusion coefficient and drift are
provided.
",0,1,0,0,0,0
17384,Alternating minimization for dictionary learning with random initialization,"  We present theoretical guarantees for an alternating minimization algorithm
for the dictionary learning/sparse coding problem. The dictionary learning
problem is to factorize vector samples $y^{1},y^{2},\ldots, y^{n}$ into an
appropriate basis (dictionary) $A^*$ and sparse vectors $x^{1*},\ldots,x^{n*}$.
Our algorithm is a simple alternating minimization procedure that switches
between $\ell_1$ minimization and gradient descent in alternate steps.
Dictionary learning and specifically alternating minimization algorithms for
dictionary learning are well studied both theoretically and empirically.
However, in contrast to previous theoretical analyses for this problem, we
replace the condition on the operator norm (that is, the largest magnitude
singular value) of the true underlying dictionary $A^*$ with a condition on the
matrix infinity norm (that is, the largest magnitude term). This not only
allows us to get convergence rates for the error of the estimated dictionary
measured in the matrix infinity norm, but also ensures that a random
initialization will provably converge to the global optimum. Our guarantees are
under a reasonable generative model that allows for dictionaries with growing
operator norms, and can handle an arbitrary level of overcompleteness, while
having sparsity that is information theoretically optimal. We also establish
upper bounds on the sample complexity of our algorithm.
",1,0,0,1,0,0
9826,t-SNE-CUDA: GPU-Accelerated t-SNE and its Applications to Modern Data,"  Modern datasets and models are notoriously difficult to explore and analyze
due to their inherent high dimensionality and massive numbers of samples.
Existing visualization methods which employ dimensionality reduction to two or
three dimensions are often inefficient and/or ineffective for these datasets.
This paper introduces t-SNE-CUDA, a GPU-accelerated implementation of
t-distributed Symmetric Neighbor Embedding (t-SNE) for visualizing datasets and
models. t-SNE-CUDA significantly outperforms current implementations with
50-700x speedups on the CIFAR-10 and MNIST datasets. These speedups enable, for
the first time, visualization of the neural network activations on the entire
ImageNet dataset - a feat that was previously computationally intractable. We
also demonstrate visualization performance in the NLP domain by visualizing the
GloVe embedding vectors. From these visualizations, we can draw interesting
conclusions about using the L2 metric in these embedding spaces. t-SNE-CUDA is
publicly available atthis https URL
",1,0,0,1,0,0
3673,Nano-jet Related to Bessel Beams and to Super-Resolutions in Micro-sphere Optical Experiments,"  The appearance of a Nano-jet in the micro-sphere optical experiments is
analyzed by relating this effect to non-diffracting Bessel beams. By inserting
a circular aperture with a radius which is in the order of subwavelength in the
EM waist, and sending the transmitted light into a confocal microscope, EM
fluctuations by the different Bessel beams are avoided. On this constant EM
field evanescent waves are superposed. While this effect improves the
optical-depth of the imaging process, the object fine-structures are obtained,
from the modulation of the EM fields by the evanescent waves. The use of a
combination of the micro-sphere optical system with an interferometer for phase
contrast measurements is described.
",0,1,0,0,0,0
14759,Hyperpolarizability and operational magic wavelength in an optical lattice clock,"  Optical clocks benefit from tight atomic confinement enabling extended
interrogation times as well as Doppler- and recoil-free operation. However,
these benefits come at the cost of frequency shifts that, if not properly
controlled, may degrade clock accuracy. Numerous theoretical studies have
predicted optical lattice clock frequency shifts that scale nonlinearly with
trap depth. To experimentally observe and constrain these shifts in an
$^{171}$Yb optical lattice clock, we construct a lattice enhancement cavity
that exaggerates the light shifts. We observe an atomic temperature that is
proportional to the optical trap depth, fundamentally altering the scaling of
trap-induced light shifts and simplifying their parametrization. We identify an
""operational"" magic wavelength where frequency shifts are insensitive to
changes in trap depth. These measurements and scaling analysis constitute an
essential systematic characterization for clock operation at the $10^{-18}$
level and beyond.
",0,1,0,0,0,0
11616,Debiasing the Debiased Lasso with Bootstrap,"  In this paper, we prove that under proper conditions, bootstrap can further
debias the debiased Lasso estimator for statistical inference of
low-dimensional parameters in high-dimensional linear regression. We prove that
the required sample size for inference with bootstrapped debiased Lasso, which
involves the number of small coefficients, can be of smaller order than the
existing ones for the debiased Lasso. Therefore, our results reveal the
benefits of having strong signals. Our theory is supported by results of
simulation experiments, which compare coverage probabilities and lengths of
confidence intervals with and without bootstrap, with and without debiasing.
",0,0,1,1,0,0
20548,Multimodal Nonlinear Microscope based on a Compact Fiber-format Laser Source,"  We present a multimodal non-linear optical (NLO) laser-scanning microscope,
based on a compact fiber-format excitation laser and integrating coherent
anti-Stokes Raman scattering (CARS), stimulated Raman scattering (SRS) and
two-photon-excitation fluorescence (TPEF) on a single platform. We demonstrate
its capabilities in simultaneously acquiring CARS and SRS images of a blend of
6-{\mu}m poly(methyl methacrylate) beads and 3-{\mu}m polystyrene beads. We
then apply it to visualize cell walls and chloroplast of an unprocessed fresh
leaf of Elodea aquatic plant via SRS and TPEF modalities, respectively. The
presented NLO microscope, developed in house using off-the-shelf components,
offers full accessibility to the optical path and ensures its easy
re-configurability and flexibility.
",0,1,0,0,0,0
5413,Short-term Memory of Deep RNN,"  The extension of deep learning towards temporal data processing is gaining an
increasing research interest. In this paper we investigate the properties of
state dynamics developed in successive levels of deep recurrent neural networks
(RNNs) in terms of short-term memory abilities. Our results reveal interesting
insights that shed light on the nature of layering as a factor of RNN design.
Noticeably, higher layers in a hierarchically organized RNN architecture
results to be inherently biased towards longer memory spans even prior to
training of the recurrent connections. Moreover, in the context of Reservoir
Computing framework, our analysis also points out the benefit of a layered
recurrent organization as an efficient approach to improve the memory skills of
reservoir models.
",0,0,0,1,0,0
6444,A formalization of convex polyhedra based on the simplex method,"  We present a formalization of convex polyhedra in the proof assistant Coq.
The cornerstone of our work is a complete implementation of the simplex method,
together with the proof of its correctness and termination. This allows us to
define the basic predicates over polyhedra in an effective way (i.e., as
programs), and relate them with the corresponding usual logical counterparts.
To this end, we make an extensive use of the Boolean reflection methodology.
The benefit of this approach is that we can easily derive the proof of several
fundamental results on polyhedra, such as Farkas' Lemma, the duality theorem of
linear programming, and Minkowski's Theorem.
",1,0,1,0,0,0
13957,A high-order nonconservative approach for hyperbolic equations in fluid dynamics,"  It is well known, thanks to Lax-Wendroff theorem, that the local conservation
of a numerical scheme for a conservative hyperbolic system is a simple and
systematic way to guarantee that, if stable, a scheme will provide a sequence
of solutions that will converge to a weak solution of the continuous problem.
In [1], it is shown that a nonconservative scheme will not provide a good
solution. The question of using, nevertheless, a nonconservative formulation of
the system and getting the correct solution has been a long-standing debate. In
this paper, we show how get a relevant weak solution from a pressure-based
formulation of the Euler equations of fluid mechanics. This is useful when
dealing with nonlinear equations of state because it is easier to compute the
internal energy from the pressure than the opposite. This makes it possible to
get oscillation free solutions, contrarily to classical conservative methods.
An extension to multiphase flows is also discussed, as well as a
multidimensional extension.
",0,0,1,0,0,0
18538,Glass Transition in Supercooled Liquids with Medium Range Crystalline Order,"  The origins of rapid dynamical slow down in glass forming liquids in the
growth of static length scales, possibly associated with identifiable
structural ordering, is a much debated issue. Growth of medium range
crystalline order (MRCO) has been observed in various model systems to be
associated with glassy behaviour. Such observations raise the question about
the eventual state reached by a glass former, if allowed to relax for
sufficiently long times. Is a slowly growing crystalline order responsible for
slow dynamics? Are the molecular mechanisms for glass transition in liquids
with and without MRCO the same? If yes, glass formers with MRCO provide a
paradigm for understanding glassy behaviour generically. If not, systems with
MRCO form a new class of glass forming materials whose molecular mechanism for
slow dynamics may be easier to understand in terms of growing crystalline
order, and should be approached in that manner, even while they will not
provide generic insights. In this study we perform extensive molecular dynamics
simulations of a number of glass forming liquids in two dimensions and show
that the static and dynamic properties of glasses with MRCO are different from
other glass forming liquids with no predominant local order. We also resolve an
important issue regarding the so-called Point-to-set method for determining
static length scales, and demonstrate it to be a robust, order agnostic, method
for determining static correlation lengths in glass formers.
",0,1,0,0,0,0
9468,Spiral arms and disc stability in the Andromeda galaxy,"  Aims: Density waves are often considered as the triggering mechanism of star
formation in spiral galaxies. Our aim is to study relations between different
star formation tracers (stellar UV and near-IR radiation and emission from HI,
CO and cold dust) in the spiral arms of M31, to calculate stability conditions
in the galaxy disc and to draw conclusions about possible star formation
triggering mechanisms.
Methods: We select fourteen spiral arm segments from the de-projected data
maps and compare emission distributions along the cross sections of the
segments in different datasets to each other, in order to detect spatial
offsets between young stellar populations and the star forming medium. By using
the disc stability condition as a function of perturbation wavelength and
distance from the galaxy centre we calculate the effective disc stability
parameters and the least stable wavelengths at different distances. For this we
utilise a mass distribution model of M31 with four disc components (old and
young stellar discs, cold and warm gaseous discs) embedded within the external
potential of the bulge, the stellar halo and the dark matter halo. Each
component is considered to have a realistic finite thickness.
Results: No systematic offsets between the observed UV and CO/far-IR emission
across the spiral segments are detected. The calculated effective stability
parameter has a minimal value Q_{eff} ~ 1.8 at galactocentric distances 12 - 13
kpc. The least stable wavelengths are rather long, with the minimal values
starting from ~ 3 kpc at distances R > 11 kpc.
Conclusions: The classical density wave theory is not a realistic explanation
for the spiral structure of M31. Instead, external causes should be considered,
e.g. interactions with massive gas clouds or dwarf companions of M31.
",0,1,0,0,0,0
2290,MM Algorithms for Variance Component Estimation and Selection in Logistic Linear Mixed Model,"  Logistic linear mixed model is widely used in experimental designs and
genetic analysis with binary traits. Motivated by modern applications, we
consider the case with many groups of random effects and each group corresponds
to a variance component. When the number of variance components is large,
fitting the logistic linear mixed model is challenging. We develop two
efficient and stable minorization-maximization (MM) algorithms for the
estimation of variance components based on the Laplace approximation of the
logistic model. One of them leads to a simple iterative soft-thresholding
algorithm for variance component selection using maximum penalized approximated
likelihood. We demonstrate the variance component estimation and selection
performance of our algorithms by simulation studies and a real data analysis.
",0,0,0,1,0,0
18379,EmbNum: Semantic labeling for numerical values with deep metric learning,"  Semantic labeling for numerical values is a task of assigning semantic labels
to unknown numerical attributes. The semantic labels could be numerical
properties in ontologies, instances in knowledge bases, or labeled data that
are manually annotated by domain experts. In this paper, we refer to semantic
labeling as a retrieval setting where the label of an unknown attribute is
assigned by the label of the most relevant attribute in labeled data. One of
the greatest challenges is that an unknown attribute rarely has the same set of
values with the similar one in the labeled data. To overcome the issue,
statistical interpretation of value distribution is taken into account.
However, the existing studies assume a specific form of distribution. It is not
appropriate in particular to apply open data where there is no knowledge of
data in advance. To address these problems, we propose a neural numerical
embedding model (EmbNum) to learn useful representation vectors for numerical
attributes without prior assumptions on the distribution of data. Then, the
""semantic similarities"" between the attributes are measured on these
representation vectors by the Euclidean distance. Our empirical experiments on
City Data and Open Data show that EmbNum significantly outperforms
state-of-the-art methods for the task of numerical attribute semantic labeling
regarding effectiveness and efficiency.
",0,0,0,1,0,0
5013,On non-Abelian Lie Bracket of Generalized Covariant Hamilton Systems,"  This is a theoretical paper, which is a continuation of [arXiv:1710.10597],
it considers the non-abelian Lie algebra $\mathcal{G}$ of Lie groups for
$\left[ {{X}_{i}},{{X}_{j}} \right]=c_{ij}^{k}{{X}_{k}}\in \mathcal{G}$ on the
foundation of the GCHS, where $c_{ij}^{k}\in {{C}^{\infty }}\left( U,R \right)$
are the structure constants. The GPWB [arXiv:1710.10597] is nonlinear bracket
applying to the non-Euclidean space, the second order (2,0) form antisymmetric
curvature tensor ${{F}_{ij}}=c_{ij}^{k}{{D}_{k}}$, and Qsu quantity
${{q}_{i}}=w_{i}^{k}{{D}_{k}}$ are accordingly obtained by using the
non-abelian Lie bracket. The GCHS $\left\{ H,f \right\}\in {{C}^{\infty
}}\left( M,\mathbb{R} \right)$ holds for the non-symplectic vector field
$X_{H}^{M}\in \mathcal{G}$ and $f\in {{C}^{\infty }}\left( M,\mathbb{R}
\right)$ that implies the covariant evolution equation consists of two parts,
NGHS and W dynamics along with the second order invariant operator
$\frac{{\mathcal{D}^{2}}}{d{{t}^{2}}}=\frac{{{d}^{2}}}{d{{t}^{2}}}+2w\frac{d}{dt}+\beta$.
",0,0,1,0,0,0
9919,End-to-end Lung Nodule Detection in Computed Tomography,"  Computer aided diagnostic (CAD) system is crucial for modern med-ical
imaging. But almost all CAD systems operate on reconstructed images, which were
optimized for radiologists. Computer vision can capture features that is subtle
to human observers, so it is desirable to design a CAD system op-erating on the
raw data. In this paper, we proposed a deep-neural-network-based detection
system for lung nodule detection in computed tomography (CT). A
primal-dual-type deep reconstruction network was applied first to convert the
raw data to the image space, followed by a 3-dimensional convolutional neural
network (3D-CNN) for the nodule detection. For efficient network training, the
deep reconstruction network and the CNN detector was trained sequentially
first, then followed by one epoch of end-to-end fine tuning. The method was
evaluated on the Lung Image Database Consortium image collection (LIDC-IDRI)
with simulated forward projections. With 144 multi-slice fanbeam pro-jections,
the proposed end-to-end detector could achieve comparable sensitivity with the
reference detector, which was trained and applied on the fully-sampled image
data. It also demonstrated superior detection performance compared to detectors
trained on the reconstructed images. The proposed method is general and could
be expanded to most detection tasks in medical imaging.
",1,0,0,1,0,0
16491,Multiphoton-Excited Fluorescence of Silicon-Vacancy Color Centers in Diamond,"  Silicon-vacancy color centers in nanodiamonds are promising as fluorescent
labels for biological applications, with a narrow, non-bleaching emission line
at 738\,nm. Two-photon excitation of this fluorescence offers the possibility
of low-background detection at significant tissue depth with high
three-dimensional spatial resolution. We have measured the two-photon
fluorescence cross section of a negatively-charged silicon vacancy (SiV$^-$) in
ion-implanted bulk diamond to be $0.74(19) \times 10^{-50}{\rm cm^4\;s/photon}$
at an excitation wavelength of 1040\,nm. In comparison to the diamond nitrogen
vacancy (NV) center, the expected detection threshold of a two-photon excited
SiV center is more than an order of magnitude lower, largely due to its much
narrower linewidth. We also present measurements of two- and three-photon
excitation spectra, finding an increase in the two-photon cross section with
decreasing wavelength, and discuss the physical interpretation of the spectra
in the context of existing models of the SiV energy-level structure.
",0,1,0,0,0,0
7970,The Brauer trees of unipotent blocks,"  In this paper we complete the determination of the Brauer trees of unipotent
blocks (with cyclic defect groups) of finite groups of Lie type. These trees
were conjectured by the first author. As a consequence, the Brauer trees of
principal $\ell$-blocks of finite groups are known for $\ell>71$.
",0,0,1,0,0,0
13289,"Fairer and more accurate, but for whom?","  Complex statistical machine learning models are increasingly being used or
considered for use in high-stakes decision-making pipelines in domains such as
financial services, health care, criminal justice and human services. These
models are often investigated as possible improvements over more classical
tools such as regression models or human judgement. While the modeling approach
may be new, the practice of using some form of risk assessment to inform
decisions is not. When determining whether a new model should be adopted, it is
therefore essential to be able to compare the proposed model to the existing
approach across a range of task-relevant accuracy and fairness metrics. Looking
at overall performance metrics, however, may be misleading. Even when two
models have comparable overall performance, they may nevertheless disagree in
their classifications on a considerable fraction of cases. In this paper we
introduce a model comparison framework for automatically identifying subgroups
in which the differences between models are most pronounced. Our primary focus
is on identifying subgroups where the models differ in terms of
fairness-related quantities such as racial or gender disparities. We present
experimental results from a recidivism prediction task and a hypothetical
lending example.
",1,0,0,1,0,0
12104,A Projected Inverse Dynamics Approach for Dual-arm Cartesian Impedance Control,"  We propose a method for dual-arm manipulation of rigid objects, subject to
external disturbance. The problem is formulated as a Cartesian impedance
controller within a projected inverse dynamics framework. We use the
constrained component of the controller to enforce contact and the
unconstrained controller to accomplish the task with a desired 6-DOF impedance
behaviour. Furthermore, the proposed method optimises the torque required to
maintain contact, subject to unknown disturbances, and can do so without direct
measurement of external force. The techniques are evaluated on a single-arm
wiping a table and a dual-arm platform manipulating a rigid object of unknown
mass and with human interaction.
",1,0,0,0,0,0
17616,Classification via Tensor Decompositions of Echo State Networks,"  This work introduces a tensor-based method to perform supervised
classification on spatiotemporal data processed in an echo state network.
Typically when performing supervised classification tasks on data processed in
an echo state network, the entire collection of hidden layer node states from
the training dataset is shaped into a matrix, allowing one to use standard
linear algebra techniques to train the output layer. However, the collection of
hidden layer states is multidimensional in nature, and representing it as a
matrix may lead to undesirable numerical conditions or loss of spatial and
temporal correlations in the data.
This work proposes a tensor-based supervised classification method on echo
state network data that preserves and exploits the multidimensional nature of
the hidden layer states. The method, which is based on orthogonal Tucker
decompositions of tensors, is compared with the standard linear output weight
approach in several numerical experiments on both synthetic and natural data.
The results show that the tensor-based approach tends to outperform the
standard approach in terms of classification accuracy.
",1,0,0,1,0,0
14111,Simulation and stability analysis of oblique shock wave/boundary layer interactions at Mach 5.92,"  We investigate flow instability created by an oblique shock wave impinging on
a Mach 5.92 laminar boundary layer at a transitional Reynolds number. The
adverse pressure gradient of the oblique shock causes the boundary layer to
separate from the wall, resulting in the formation of a recirculation bubble.
For sufficiently large oblique shock angles, the recirculation bubble is
unstable to three-dimensional perturbations and the flow bifurcates from its
original laminar state. We utilize Direct Numerical Simulation (DNS) and Global
Stability Analysis (GSA) to show that this first occurs at a critical shock
angle of $\theta = 12.9^o$. At bifurcation, the least stable global mode is
non-oscillatory, and it takes place at a spanwise wavenumber $\beta=0.25$, in
good agreement with DNS results. Examination of the critical global mode
reveals that it originates from an interaction between small spanwise
corrugations at the base of the incident shock, streamwise vortices inside the
recirculation bubble, and spanwise modulation of the bubble strength. The
global mode drives the formation of long streamwise streaks downstream of the
bubble. While the streaks may be amplified by either the lift-up effect or by
Görtler instability, we show that centrifugal instability plays no role in
the upstream self-sustaining mechanism of the global mode. We employ an adjoint
solver to corroborate our physical interpretation by showing that the critical
global mode is most sensitive to base flow modifications that are entirely
contained inside the recirculation bubble.
",0,1,0,0,0,0
1957,SCAV'18: Report of the 2nd International Workshop on Safe Control of Autonomous Vehicles,"  This report summarizes the discussions, open issues, take-away messages, and
conclusions of the 2nd SCAV workshop.
",1,0,0,0,0,0
893,On perpetuities with gamma-like tails,"  An infinite convergent sum of independent and identically distributed random
variables discounted by a multiplicative random walk is called perpetuity,
because of a possible actuarial application. We give three disjoint groups of
sufficient conditions which ensure that the distribution right tail of a
perpetuity $\mathbb{P}\{X>x\}$ is asymptotic to $ax^ce^{-bx}$ as $x\to\infty$
for some $a,b>0$ and $c\in\mathbb{R}$. Our results complement those of Denisov
and Zwart [J. Appl. Probab. 44 (2007), 1031--1046]. As an auxiliary tool we
provide criteria for the finiteness of the one-sided exponential moments of
perpetuities. Several examples are given in which the distributions of
perpetuities are explicitly identified.
",0,0,1,0,0,0
6148,Path Cover and Path Pack Inequalities for the Capacitated Fixed-Charge Network Flow Problem,"  Capacitated fixed-charge network flows are used to model a variety of
problems in telecommunication, facility location, production planning and
supply chain management. In this paper, we investigate capacitated path
substructures and derive strong and easy-to-compute \emph{path cover and path
pack inequalities}. These inequalities are based on an explicit
characterization of the submodular inequalities through a fast computation of
parametric minimum cuts on a path, and they generalize the well-known flow
cover and flow pack inequalities for the single-node relaxations of
fixed-charge flow models. We provide necessary and sufficient facet conditions.
Computational results demonstrate the effectiveness of the inequalities when
used as cuts in a branch-and-cut algorithm.
",1,0,1,0,0,0
3182,Distributed Time Synchronization for Networks with Random Delays and Measurement Noise,"  In this paper a new distributed asynchronous algorithm is proposed for time
synchronization in networks with random communication delays, measurement noise
and communication dropouts. Three different types of the drift correction
algorithm are introduced, based on different kinds of local time increments.
Under nonrestrictive conditions concerning network properties, it is proved
that all the algorithm types provide convergence in the mean square sense and
with probability one (w.p.1) of the corrected drifts of all the nodes to the
same value (consensus). An estimate of the convergence rate of these algorithms
is derived. For offset correction, a new algorithm is proposed containing a
compensation parameter coping with the influence of random delays and special
terms taking care of the influence of both linearly increasing time and drift
correction. It is proved that the corrected offsets of all the nodes converge
in the mean square sense and w.p.1. An efficient offset correction algorithm
based on consensus on local compensation parameters is also proposed. It is
shown that the overall time synchronization algorithm can also be implemented
as a flooding algorithm with one reference node. It is proved that it is
possible to achieve bounded error between local corrected clocks in the mean
square sense and w.p.1. Simulation results provide an additional practical
insight into the algorithm properties and show its advantage over the existing
methods.
",1,0,0,0,0,0
4460,High order conformal symplectic and ergodic schemes for stochastic Langevin equation via generating functions,"  In this paper, we consider the stochastic Langevin equation with additive
noises, which possesses both conformal symplectic geometric structure and
ergodicity. We propose a methodology of constructing high weak order conformal
symplectic schemes by converting the equation into an equivalent autonomous
stochastic Hamiltonian system and modifying the associated generating function.
To illustrate this approach, we construct a specific second order numerical
scheme, and prove that its symplectic form dissipates exponentially. Moreover,
for the linear case, the proposed scheme is also shown to inherit the
ergodicity of the original system, and the temporal average of the numerical
solution is a proper approximation of the ergodic limit over long time.
Numerical experiments are given to verify these theoretical results.
",0,0,1,0,0,0
20479,First Results from CUORE: A Search for Lepton Number Violation via $0νββ$ Decay of $^{130}$Te,"  The CUORE experiment, a ton-scale cryogenic bolometer array, recently began
operation at the Laboratori Nazionali del Gran Sasso in Italy. The array
represents a significant advancement in this technology, and in this work we
apply it for the first time to a high-sensitivity search for a
lepton-number--violating process: $^{130}$Te neutrinoless double-beta decay.
Examining a total TeO$_2$ exposure of 86.3 kg$\cdot$yr, characterized by an
effective energy resolution of (7.7 $\pm$ 0.5) keV FWHM and a background in the
region of interest of (0.014 $\pm$ 0.002) counts/(keV$\cdot$kg$\cdot$yr), we
find no evidence for neutrinoless double-beta decay. The median statistical
sensitivity of this search is $7.0\times10^{24}$ yr. Including systematic
uncertainties, we place a lower limit on the decay half-life of
$T^{0\nu}_{1/2}$($^{130}$Te) > $1.3\times 10^{25}$ yr (90% C.L.). Combining
this result with those of two earlier experiments, Cuoricino and CUORE-0, we
find $T^{0\nu}_{1/2}$($^{130}$Te) > $1.5\times 10^{25}$ yr (90% C.L.), which is
the most stringent limit to date on this decay. Interpreting this result as a
limit on the effective Majorana neutrino mass, we find $m_{\beta\beta}<(110 -
520)$ meV, where the range reflects the nuclear matrix element estimates
employed.
",0,1,0,0,0,0
7193,TRAMP: Tracking by a Real-time AMbisonic-based Particle filter,"  This article presents a multiple sound source localization and tracking
system, fed by the Eigenmike array. The First Order Ambisonics (FOA) format is
used to build a pseudointensity-based spherical histogram, from which the
source position estimates are deduced. These instantaneous estimates are
processed by a wellknown tracking system relying on a set of particle filters.
While the novelty within localization and tracking is incremental, the
fully-functional, complete and real-time running system based on these
algorithms is proposed for the first time. As such, it could serve as an
additional baseline method of the LOCATA challenge.
",1,0,0,0,0,0
20476,Learning in the Repeated Secretary Problem,"  In the classical secretary problem, one attempts to find the maximum of an
unknown and unlearnable distribution through sequential search. In many
real-world searches, however, distributions are not entirely unknown and can be
learned through experience. To investigate learning in such a repeated
secretary problem we conduct a large-scale behavioral experiment in which
people search repeatedly from fixed distributions. In contrast to prior
investigations that find no evidence for learning in the classical scenario, in
the repeated setting we observe substantial learning resulting in near-optimal
stopping behavior. We conduct a Bayesian comparison of multiple behavioral
models which shows that participants' behavior is best described by a class of
threshold-based models that contains the theoretically optimal strategy.
Fitting such a threshold-based model to data reveals players' estimated
thresholds to be surprisingly close to the optimal thresholds after only a
small number of games.
",1,0,0,0,0,0
4380,Engineering a flux-dependent mobility edge in disordered zigzag chains,"  There has been great interest in realizing quantum simulators of charged
particles in artificial gauge fields. Here, we perform the first quantum
simulation explorations of the combination of artificial gauge fields and
disorder. Using synthetic lattice techniques based on parametrically-coupled
atomic momentum states, we engineer zigzag chains with a tunable homogeneous
flux. The breaking of time-reversal symmetry by the applied flux leads to
analogs of spin-orbit coupling and spin-momentum locking, which we observe
directly through the chiral dynamics of atoms initialized to single lattice
sites. We additionally introduce precisely controlled disorder in the site
energy landscape, allowing us to explore the interplay of disorder and large
effective magnetic fields. The combination of correlated disorder and
controlled intra- and inter-row tunneling in this system naturally supports
energy-dependent localization, relating to a single-particle mobility edge. We
measure the localization properties of the extremal eigenstates of this system,
the ground state and the most-excited state, and demonstrate clear evidence for
a flux-dependent mobility edge. These measurements constitute the first direct
evidence for energy-dependent localization in a lower-dimensional system, as
well as the first explorations of the combined influence of artificial gauge
fields and engineered disorder. Moreover, we provide direct evidence for
interaction shifts of the localization transitions for both low- and
high-energy eigenstates in correlated disorder, relating to the presence of a
many-body mobility edge. The unique combination of strong interactions,
controlled disorder, and tunable artificial gauge fields present in this
synthetic lattice system should enable myriad explorations into intriguing
correlated transport phenomena.
",0,1,0,0,0,0
8149,Scalable Inference for Space-Time Gaussian Cox Processes,"  The log-Gaussian Cox process is a flexible and popular class of point pattern
models for capturing spatial and space-time dependence for point patterns.
Model fitting requires approximation of stochastic integrals which is
implemented through discretization over the domain of interest. With fine scale
discretization, inference based on Markov chain Monte Carlo is computationally
burdensome because of the cost of matrix decompositions and storage, such as
the Cholesky, for high dimensional covariance matrices associated with latent
Gaussian variables. This article addresses these computational bottlenecks by
combining two recent developments: (i) a data augmentation strategy that has
been proposed for space-time Gaussian Cox processes that is based on exact
Bayesian inference and does not require fine grid approximations for infinite
dimensional integrals, and (ii) a recently developed family of
sparsity-inducing Gaussian processes, called nearest-neighbor Gaussian
processes, to avoid expensive matrix computations. Our inference is delivered
within the fully model-based Bayesian paradigm and does not sacrifice the
richness of traditional log-Gaussian Cox processes. We apply our method to
crime event data in San Francisco and investigate the recovery of the intensity
surface.
",0,0,0,1,0,0
2864,Towards Plan Transformations for Real-World Pick and Place Tasks,"  In this paper, we investigate the possibility of applying plan
transformations to general manipulation plans in order to specialize them to
the specific situation at hand. We present a framework for optimizing execution
and achieving higher performance by autonomously transforming robot's behavior
at runtime. We show that plans employed by robotic agents in real-world
environments can be transformed, despite their control structures being very
complex due to the specifics of acting in the real world. The evaluation is
carried out on a plan of a PR2 robot performing pick and place tasks, to which
we apply three example transformations, as well as on a large amount of
experiments in a fast plan projection environment.
",1,0,0,0,0,0
15554,Superior lattice thermal conductance of single layer borophene,"  By way of the nonequilibrium Green's function simulations and first
principles calculations, we report that borophene, a single layer of boron
atoms that was fabricated recently, possesses an extraordinarily high lattice
thermal conductance in the ballistic transport regime, which even exceeds
graphene. In addition to the obvious reasons of light mass and strong bonding
of boron atoms, the superior thermal conductance is mainly rooted in its strong
structural anisotropy and unusual phonon transmission. For low-frequency
phonons, the phonon transmission within borophene is nearly isotropic, similar
to that of graphene. For high frequency phonons, however, the transmission is
one dimensional, that is, all the phonons travel in one direction, giving rise
to its ultrahigh thermal conductance. The present study suggests that borophene
is promising for applications in efficient heat dissipation and thermal
management, and also an ideal material for revealing fundamentals of
dimensionality effect on phonon transport in ballistic regime.
",0,1,0,0,0,0
14567,"Network Slicing for 5G with SDN/NFV: Concepts, Architectures and Challenges","  The fifth generation of mobile communications is anticipated to open up
innovation opportunities for new industries such as vertical markets. However,
these verticals originate myriad use cases with diverging requirements that
future 5G networks have to efficiently support. Network slicing may be a
natural solution to simultaneously accommodate over a common network
infrastructure the wide range of services that vertical-specific use cases will
demand. In this article, we present the network slicing concept, with a
particular focus on its application to 5G systems. We start by summarizing the
key aspects that enable the realization of so-called network slices. Then, we
give a brief overview on the SDN architecture proposed by the ONF and show that
it provides tools to support slicing. We argue that although such architecture
paves the way for network slicing implementation, it lacks some essential
capabilities that can be supplied by NFV. Hence, we analyze a proposal from the
ETSI to incorporate the capabilities of SDN into the NFV architecture.
Additionally, we present an example scenario that combines SDN and NFV
technologies to address the realization of network slices. Finally, we
summarize the open research issues with the purpose of motivating new advances
in this field.
",1,0,0,0,0,0
18143,A Multiple Linear Regression Approach For Estimating the Market Value of Football Players in Forward Position,"  In this paper, market values of the football players in the forward positions
are estimated using multiple linear regression by including the physical and
performance factors in 2017-2018 season. Players from 4 major leagues of Europe
are examined, and by applying the test for homoscedasticity, a reasonable
regression model within 0.10 significance level is built, and the most and the
least affecting factors are explained in detail.
",0,0,0,1,0,0
15226,"Topological Semimetals carrying Arbitrary Hopf Numbers: Hopf-Link, Solomon's-Knot, Trefoil-Knot and Other Semimetals","  We propose a new type of Hopf semimetals indexed by a pair of numbers
$(p,q)$, where the Hopf number is given by $pq$. The Fermi surface is given by
the preimage of the Hopf map, which is nontrivially linked for a nonzero Hopf
number. The Fermi surface forms a torus link, whose examples are the Hopf link
indexed by $(1,1)$, the Solomon's knot $(2,1)$, the double Hopf-link $(2,2)$
and the double trefoil-knot $(3,2)$. We may choose $p$ or $q$ as a half
integer, where torus-knot Fermi surfaces such as the trefoil knot $(3/2,1)$ are
realized. It is even possible to make the Hopf number an arbitrary rational
number, where a semimetal whose Fermi surface forms open strings is generated.
",0,1,0,0,0,0
14192,A note on a separating system of rational invariants for finite dimensional generic algebras,"  The paper deals with a construction of a separating system of rational
invariants for finite dimensional generic algebras. In the process of dealing
an approach to a rough classification of finite dimensional algebras is offered
by attaching them some quadratic forms.
",0,0,1,0,0,0
7866,Nonlinear Modal Decoupling Based Power System Transient Stability Analysis,"  Nonlinear modal decoupling (NMD) was recently proposed to nonlinearly
transform a multi-oscillator system into a number of decoupled oscillators
which together behave the same as the original system in an extended
neighborhood of the equilibrium. Each oscillator has just one degree of freedom
and hence can easily be analyzed to infer the stability of the original system
associated with one electromechanical mode. As the first attempt of applying
the NMD methodology to realistic power system models, this paper proposes an
NMD-based transient stability analysis approach. For a multi-machine power
system, the approach first derives decoupled nonlinear oscillators by a
coordinates transformation, and then applies Lyapunov stability analysis to
oscillators to assess the stability of the original system. Nonlinear modal
interaction is also considered. The approach can be efficiently applied to a
large-scale power grid by conducting NMD regarding only selected modes. Case
studies on a 3-machine 9-bus system and an NPCC 48-machine 140-bus system show
the potentials of the approach in transient stability analysis for
multi-machine systems.
",1,0,0,0,0,0
20013,Compressive Statistical Learning with Random Feature Moments,"  We describe a general framework --compressive statistical learning-- for
resource-efficient large-scale learning: the training collection is compressed
in one pass into a low-dimensional sketch (a vector of random empirical
generalized moments) that captures the information relevant to the considered
learning task. A near-minimizer of the risk is computed from the sketch through
the solution of a nonlinear least squares problem. We investigate sufficient
sketch sizes to control the generalization error of this procedure. The
framework is illustrated on compressive clustering, compressive Gaussian
mixture Modeling with fixed known variance, and compressive PCA.
",1,0,1,1,0,0
10151,Some remarks on protolocalizations and protoadditive reflections,"  We investigate additional properties of protolocalizations, introduced and
studied by F. Borceux, M. M. Clementino, M. Gran, and L. Sousa, and of
protoadditive reflections, introduced and studied by T. Everaert and M. Gran.
Among other things we show that there are no non-trivial (protolocalizations
and) protoadditive reflections of the category of groups, and establish a
connection between protolocalizations and Kurosh--Amitsur radicals of groups
with multiple operators whose semisimple classes form subvarieties.
",0,0,1,0,0,0
17997,The center problem for the Lotka reactions with generalized mass-action kinetics,"  Chemical reaction networks with generalized mass-action kinetics lead to
power-law dynamical systems. As a simple example, we consider the Lotka
reactions and the resulting planar ODE. We characterize the parameters
(positive coefficients and real exponents) for which the unique positive
equilibrium is a center.
",0,0,1,0,0,0
4779,Open Vocabulary Scene Parsing,"  Recognizing arbitrary objects in the wild has been a challenging problem due
to the limitations of existing classification models and datasets. In this
paper, we propose a new task that aims at parsing scenes with a large and open
vocabulary, and several evaluation metrics are explored for this problem. Our
proposed approach to this problem is a joint image pixel and word concept
embeddings framework, where word concepts are connected by semantic relations.
We validate the open vocabulary prediction ability of our framework on ADE20K
dataset which covers a wide variety of scenes and objects. We further explore
the trained joint embedding space to show its interpretability.
",1,0,0,0,0,0
17517,Reconstructing the gravitational field of the local universe,"  Tests of gravity at the galaxy scale are in their infancy. As a first step to
systematically uncovering the gravitational significance of galaxies, we map
three fundamental gravitational variables -- the Newtonian potential,
acceleration and curvature -- over the galaxy environments of the local
universe to a distance of approximately 200 Mpc. Our method combines the
contributions from galaxies in an all-sky redshift survey, halos from an N-body
simulation hosting low-luminosity objects, and linear and quasi-linear modes of
the density field. We use the ranges of these variables to determine the extent
to which galaxies expand the scope of generic tests of gravity and are capable
of constraining specific classes of model for which they have special
significance. Finally, we investigate the improvements afforded by upcoming
galaxy surveys.
",0,1,0,0,0,0
20211,G-Deformations of maps into projective space,"  $G$-deformability of maps into projective space is characterised by the
existence of certain Lie algebra valued 1-forms. This characterisation gives a
unified way to obtain well known results regarding deformability in different
geometries.
",0,0,1,0,0,0
6113,Connections on parahoric torsors over curves,"  We define parahoric $\cG$--torsors for certain Bruhat--Tits group scheme
$\cG$ on a smooth complex projective curve $X$ when the weights are real, and
also define connections on them. We prove that a $\cG$--torsor is given by a
homomorphism from $\pi_1(X\setminus D)$ to a maximal compact subgroup of $G$,
where $D\, \subset\, X$ is the parabolic divisor, if and only if the torsor is
polystable.
",0,0,1,0,0,0
14658,"Analysis and Design of Cost-Effective, High-Throughput LDPC Decoders","  This paper introduces a new approach to cost-effective, high-throughput
hardware designs for Low Density Parity Check (LDPC) decoders. The proposed
approach, called Non-Surjective Finite Alphabet Iterative Decoders (NS-FAIDs),
exploits the robustness of message-passing LDPC decoders to inaccuracies in the
calculation of exchanged messages, and it is shown to provide a unified
framework for several designs previously proposed in the literature. NS-FAIDs
are optimized by density evolution for regular and irregular LDPC codes, and
are shown to provide different trade-offs between hardware complexity and
decoding performance. Two hardware architectures targeting high-throughput
applications are also proposed, integrating both Min-Sum (MS) and NS-FAID
decoding kernels. ASIC post synthesis implementation results on 65nm CMOS
technology show that NS-FAIDs yield significant improvements in the throughput
to area ratio, by up to 58.75% with respect to the MS decoder, with even better
or only slightly degraded error correction performance.
",1,0,0,0,0,0
10480,The Vanishing viscosity limit for some symmetric flows,"  The focus of this paper is on the analysis of the boundary layer and the
associated vanishing viscosity limit for two classes of flows with symmetry,
namely, Plane-Parallel Channel Flows and Parallel Pipe Flows. We construct
explicit boundary layer correctors, which approximate the difference between
the Navier-Stokes and the Euler solutions. Using properties of these
correctors, we establish convergence of the Navier-Stokes solution to the Euler
solution as viscosity vanishes with optimal rates of convergence. In addition,
we investigate vorticity production on the boundary in the limit of vanishing
viscosity. Our work significantly extends prior work in the literature.
",0,0,1,0,0,0
18048,Magnetic field influenced electron-impurity scattering and magnetotransport,"  We formulate a quasiclassical theory ($\omega_c\tau \lesssim 1$ with
$\omega_c$ as the cyclotron frequency and $\tau$ as the relaxation time) to
study the influence of magnetic field on electron-impurity scattering process
in the two-dimensional electron gas. We introduce a general recipe based on an
abstraction of the detailed impurity scattering process to define the
scattering parameter such as the incoming and outgoing momentum and coordinate
jump. In this picture, we can conveniently describe the skew scattering and
coordinate jump, which will eventually modify the Boltzmann equation. We find
an anomalous Hall resistivity different from the conventional Boltzmann-Drude
result and a negative magnetoresistivity parabolic in magnetic field. The
origin of these results has been analyzed. The relevance between our theory and
recent simulation and experimental works is also discussed. Our theory
dominates in dilute impurity system where the correlation effect is negligible.
",0,1,0,0,0,0
15808,Complexity of products: the effect of data regularisation,"  Among several developments, the field of Economic Complexity (EC) has notably
seen the introduction of two new techniques. One is the Bootstrapped Selective
Predictability Scheme (SPSb), which can provide quantitative forecasts of the
Gross Domestic Product of countries. The other, Hidden Markov Model (HMM)
regularisation, denoises the datasets typically employed in the literature. We
contribute to EC along three different directions. First, we prove the
convergence of the SPSb algorithm to a well-known statistical learning
technique known as Nadaraya-Watson Kernel regression. The latter has
significantly lower time complexity, produces deterministic results, and it is
interchangeable with SPSb for the purpose of making predictions. Second, we
study the effects of HMM regularization on the Product Complexity and logPRODY
metrics, for which a model of time evolution has been recently proposed. We
find confirmation for the original interpretation of the logPRODY model as
describing the change in the global market structure of products with new
insights allowing a new interpretation of the Complexity measure, for which we
propose a modification. Third, we explore new effects of regularisation on the
data. We find that it reduces noise, and observe for the first time that it
increases nestedness in the export network adjacency matrix.
",0,0,0,0,0,1
3916,"Systematic Identification of LAEs for Visible Exploration and Reionization Research Using Subaru HSC (SILVERRUSH). I. Program Strategy and Clustering Properties of ~2,000 Lya Emitters at z=6-7 over the 0.3-0.5 Gpc$^2$ Survey Area","  We present the SILVERRUSH program strategy and clustering properties
investigated with $\sim 2,000$ Ly$\alpha$ emitters at $z=5.7$ and $6.6$ found
in the early data of the Hyper Suprime-Cam (HSC) Subaru Strategic Program
survey exploiting the carefully designed narrowband filters. We derive angular
correlation functions with the unprecedentedly large samples of LAEs at $z=6-7$
over the large total area of $14-21$ deg$^2$ corresponding to $0.3-0.5$
comoving Gpc$^2$. We obtain the average large-scale bias values of $b_{\rm
avg}=4.1\pm 0.2$ ($4.5\pm 0.6$) at $z=5.7$ ($z=6.6$) for $\gtrsim L^*$ LAEs,
indicating the weak evolution of LAE clustering from $z=5.7$ to $6.6$. We
compare the LAE clustering results with two independent theoretical models that
suggest an increase of an LAE clustering signal by the patchy ionized bubbles
at the epoch of reionization (EoR), and estimate the neutral hydrogen fraction
to be $x_{\rm HI}=0.15^{+0.15}_{-0.15}$ at $z=6.6$. Based on the halo
occupation distribution models, we find that the $\gtrsim L^*$ LAEs are hosted
by the dark-matter halos with the average mass of $\log (\left < M_{\rm h}
\right >/M_\odot) =11.1^{+0.2}_{-0.4}$ ($10.8^{+0.3}_{-0.5}$) at $z=5.7$
($6.6$) with a Ly$\alpha$ duty cycle of 1 % or less, where the results of
$z=6.6$ LAEs may be slightly biased, due to the increase of the clustering
signal at the EoR. Our clustering analysis reveals the low-mass nature of
$\gtrsim L^*$ LAEs at $z=6-7$, and that these LAEs probably evolve into massive
super-$L^*$ galaxies in the present-day universe.
",0,1,0,0,0,0
243,Bayesian Metabolic Flux Analysis reveals intracellular flux couplings,"  Metabolic flux balance analyses are a standard tool in analysing metabolic
reaction rates compatible with measurements, steady-state and the metabolic
reaction network stoichiometry. Flux analysis methods commonly place
unrealistic assumptions on fluxes due to the convenience of formulating the
problem as a linear programming model, and most methods ignore the notable
uncertainty in flux estimates. We introduce a novel paradigm of Bayesian
metabolic flux analysis that models the reactions of the whole genome-scale
cellular system in probabilistic terms, and can infer the full flux vector
distribution of genome-scale metabolic systems based on exchange and
intracellular (e.g. 13C) flux measurements, steady-state assumptions, and
target function assumptions. The Bayesian model couples all fluxes jointly
together in a simple truncated multivariate posterior distribution, which
reveals informative flux couplings. Our model is a plug-in replacement to
conventional metabolic balance methods, such as flux balance analysis (FBA).
Our experiments indicate that we can characterise the genome-scale flux
covariances, reveal flux couplings, and determine more intracellular unobserved
fluxes in C. acetobutylicum from 13C data than flux variability analysis. The
COBRA compatible software is available at github.com/markusheinonen/bamfa
",0,0,0,1,0,0
13690,Trajectory Tracking Using Motion Primitives for the Purcell's Swimmer,"  Locomotion at low Reynolds numbers is a topic of growing interest, spurred by
its various engineering and medical applications. This paper presents a novel
prototype and a locomotion algorithm for the 3-link planar Purcell's swimmer
based on Lie algebraic notions. The kinematic model based on Cox theory of the
prototype swimmer is a driftless control-affine system. Using the existing
strong controllability and related results, the existence of motion primitives
is initially shown. The Lie algebra of the control vector fields is then used
to synthesize control profiles to generate motions along the basis of the Lie
algebra associated with the structure group of the system. An open loop control
system with vision-based positioning is successfully implemented which allows
tracking any given continuous trajectory of the position and orientation of the
swimmer's base link. Alongside, the paper also provides a theoretical
interpretation of the symmetry arguments presented in the existing literature
to generate the control profiles of the swimmer.
",1,0,0,0,0,0
2591,Reach and speed of judgment propagation in the laboratory,"  In recent years, a large body of research has demonstrated that judgments and
behaviors can propagate from person to person. Phenomena as diverse as
political mobilization, health practices, altruism, and emotional states
exhibit similar dynamics of social contagion. The precise mechanisms of
judgment propagation are not well understood, however, because it is difficult
to control for confounding factors such as homophily or dynamic network
structures. We introduce a novel experimental design that renders possible the
stringent study of judgment propagation. In this design, experimental chains of
individuals can revise their initial judgment in a visual perception task after
observing a predecessor's judgment. The positioning of a very good performer at
the top of a chain created a performance gap, which triggered waves of judgment
propagation down the chain. We evaluated the dynamics of judgment propagation
experimentally. Despite strong social influence within pairs of individuals,
the reach of judgment propagation across a chain rarely exceeded a social
distance of three to four degrees of separation. Furthermore, computer
simulations showed that the speed of judgment propagation decayed exponentially
with the social distance from the source. We show that information distortion
and the overweighting of other people's errors are two individual-level
mechanisms hindering judgment propagation at the scale of the chain. Our
results contribute to the understanding of social contagion processes, and our
experimental method offers numerous new opportunities to study judgment
propagation in the laboratory.
",1,1,0,0,0,0
1530,New Determinant Expressions of the Multi-indexed Orthogonal Polynomials in Discrete Quantum Mechanics,"  The multi-indexed orthogonal polynomials (the Meixner, little $q$-Jacobi
(Laguerre), ($q$-)Racah, Wilson, Askey-Wilson types) satisfying second order
difference equations were constructed in discrete quantum mechanics. They are
polynomials in the sinusoidal coordinates $\eta(x)$ ($x$ is the coordinate of
quantum system) and expressed in terms of the Casorati determinants whose
matrix elements are functions of $x$ at various points. By using shape
invariance properties, we derive various equivalent determinant expressions,
especially those whose matrix elements are functions of the same point $x$.
Except for the ($q$-)Racah case, they can be expressed in terms of $\eta$ only,
without explicit $x$-dependence.
",0,1,1,0,0,0
4375,Generalized Concomitant Multi-Task Lasso for sparse multimodal regression,"  In high dimension, it is customary to consider Lasso-type estimators to
enforce sparsity. For standard Lasso theory to hold, the regularization
parameter should be proportional to the noise level, yet the latter is
generally unknown in practice. A possible remedy is to consider estimators,
such as the Concomitant/Scaled Lasso, which jointly optimize over the
regression coefficients as well as over the noise level, making the choice of
the regularization independent of the noise level. However, when data from
different sources are pooled to increase sample size, or when dealing with
multimodal datasets, noise levels typically differ and new dedicated estimators
are needed. In this work we provide new statistical and computational solutions
to deal with such heteroscedastic regression models, with an emphasis on
functional brain imaging with combined magneto- and electroencephalographic
(M/EEG) signals. Adopting the formulation of Concomitant Lasso-type estimators,
we propose a jointly convex formulation to estimate both the regression
coefficients and the (square root of the) noise covariance. When our framework
is instantiated to de-correlated noise, it leads to an efficient algorithm
whose computational cost is not higher than for the Lasso and Concomitant
Lasso, while addressing more complex noise structures. Numerical experiments
demonstrate that our estimator yields improved prediction and support
identification while correctly estimating the noise (square root) covariance.
Results on multimodal neuroimaging problems with M/EEG data are also reported.
",0,0,1,1,0,0
8595,Regularity of Lie Groups,"  We solve the regularity problem for Milnor's infinite dimensional Lie groups
in the $C^0$-topological context, and provide necessary and sufficient
regularity conditions for the standard setting ($C^k$-topology). We prove that
the evolution map is $C^0$-continuous on its domain $\textit{iff}\hspace{1pt}$
the Lie group $G$ is locally $\mu$-convex. We furthermore show that if the
evolution map is defined on all smooth curves, then $G$ is Mackey complete -
This is a completeness condition formulated in terms of the Lie group
operations that generalizes Mackey completeness as defined for locally convex
vector spaces. Then, under the presumption that $G$ is locally $\mu$-convex, we
show that each $C^k$-curve, for $k\in \mathbb{N}_{\geq
1}\sqcup\{\mathrm{lip},\infty\}$, is integrable (contained in the domain of the
evolution map) $\textit{iff}\hspace{1pt}$ $G$ is Mackey complete and
$\mathrm{k}$-confined. The latter condition states that each $C^k$-curve in the
Lie algebra $\mathfrak{g}$ of $G$ can be uniformly approximated by a special
type of sequence consisting of piecewise integrable curves - A similar result
is proven for the case $k\equiv 0$; and we provide several mild conditions that
ensure that $G$ is $\mathrm{k}$-confined for each $k\in
\mathbb{N}\sqcup\{\mathrm{lip},\infty\}$. We finally discuss the
differentiation of parameter-dependent integrals in the standard topological
context. In particular, we show that if the evolution map is well defined and
continuous on $C^k([0,1],\mathfrak{g})$ for $k\in \mathbb{N}\sqcup\{\infty\}$,
then it is smooth thereon $\textit{iff}\hspace{1pt}$ $\mathfrak{g}$ is
$\hspace{0.2pt}$ Mackey complete for $k\in \mathbb{N}_{\geq 1}\sqcup\{\infty\}$
$\hspace{1pt}/\hspace{1pt}$ integral complete for $k\equiv 0$. This result is
obtained by calculating the directional derivatives explicitly - recovering the
standard formulas that hold in the Banach case.
",0,0,1,0,0,0
12676,Loop conditions,"  We discuss such Maltsev conditions that consist of just one linear equation,
we call them loop conditions. To every such condition can be assigned a graph.
We provide a classification of conditions with undirected graphs. It follows
that the Siggers term is the weakest non-trivial loop condition.
",0,0,1,0,0,0
1900,Dissipativity Theory for Accelerating Stochastic Variance Reduction: A Unified Analysis of SVRG and Katyusha Using Semidefinite Programs,"  Techniques for reducing the variance of gradient estimates used in stochastic
programming algorithms for convex finite-sum problems have received a great
deal of attention in recent years. By leveraging dissipativity theory from
control, we provide a new perspective on two important variance-reduction
algorithms: SVRG and its direct accelerated variant Katyusha. Our perspective
provides a physically intuitive understanding of the behavior of SVRG-like
methods via a principle of energy conservation. The tools discussed here allow
us to automate the convergence analysis of SVRG-like methods by capturing their
essential properties in small semidefinite programs amenable to standard
analysis and computational techniques. Our approach recovers existing
convergence results for SVRG and Katyusha and generalizes the theory to
alternative parameter choices. We also discuss how our approach complements the
linear coupling technique. Our combination of perspectives leads to a better
understanding of accelerated variance-reduced stochastic methods for finite-sum
problems.
",0,0,0,1,0,0
10718,An effective likelihood-free approximate computing method with statistical inferential guarantees,"  Approximate Bayesian computing is a powerful likelihood-free method that has
grown increasingly popular since early applications in population genetics.
However, complications arise in the theoretical justification for Bayesian
inference conducted from this method with a non-sufficient summary statistic.
In this paper, we seek to re-frame approximate Bayesian computing within a
frequentist context and justify its performance by standards set on the
frequency coverage rate. In doing so, we develop a new computational technique
called approximate confidence distribution computing, yielding theoretical
support for the use of non-sufficient summary statistics in likelihood-free
methods. Furthermore, we demonstrate that approximate confidence distribution
computing extends the scope of approximate Bayesian computing to include
data-dependent priors without damaging the inferential integrity. This
data-dependent prior can be viewed as an initial `distribution estimate' of the
target parameter which is updated with the results of the approximate
confidence distribution computing method. A general strategy for constructing
an appropriate data-dependent prior is also discussed and is shown to often
increase the computing speed while maintaining statistical inferential
guarantees. We supplement the theory with simulation studies illustrating the
benefits of the proposed method, namely the potential for broader applications
and the increased computing speed compared to the standard approximate Bayesian
computing methods.
",0,0,1,1,0,0
836,Affine Rough Models,"  The goal of this survey article is to explain and elucidate the affine
structure of recent models appearing in the rough volatility literature, and
show how it leads to exponential-affine transform formulas.
",0,0,0,0,0,1
13234,Left-invariant Grauert tubes on SU(2),"  Let M be a real analytic Riemannian manifold. An adapted complex structure on
TM is a complex structure on a neighborhood of the zero section such that the
leaves of the Riemann foliation are complex submanifolds. This structure is
called entire if it may be extended to the whole of TM. We call such manifolds
Grauert tubes, or simply tubes. We consider here the case of M = G a compact
connected Lie group with a left-invariant metric, and try to determine for
which such metrics the associated tube is entire. It is well-known that the
Grauert tube of a bi-invariant metric on a Lie group is entire. The case of the
smallest group SU(2) is treated completely, thanks to the complete
integrability of the geodesic flow for such a metric, a standard result in
classical mechanics. Along the way we find a new obstruction to tubes being
entire which is made visible by the complete integrability. (New reference and
exposition shortened, 11/17/2017.)
",0,0,1,0,0,0
9641,A Variational Projection Scheme for Nonmatching Surface-to-Line Coupling between 3D Flexible Multibody System and Incompressible Turbulent Flow,"  This paper is concerned with the partitioned iterative formulation to
simulate the fluid-structure interaction of a nonlinear multibody system in an
incompressible turbulent flow. The proposed formulation relies on a
three-dimensional (3D) incompressible turbulent flow solver, a nonlinear
monolithic elastic structural solver for constrained flexible multibody system
and the nonlinear iterative force correction scheme for coupling of the
turbulent fluid-flexible multibody system with nonmatching interface meshes.
While the fluid equations are discretized using a stabilized Petrov-Galerkin
formulation in space and the generalized-$\alpha$ updates in time, the
multibody system utilizes a discontinuous space-time Galerkin finite element
method. We address two key challenges in the present formulation. Firstly, the
coupling of the incompressible turbulent flow with a system of nonlinear
elastic bodies described in a co-rotated frame. Secondly, the projection of the
tractions and displacements across the nonmatching 3D fluid surface elements
and the one-dimensional line elements for the flexible multibody system in a
conservative manner. Through the nonlinear iterative correction and the
conservative projection, the developed fluid-flexible multibody interaction
solver is stable for problems involving strong inertial effects between the
fluid-flexible multibody system and the coupled interactions among each
multibody component. The accuracy of the proposed coupled finite element
framework is validated against the available experimental data for a long
flexible cylinder undergoing vortex-induced vibration in a uniform current flow
condition. Finally, a practical application of the proposed framework is
demonstrated by simulating the flow-induced vibration of a realistic offshore
floating platform connected to a long riser and an elastic mooring system.
",0,1,0,0,0,0
5882,Formal Verification of Neural Network Controlled Autonomous Systems,"  In this paper, we consider the problem of formally verifying the safety of an
autonomous robot equipped with a Neural Network (NN) controller that processes
LiDAR images to produce control actions. Given a workspace that is
characterized by a set of polytopic obstacles, our objective is to compute the
set of safe initial conditions such that a robot trajectory starting from these
initial conditions is guaranteed to avoid the obstacles. Our approach is to
construct a finite state abstraction of the system and use standard
reachability analysis over the finite state abstraction to compute the set of
the safe initial states. The first technical problem in computing the finite
state abstraction is to mathematically model the imaging function that maps the
robot position to the LiDAR image. To that end, we introduce the notion of
imaging-adapted sets as partitions of the workspace in which the imaging
function is guaranteed to be affine. We develop a polynomial-time algorithm to
partition the workspace into imaging-adapted sets along with computing the
corresponding affine imaging functions. Given this workspace partitioning, a
discrete-time linear dynamics of the robot, and a pre-trained NN controller
with Rectified Linear Unit (ReLU) nonlinearity, the second technical challenge
is to analyze the behavior of the neural network. To that end, we utilize a
Satisfiability Modulo Convex (SMC) encoding to enumerate all the possible
segments of different ReLUs. SMC solvers then use a Boolean satisfiability
solver and a convex programming solver and decompose the problem into smaller
subproblems. To accelerate this process, we develop a pre-processing algorithm
that could rapidly prune the space feasible ReLU segments. Finally, we
demonstrate the efficiency of the proposed algorithms using numerical
simulations with increasing complexity of the neural network controller.
",1,0,0,0,0,0
6899,BCS quantum critical phenomena,"  Theoretically, we recently showed that the scaling relation between the
transition temperature T_c and the superfluid density at zero temperature n_s
(0) might exhibit a parabolic pattern [Scientific Reports 6 (2016) 23863]. It
is significantly different from the linear scaling described by Homes' law,
which is well known as a mean-field result. More recently, Bozovic et al. have
observed such a parabolic scaling in the overdoped copper oxides with a
sufficiently low transition temperature T_c [Nature 536 (2016) 309-311]. They
further point out that this experimental finding is incompatible with the
standard Bardeen-Cooper-Schrieffer (BCS) description. Here we report that if
T_c is sufficiently low, applying the renormalization group approach into the
BCS action at zero temperature will naturally lead to the parabolic scaling.
Our result indicates that when T_c sufficiently approaches zero, quantum
fluctuations will be overwhelmingly amplified so that the mean-field
approximation may break down at zero temperature.
",0,1,0,0,0,0
14767,On the Transformation of Latent Space in Autoencoders,"  Noting the importance of the latent variables in inference and learning, we
propose a novel framework for autoencoders based on the homeomorphic
transformation of latent variables --- which could reduce the distance between
vectors in the transformed space, while preserving the topological properties
of the original space --- and investigate the effect of the transformation in
both learning generative models and denoising corrupted data. The results of
our experiments show that the proposed model can work as both a generative
model and a denoising model with improved performance due to the transformation
compared to conventional variational and denoising autoencoders.
",1,0,0,1,0,0
10440,Exchange constants in molecule-based magnets derived from density functional methods,"  Cu(pyz)(NO3)2 is a quasi one-dimensional molecular antiferromagnet that
exhibits three dimensional long-range magnetic order below TN=110 mK due to the
presence of weak inter-chain exchange couplings. Here we compare calculations
of the three largest exchange coupling constants in this system using two
techniques based on plane-wave basis-set density functional theory: (i) a dimer
fragment approach and (ii) an approach using periodic boundary conditions. The
calculated values of the large intrachain coupling constant are found to be
consistent with experiment, showing the expected level of variation between
different techniques and implementations. However, the interchain coupling
constants are found to be smaller than the current limits on the resolution of
the calculations. This is due to the computational limitations on convergence
of absolute energy differences with respect to basis set, which are larger than
the inter-chain couplings themselves. Our results imply that errors resulting
from such limitations are inherent in the evaluation of small exchange
constants in systems of this sort, and that many previously reported results
should therefore be treated with caution.
",0,1,0,0,0,0
1387,Automated Problem Identification: Regression vs Classification via Evolutionary Deep Networks,"  Regression or classification? This is perhaps the most basic question faced
when tackling a new supervised learning problem. We present an Evolutionary
Deep Learning (EDL) algorithm that automatically solves this by identifying the
question type with high accuracy, along with a proposed deep architecture.
Typically, a significant amount of human insight and preparation is required
prior to executing machine learning algorithms. For example, when creating deep
neural networks, the number of parameters must be selected in advance and
furthermore, a lot of these choices are made based upon pre-existing knowledge
of the data such as the use of a categorical cross entropy loss function.
Humans are able to study a dataset and decide whether it represents a
classification or a regression problem, and consequently make decisions which
will be applied to the execution of the neural network. We propose the
Automated Problem Identification (API) algorithm, which uses an evolutionary
algorithm interface to TensorFlow to manipulate a deep neural network to decide
if a dataset represents a classification or a regression problem. We test API
on 16 different classification, regression and sentiment analysis datasets with
up to 10,000 features and up to 17,000 unique target values. API achieves an
average accuracy of $96.3\%$ in identifying the problem type without hardcoding
any insights about the general characteristics of regression or classification
problems. For example, API successfully identifies classification problems even
with 1000 target values. Furthermore, the algorithm recommends which loss
function to use and also recommends a neural network architecture. Our work is
therefore a step towards fully automated machine learning.
",1,0,0,1,0,0
9951,Cosmological Simulations in Exascale Era,"  The architecture of Exascale computing facilities, which involves millions of
heterogeneous processing units, will deeply impact on scientific applications.
Future astrophysical HPC applications must be designed to make such computing
systems exploitable. The ExaNeSt H2020 EU-funded project aims to design and
develop an exascale ready prototype based on low-energy-consumption ARM64 cores
and FPGA accelerators. We participate to the design of the platform and to the
validation of the prototype with cosmological N-body and hydrodynamical codes
suited to perform large-scale, high-resolution numerical simulations of cosmic
structures formation and evolution. We discuss our activities on astrophysical
applications to take advantage of the underlying architecture.
",1,1,0,0,0,0
10992,The Mechanism of Electrolyte Gating on High-Tc Cuprates: The Role of Oxygen Migration and Electrostatics,"  Electrolyte gating is widely used to induce large carrier density modulation
on solid surfaces to explore various properties. Most of past works have
attributed the charge modulation to electrostatic field effect. However, some
recent reports have argued that the electrolyte gating effect in VO2, TiO2 and
SrTiO3 originated from field-induced oxygen vacancy formation. This gives rise
to a controversy about the gating mechanism, and it is therefore vital to
reveal the relationship between the role of electrolyte gating and the
intrinsic properties of materials. Here, we report entirely different
mechanisms of electrolyte gating on two high-Tc cuprates, NdBa2Cu3O7-{\delta}
(NBCO) and Pr2-xCexCuO4 (PCCO), with different crystal structures. We show that
field-induced oxygen vacancy formation in CuO chains of NBCO plays the dominant
role while it is mainly an electrostatic field effect in the case of PCCO. The
possible reason is that NBCO has mobile oxygen in CuO chains while PCCO does
not. Our study helps clarify the controversy relating to the mechanism of
electrolyte gating, leading to a better understanding of the role of oxygen
electro migration which is very material specific.
",0,1,0,0,0,0
13421,Semi-Parametric Empirical Best Prediction for small area estimation of unemployment indicators,"  The Italian National Institute for Statistics regularly provides estimates of
unemployment indicators using data from the Labor Force Survey. However, direct
estimates of unemployment incidence cannot be released for Local Labor Market
Areas. These are unplanned domains defined as clusters of municipalities; many
are out-of-sample areas and the majority is characterized by a small sample
size, which render direct estimates inadequate. The Empirical Best Predictor
represents an appropriate, model-based, alternative. However, for non-Gaussian
responses, its computation and the computation of the analytic approximation to
its Mean Squared Error require the solution of (possibly) multiple integrals
that, generally, have not a closed form. To solve the issue, Monte Carlo
methods and parametric bootstrap are common choices, even though the
computational burden is a non trivial task. In this paper, we propose a
Semi-Parametric Empirical Best Predictor for a (possibly) non-linear mixed
effect model by leaving the distribution of the area-specific random effects
unspecified and estimating it from the observed data. This approach is known to
lead to a discrete mixing distribution which helps avoid unverifiable
parametric assumptions and heavy integral approximations. We also derive a
second-order, bias-corrected, analytic approximation to the corresponding Mean
Squared Error. Finite sample properties of the proposed approach are tested via
a large scale simulation study. Furthermore, the proposal is applied to
unit-level data from the 2012 Italian Labor Force Survey to estimate
unemployment incidence for 611 Local Labor Market Areas using auxiliary
information from administrative registers and the 2011 Census.
",0,0,0,1,0,0
13894,A family of monogenic $S_4$ quartic fields arising from elliptic curves,"  We consider partial torsion fields (fields generated by a root of a division
polynomial) for elliptic curves. By analysing the reduction properties of
elliptic curves, and applying the Montes Algorithm, we obtain information about
the ring of integers. In particular, for the partial $3$-torsion fields for a
certain one-parameter family of non-CM elliptic curves, we describe a power
basis. As a result, we show that the one-parameter family of quartic $S_4$
fields given by $T^4 - 6T^2 - \alpha T - 3$ for $\alpha \in \mathbb{Z}$ such
that $\alpha \pm 8$ are squarefree, are monogenic.
",0,0,1,0,0,0
4269,MRI-PET Registration with Automated Algorithm in Pre-clinical Studies,"  Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET)
automatic 3-D registration is implemented and validated for small animal image
volumes so that the high-resolution anatomical MRI information can be fused
with the low spatial resolution of functional PET information for the
localization of lesion that is currently in high demand in the study of tumor
of cancer (oncology) and its corresponding preparation of pharmaceutical drugs.
Though many registration algorithms are developed and applied on human brain
volumes, these methods may not be as efficient on small animal datasets due to
lack of intensity information and often the high anisotropy in voxel
dimensions. Therefore, a fully automatic registration algorithm which can
register not only assumably rigid small animal volumes such as brain but also
deformable organs such as kidney, cardiac and chest is developed using a
combination of global affine and local B-spline transformation models in which
mutual information is used as a similarity criterion. The global affine
registration uses a multi-resolution pyramid on image volumes of 3 levels
whereas in local B-spline registration, a multi-resolution scheme is applied on
the B-spline grid of 2 levels on the finest resolution of the image volumes in
which only the transform itself is affected rather than the image volumes.
Since mutual information lacks sufficient spatial information, PCA is used to
inject it by estimating initial translation and rotation parameters. It is
computationally efficient since it is implemented using C++ and ITK library,
and is qualitatively and quantitatively shown that this PCA-initialized global
registration followed by local registration is in close agreement with expert
manual registration and outperforms the one without PCA initialization tested
on small animal brain and kidney.
",1,0,0,0,0,0
13378,Espresso: Brewing Java For More Non-Volatility with Non-volatile Memory,"  Fast, byte-addressable non-volatile memory (NVM) embraces both near-DRAM
latency and disk-like persistence, which has generated considerable interests
to revolutionize system software stack and programming models. However, it is
less understood how NVM can be combined with managed runtime like Java virtual
machine (JVM) to ease persistence management. This paper proposes Espresso, a
holistic extension to Java and its runtime, to enable Java programmers to
exploit NVM for persistence management with high performance. Espresso first
provides a general persistent heap design called Persistent Java Heap (PJH) to
manage persistent data as normal Java objects. The heap is then strengthened
with a recoverable mechanism to provide crash consistency for heap metadata. It
then provides a new abstraction called Persistent Java Object (PJO) to provide
an easy-to-use but safe persistent programming model for programmers to persist
application data. The evaluation confirms that Espresso significantly
outperforms state-of-art NVM support for Java (i.e., JPA and PCJ) while being
compatible to existing data structures in Java programs.
",1,0,0,0,0,0
1770,On effective Birkhoff's ergodic theorem for computable actions of amenable groups,"  We introduce computable actions of computable groups and prove the following
versions of effective Birkhoff's ergodic theorem. Let $\Gamma$ be a computable
amenable group, then there always exists a canonically computable tempered
two-sided F{\o}lner sequence $(F_n)_{n \geq
1}$ in $\Gamma$. For a computable, measure-preserving, ergodic action of
$\Gamma$ on a Cantor space $\{0,1\}^{\mathbb N}$ endowed with a computable
probability measure $\mu$, it is shown that for every bounded lower
semicomputable function $f$ on $\{0,1\}^{\mathbb N}$ and for every Martin-Löf
random $\omega \in \{0,1\}^{\mathbb N}$ the equality \[ \lim\limits_{n \to
\infty} \frac{1}{|F_n|} \sum\limits_{g \in F_n} f(g \cdot \omega) = \int\limits
f d \mu \] holds, where the averages are taken with respect to a canonically
computable tempered two-sided F{\o}lner sequence $(F_n)_{n \geq
1}$. We also prove the same identity for all lower semicomputable $f$'s in
the special case when $\Gamma$ is a computable group of polynomial growth and
$F_n:=\mathrm{B}(n)$ is the F{\o}lner sequence of balls around the neutral
element of $\Gamma$.
",0,0,1,0,0,0
18446,Kernel theorems for modulation spaces,"  We deal with kernel theorems for modulation spaces. We completely
characterize the continuity of a linear operator on the modulation spaces $M^p$
for every $1\leq p\leq\infty$, by the membership of its kernel to (mixed)
modulation spaces. Whereas Feichtinger's kernel theorem (which we recapture as
a special case) is the modulation space counterpart of Schwartz' kernel theorem
for temperate distributions, our results do not have a couterpart in
distribution theory. This reveals the superiority, in some respects, of the
modulation space formalism upon distribution theory, as already emphasized in
Feichtinger's manifesto for a post-modern harmonic analysis, tailored to the
needs of mathematical signal processing. The proof uses in an essential way a
discretization of the problem by means of Gabor frames. We also show the
equivalence of the operator norm and the modulation space norm of the
corresponding kernel. For operators acting on $M^{p,q}$ a similar
characterization is not expected, but sufficient conditions for boundedness can
be sated in the same spirit.
",0,0,1,0,0,0
2695,Deep Reinforcement Learning for Event-Driven Multi-Agent Decision Processes,"  The incorporation of macro-actions (temporally extended actions) into
multi-agent decision problems has the potential to address the curse of
dimensionality associated with such decision problems. Since macro-actions last
for stochastic durations, multiple agents executing decentralized policies in
cooperative environments must act asynchronously. We present an algorithm that
modifies Generalized Advantage Estimation for temporally extended actions,
allowing a state-of-the-art policy optimization algorithm to optimize policies
in Dec-POMDPs in which agents act asynchronously. We show that our algorithm is
capable of learning optimal policies in two cooperative domains, one involving
real-time bus holding control and one involving wildfire fighting with unmanned
aircraft. Our algorithm works by framing problems as ""event-driven decision
processes,"" which are scenarios where the sequence and timing of actions and
events are random and governed by an underlying stochastic process. In addition
to optimizing policies with continuous state and action spaces, our algorithm
also facilitates the use of event-driven simulators, which do not require time
to be discretized into time-steps. We demonstrate the benefit of using
event-driven simulation in the context of multiple agents taking asynchronous
actions. We show that fixed time-step simulation risks obfuscating the sequence
in which closely-separated events occur, adversely affecting the policies
learned. Additionally, we show that arbitrarily shrinking the time-step scales
poorly with the number of agents.
",1,0,0,0,0,0
7240,Full-angle Negative Reflection with An Ultrathin Acoustic Gradient Metasurface: Floquet-Bloch Modes Perspective and Experimental Verification,"  Metasurface with gradient phase response offers new alternative for steering
the propagation of waves. Conventional Snell's law has been revised by taking
the contribution of local phase gradient into account. However, the requirement
of momentum matching along the metasurface sets its nontrivial beam
manipulation functionality within a limited-angle incidence. In this work, we
theoretically and experimentally demonstrate that the acoustic gradient
metasurface supports the negative reflection for full-angle incidence. The mode
expansion theory is developed to help understand how the gradient metasurface
tailors the incident beams, and the full-angle negative reflection occurs when
the first negative order Floquet-Bloch mode dominates. The coiling-up space
structures are utilized to build desired acoustic gradient metasurface and the
full-angle negative reflections have been perfectly verified by experimental
measurements. Our work offers the Floquet-Bloch modes perspective for
qualitatively understanding the reflection behaviors of the acoustic gradient
metasurface and enables a new degree of the acoustic wave manipulating.
",0,1,0,0,0,0
16139,Compression-Based Regularization with an Application to Multi-Task Learning,"  This paper investigates, from information theoretic grounds, a learning
problem based on the principle that any regularity in a given dataset can be
exploited to extract compact features from data, i.e., using fewer bits than
needed to fully describe the data itself, in order to build meaningful
representations of a relevant content (multiple labels). We begin by
introducing the noisy lossy source coding paradigm with the log-loss fidelity
criterion which provides the fundamental tradeoffs between the
\emph{cross-entropy loss} (average risk) and the information rate of the
features (model complexity). Our approach allows an information theoretic
formulation of the \emph{multi-task learning} (MTL) problem which is a
supervised learning framework in which the prediction models for several
related tasks are learned jointly from common representations to achieve better
generalization performance. Then, we present an iterative algorithm for
computing the optimal tradeoffs and its global convergence is proven provided
that some conditions hold. An important property of this algorithm is that it
provides a natural safeguard against overfitting, because it minimizes the
average risk taking into account a penalization induced by the model
complexity. Remarkably, empirical results illustrate that there exists an
optimal information rate minimizing the \emph{excess risk} which depends on the
nature and the amount of available training data. An application to
hierarchical text categorization is also investigated, extending previous
works.
",1,0,0,1,0,0
17050,Lorentzian surfaces and the curvature of the Schmidt metric,"  The b-boundary is a mathematical tool used to attach a topological boundary
to incomplete Lorentzian manifolds using a Riemaniann metric called the Schmidt
metric on the frame bundle. In this paper, we give the general form of the
Schmidt metric in the case of Lorentzian surfaces. Furthermore, we write the
Ricci scalar of the Schmidt metric in terms of the Ricci scalar of the
Lorentzian manifold and give some examples. Finally, we discuss some
applications to general relativity.
",0,0,1,0,0,0
15381,Initial-boundary value problem to 2D Boussinesq equations for MHD convection with stratification effects,"  This paper is concerned with the initial-boundary value problem to 2D
magnetohydrodynamics-Boussinesq system with the temperature-dependent
viscosity, thermal diffusivity and electrical conductivity. First, we establish
the global weak solutions under the minimal initial assumption. Then by
imposing higher regularity assumption on the initial data, we obtain the global
strong solution with uniqueness. Moreover, the exponential decay estimate of
the solution is obtained.
",0,0,1,0,0,0
8572,Budgeted Experiment Design for Causal Structure Learning,"  We study the problem of causal structure learning when the experimenter is
limited to perform at most $k$ non-adaptive experiments of size $1$. We
formulate the problem of finding the best intervention target set as an
optimization problem, which aims to maximize the average number of edges whose
directions are resolved. We prove that the corresponding objective function is
submodular and a greedy algorithm suffices to achieve
$(1-\frac{1}{e})$-approximation of the optimal value. We further present an
accelerated variant of the greedy algorithm, which can lead to orders of
magnitude performance speedup. We validate our proposed approach on synthetic
and real graphs. The results show that compared to the purely observational
setting, our algorithm orients the majority of the edges through a considerably
small number of interventions.
",1,0,0,1,0,0
11737,A Diophantine approximation problem with two primes and one $k$-th power of a prime,"  We refine a result of the last two Authors of [8] on a Diophantine
approximation problem with two primes and a $k$-th power of a prime which was
only proved to hold for $1<k<4/3$. We improve the $k$-range to $1<k\le 3$ by
combining Harman's technique on the minor arc with a suitable estimate for the
$L^4$-norm of the relevant exponential sum over primes $S_k$. In the common
range we also give a stronger bound for the approximation.
",0,0,1,0,0,0
10716,The Pragmatics of Indirect Commands in Collaborative Discourse,"  Today's artificial assistants are typically prompted to perform tasks through
direct, imperative commands such as \emph{Set a timer} or \emph{Pick up the
box}. However, to progress toward more natural exchanges between humans and
these assistants, it is important to understand the way non-imperative
utterances can indirectly elicit action of an addressee. In this paper, we
investigate command types in the setting of a grounded, collaborative game. We
focus on a less understood family of utterances for eliciting agent action,
locatives like \emph{The chair is in the other room}, and demonstrate how these
utterances indirectly command in specific game state contexts. Our work shows
that models with domain-specific grounding can effectively realize the
pragmatic reasoning that is necessary for more robust natural language
interaction.
",1,0,0,0,0,0
13145,Unbounded product-form Petri nets,"  Computing steady-state distributions in infinite-state stochastic systems is
in general a very dificult task. Product-form Petri nets are those Petri nets
for which the steady-state distribution can be described as a natural product
corresponding, up to a normalising constant, to an exponentiation of the
markings. However, even though some classes of nets are known to have a
product-form distribution, computing the normalising constant can be hard. The
class of (closed) {\Pi}3-nets has been proposed in an earlier work, for which
it is shown that one can compute the steady-state distribution efficiently.
However these nets are bounded. In this paper, we generalise queuing Markovian
networks and closed {\Pi}3-nets to obtain the class of open {\Pi}3-nets, that
generate infinite-state systems. We show interesting properties of these nets:
(1) we prove that liveness can be decided in polynomial time, and that
reachability in live {\Pi}3-nets can be decided in polynomial time; (2) we show
that we can decide ergodicity of such nets in polynomial time as well; (3) we
provide a pseudo-polynomial time algorithm to compute the normalising constant.
",1,0,0,0,0,0
18727,Entropy? Honest!,"  Here we deconstruct, and then in a reasoned way reconstruct, the concept of
""entropy of a system,"" paying particular attention to where the randomness may
be coming from. We start with the core concept of entropy as a COUNT associated
with a DESCRIPTION; this count (traditionally expressed in logarithmic form for
a number of good reasons) is in essence the number of possibilities---specific
instances or ""scenarios,"" that MATCH that description. Very natural (and
virtually inescapable) generalizations of the idea of description are the
probability distribution and of its quantum mechanical counterpart, the density
operator.
We track the process of dynamically updating entropy as a system evolves.
Three factors may cause entropy to change: (1) the system's INTERNAL DYNAMICS;
(2) unsolicited EXTERNAL INFLUENCES on it; and (3) the approximations one has
to make when one tries to predict the system's future state. The latter task is
usually hampered by hard-to-quantify aspects of the original description,
limited data storage and processing resource, and possibly algorithmic
inadequacy. Factors 2 and 3 introduce randomness into one's predictions and
accordingly degrade them. When forecasting, as long as the entropy bookkeping
is conducted in an HONEST fashion, this degradation will ALWAYS lead to an
entropy increase.
To clarify the above point we introduce the notion of HONEST ENTROPY, which
coalesces much of what is of course already done, often tacitly, in responsible
entropy-bookkeping practice. This notion, we believe, will help to fill an
expressivity gap in scientific discourse. With its help we shall prove that ANY
dynamical system---not just our physical universe---strictly obeys Clausius's
original formulation of the second law of thermodynamics IF AND ONLY IF it is
invertible. Thus this law is a TAUTOLOGICAL PROPERTY of invertible systems!
",0,1,0,0,0,0
3698,An Infinite Hidden Markov Model With Similarity-Biased Transitions,"  We describe a generalization of the Hierarchical Dirichlet Process Hidden
Markov Model (HDP-HMM) which is able to encode prior information that state
transitions are more likely between ""nearby"" states. This is accomplished by
defining a similarity function on the state space and scaling transition
probabilities by pair-wise similarities, thereby inducing correlations among
the transition distributions. We present an augmented data representation of
the model as a Markov Jump Process in which: (1) some jump attempts fail, and
(2) the probability of success is proportional to the similarity between the
source and destination states. This augmentation restores conditional conjugacy
and admits a simple Gibbs sampler. We evaluate the model and inference method
on a speaker diarization task and a ""harmonic parsing"" task using four-part
chorale data, as well as on several synthetic datasets, achieving favorable
comparisons to existing models.
",1,0,0,1,0,0
3281,Comparison of methods for early-readmission prediction in a high-dimensional heterogeneous covariates and time-to-event outcome framework,"  Background: Choosing the most performing method in terms of outcome
prediction or variables selection is a recurring problem in prognosis studies,
leading to many publications on methods comparison. But some aspects have
received little attention. First, most comparison studies treat prediction
performance and variable selection aspects separately. Second, methods are
either compared within a binary outcome setting (based on an arbitrarily chosen
delay) or within a survival setting, but not both. In this paper, we propose a
comparison methodology to weight up those different settings both in terms of
prediction and variables selection, while incorporating advanced machine
learning strategies. Methods: Using a high-dimensional case study on a
sickle-cell disease (SCD) cohort, we compare 8 statistical methods. In the
binary outcome setting, we consider logistic regression (LR), support vector
machine (SVM), random forest (RF), gradient boosting (GB) and neural network
(NN); while on the survival analysis setting, we consider the Cox Proportional
Hazards (PH), the CURE and the C-mix models. We then compare performances of
all methods both in terms of risk prediction and variable selection, with a
focus on the use of Elastic-Net regularization technique. Results: Among all
assessed statistical methods assessed, the C-mix model yields the better
performances in both the two considered settings, as well as interesting
interpretation aspects. There is some consistency in selected covariates across
methods within a setting, but not much across the two settings. Conclusions: It
appears that learning withing the survival setting first, and then going back
to a binary prediction using the survival estimates significantly enhance
binary predictions.
",0,0,0,1,0,0
7481,Zero divisor and unit elements with support of size 4 in group algebras of torsion free groups,"  Kaplansky Zero Divisor Conjecture states that if $G $ is a torsion free group
and $ \mathbb{F} $ is a field, then the group ring $\mathbb{F}[G]$ contains no
zero divisor and Kaplansky Unit Conjecture states that if $G $ is a torsion
free group and $ \mathbb{F} $ is a field, then $\mathbb{F}[G]$ contains no
non-trivial units. The support of an element $ \alpha= \sum_{x\in G}\alpha_xx$
in $\mathbb{F}[G] $, denoted by $supp(\alpha)$, is the set $ \{x \in
G|\alpha_x\neq 0\} $. In this paper we study possible zero divisors and units
with supports of size $ 4 $ in $\mathbb{F}[G]$. We prove that if
$ \alpha, \beta $ are non-zero elements in $ \mathbb{F}[G] $ for a possible
torsion free group $ G $ and an arbitrary field $ \mathbb{F} $ such that $
|supp(\alpha)|=4 $ and $ \alpha\beta=0 $, then $|supp(\beta)|\geq 7 $. In [J.
Group Theory, $16$ $ (2013),$ no. $5$, $667$-$693$], it is proved that if $
\mathbb{F}=\mathbb{F}_2 $ is the field with two elements, $ G $ is a torsion
free group and $ \alpha,\beta \in \mathbb{F}_2[G]\setminus \{0\}$ such that
$|supp(\alpha)|=4 $ and $ \alpha\beta =0 $, then $|supp(\beta)|\geq 8$. We
improve the latter result to $|supp(\beta)|\geq 9$. Also, concerning the Unit
Conjecture, we prove that if $\mathsf{a}\mathsf{b}=1$ for some
$\mathsf{a},\mathsf{b}\in \mathbb{F}[G]$ and $|supp(\mathsf{a})|=4$, then
$|supp(\mathsf{b})|\geq 6$.
",0,0,1,0,0,0
4100,Reexamination of Tolman's law and the Gibbs adsorption equation for curved interfaces,"  The influence of the surface curvature on the surface tension of small
droplets in equilibrium with a surrounding vapour, or small bubbles in
equilibrium with a surrounding liquid, can be expanded as $\gamma(R) = \gamma_0
+ c_1\gamma_0/R + O(1/R^2)$, where $R = R_\gamma$ is the radius of the surface
of tension and $\gamma_0$ is the surface tension of the planar interface,
corresponding to zero curvature. According to Tolman's law, the first-order
coefficient in this expansion is assumed to be related to the planar limit
$\delta_0$ of the Tolman length, i.e., the difference $\delta = R_\rho -
R_\gamma$ between the equimolar radius and the radius of the surface of
tension, by $c_1 = -2\delta_0$.
We show here that the deduction of Tolman's law from interfacial
thermodynamics relies on an inaccurate application of the Gibbs adsorption
equation to dispersed phases (droplets or bubbles). A revision of the
underlying theory reveals that the adsorption equation needs to be employed in
an alternative manner to that suggested by Tolman. Accordingly, we develop a
generalized Gibbs adsorption equation which consistently takes the size
dependence of interfacial properties into account, and show that from this
equation, a relation between the Tolman length and the influence of the size of
the dispersed phase on the surface tension cannot be deduced, invalidating the
argument which was put forward by Tolman [J. Chem. Phys. 17 (1949) 333].
",0,1,0,0,0,0
9386,A five-decision testing procedure to infer on unidimensional parameter,"  A statistical test can be seen as a procedure to produce a decision based on
observed data, where some decisions consist of rejecting a hypothesis (yielding
a significant result) and some do not, and where one controls the probability
to make a wrong rejection at some pre-specified significance level. Whereas
traditional hypothesis testing involves only two possible decisions (to reject
or not a null hypothesis), Kaiser's directional two-sided test as well as the
more recently introduced Jones and Tukey's testing procedure involve three
possible decisions to infer on unidimensional parameter. The latter procedure
assumes that a point null hypothesis is impossible (e.g. that two treatments
cannot have exactly the same effect), allowing a gain of statistical power.
There are however situations where a point hypothesis is indeed plausible, for
example when considering hypotheses derived from Einstein's theories. In this
article, we introduce a five-decision rule testing procedure, which combines
the advantages of the testing procedures of Kaiser (no assumption on a point
hypothesis being impossible) and of Jones and Tukey (higher power), allowing
for a non-negligible (typically 20%) reduction of the sample size needed to
reach a given statistical power to get a significant result, compared to the
traditional approach.
",0,0,1,1,0,0
8208,Active sorting of orbital angular momentum states of light with cascaded tunable resonators,"  Light carrying orbital angular momentum (OAM) has been shown to be of use in
a disparate range of fields ranging from astronomy to optical trapping, and as
a promising new dimension for multiplexing signals in optical communications
and data storage. A challenge to many of these applications is a reliable and
dynamic method that sorts incident OAM states without altering them. Here we
report a wavelength-independent technique capable of dynamically filtering
individual OAM states based on the resonant transmission of a tunable optical
cavity. The cavity length is piezo-controlled to facilitate dynamic
reconfiguration, and the sorting process leaves both the transmitted and
reflected signals in their original states for subsequent processing. As a
result, we also show that a reconfigurable sorting network can be constructed
by cascading such optical resonators to handle multiple OAM states
simultaneously. This approach to sorting OAM states is amenable to integration
into optical communication networks and has implications in quantum optics,
astronomy, optical data storage and optical trapping.
",0,1,0,0,0,0
3310,Benchmarking gate-based quantum computers,"  With the advent of public access to small gate-based quantum processors, it
becomes necessary to develop a benchmarking methodology such that independent
researchers can validate the operation of these processors. We explore the
usefulness of a number of simple quantum circuits as benchmarks for gate-based
quantum computing devices and show that circuits performing identity operations
are very simple, scalable and sensitive to gate errors and are therefore very
well suited for this task. We illustrate the procedure by presenting benchmark
results for the IBM Quantum Experience, a cloud-based platform for gate-based
quantum computing.
",0,1,0,0,0,0
10778,Democratizing Design for Future Computing Platforms,"  Information and communications technology can continue to change our world.
These advances will partially depend upon designs that synergistically combine
software with specialized hardware. Today open-source software incubates rapid
software-only innovation. The government can unleash software-hardware
innovation with programs to develop open hardware components, tools, and design
flows that simplify and reduce the cost of hardware design. Such programs will
speed development for startup companies, established industry leaders,
education, scientific research, and for government intelligence and defense
platforms.
",1,0,0,0,0,0
20199,Connecting HL Tau to the Observed Exoplanet Sample,"  The Atacama Large Millimeter/submilimeter Array (ALMA) recently revealed a
set of nearly concentric gaps in the protoplanetary disk surrounding the young
star HL Tau. If these are carved by forming gas giants, this provides the first
set of orbital initial conditions for planets as they emerge from their birth
disks. Using N-body integrations, we have followed the evolution of the system
for 5 Gyr to explore the possible outcomes. We find that HL Tau initial
conditions scaled down to the size of typically observed exoplanet orbits
naturally produce several populations in the observed exoplanet sample. First,
for a plausible range of planetary masses, we can match the observed
eccentricity distribution of dynamically excited radial velocity giant planets
with eccentricities $>$ 0.2. Second, we roughly obtain the observed rate of hot
Jupiters around FGK stars. Finally, we obtain a large efficiency of planetary
ejections of $\approx 2$ per HL Tau-like system, but the small fraction of
stars observed to host giant planets makes it hard to match the rate of
free-floating planets inferred from microlensing observations. In view of
upcoming GAIA results, we also provide predictions for the expected mutual
inclination distribution, which is significantly broader than the absolute
inclination distributions typically considered by previous studies.
",0,1,0,0,0,0
10547,"On distributions determined by their upward, space-time Wiener-Hopf factor","  According to the Wiener-Hopf factorization, the characteristic function
$\varphi$ of any probability distribution $\mu$ on $\mathbb{R}$ can be
decomposed in a unique way as
\[1-s\varphi(t)=[1-\chi_-(s,it)][1-\chi_+(s,it)]\,,\;\;\;|s|\le1,\,t\in\mathbb{R}\,,\]
where $\chi_-(e^{iu},it)$ and $\chi_+(e^{iu},it)$ are the characteristic
functions of possibly defective distributions in
$\mathbb{Z}_+\times(-\infty,0)$ and $\mathbb{Z}_+\times[0,\infty)$,
respectively.
We prove that $\mu$ can be characterized by the sole data of the upward
factor $\chi_+(s,it)$, $s\in[0,1)$, $t\in\mathbb{R}$ in many cases including
the cases where:
1) $\mu$ has some exponential moments;
2) the function $t\mapsto\mu(t,\infty)$ is completely monotone on
$(0,\infty)$;
3) the density of $\mu$ on $[0,\infty)$ admits an analytic continuation on
$\mathbb{R}$.
We conjecture that any probability distribution is actually characterized by
its upward factor. This conjecture is equivalent to the following: {\it Any
probability measure $\mu$ on $\mathbb{R}$ whose support is not included in
$(-\infty,0)$ is determined by its convolution powers $\mu^{*n}$, $n\ge1$
restricted to $[0,\infty)$}. We show that in many instances, the sole knowledge
of $\mu$ and $\mu^{*2}$ restricted to $[0,\infty)$ is actually sufficient to
determine $\mu$. Then we investigate the analogous problem in the framework of
infinitely divisible distributions.
",0,0,1,0,0,0
18239,Classical Entanglement Structure in the Wavefunction of Inflationary Fluctuations,"  We argue that the preferred classical variables that emerge from a pure
quantum state are determined by its entanglement structure in the form of
redundant records: information shared between many subsystems. Focusing on the
early universe, we ask how classical metric perturbations emerge from vacuum
fluctuations in an inflationary background. We show that the squeezing of the
quantum state for super-horizon modes, along with minimal gravitational
interactions, leads to decoherence and to an exponential number of records of
metric fluctuations on very large scales, $\lambda/\lambda_{\rm
Hubble}>\Delta_\zeta^{-2/3}$, where $\Delta_\zeta\lesssim 10^{-5}$ is the
amplitude of scalar metric fluctuations. This determines a preferred
decomposition of the inflationary wavefunction into orthogonal ""branches""
corresponding to classical metric perturbations, which defines an inflationary
entropy production rate and accounts for the emergence of stochastic,
inhomogeneous spacetime geometry.
",0,1,0,0,0,0
11088,Polar Coding for the Binary Erasure Channel with Deletions,"  We study the application of polar codes in deletion channels by analyzing the
cascade of a binary erasure channel (BEC) and a deletion channel. We show how
polar codes can be used effectively on a BEC with a single deletion, and
propose a list decoding algorithm with a cyclic redundancy check for this case.
The decoding complexity is $O(N^2\log N)$, where $N$ is the blocklength of the
code. An important contribution is an optimization of the amount of redundancy
added to minimize the overall error probability. Our theoretical results are
corroborated by numerical simulations which show that the list size can be
reduced to one and the original message can be recovered with high probability
as the length of the code grows.
",1,0,1,0,0,0
17778,The late-time light curve of the type Ia supernova SN 2011fe,"  We present late-time optical $R$-band imaging data from the Palomar Transient
Factory (PTF) for the nearby type Ia supernova SN 2011fe. The stacked PTF light
curve provides densely sampled coverage down to $R\simeq22$ mag over 200 to 620
days past explosion. Combining with literature data, we estimate the
pseudo-bolometric light curve for this event from 200 to 1600 days after
explosion, and constrain the likely near-infrared contribution. This light
curve shows a smooth decline consistent with radioactive decay, except over
~450 to ~600 days where the light curve appears to decrease faster than
expected based on the radioactive isotopes presumed to be present, before
flattening at around 600 days. We model the 200-1600d pseudo-bolometric light
curve with the luminosity generated by the radioactive decay chains of
$^{56}$Ni, $^{57}$Ni and $^{55}$Co, and find it is not consistent with models
that have full positron trapping and no infrared catastrophe (IRC); some
additional energy escape other than optical/near-IR photons is required.
However, the light curve is consistent with models that allow for positron
escape (reaching 75% by day 500) and/or an IRC (with 85% of the flux emerging
in non-optical wavelengths by day 600). The presence of the $^{57}$Ni decay
chain is robustly detected, but the $^{55}$Co decay chain is not formally
required, with an upper mass limit estimated at 0.014 M$_{\odot}$. The
measurement of the $^{57}$Ni/$^{56}$Ni mass ratio is subject to significant
systematic uncertainties, but all of our fits require a high ratio >0.031 (>1.3
in solar abundances).
",0,1,0,0,0,0
2134,Congenial Causal Inference with Binary Structural Nested Mean Models,"  Structural nested mean models (SNMMs) are among the fundamental tools for
inferring causal effects of time-dependent exposures from longitudinal studies.
With binary outcomes, however, current methods for estimating multiplicative
and additive SNMM parameters suffer from variation dependence between the
causal SNMM parameters and the non-causal nuisance parameters. Estimating
methods for logistic SNMMs do not suffer from this dependence. Unfortunately,
in contrast with the multiplicative and additive models, unbiased estimation of
the causal parameters of a logistic SNMM rely on additional modeling
assumptions even when the treatment probabilities are known. These difficulties
have hindered the uptake of SNMMs in epidemiological practice, where binary
outcomes are common. We solve the variation dependence problem for the binary
multiplicative SNMM by a reparametrization of the non-causal nuisance
parameters. Our novel nuisance parameters are variation independent of the
causal parameters, and hence allows the fitting of a multiplicative SNMM by
unconstrained maximum likelihood. It also allows one to construct true (i.e.
congenial) doubly robust estimators of the causal parameters. Along the way, we
prove that an additive SNMM with binary outcomes does not admit a variation
independent parametrization, thus explaining why we restrict ourselves to the
multiplicative SNMM.
",0,0,0,1,0,0
13963,Two-Dimensional Systolic Complexes Satisfy Property A,"  We show that 2-dimensional systolic complexes are quasi-isometric to quadric
complexes with flat intervals. We use this fact along with the weight function
of Brodzki, Campbell, Guentner, Niblo and Wright to prove that 2-dimensional
systolic complexes satisfy Property A.
",0,0,1,0,0,0
18327,Group Sparse Bayesian Learning for Active Surveillance on Epidemic Dynamics,"  Predicting epidemic dynamics is of great value in understanding and
controlling diffusion processes, such as infectious disease spread and
information propagation. This task is intractable, especially when surveillance
resources are very limited. To address the challenge, we study the problem of
active surveillance, i.e., how to identify a small portion of system components
as sentinels to effect monitoring, such that the epidemic dynamics of an entire
system can be readily predicted from the partial data collected by such
sentinels. We propose a novel measure, the gamma value, to identify the
sentinels by modeling a sentinel network with row sparsity structure. We design
a flexible group sparse Bayesian learning algorithm to mine the sentinel
network suitable for handling both linear and non-linear dynamical systems by
using the expectation maximization method and variational approximation. The
efficacy of the proposed algorithm is theoretically analyzed and empirically
validated using both synthetic and real-world data.
",1,0,0,1,0,0
3651,Disentangling group and link persistence in Dynamic Stochastic Block models,"  We study the inference of a model of dynamic networks in which both
communities and links keep memory of previous network states. By considering
maximum likelihood inference from single snapshot observations of the network,
we show that link persistence makes the inference of communities harder,
decreasing the detectability threshold, while community persistence tends to
make it easier. We analytically show that communities inferred from single
network snapshot can share a maximum overlap with the underlying communities of
a specific previous instant in time. This leads to time-lagged inference: the
identification of past communities rather than present ones. Finally we compute
the time lag and propose a corrected algorithm, the Lagged Snapshot Dynamic
(LSD) algorithm, for community detection in dynamic networks. We analytically
and numerically characterize the detectability transitions of such algorithm as
a function of the memory parameters of the model and we make a comparison with
a full dynamic inference.
",1,1,0,1,0,0
16828,A finite temperature study of ideal quantum gases in the presence of one dimensional quasi-periodic potential,"  We study the thermodynamics of ideal Bose gas as well as the transport
properties of non interacting bosons and fermions in a one dimensional
quasi-periodic potential, namely Aubry-André (AA) model at finite
temperature. For bosons in finite size systems, the effect of quasi-periodic
potential on the crossover phenomena corresponding to Bose-Einstein
condensation (BEC), superfluidity and localization phenomena at finite
temperatures are investigated. From the ground state number fluctuation we
calculate the crossover temperature of BEC which exhibits a non monotonic
behavior with the strength of AA potential and vanishes at the self-dual
critical point following power law. Appropriate rescaling of the crossover
temperatures reveals universal behavior which is studied for different
quasi-periodicity of the AA model. Finally, we study the temperature and flux
dependence of the persistent current of fermions in presence of a
quasi-periodic potential to identify the localization at the Fermi energy from
the decay of the current.
",0,1,0,0,0,0
1980,Small Resolution Proofs for QBF using Dependency Treewidth,"  In spite of the close connection between the evaluation of quantified Boolean
formulas (QBF) and propositional satisfiability (SAT), tools and techniques
which exploit structural properties of SAT instances are known to fail for QBF.
This is especially true for the structural parameter treewidth, which has
allowed the design of successful algorithms for SAT but cannot be
straightforwardly applied to QBF since it does not take into account the
interdependencies between quantified variables.
In this work we introduce and develop dependency treewidth, a new structural
parameter based on treewidth which allows the efficient solution of QBF
instances. Dependency treewidth pushes the frontiers of tractability for QBF by
overcoming the limitations of previously introduced variants of treewidth for
QBF. We augment our results by developing algorithms for computing the
decompositions that are required to use the parameter.
",1,0,0,0,0,0
9325,Adversarial Training for Disease Prediction from Electronic Health Records with Missing Data,"  Electronic health records (EHRs) have contributed to the computerization of
patient records and can thus be used not only for efficient and systematic
medical services, but also for research on biomedical data science. However,
there are many missing values in EHRs when provided in matrix form, which is an
important issue in many biomedical EHR applications. In this paper, we propose
a two-stage framework that includes missing data imputation and disease
prediction to address the missing data problem in EHRs. We compared the disease
prediction performance of generative adversarial networks (GANs) and
conventional learning algorithms in combination with missing data prediction
methods. As a result, we obtained a level of accuracy of 0.9777, sensitivity of
0.9521, specificity of 0.9925, area under the receiver operating characteristic
curve (AUC-ROC) of 0.9889, and F-score of 0.9688 with a stacked autoencoder as
the missing data prediction method and an auxiliary classifier GAN (AC-GAN) as
the disease prediction method. The comparison results show that a combination
of a stacked autoencoder and an AC-GAN significantly outperforms other existing
approaches. Our results suggest that the proposed framework is more robust for
disease prediction from EHRs with missing data.
",1,0,0,1,0,0
10698,Disturbance-to-State Stabilization and Quantized Control for Linear Hyperbolic Systems,"  We consider a system of linear hyperbolic PDEs where the state at one of the
boundary points is controlled using the measurements of another boundary point.
Because of the disturbances in the measurement, the problem of designing
dynamic controllers is considered so that the closed-loop system is robust with
respect to measurement errors. Assuming that the disturbance is a locally
essentially bounded measurable function of time, we derive a
disturbance-to-state estimate which provides an upper bound on the maximum norm
of the state (with respect to the spatial variable) at each time in terms of
$\mathcal{L}^\infty$-norm of the disturbance up to that time. The analysis is
based on constructing a Lyapunov function for the closed-loop system, which
leads to controller synthesis and the conditions on system dynamics required
for stability. As an application of this stability notion, the problem of
quantized control for hyperbolic PDEs is considered where the measurements sent
to the controller are communicated using a quantizer of finite length. The
presence of quantizer yields practical stability only, and the ultimate bounds
on the norm of the state trajectory are also derived.
",1,0,1,0,0,0
14933,Transversal magnetoresistance and Shubnikov-de Haas oscillations in Weyl semimetals,"  We explore theoretically the magnetoresistance of Weyl semimetals in
transversal magnetic fields away from charge neutrality. The analysis within
the self-consistent Born approximation is done for the two different models of
disorder: (i) short-range impurties and (ii) charged (Coulomb) impurities. For
these models of disorder, we calculate the conductivity away from charge
neutrality point as well as the Hall conductivity, and analyze the transversal
magnetoresistance (TMR) and Shubnikov-de Haas oscillations for both types of
disorder. We further consider a model with Weyl nodes shifted in energy with
respect to each other (as found in various materials) with the chemical
potential corresponding to the total charge neutrality. In the experimentally
most relevant case of Coulomb impurities, we find in this model a large TMR in
a broad range of quantizing magnetic fields. More specifically, in the
ultra-quantum limit, where only the zeroth Landau level is effective, the TMR
is linear in magnetic field. In the regime of moderate (but still quantizing)
magnetic fields, where the higher Landau levels are relevant, the rapidly
growing TMR is supplemented by strong Shubnikov-de Haas oscillations,
consistent with experimental observations.
",0,1,0,0,0,0
8647,Simulation-based reachability analysis for nonlinear systems using componentwise contraction properties,"  A shortcoming of existing reachability approaches for nonlinear systems is
the poor scalability with the number of continuous state variables. To mitigate
this problem we present a simulation-based approach where we first sample a
number of trajectories of the system and next establish bounds on the
convergence or divergence between the samples and neighboring trajectories. We
compute these bounds using contraction theory and reduce the conservatism by
partitioning the state vector into several components and analyzing contraction
properties separately in each direction. Among other benefits this allows us to
analyze the effect of constant but uncertain parameters by treating them as
state variables and partitioning them into a separate direction. We next
present a numerical procedure to search for weighted norms that yield a
prescribed contraction rate, which can be incorporated in the reachability
algorithm to adjust the weights to minimize the growth of the reachable set.
",1,0,0,0,0,0
31,mixup: Beyond Empirical Risk Minimization,"  Large deep neural networks are powerful, but exhibit undesirable behaviors
such as memorization and sensitivity to adversarial examples. In this work, we
propose mixup, a simple learning principle to alleviate these issues. In
essence, mixup trains a neural network on convex combinations of pairs of
examples and their labels. By doing so, mixup regularizes the neural network to
favor simple linear behavior in-between training examples. Our experiments on
the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show
that mixup improves the generalization of state-of-the-art neural network
architectures. We also find that mixup reduces the memorization of corrupt
labels, increases the robustness to adversarial examples, and stabilizes the
training of generative adversarial networks.
",1,0,0,1,0,0
8823,Optimal bounds and extremal trajectories for time averages in nonlinear dynamical systems,"  For any quantity of interest in a system governed by ordinary differential
equations, it is natural to seek the largest (or smallest) long-time average
among solution trajectories, as well as the extremal trajectories themselves.
Upper bounds on time averages can be proved a priori using auxiliary functions,
the optimal choice of which is a convex optimization problem. We prove that the
problems of finding maximal trajectories and minimal auxiliary functions are
strongly dual. Thus, auxiliary functions provide arbitrarily sharp upper bounds
on time averages. Moreover, any nearly minimal auxiliary function provides
phase space volumes in which all nearly maximal trajectories are guaranteed to
lie. For polynomial equations, auxiliary functions can be constructed by
semidefinite programming, which we illustrate using the Lorenz system.
",0,1,1,0,0,0
795,Excited states of defect lines in silicon: A first-principles study based on hydrogen cluster analogues,"  Excited states of a single donor in bulk silicon have previously been studied
extensively based on effective mass theory. However, a proper theoretical
description of the excited states of a donor cluster is still scarce. Here we
study the excitations of lines of defects within a single-valley spherical band
approximation, thus mapping the problem to a scaled hydrogen atom array. A
series of detailed full configuration-interaction and time-dependent hybrid
density-functional theory calculations have been performed to understand linear
clusters of up to 10 donors. Our studies illustrate the generic features of
their excited states, addressing the competition between formation of
inter-donor ionic states and intra-donor atomic excited states. At short
inter-donor distances, excited states of donor molecules are dominant, at
intermediate distances ionic states play an important role, and at long
distances the intra-donor excitations are predominant as expected. The
calculations presented here emphasise the importance of correlations between
donor electrons, and are thus complementary to other recent approaches that
include effective mass anisotropy and multi-valley effects. The exchange
splittings between relevant excited states have also been estimated for a donor
pair and for a three-donor arrays; the splittings are much larger than those in
the ground state in the range of donor separations between 10 and 20 nm. This
establishes a solid theoretical basis for the use of excited-state exchange
interactions for controllable quantum gate operations in silicon.
",0,1,0,0,0,0
18511,Observation of a Lamb band gap in a polymer waveguide with periodic cross-like cavities,"  The quest for large and low frequency band gaps is one of the principal
objectives pursued in a number of engineering applications, ranging from noise
absorption to vibration control, to seismic wave abatement. For this purpose, a
plethora of complex architectures (including multi-phase materials) and
multi-physics approaches have been proposed in the past, often involving
difficulties in their practical realization.
To address this issue, in this work we propose an easy-to-manufacture design
able to open large, low frequency complete Lamb band gaps exploiting a suitable
arrangement of masses and stiffnesses produced by cavities in a monolithic
material. The performance of the designed structure is evaluated by numerical
simulations and confirmed by Scanning Laser Doppler Vibrometer (SLDV)
measurements on an isotropic polyvinyl chloride plate in which a square ring
region of cross-like cavities is fabricated. The full wave field reconstruction
clearly confirms the ability of even a limited number of unit cell rows of the
proposed design to efficiently attenuate Lamb waves. In addition, numerical
simulations show that the structure allows to shift of the central frequency of
the BG through geometrical modifications. The design may be of interest for
applications in which large BGs at low frequencies are required.
",0,1,0,0,0,0
11152,Gamma-Band Correlations in Primary Visual Cortex,"  Neural field theory is used to quantitatively analyze the two-dimensional
spatiotemporal correlation properties of gamma-band (30 -- 70 Hz) oscillations
evoked by stimuli arriving at the primary visual cortex (V1), and modulated by
patchy connectivities that depend on orientation preference (OP). Correlation
functions are derived analytically under different stimulus and measurement
conditions. The predictions reproduce a range of published experimental
results, including the existence of two-point oscillatory temporal
cross-correlations with zero time-lag between neurons with similar OP, the
influence of spatial separation of neurons on the strength of the correlations,
and the effects of differing stimulus orientations.
",0,0,0,0,1,0
20446,White light emission from silicon nanoparticles,"  As one of the most important semiconductors, silicon (Si) has been used to
fabricate electronic devices, waveguides, detectors, and solar cells etc.
However, its indirect bandgap hinders the use of Si for making good emitters1.
For integrated photonic circuits, Si-based emitters with sizes in the range of
100-300 nm are highly desirable. Here, we show that efficient white light
emission can be realized in spherical and cylindrical Si nanoparticles with
feature sizes of ~200 nm. The up-converted luminescence appears at the magnetic
and electric multipole resonances when the nanoparticles are resonantly excited
at their magnetic and electric dipole resonances by using femtosecond (fs)
laser pulses with ultralow low energy of ~40 pJ. The lifetime of the white
light is as short as ~52 ps, almost three orders of magnitude smaller than the
state-of-the-art results reported so far for Si (~10 ns). Our finding paves the
way for realizing efficient Si-based emitters compatible with current
semiconductor fabrication technology, which can be integrated to photonic
circuits.
",0,1,0,0,0,0
19736,An analytical Model which Determines the Apparent T1 for Modified Look-Locker Inversion Recovery (MOLLI) -- Analysis of the Longitudinal Relaxation under the Influence of Discontinuous Balanced and Spoiled Gradient Echo Readouts,"  Quantitative nuclear magnetic resonance imaging (MRI) shifts more and more
into the focus of clinical research. Especially determination of relaxation
times without/and with contrast agents becomes the foundation of tissue
characterization, e.g. in cardiac MRI for myocardial fibrosis. Techniques which
assess longitudinal relaxation times rely on repetitive application of readout
modules, which are interrupted by free relaxation periods, e.g. the Modified
Look-Locker Inversion Recovery = MOLLI sequence. These discontinuous sequences
reveal an apparent relaxation time, and, by techniques extrapolated from
continuous readout sequences, the real T1 is determined. What is missing is a
rigorous analysis of the dependence of the apparent relaxation time on its real
partner, readout sequence parameters and biological parameters as heart rate.
This is provided in this paper for the discontinuous balanced steady state free
precession (bSSFP) and spoiled gradient echo readouts. It turns out that the
apparente longitudinal relaxation rate is the time average of the relaxation
rates during the readout module, and free relaxation period. Knowing the heart
rate our results vice versa allow to determine the real T1 from its measured
apparent partner.
",0,1,0,0,0,0
7686,Accumulation Bit-Width Scaling For Ultra-Low Precision Training Of Deep Networks,"  Efforts to reduce the numerical precision of computations in deep learning
training have yielded systems that aggressively quantize weights and
activations, yet employ wide high-precision accumulators for partial sums in
inner-product operations to preserve the quality of convergence. The absence of
any framework to analyze the precision requirements of partial sum
accumulations results in conservative design choices. This imposes an
upper-bound on the reduction of complexity of multiply-accumulate units. We
present a statistical approach to analyze the impact of reduced accumulation
precision on deep learning training. Observing that a bad choice for
accumulation precision results in loss of information that manifests itself as
a reduction in variance in an ensemble of partial sums, we derive a set of
equations that relate this variance to the length of accumulation and the
minimum number of bits needed for accumulation. We apply our analysis to three
benchmark networks: CIFAR-10 ResNet 32, ImageNet ResNet 18 and ImageNet
AlexNet. In each case, with accumulation precision set in accordance with our
proposed equations, the networks successfully converge to the single precision
floating-point baseline. We also show that reducing accumulation precision
further degrades the quality of the trained network, proving that our equations
produce tight bounds. Overall this analysis enables precise tailoring of
computation hardware to the application, yielding area- and power-optimal
systems.
",1,0,0,1,0,0
12682,SPASS: Scientific Prominence Active Search System with Deep Image Captioning Network,"  Planetary exploration missions with Mars rovers are complicated, which
generally require elaborated task planning by human experts, from the path to
take to the images to capture. NASA has been using this process to acquire over
22 million images from the planet Mars. In order to improve the degree of
automation and thus efficiency in this process, we propose a system for
planetary rovers to actively search for prominence of prespecified scientific
features in captured images. Scientists can prespecify such search tasks in
natural language and upload them to a rover, on which the deployed system
constantly captions captured images with a deep image captioning network and
compare the auto-generated captions to the prespecified search tasks by certain
metrics so as to prioritize those images for transmission. As a beneficial side
effect, the proposed system can also be deployed to ground-based planetary data
systems as a content-based search engine.
",1,0,0,1,0,0
12101,VIP: Vortex Image Processing package for high-contrast direct imaging,"  We present the Vortex Image Processing (VIP) library, a python package
dedicated to astronomical high-contrast imaging. Our package relies on the
extensive python stack of scientific libraries and aims to provide a flexible
framework for high-contrast data and image processing. In this paper, we
describe the capabilities of VIP related to processing image sequences acquired
using the angular differential imaging (ADI) observing technique. VIP
implements functionalities for building high-contrast data processing
pipelines, encompass- ing pre- and post-processing algorithms, potential
sources position and flux estimation, and sensitivity curves generation. Among
the reference point-spread function subtraction techniques for ADI
post-processing, VIP includes several flavors of principal component analysis
(PCA) based algorithms, such as annular PCA and incremental PCA algorithm
capable of processing big datacubes (of several gigabytes) on a computer with
limited memory. Also, we present a novel ADI algorithm based on non-negative
matrix factorization (NMF), which comes from the same family of low-rank matrix
approximations as PCA and provides fairly similar results. We showcase the ADI
capabilities of the VIP library using a deep sequence on HR8799 taken with the
LBTI/LMIRCam and its recently commissioned L-band vortex coronagraph. Using VIP
we investigated the presence of additional companions around HR8799 and did not
find any significant additional point source beyond the four known planets. VIP
is available at this http URL and is accompanied with
Jupyter notebook tutorials illustrating the main functionalities of the
library.
",0,1,0,0,0,0
3064,Ergodicity of spherically symmetric fluid flows outside of a Schwarzschild black hole with random boundary forcing,"  We consider the Burgers equation posed on the outer communication region of a
Schwarzschild black hole spacetime. Assuming spherical symmetry for the fluid
flow under consideration, we study the propagation and interaction of shock
waves under the effect of random forcing. First of all, considering the initial
and boundary value problem with boundary data prescribed in the vicinity of the
horizon, we establish a generalization of the Hopf--Lax--Oleinik formula, which
takes the curved geometry into account and allows us to establish the existence
of bounded variation solutions. To this end, we analyze the global behavior of
the characteristic curves in the Schwarzschild geometry, including their
behavior near the black hole horizon. In a second part, we investigate the
long-term statistical properties of solutions when a random forcing is imposed
near the black hole horizon and study the ergodicity of the fluid flow under
consideration. We prove the existence of a random global attractor and, for the
Burgers equation outside of a Schwarzschild black hole, we are able to validate
the so-called `one-force-one-solution' principle. Furthermore, all of our
results are also established for a pressureless Euler model which consists of
two balance laws and includes a transport equation satisfied by the integrated
fluid density.
",0,0,1,0,0,0
16106,Revisiting the cavity-method threshold for random 3-SAT,"  A detailed Monte Carlo-study of the satisfiability threshold for random 3-SAT
has been undertaken. In combination with a monotonicity assumption we find that
the threshold for random 3-SAT satisfies $\alpha_3 \leq 4.262$. If the
assumption is correct, this means that the actual threshold value for $k=3$ is
lower than that given by the cavity method. In contrast the latter has recently
been shown to give the correct value for large $k$. Our result thus indicate
that there are distinct behaviors for $k$ above and below some critical $k_c$,
and the cavity method may provide a correct mean-field picture for the range
above $k_c$.
",0,1,0,0,0,0
3864,Small presentations of model categories and Vopěnka's principle,"  We prove existence results for small presentations of model categories
generalizing a theorem of D. Dugger from combinatorial model categories to more
general model categories. Some of these results are shown under the assumption
of Vopěnka's principle. Our main theorem applies in particular to
cofibrantly generated model categories where the domains of the generating
cofibrations satisfy a slightly stronger smallness condition. As a consequence,
assuming Vopěnka's principle, such a cofibrantly generated model category
is Quillen equivalent to a combinatorial model category. Moreover, if there are
generating sets which consist of presentable objects, then the same conclusion
holds without the assumption of Vopěnka's principle. We also correct a
mistake from previous work that made similar claims.
",0,0,1,0,0,0
18728,Prior Convictions: Black-Box Adversarial Attacks with Bandits and Priors,"  We study the problem of generating adversarial examples in a black-box
setting in which only loss-oracle access to a model is available. We introduce
a framework that conceptually unifies much of the existing work on black-box
attacks, and we demonstrate that the current state-of-the-art methods are
optimal in a natural sense. Despite this optimality, we show how to improve
black-box attacks by bringing a new element into the problem: gradient priors.
We give a bandit optimization-based algorithm that allows us to seamlessly
integrate any such priors, and we explicitly identify and incorporate two
examples. The resulting methods use two to four times fewer queries and fail
two to five times less often than the current state-of-the-art.
",0,0,0,1,0,0
3492,Probabilistic Line Searches for Stochastic Optimization,"  In deterministic optimization, line searches are a standard tool ensuring
stability and efficiency. Where only stochastic gradients are available, no
direct equivalent has so far been formulated, because uncertain gradients do
not allow for a strict sequence of decisions collapsing the search space. We
construct a probabilistic line search by combining the structure of existing
deterministic methods with notions from Bayesian optimization. Our method
retains a Gaussian process surrogate of the univariate optimization objective,
and uses a probabilistic belief over the Wolfe conditions to monitor the
descent. The algorithm has very low computational cost, and no user-controlled
parameters. Experiments show that it effectively removes the need to define a
learning rate for stochastic gradient descent.
",1,0,0,1,0,0
15199,Simple Policy Evaluation for Data-Rich Iterative Tasks,"  A data-based policy for iterative control task is presented. The proposed
strategy is model-free and can be applied whenever safe input and state
trajectories of a system performing an iterative task are available. These
trajectories, together with a user-defined cost function, are exploited to
construct a piecewise affine approximation to the value function. Approximated
value functions are then used to evaluate the control policy by solving a
linear program. We show that for linear system subject to convex cost and
constraints, the proposed strategy guarantees closed-loop constraint
satisfaction and performance bounds on the closed-loop trajectory. We evaluate
the proposed strategy in simulations and experiments, the latter carried out on
the Berkeley Autonomous Race Car (BARC) platform. We show that the proposed
strategy is able to reduce the computation time by one order of magnitude while
achieving the same performance as our model-based control algorithm.
",1,0,0,0,0,0
13087,Deterministic Browser,"  Timing attacks have been a continuous threat to users' privacy in modern
browsers. To mitigate such attacks, existing approaches, such as Tor Browser
and Fermata, add jitters to the browser clock so that an attacker cannot
accurately measure an event. However, such defenses only raise the bar for an
attacker but do not fundamentally mitigate timing attacks, i.e., it just takes
longer than previous to launch a timing attack. In this paper, we propose a
novel approach, called deterministic browser, which can provably prevent timing
attacks in modern browsers. Borrowing from Physics, we introduce several
concepts, such as an observer and a reference frame. Specifically, a snippet of
JavaScript, i.e., an observer in JavaScript reference frame, will always obtain
the same, fixed timing information so that timing attacks are prevented; at
contrast, a user, i.e., an oracle observer, will perceive the JavaScript
differently and do not experience the performance slowdown. We have implemented
a prototype called DeterFox and our evaluation shows that the prototype can
defend against browser-related timing attacks.
",1,0,0,0,0,0
4802,Ultracold atoms in multiple-radiofrequency dressed adiabatic potentials,"  We present the first experimental demonstration of a multiple-radiofrequency
dressed potential for the configurable magnetic confinement of ultracold atoms.
We load cold $^{87}$Rb atoms into a double well potential with an adjustable
barrier height, formed by three radiofrequencies applied to atoms in a static
quadrupole magnetic field. Our multiple-radiofrequency approach gives precise
control over the double well characteristics, including the depth of individual
wells and the height of the barrier, and enables reliable transfer of atoms
between the available trapping geometries. We have characterised the
multiple-radiofrequency dressed system using radiofrequency spectroscopy,
finding good agreement with the eigenvalues numerically calculated using
Floquet theory. This method creates trapping potentials that can be
reconfigured by changing the amplitudes, polarizations and frequencies of the
applied dressing fields, and easily extended with additional dressing
frequencies.
",0,1,0,0,0,0
20867,"Reminiscences of Julian Schwinger: Late Harvard, Early UCLA Years (1968-1981)","  These are reminiscences of my interactions with Julian Schwinger from 1968
through 1981 and beyond.
",0,1,0,0,0,0
4489,Stateless Puzzles for Real Time Online Fraud Preemption,"  The profitability of fraud in online systems such as app markets and social
networks marks the failure of existing defense mechanisms. In this paper, we
propose FraudSys, a real-time fraud preemption approach that imposes
Bitcoin-inspired computational puzzles on the devices that post online system
activities, such as reviews and likes. We introduce and leverage several novel
concepts that include (i) stateless, verifiable computational puzzles, that
impose minimal performance overhead, but enable the efficient verification of
their authenticity, (ii) a real-time, graph-based solution to assign fraud
scores to user activities, and (iii) mechanisms to dynamically adjust puzzle
difficulty levels based on fraud scores and the computational capabilities of
devices. FraudSys does not alter the experience of users in online systems, but
delays fraudulent actions and consumes significant computational resources of
the fraudsters. Using real datasets from Google Play and Facebook, we
demonstrate the feasibility of FraudSys by showing that the devices of honest
users are minimally impacted, while fraudster controlled devices receive daily
computational penalties of up to 3,079 hours. In addition, we show that with
FraudSys, fraud does not pay off, as a user equipped with mining hardware
(e.g., AntMiner S7) will earn less than half through fraud than from honest
Bitcoin mining.
",1,0,0,0,0,0
2672,Modeling Human Categorization of Natural Images Using Deep Feature Representations,"  Over the last few decades, psychologists have developed sophisticated formal
models of human categorization using simple artificial stimuli. In this paper,
we use modern machine learning methods to extend this work into the realm of
naturalistic stimuli, enabling human categorization to be studied over the
complex visual domain in which it evolved and developed. We show that
representations derived from a convolutional neural network can be used to
model behavior over a database of >300,000 human natural image classifications,
and find that a group of models based on these representations perform well,
near the reliability of human judgments. Interestingly, this group includes
both exemplar and prototype models, contrasting with the dominance of exemplar
models in previous work. We are able to improve the performance of the
remaining models by preprocessing neural network representations to more
closely capture human similarity judgments.
",1,0,0,1,0,0
14319,Volumes of $\mathrm{SL}_n\mathbb{C}$-representations of hyperbolic 3-manifolds,"  Let $M$ be a compact oriented three-manifold whose interior is hyperbolic of
finite volume. We prove a variation formula for the volume on the variety of
representations of $M$ in $\operatorname{SL}_n(\mathbb C)$. Our proof follows
the strategy of Reznikov's rigidity when $M$ is closed, in particular we use
Fuks' approach to variations by means of Lie algebra cohomology. When $n=2$, we
get back Hodgson's formula for variation of volume on the space of hyperbolic
Dehn fillings. Our formula also yields the variation of volume on the space of
decorated triangulations obtained by Bergeron-Falbel-Guillou and
Dimofte-Gabella-Goncharov.
",0,0,1,0,0,0
14206,Semi-automated Signal Surveying Using Smartphones and Floorplans,"  Location fingerprinting locates devices based on pattern matching signal
observations to a pre-defined signal map. This paper introduces a technique to
enable fast signal map creation given a dedicated surveyor with a smartphone
and floorplan. Our technique (PFSurvey) uses accelerometer, gyroscope and
magnetometer data to estimate the surveyor's trajectory post-hoc using
Simultaneous Localisation and Mapping and particle filtering to incorporate a
building floorplan. We demonstrate conventional methods can fail to recover the
survey path robustly and determine the room unambiguously. To counter this we
use a novel loop closure detection method based on magnetic field signals and
propose to incorporate the magnetic loop closures and straight-line constraints
into the filtering process to ensure robust trajectory recovery. We show this
allows room ambiguities to be resolved.
An entire building can be surveyed by the proposed system in minutes rather
than days. We evaluate in a large office space and compare to state-of-the-art
approaches. We achieve trajectories within 1.1 m of the ground truth 90% of the
time. Output signal maps well approximate those built from conventional,
laborious manual survey. We also demonstrate that the signal maps built by
PFSurvey provide similar or even better positioning performance than the manual
signal maps.
",1,0,0,0,0,0
7762,"Interacting superradiance samples: modified intensities and timescales, and frequency shifts","  We consider the interaction between distinct superradiance (SR) systems and
use the dressed state formalism to solve the case of two interacting two-atom
SR samples at resonance. We show that the ensuing entanglement modifies the
transition rates and intensities of radiation, as well as introduces a
potentially measurable frequency chirp in the SR cascade, the magnitude of
which being a function of the separation between the samples. For the dominant
SR cascade we find a significant reduction in the duration and an increase of
the intensity of the SR pulse relative to the case of a single two-atom SR
sample.
",0,1,0,0,0,0
10107,Fine-grained ECG Classification Based on Deep CNN and Online Decision Fusion,"  Early recognition of abnormal rhythm in ECG signals is crucial for monitoring
or diagnosing patients' cardiac conditions and increasing the success rate of
the treatment. Classifying abnormal rhythms into fine-grained categories is
very challenging due to the the broad taxonomy of rhythms, noises and lack of
real-world data and annotations from large number of patients. This paper
presents a new ECG classification method based on Deep Convolutional Neural
Networks (DCNN) and online decision fusion. Different from previous methods
which utilize hand-crafted features or learn features from the original signal
domain, the proposed DCNN based method learns features and classifiers from the
time-frequency domain in an end-to-end manner. First, the ECG wave signal is
transformed to time-frequency domain by using Short-Time Fourier Transform.
Next, specific DCNN models are trained on ECG samples of specific length.
Finally, an online decision fusion method is proposed to fuse past and current
decisions from different models into a more accurate one. Experimental results
on both synthetic and real-world ECG datasets convince the effectiveness and
efficiency of the proposed method.
",1,0,0,1,0,0
17834,Maximum likelihood estimation of determinantal point processes,"  Determinantal point processes (DPPs) have wide-ranging applications in
machine learning, where they are used to enforce the notion of diversity in
subset selection problems. Many estimators have been proposed, but surprisingly
the basic properties of the maximum likelihood estimator (MLE) have received
little attention. The difficulty is that it is a non-concave maximization
problem, and such functions are notoriously difficult to understand in high
dimensions, despite their importance in modern machine learning. Here we study
both the local and global geometry of the expected log-likelihood function. We
prove several rates of convergence for the MLE and give a complete
characterization of the case where these are parametric. We also exhibit a
potential curse of dimensionality where the asymptotic variance of the MLE
scales exponentially with the dimension of the problem. Moreover, we exhibit an
exponential number of saddle points, and give evidence that these may be the
only critical points.
",0,0,1,1,0,0
13507,Bayesian Uncertainty Quantification and Information Fusion in CALPHAD-based Thermodynamic Modeling,"  Calculation of phase diagrams is one of the fundamental tools in alloy
design---more specifically under the framework of Integrated Computational
Materials Engineering. Uncertainty quantification of phase diagrams is the
first step required to provide confidence for decision making in property- or
performance-based design. As a manner of illustration, a thorough probabilistic
assessment of the CALPHAD model parameters is performed against the available
data for a Hf-Si binary case study using a Markov Chain Monte Carlo sampling
approach. The plausible optimum values and uncertainties of the parameters are
thus obtained, which can be propagated to the resulting phase diagram. Using
the parameter values obtained from deterministic optimization in a
computational thermodynamic assessment tool (in this case Thermo-Calc) as the
prior information for the parameter values and ranges in the sampling process
is often necessary to achieve a reasonable cost for uncertainty quantification.
This brings up the problem of finding an appropriate CALPHAD model with
high-level of confidence which is a very hard and costly task that requires
considerable expert skill. A Bayesian hypothesis testing based on Bayes'
factors is proposed to fulfill the need of model selection in this case, which
is applied to compare four recommended models for the Hf-Si system. However, it
is demonstrated that information fusion approaches, i.e., Bayesian model
averaging and an error correlation-based model fusion, can be used to combine
the useful information existing in all the given models rather than just using
the best selected model, which may lack some information about the system being
modelled.
",0,0,0,1,0,0
1891,On a representation of fractional Brownian motion and the limit distributions of statistics arising in cusp statistical models,"  We discuss some extensions of results from the recent paper by Chernoyarov et
al. (Ann. Inst. Stat. Math., October 2016) concerning limit distributions of
Bayesian and maximum likelihood estimators in the model ""signal plus white
noise"" with irregular cusp-type signals. Using a new representation of
fractional Brownian motion (fBm) in terms of cusp functions we show that as the
noise intensity tends to zero, the limit distributions are expressed in terms
of fBm for the full range of asymmetric cusp-type signals correspondingly with
the Hurst parameter H, 0<H<1. Simulation results for the densities and
variances of the limit distributions of Bayesian and maximum likelihood
estimators are also provided.
",0,0,1,1,0,0
14093,Duality of Graphical Models and Tensor Networks,"  In this article we show the duality between tensor networks and undirected
graphical models with discrete variables. We study tensor networks on
hypergraphs, which we call tensor hypernetworks. We show that the tensor
hypernetwork on a hypergraph exactly corresponds to the graphical model given
by the dual hypergraph. We translate various notions under duality. For
example, marginalization in a graphical model is dual to contraction in the
tensor network. Algorithms also translate under duality. We show that belief
propagation corresponds to a known algorithm for tensor network contraction.
This article is a reminder that the research areas of graphical models and
tensor networks can benefit from interaction.
",1,0,1,1,0,0
197,Epidemic Spreading and Aging in Temporal Networks with Memory,"  Time-varying network topologies can deeply influence dynamical processes
mediated by them. Memory effects in the pattern of interactions among
individuals are also known to affect how diffusive and spreading phenomena take
place. In this paper we analyze the combined effect of these two ingredients on
epidemic dynamics on networks. We study the susceptible-infected-susceptible
(SIS) and the susceptible-infected-removed (SIR) models on the recently
introduced activity-driven networks with memory. By means of an activity-based
mean-field approach we derive, in the long time limit, analytical predictions
for the epidemic threshold as a function of the parameters describing the
distribution of activities and the strength of the memory effects. Our results
show that memory reduces the threshold, which is the same for SIS and SIR
dynamics, therefore favouring epidemic spreading. The theoretical approach
perfectly agrees with numerical simulations in the long time asymptotic regime.
Strong aging effects are present in the preasymptotic regime and the epidemic
threshold is deeply affected by the starting time of the epidemics. We discuss
in detail the origin of the model-dependent preasymptotic corrections, whose
understanding could potentially allow for epidemic control on correlated
temporal networks.
",1,0,0,0,1,0
6137,Speaking Style Authentication Using Suprasegmental Hidden Markov Models,"  The importance of speaking style authentication from human speech is gaining
an increasing attention and concern from the engineering community. The
importance comes from the demand to enhance both the naturalness and efficiency
of spoken language human-machine interface. Our work in this research focuses
on proposing, implementing, and testing speaker-dependent and text-dependent
speaking style authentication (verification) systems that accept or reject the
identity claim of a speaking style based on suprasegmental hidden Markov models
(SPHMMs). Based on using SPHMMs, our results show that the average speaking
style authentication performance is: 99%, 37%, 85%, 60%, 61%, 59%, 41%, 61%,
and 57% belonging respectively to the speaking styles: neutral, shouted, slow,
loud, soft, fast, angry, happy, and fearful.
",1,0,0,0,0,0
8879,Objectness Scoring and Detection Proposals in Forward-Looking Sonar Images with Convolutional Neural Networks,"  Forward-looking sonar can capture high resolution images of underwater
scenes, but their interpretation is complex. Generic object detection in such
images has not been solved, specially in cases of small and unknown objects. In
comparison, detection proposal algorithms have produced top performing object
detectors in real-world color images. In this work we develop a Convolutional
Neural Network that can reliably score objectness of image windows in
forward-looking sonar images and by thresholding objectness, we generate
detection proposals. In our dataset of marine garbage objects, we obtain 94%
recall, generating around 60 proposals per image. The biggest strength of our
method is that it can generalize to previously unseen objects. We show this by
detecting chain links, walls and a wrench without previous training in such
objects. We strongly believe our method can be used for class-independent
object detection, with many real-world applications such as chain following and
mine detection.
",1,0,0,0,0,0
8412,A Discontinuity Adjustment for Subdistribution Function Confidence Bands Applied to Right-Censored Competing Risks Data,"  The wild bootstrap is the resampling method of choice in survival analytic
applications. Theoretic justifications rely on the assumption of existing
intensity functions which is equivalent to an exclusion of ties among the event
times. However, such ties are omnipresent in practical studies. It turns out
that the wild bootstrap should only be applied in a modified manner that
corrects for altered limit variances and emerging dependencies. This again
ensures the asymptotic exactness of inferential procedures. An analogous
necessity is the use of the Greenwood-type variance estimator for Nelson-Aalen
estimators which is particularly preferred in tied data regimes. All theoretic
arguments are transferred to bootstrapping Aalen-Johansen estimators for
cumulative incidence functions in competing risks. An extensive simulation
study as well as an application to real competing risks data of male intensive
care unit patients suffering from pneumonia illustrate the practicability of
the proposed technique.
",0,0,1,1,0,0
8693,Randomization-based Inference for Bernoulli-Trial Experiments and Implications for Observational Studies,"  We present a randomization-based inferential framework for experiments
characterized by a strongly ignorable assignment mechanism where units have
independent probabilities of receiving treatment. Previous works on
randomization tests often assume these probabilities are equal within blocks of
units. We consider the general case where they differ across units and show how
to perform randomization tests and obtain point estimates and confidence
intervals. Furthermore, we develop a rejection-sampling algorithm to conduct
randomization-based inference conditional on ancillary statistics, covariate
balance, or other statistics of interest. Through simulation we demonstrate how
our algorithm can yield powerful randomization tests and thus precise
inference. Our work also has implications for observational studies, which
commonly assume a strongly ignorable assignment mechanism. Most methodologies
for observational studies make additional modeling or asymptotic assumptions,
while our framework only assumes the strongly ignorable assignment mechanism,
and thus can be considered a minimal-assumption approach.
",0,0,0,1,0,0
4980,BinPro: A Tool for Binary Source Code Provenance,"  Enforcing open source licenses such as the GNU General Public License (GPL),
analyzing a binary for possible vulnerabilities, and code maintenance are all
situations where it is useful to be able to determine the source code
provenance of a binary. While previous work has either focused on computing
binary-to-binary similarity or source-to-source similarity, BinPro is the first
work we are aware of to tackle the problem of source-to-binary similarity.
BinPro can match binaries with their source code even without knowing which
compiler was used to produce the binary, or what optimization level was used
with the compiler. To do this, BinPro utilizes machine learning to compute
optimal code features for determining binary-to-source similarity and a static
analysis pipeline to extract and compute similarity based on those features.
Our experiments show that on average BinPro computes a similarity of 81% for
matching binaries and source code of the same applications, and an average
similarity of 25% for binaries and source code of similar but different
applications. This shows that BinPro's similarity score is useful for
determining if a binary was derived from a particular source code.
",1,0,0,0,0,0
7144,Physics-Informed Regularization of Deep Neural Networks,"  This paper presents a novel physics-informed regularization method for
training of deep neural networks (DNNs). In particular, we focus on the DNN
representation for the response of a physical or biological system, for which a
set of governing laws are known. These laws often appear in the form of
differential equations, derived from first principles, empirically-validated
laws, and/or domain expertise. We propose a DNN training approach that utilizes
these known differential equations in addition to the measurement data, by
introducing a penalty term to the training loss function to penalize divergence
form the governing laws. Through three numerical examples, we will show that
the proposed regularization produces surrogates that are physically
interpretable with smaller generalization errors, when compared to other common
regularization methods.
",1,0,0,0,0,0
6909,Comparing anticyclotomic Selmer groups of positive coranks for congruent modular forms,"  We study the variation of Iwasawa invariants of the anticyclotomic Selmer
groups of congruent modular forms under the Heegner hypothesis. In particular,
we show that even if the Selmer groups we study may have positive coranks, the
mu-invariant vanishes for one modular form if and only if it vanishes for the
other, and that their lambda-invariants are related by an explicit formula.
This generalizes results of Greenberg-Vatsal for the cyclotomic extension, as
well as results of Pollack-Weston and Castella-Kim-Longo for the anticyclotomic
extension when the Selmer groups in question are cotorsion.
",0,0,1,0,0,0
11384,Bulk diffusion in a kinetically constrained lattice gas,"  In the hydrodynamic regime, the evolution of a stochastic lattice gas with
symmetric hopping rules is described by a diffusion equation with
density-dependent diffusion coefficient encapsulating all microscopic details
of the dynamics. This diffusion coefficient is, in principle, determined by a
Green-Kubo formula. In practice, even when the equilibrium properties of a
lattice gas are analytically known, the diffusion coefficient cannot be
computed except when a lattice gas additionally satisfies the gradient
condition. We develop a procedure to systematically obtain analytical
approximations for the diffusion coefficient for non-gradient lattice gases
with known equilibrium. The method relies on a variational formula found by
Varadhan and Spohn which is a version of the Green-Kubo formula particularly
suitable for diffusive lattice gases. Restricting the variational formula to
finite-dimensional sub-spaces allows one to perform the minimization and gives
upper bounds for the diffusion coefficient. We apply this approach to a
kinetically constrained non-gradient lattice gas, viz. to the Kob-Andersen
model on the square lattice.
",0,1,0,0,0,0
13336,On the computability of graph Turing machines,"  We consider graph Turing machines, a model of parallel computation on a
graph, in which each vertex is only capable of performing one of a finite
number of operations. This model of computation is a natural generalization of
several well-studied notions of computation, including ordinary Turing
machines, cellular automata, and parallel graph dynamical systems. We analyze
the power of computations that can take place in this model, both in terms of
the degrees of computability of the functions that can be computed, and the
time and space resources needed to carry out these computations. We further
show that properties of the underlying graph have significant consequences for
the power of computation thereby obtained. In particular, we show that every
arithmetically definable set can be computed by a graph Turing machine in
constant time, and that every computably enumerable Turing degree can be
computed in constant time and linear space by a graph Turing machine whose
underlying graph has finite degree.
",1,0,1,0,0,0
13371,Stochastic Feedback Control of Systems with Unknown Nonlinear Dynamics,"  This paper studies the stochastic optimal control problem for systems with
unknown dynamics. First, an open-loop deterministic trajectory optimization
problem is solved without knowing the explicit form of the dynamical system.
Next, a Linear Quadratic Gaussian (LQG) controller is designed for the nominal
trajectory-dependent linearized system, such that under a small noise
assumption, the actual states remain close to the optimal trajectory. The
trajectory-dependent linearized system is identified using input-output
experimental data consisting of the impulse responses of the nominal system. A
computational example is given to illustrate the performance of the proposed
approach.
",1,0,0,0,0,0
831,Optimal Timing of Decisions: A General Theory Based on Continuation Values,"  Building on insights of Jovanovic (1982) and subsequent authors, we develop a
comprehensive theory of optimal timing of decisions based around continuation
value functions and operators that act on them. Optimality results are provided
under general settings, with bounded or unbounded reward functions. This
approach has several intrinsic advantages that we exploit in developing the
theory. One is that continuation value functions are smoother than value
functions, allowing for sharper analysis of optimal policies and more efficient
computation. Another is that, for a range of problems, the continuation value
function exists in a lower dimensional space than the value function,
mitigating the curse of dimensionality. In one typical experiment, this reduces
the computation time from over a week to less than three minutes.
",0,0,1,0,0,0
214,Sparse mean localization by information theory,"  Sparse feature selection is necessary when we fit statistical models, we have
access to a large group of features, don't know which are relevant, but assume
that most are not. Alternatively, when the number of features is larger than
the available data the model becomes over parametrized and the sparse feature
selection task involves selecting the most informative variables for the model.
When the model is a simple location model and the number of relevant features
does not grow with the total number of features, sparse feature selection
corresponds to sparse mean estimation. We deal with a simplified mean
estimation problem consisting of an additive model with gaussian noise and mean
that is in a restricted, finite hypothesis space. This restriction simplifies
the mean estimation problem into a selection problem of combinatorial nature.
Although the hypothesis space is finite, its size is exponential in the
dimension of the mean. In limited data settings and when the size of the
hypothesis space depends on the amount of data or on the dimension of the data,
choosing an approximation set of hypotheses is a desirable approach. Choosing a
set of hypotheses instead of a single one implies replacing the bias-variance
trade off with a resolution-stability trade off. Generalization capacity
provides a resolution selection criterion based on allowing the learning
algorithm to communicate the largest amount of information in the data to the
learner without error. In this work the theory of approximation set coding and
generalization capacity is explored in order to understand this approach. We
then apply the generalization capacity criterion to the simplified sparse mean
estimation problem and detail an importance sampling algorithm which at once
solves the difficulty posed by large hypothesis spaces and the slow convergence
of uniform sampling algorithms.
",0,0,0,1,0,0
18096,Twin Learning for Similarity and Clustering: A Unified Kernel Approach,"  Many similarity-based clustering methods work in two separate steps including
similarity matrix computation and subsequent spectral clustering. However,
similarity measurement is challenging because it is usually impacted by many
factors, e.g., the choice of similarity metric, neighborhood size, scale of
data, noise and outliers. Thus the learned similarity matrix is often not
suitable, let alone optimal, for the subsequent clustering. In addition,
nonlinear similarity often exists in many real world data which, however, has
not been effectively considered by most existing methods. To tackle these two
challenges, we propose a model to simultaneously learn cluster indicator matrix
and similarity information in kernel spaces in a principled way. We show
theoretical relationships to kernel k-means, k-means, and spectral clustering
methods. Then, to address the practical issue of how to select the most
suitable kernel for a particular clustering task, we further extend our model
with a multiple kernel learning ability. With this joint model, we can
automatically accomplish three subtasks of finding the best cluster indicator
matrix, the most accurate similarity relations and the optimal combination of
multiple kernels. By leveraging the interactions between these three subtasks
in a joint framework, each subtask can be iteratively boosted by using the
results of the others towards an overall optimal solution. Extensive
experiments are performed to demonstrate the effectiveness of our method.
",1,0,0,1,0,0
6152,Real elliptic curves and cevian geometry,"  We study the elliptic curve $E_a: (ax+1)y^2+(ax+1)(x-1)y+x^2-x=0$, which we
call the geometric normal form of an elliptic curve. We show that any elliptic
curve whose $j$-invariant is real is isomorphic to a curve $E_a$ in geometric
normal form, and show that for $a \notin \{0, -1, -9\}$, the points on $E_a$,
minus a set of $6$ points, can be characterized in terms of the cevian geometry
of a triangle.
",0,0,1,0,0,0
18816,Localized-endemic state transition in the susceptible-infected-susceptible model on networks,"  It is a longstanding debate concerning the absence of threshold for the
susceptible-infected-susceptible spreading model on networks with localized
state. The key to resolve this controversy is the dynamical interaction
pattern, which has not been uncovered. Here we show that the interaction
driving the localized-endemic state transition is not the global interaction
between a node and all the other nodes on the network, but exists at the level
of super node composed of highly connected node and its neighbors. The internal
interactions within a super node induce localized state with limited lifetime,
while the interactions between neighboring super nodes via a path of two hops
enable them to avoid trapping in the absorbing state, marking the onset of
endemic state. The hybrid interactions render highly connected nodes
exponentially increasing infection density, which truly account for the null
threshold. These results are crucial for correctly understanding diverse
recurrent contagion phenomena
",0,1,0,0,0,0
10453,On the Inverse of Forward Adjacency Matrix,"  During routine state space circuit analysis of an arbitrarily connected set
of nodes representing a lossless LC network, a matrix was formed that was
observed to implicitly capture connectivity of the nodes in a graph similar to
the conventional incidence matrix, but in a slightly different manner. This
matrix has only 0, 1 or -1 as its elements. A sense of direction (of the graph
formed by the nodes) is inherently encoded in the matrix because of the
presence of -1. It differs from the incidence matrix because of leaving out the
datum node from the matrix. Calling this matrix as forward adjacency matrix, it
was found that its inverse also displays useful and interesting physical
properties when a specific style of node-indexing is adopted for the nodes in
the graph. The graph considered is connected but does not have any closed
loop/cycle (corresponding to closed loop of inductors in a circuit) as with its
presence the matrix is not invertible. Incidentally, by definition the graph
being considered is a tree. The properties of the forward adjacency matrix and
its inverse, along with rigorous proof, are presented.
",1,0,0,0,0,0
4650,Nauticle: a general-purpose particle-based simulation tool,"  Nauticle is a general-purpose simulation tool for the flexible and highly
configurable application of particle-based methods of either discrete or
continuum phenomena. It is presented that Nauticle has three distinct layers
for users and developers, then the top two layers are discussed in detail. The
paper introduces the Symbolic Form Language (SFL) of Nauticle, which
facilitates the formulation of user-defined numerical models at the top level
in text-based configuration files and provides simple application examples of
use. On the other hand, at the intermediate level, it is shown that the SFL can
be intuitively extended with new particle methods without tedious recoding or
even the knowledge of the bottom level. Finally, the efficiency of the code is
also tested through a performance benchmark.
",1,1,0,0,0,0
14205,"Efficient Propagation of Uncertainties in Manufacturing Supply Chains: Time Buckets, L-leap and Multilevel Monte Carlo","  Uncertainty propagation of large scale discrete supply chains can be
prohibitive when a large number of events occur during the simulated period and
discrete event simulations (DES) are costly. We present a time bucket method to
approximate and accelerate the DES of supply chains. Its stochastic version,
which we call the L(logistic)-leap method, can be viewed as an extension of the
leap methods, e.g., tau-leap, D-leap, developed in the chemical engineering
community for the acceleration of stochastic DES of chemical reactions. The
L-leap method instantaneously updates the system state vector at discrete time
points and the production rates and policies of a supply chain are assumed to
be stationary during each time bucket. We propose to use Multilevel Monte Carlo
(MLMC) to efficiently propagate the uncertainties in a supply chain network,
where the levels are naturally defined by the sizes of the time buckets of the
simulations. We demonstrate the efficiency and accuracy of our methods using
four numerical examples derived from a real world manufacturing material flow.
In these examples, our multilevel L-leap approach can be faster than the
standard Monte Carlo (MC) method by one or two orders of magnitudes without
compromising the accuracy.
",0,0,0,1,0,0
10037,Rank modulation codes for DNA storage,"  Synthesis of DNA molecules offers unprecedented advances in storage
technology. Yet, the microscopic world in which these molecules reside induces
error patterns that are fundamentally different from their digital
counterparts. Hence, to maintain reliability in reading and writing, new coding
schemes must be developed. In a reading technique called shotgun sequencing, a
long DNA string is read in a sliding window fashion, and a profile vector is
produced. It was recently suggested by Kiah et al. that such a vector can
represent the permutation which is induced by its entries, and hence a
rank-modulation scheme arises. Although this interpretation suggests high error
tolerance, it is unclear which permutations are feasible, and how to produce a
DNA string whose profile vector induces a given permutation. In this paper, by
observing some necessary conditions, an upper bound for the number of feasible
permutations is given. Further, a technique for deciding the feasibility of a
permutation is devised. By using insights from this technique, an algorithm for
producing a considerable number of feasible permutations is given, which
applies to any alphabet size and any window length.
",1,0,0,0,0,0
3627,"Diamond-colored distributive lattices, move-minimizing games, and fundamental Weyl symmetric functions: The type $\mathsf{A}$ case","  We present some elementary but foundational results concerning
diamond-colored modular and distributive lattices and connect these structures
to certain one-player combinatorial ""move-minimizing games,"" in particular, a
so-called ""domino game."" The objective of this game is to find, if possible,
the least number of ""domino moves"" to get from one partition to another, where
a domino move is, with one exception, the addition or removal of a
domino-shaped pair of tiles. We solve this domino game by demonstrating the
somewhat surprising fact that the associated ""game graphs"" coincide with a
well-known family of diamond-colored distributive lattices which shall be
referred to as the ""type $\mathsf{A}$ fundamental lattices."" These lattices
arise as supporting graphs for the fundamental representations of the special
linear Lie algebras and as splitting posets for type $\mathsf{A}$ fundamental
symmetric functions, connections which are further explored in sequel papers
for types $\mathsf{A}$, $\mathsf{C}$, and $\mathsf{B}$. In this paper, this
connection affords a solution to the proposed domino game as well as new
descriptions of the type $\mathsf{A}$ fundamental lattices.
",0,0,1,0,0,0
16470,Modeling Perceptual Aliasing in SLAM via Discrete-Continuous Graphical Models,"  Perceptual aliasing is one of the main causes of failure for Simultaneous
Localization and Mapping (SLAM) systems operating in the wild. Perceptual
aliasing is the phenomenon where different places generate a similar visual
(or, in general, perceptual) footprint. This causes spurious measurements to be
fed to the SLAM estimator, which typically results in incorrect localization
and mapping results. The problem is exacerbated by the fact that those outliers
are highly correlated, in the sense that perceptual aliasing creates a large
number of mutually-consistent outliers. Another issue stems from the fact that
most state-of-the-art techniques rely on a given trajectory guess (e.g., from
odometry) to discern between inliers and outliers and this makes the resulting
pipeline brittle, since the accumulation of error may result in incorrect
choices and recovery from failures is far from trivial. This work provides a
unified framework to model perceptual aliasing in SLAM and provides practical
algorithms that can cope with outliers without relying on any initial guess. We
present two main contributions. The first is a Discrete-Continuous Graphical
Model (DC-GM) for SLAM: the continuous portion of the DC-GM captures the
standard SLAM problem, while the discrete portion describes the selection of
the outliers and models their correlation. The second contribution is a
semidefinite relaxation to perform inference in the DC-GM that returns
estimates with provable sub-optimality guarantees. Experimental results on
standard benchmarking datasets show that the proposed technique compares
favorably with state-of-the-art methods while not relying on an initial guess
for optimization.
",1,0,0,0,0,0
12783,Dynamics of quantum information in many-body localized systems,"  We characterize the information dynamics of strongly disordered systems using
a combination of analytics, exact diagonalization, and matrix product operator
simulations. More specifically, we study the spreading of quantum information
in three different scenarios: thermalizing, Anderson localized, and many-body
localized. We qualitatively distinguish these cases by quantifying the amount
of remnant information in a local region. The nature of the dynamics is further
explored by computing the propagation of mutual information with respect to
varying partitions. Finally, we demonstrate that classical simulability, as
captured by the magnitude of MPO truncation errors, exhibits enhanced
fluctuations near the localization transition, suggesting the possibility of
its use as a diagnostic of the critical point.
",0,1,0,0,0,0
1311,"Gender Disparities in Science? Dropout, Productivity, Collaborations and Success of Male and Female Computer Scientists","  Scientific collaborations shape ideas as well as innovations and are both the
substrate for, and the outcome of, academic careers. Recent studies show that
gender inequality is still present in many scientific practices ranging from
hiring to peer-review processes and grant applications. In this work, we
investigate gender-specific differences in collaboration patterns of more than
one million computer scientists over the course of 47 years. We explore how
these patterns change over years and career ages and how they impact scientific
success. Our results highlight that successful male and female scientists
reveal the same collaboration patterns: compared to scientists in the same
career age, they tend to collaborate with more colleagues than other
scientists, seek innovations as brokers and establish longer-lasting and more
repetitive collaborations. However, women are on average less likely to adapt
the collaboration patterns that are related with success, more likely to embed
into ego networks devoid of structural holes, and they exhibit stronger gender
homophily as well as a consistently higher dropout rate than men in all career
ages.
",1,1,0,0,0,0
14020,"(p,q)-webs of DIM representations, 5d N=1 instanton partition functions and qq-characters","  Instanton partition functions of $\mathcal{N}=1$ 5d Super Yang-Mills reduced
on $S^1$ can be engineered in type IIB string theory from the $(p,q)$-branes
web diagram. To this diagram is superimposed a web of representations of the
Ding-Iohara-Miki (DIM) algebra that acts on the partition function. In this
correspondence, each segment is associated to a representation, and the
(topological string) vertex is identified with the intertwiner operator
constructed by Awata, Feigin and Shiraishi. We define a new intertwiner acting
on the representation spaces of levels $(1,n)\otimes(0,m)\to(1,n+m)$, thereby
generalizing to higher rank $m$ the original construction. It allows us to use
a folded version of the usual $(p,q)$-web diagram, bringing great
simplifications to actual computations. As a result, the characterization of
Gaiotto states and vertical intertwiners, previously obtained by some of the
authors, is uplifted to operator relations acting in the Fock space of
horizontal representations. We further develop a method to build qq-characters
of linear quivers based on the horizontal action of DIM elements. While
fundamental qq-characters can be built using the coproduct, higher ones require
the introduction of a (quantum) Weyl reflection acting on tensor products of
DIM generators.
",0,0,1,0,0,0
12687,Communication-Avoiding Optimization Methods for Distributed Massive-Scale Sparse Inverse Covariance Estimation,"  Across a variety of scientific disciplines, sparse inverse covariance
estimation is a popular tool for capturing the underlying dependency
relationships in multivariate data. Unfortunately, most estimators are not
scalable enough to handle the sizes of modern high-dimensional data sets (often
on the order of terabytes), and assume Gaussian samples. To address these
deficiencies, we introduce HP-CONCORD, a highly scalable optimization method
for estimating a sparse inverse covariance matrix based on a regularized
pseudolikelihood framework, without assuming Gaussianity. Our parallel proximal
gradient method uses a novel communication-avoiding linear algebra algorithm
and runs across a multi-node cluster with up to 1k nodes (24k cores), achieving
parallel scalability on problems with up to ~819 billion parameters (1.28
million dimensions); even on a single node, HP-CONCORD demonstrates
scalability, outperforming a state-of-the-art method. We also use HP-CONCORD to
estimate the underlying dependency structure of the brain from fMRI data, and
use the result to identify functional regions automatically. The results show
good agreement with a clustering from the neuroscience literature.
",1,0,0,1,0,0
14394,Fracton topological order via coupled layers,"  In this work, we develop a coupled layer construction of fracton topological
orders in $d=3$ spatial dimensions. These topological phases have sub-extensive
topological ground-state degeneracy and possess excitations whose movement is
restricted in interesting ways. Our coupled layer approach is used to construct
several different fracton topological phases, both from stacked layers of
simple $d=2$ topological phases and from stacks of $d=3$ fracton topological
phases. This perspective allows us to shed light on the physics of the X-cube
model recently introduced by Vijay, Haah, and Fu, which we demonstrate can be
obtained as the strong-coupling limit of a coupled three-dimensional stack of
toric codes. We also construct two new models of fracton topological order: a
semionic generalization of the X-cube model, and a model obtained by coupling
together four interpenetrating X-cube models, which we dub the ""Four Color Cube
model."" The couplings considered lead to fracton topological orders via
mechanisms we dub ""p-string condensation"" and ""p-membrane condensation,"" in
which strings or membranes built from particle excitations are driven to
condense. This allows the fusion properties, braiding statistics, and
ground-state degeneracy of the phases we construct to be easily studied in
terms of more familiar degrees of freedom. Our work raises the possibility of
studying fracton topological phases from within the framework of topological
quantum field theory, which may be useful for obtaining a more complete
understanding of such phases.
",0,1,0,0,0,0
13644,On addition theorems related to elliptic integrals,"  This paper provides some explicit formulas related to addition theorems for
elliptic integrals $\int_0^x dt/R(t)$, where $R(t)$ is the square root from a
polynomial of degree 4. These integrals are related to complex elliptic genera
and are motivated by Euler's addition theorem for elliptic integrals of the
first kind.
",0,0,1,0,0,0
12103,Extremal copositive matrices with minimal zero supports of cardinality two,"  Let $A \in {\cal C}^n$ be an extremal copositive matrix with unit diagonal.
Then the minimal zeros of $A$ all have supports of cardinality two if and only
if the elements of $A$ are all from the set $\{-1,0,1\}$. Thus the extremal
copositive matrices with minimal zero supports of cardinality two are exactly
those matrices which can be obtained by diagonal scaling from the extremal
$\{-1,0,1\}$ unit diagonal matrices characterized by Hoffman and Pereira in
1973.
",0,0,1,0,0,0
14393,Emergence of Invariance and Disentanglement in Deep Representations,"  Using established principles from Statistics and Information Theory, we show
that invariance to nuisance factors in a deep neural network is equivalent to
information minimality of the learned representation, and that stacking layers
and injecting noise during training naturally bias the network towards learning
invariant representations. We then decompose the cross-entropy loss used during
training and highlight the presence of an inherent overfitting term. We propose
regularizing the loss by bounding such a term in two equivalent ways: One with
a Kullbach-Leibler term, which relates to a PAC-Bayes perspective; the other
using the information in the weights as a measure of complexity of a learned
model, yielding a novel Information Bottleneck for the weights. Finally, we
show that invariance and independence of the components of the representation
learned by the network are bounded above and below by the information in the
weights, and therefore are implicitly optimized during training. The theory
enables us to quantify and predict sharp phase transitions between underfitting
and overfitting of random labels when using our regularized loss, which we
verify in experiments, and sheds light on the relation between the geometry of
the loss function, invariance properties of the learned representation, and
generalization error.
",1,0,0,1,0,0
12054,Zonal Flow Magnetic Field Interaction in the Semi-Conducting Region of Giant Planets,"  All four giant planets in the Solar System feature zonal flows on the order
of 100 m/s in the cloud deck, and large-scale intrinsic magnetic fields on the
order of 1 Gauss near the surface. The vertical structure of the zonal flows
remains obscure. The end-member scenarios are shallow flows confined in the
radiative atmosphere and deep flows throughout the entire planet. The
electrical conductivity increases rapidly yet smoothly as a function of depth
inside Jupiter and Saturn. Deep zonal flows will inevitably interact with the
magnetic field, at depth with even modest electrical conductivity. Here we
investigate the interaction between zonal flows and magnetic fields in the
semi-conducting region of giant planets. Employing mean-field electrodynamics,
we show that the interaction will generate detectable poloidal magnetic field
perturbations spatially correlated with the deep zonal flows. Assuming the peak
amplitude of the dynamo alpha-effect to be 0.1 mm/s, deep zonal flows on the
order of 0.1 - 1 m/s in the semi-conducting region of Jupiter and Saturn would
generate poloidal magnetic perturbations on the order of 0.01% - 1% of the
background dipole field. These poloidal perturbations should be detectable with
the in-situ magnetic field measurements from the Juno mission and the Cassini
Grand Finale. This implies that magnetic field measurements can be employed to
constrain the properties of deep zonal flows in the semi-conducting region of
giant planets.
",0,1,0,0,0,0
6586,An Approximate Bayesian Long Short-Term Memory Algorithm for Outlier Detection,"  Long Short-Term Memory networks trained with gradient descent and
back-propagation have received great success in various applications. However,
point estimation of the weights of the networks is prone to over-fitting
problems and lacks important uncertainty information associated with the
estimation. However, exact Bayesian neural network methods are intractable and
non-applicable for real-world applications. In this study, we propose an
approximate estimation of the weights uncertainty using Ensemble Kalman Filter,
which is easily scalable to a large number of weights. Furthermore, we optimize
the covariance of the noise distribution in the ensemble update step using
maximum likelihood estimation. To assess the proposed algorithm, we apply it to
outlier detection in five real-world events retrieved from the Twitter
platform.
",1,0,0,1,0,0
2597,Multi-armed Bandit Problems with Strategic Arms,"  We study a strategic version of the multi-armed bandit problem, where each
arm is an individual strategic agent and we, the principal, pull one arm each
round. When pulled, the arm receives some private reward $v_a$ and can choose
an amount $x_a$ to pass on to the principal (keeping $v_a-x_a$ for itself). All
non-pulled arms get reward $0$. Each strategic arm tries to maximize its own
utility over the course of $T$ rounds. Our goal is to design an algorithm for
the principal incentivizing these arms to pass on as much of their private
rewards as possible.
When private rewards are stochastically drawn each round ($v_a^t \leftarrow
D_a$), we show that:
- Algorithms that perform well in the classic adversarial multi-armed bandit
setting necessarily perform poorly: For all algorithms that guarantee low
regret in an adversarial setting, there exist distributions $D_1,\ldots,D_k$
and an approximate Nash equilibrium for the arms where the principal receives
reward $o(T)$.
- Still, there exists an algorithm for the principal that induces a game
among the arms where each arm has a dominant strategy. When each arm plays its
dominant strategy, the principal sees expected reward $\mu'T - o(T)$, where
$\mu'$ is the second-largest of the means $\mathbb{E}[D_{a}]$. This algorithm
maintains its guarantee if the arms are non-strategic ($x_a = v_a$), and also
if there is a mix of strategic and non-strategic arms.
",1,0,0,1,0,0
1374,A supervised approach to time scale detection in dynamic networks,"  For any stream of time-stamped edges that form a dynamic network, an
important choice is the aggregation granularity that an analyst uses to bin the
data. Picking such a windowing of the data is often done by hand, or left up to
the technology that is collecting the data. However, the choice can make a big
difference in the properties of the dynamic network. This is the time scale
detection problem. In previous work, this problem is often solved with a
heuristic as an unsupervised task. As an unsupervised problem, it is difficult
to measure how well a given algorithm performs. In addition, we show that the
quality of the windowing is dependent on which task an analyst wants to perform
on the network after windowing. Therefore the time scale detection problem
should not be handled independently from the rest of the analysis of the
network.
We introduce a framework that tackles both of these issues: By measuring the
performance of the time scale detection algorithm based on how well a given
task is accomplished on the resulting network, we are for the first time able
to directly compare different time scale detection algorithms to each other.
Using this framework, we introduce time scale detection algorithms that take a
supervised approach: they leverage ground truth on training data to find a good
windowing of the test data. We compare the supervised approach to previous
approaches and several baselines on real data.
",1,0,0,0,0,0
7873,Delta sets for symmetric numerical semigroups with embedding dimension three,"  This work extends the results known for the Delta sets of non-symmetric
numerical semigroups with embedding dimension three to the symmetric case.
Thus, we have a fast algorithm to compute the Delta set of any embedding
dimension three numerical semigroup. Also, as a consequence of these resutls,
the sets that can be realized as Delta sets of numerical semigroups of
embedding dimension three are fully characterized.
",0,0,1,0,0,0
2058,RelNN: A Deep Neural Model for Relational Learning,"  Statistical relational AI (StarAI) aims at reasoning and learning in noisy
domains described in terms of objects and relationships by combining
probability with first-order logic. With huge advances in deep learning in the
current years, combining deep networks with first-order logic has been the
focus of several recent studies. Many of the existing attempts, however, only
focus on relations and ignore object properties. The attempts that do consider
object properties are limited in terms of modelling power or scalability. In
this paper, we develop relational neural networks (RelNNs) by adding hidden
layers to relational logistic regression (the relational counterpart of
logistic regression). We learn latent properties for objects both directly and
through general rules. Back-propagation is used for training these models. A
modular, layer-wise architecture facilitates utilizing the techniques developed
within deep learning community to our architecture. Initial experiments on
eight tasks over three real-world datasets show that RelNNs are promising
models for relational learning.
",1,0,0,1,0,0
20560,BPGrad: Towards Global Optimality in Deep Learning via Branch and Pruning,"  Understanding the global optimality in deep learning (DL) has been attracting
more and more attention recently. Conventional DL solvers, however, have not
been developed intentionally to seek for such global optimality. In this paper
we propose a novel approximation algorithm, BPGrad, towards optimizing deep
models globally via branch and pruning. Our BPGrad algorithm is based on the
assumption of Lipschitz continuity in DL, and as a result it can adaptively
determine the step size for current gradient given the history of previous
updates, wherein theoretically no smaller steps can achieve the global
optimality. We prove that, by repeating such branch-and-pruning procedure, we
can locate the global optimality within finite iterations. Empirically an
efficient solver based on BPGrad for DL is proposed as well, and it outperforms
conventional DL solvers such as Adagrad, Adadelta, RMSProp, and Adam in the
tasks of object recognition, detection, and segmentation.
",1,0,0,1,0,0
13660,Topological semimetal state and field-induced Fermi surface reconstruction in antiferromagnetic monopnictide NdSb,"  We report the experimental realization of Dirac semimetal state in NdSb, a
material with antiferromagnetic ground state. The occurrence of topological
semimetal state has been well supported by our band structure calculations and
the experimental observation of chiral anomaly induced negative
magnetoresistance. A field-induced Fermi surface reconstruction is observed, in
response to the change of spin polarization. The observation of topological
semimetal state in a magnetic material provides an opportunity to investigate
the magneto-topological phenomena.
",0,1,0,0,0,0
2429,One- and two-channel Kondo model with logarithmic Van Hove singularity: a numerical renormalization group solution,"  Simple scaling consideration and NRG solution of the one- and two-channel
Kondo model in the presence of a logarithmic Van Hove singularity at the Fermi
level is given. The temperature dependences of local and impurity magnetic
susceptibility and impurity entropy are calculated. The low-temperature
behavior of the impurity susceptibility and impurity entropy turns out to be
non-universal in the Kondo sense and independent of the $s-d$ coupling $J$. The
resonant level model solution in the strong coupling regime confirms the NRG
results. In the two-channel case the local susceptibility demonstrates a
non-Fermi-liquid power-law behavior.
",0,1,0,0,0,0
6269,Spatial localization for nonlinear dynamical stochastic models for excitable media,"  Nonlinear dynamical stochastic models are ubiquitous in different areas.
Excitable media models are typical examples with large state dimensions. Their
statistical properties are often of great interest but are also very
challenging to compute. In this article, a theoretical framework to understand
the spatial localization for a large class of stochastically coupled nonlinear
systems in high dimensions is developed. Rigorous mathematical theories show
the covariance decay behavior due to both local and nonlocal effects, which
result from the diffusion and the mean field interaction, respectively. The
analysis is based on a comparison with an appropriate linear surrogate model,
of which the covariance propagation can be computed explicitly. Two important
applications of these theoretical results are discussed. They are the spatial
averaging strategy for efficiently sampling the covariance matrix and the
localization technique in data assimilation. Test examples of a surrogate
linear model and a stochastically coupled FitzHugh-Nagumo model for excitable
media are adopted to validate the theoretical results. The latter is also used
for a systematical study of the spatial averaging strategy in efficiently
sampling the covariance matrix in different dynamical regimes.
",0,0,1,1,0,0
6432,Code-division multiplexed resistive pulse sensor networks for spatio-temporal detection of particles in microfluidic devices,"  Spatial separation of suspended particles based on contrast in their physical
or chemical properties forms the basis of various biological assays performed
on lab-on-achip devices. To electronically acquire this information, we have
recently introduced a microfluidic sensing platform, called Microfluidic CODES,
which combines the resistive pulse sensing with the code division multiple
access in multiplexing a network of integrated electrical sensors. In this
paper, we enhance the multiplexing capacity of the Microfluidic CODES by
employing sensors that generate non-orthogonal code waveforms and a new
decoding algorithm that combines machine learning techniques with minimum
mean-squared error estimation. As a proof of principle, we fabricated a
microfluidic device with a network of 10 code-multiplexed sensors and
characterized it using cells suspended in phosphate buffer saline solution.
",0,0,0,1,0,0
17177,"Size, Shape, and Phase Control in Ultrathin CdSe Nanosheets","  Ultrathin two-dimensional nanosheets raise a rapidly increasing interest due
to their unique dimensionality-dependent properties. Most of the
two-dimensional materials are obtained by exfoliation of layered bulk materials
or are grown on substrates by vapor deposition methods. To produce
free-standing nanosheets, solution-based colloidal methods are emerging as
promising routes. In this work, we demonstrate ultrathin CdSe nanosheets with
controllable size, shape and phase. The key of our approach is the use of
halogenated alkanes as additives in a hot-injection synthesis. Increasing
concentrations of bromoalkanes can tune the shape from sexangular to
quadrangular to triangular and the phase from zinc blende to wurtzite. Geometry
and crystal structure evolution of the nanosheets take place in the presence of
halide ions, acting as cadmium complexing agents and as surface X-type ligands,
according to mass spectrometry and X-ray photoelectron spectroscopies. Our
experimental findings show that the degree of these changes depends on the
molecular structure of the halogen alkanes and the type of halogen atom.
",0,1,0,0,0,0
2048,Noise Statistics Oblivious GARD For Robust Regression With Sparse Outliers,"  Linear regression models contaminated by Gaussian noise (inlier) and possibly
unbounded sparse outliers are common in many signal processing applications.
Sparse recovery inspired robust regression (SRIRR) techniques are shown to
deliver high quality estimation performance in such regression models.
Unfortunately, most SRIRR techniques assume \textit{a priori} knowledge of
noise statistics like inlier noise variance or outlier statistics like number
of outliers. Both inlier and outlier noise statistics are rarely known
\textit{a priori} and this limits the efficient operation of many SRIRR
algorithms. This article proposes a novel noise statistics oblivious algorithm
called residual ratio thresholding GARD (RRT-GARD) for robust regression in the
presence of sparse outliers. RRT-GARD is developed by modifying the recently
proposed noise statistics dependent greedy algorithm for robust de-noising
(GARD). Both finite sample and asymptotic analytical results indicate that
RRT-GARD performs nearly similar to GARD with \textit{a priori} knowledge of
noise statistics. Numerical simulations in real and synthetic data sets also
point to the highly competitive performance of RRT-GARD.
",0,0,0,1,0,0
4632,Coupling between a charge density wave and magnetism in an Heusler material,"  The Prototypical magnetic memory shape alloy Ni$_2$MnGa undergoes various
phase transitions as a function of temperature, pressure, and doping. In the
low-temperature phases below 260 K, an incommensurate structural modulation
occurs along the [110] direction which is thought to arise from softening of a
phonon mode. It is not at present clear how this phenomenon is related, if at
all, to the magnetic memory effect. Here we report time-resolved measurements
which track both the structural and magnetic components of the phase transition
from the modulated cubic phase as it is brought into the high-symmetry phase.
The results suggest that the photoinduced demagnetization modifies the Fermi
surface in regions that couple strongly to the periodicity of the structural
modulation through the nesting vector. The amplitude of the periodic lattice
distortion, however, appears to be less affected by the demagnetizaton.
",0,1,0,0,0,0
20324,Efficient determination of optimised multi-arm multi-stage experimental designs with control of generalised error-rates,"  Primarily motivated by the drug development process, several publications
have now presented methodology for the design of multi-arm multi-stage
experiments with normally distributed outcome variables of known variance.
Here, we extend these past considerations to allow the design of what we refer
to as an abcd multi-arm multi-stage experiment. We provide a proof of how
strong control of the a-generalised type-I familywise error-rate can be
ensured. We then describe how to attain the power to reject at least b out of c
false hypotheses, which is related to controlling the b-generalised type-II
familywise error-rate. Following this, we detail how a design can be optimised
for a scenario in which rejection of any d null hypotheses brings about
termination of the experiment. We achieve this by proposing a highly
computationally efficient approach for evaluating the performance of a
candidate design. Finally, using a real clinical trial as a motivating example,
we explore the effect of the design's control parameters on the statistical
operating characteristics.
",0,0,0,1,0,0
11359,"The normal closure of big Dehn twists, and plate spinning with rotating families","  We study the normal closure of a big power of one or several Dehn twists in a
Mapping Class Group. We prove that it has a presentation whose relators
consists only of commutators between twists of disjoint support, thus answering
a question of Ivanov. Our method is to use the theory of projection complexes
of Bestvina Bromberg and Fujiwara, together with the theory of rotating
families, simultaneously on several spaces.
",0,0,1,0,0,0
12320,On Atiyah-Singer and Atiyah-Bott for finite abstract simplicial complexes,"  A linear or multi-linear valuation on a finite abstract simplicial complex
can be expressed as an analytic index dim(ker(D)) -dim(ker(D^*)) of a
differential complex D:E -> F. In the discrete, a complex D can be called
elliptic if a McKean-Singer spectral symmetry applies as this implies
str(exp(-t D^2)) is t-independent. In that case, the analytic index of D is the
sum of (-1)^k b_k(D), where b_k(D) is the k'th Betti number, which by Hodge is
the nullity of the (k+1)'th block of the Hodge operator L=D^2. It can also be
written as a topological index summing K(v) over the set of zero-dimensional
simplices in G and where K is an Euler type curvature defined by G and D. This
can be interpreted as a Atiyah-Singer type correspondence between analytic and
topological index. Examples are the de Rham differential complex for the Euler
characteristic X(G) or the connection differential complex for Wu
characteristic w_k(G). Given an endomorphism T of an elliptic complex, the
Lefschetz number X(T,G,D) is defined as the super trace of T acting on
cohomology defined by E. It is equal to the sum i(v) over V which are contained
in fixed simplices of T, and i is a Brouwer type index. This Atiyah-Bott result
generalizes the Brouwer-Lefschetz fixed point theorem for an endomorphism of
the simplicial complex G. In both the static and dynamic setting, the proof is
done by heat deforming the Koopman operator U(T) to get the cohomological
picture str(exp(-t D^2) U(T)) in the limit t to infinity and then use Hodge,
and then by applying a discrete gradient flow to the simplex data defining the
valuation to push str(U(T)) to V, getting curvature K(v) or the Brouwer type
index i(v).
",1,0,1,0,0,0
14803,Security Against Impersonation Attacks in Distributed Systems,"  In a multi-agent system, transitioning from a centralized to a distributed
decision-making strategy can introduce vulnerability to adversarial
manipulation. We study the potential for adversarial manipulation in a class of
graphical coordination games where the adversary can pose as a friendly agent
in the game, thereby influencing the decision-making rules of a subset of
agents. The adversary's influence can cascade throughout the system, indirectly
influencing other agents' behavior and significantly impacting the emergent
collective behavior. The main results in this paper focus on characterizing
conditions under which the adversary's local influence can dramatically impact
the emergent global behavior, e.g., destabilize efficient Nash equilibria.
",1,0,0,0,0,0
8162,Improving galaxy morphology with machine learning,"  This paper presents machine learning experiments performed over results of
galaxy classification into elliptical (E) and spiral (S) with morphological
parameters: concetration (CN), assimetry metrics (A3), smoothness metrics (S3),
entropy (H) and gradient pattern analysis parameter (GA). Except concentration,
all parameters performed a image segmentation pre-processing. For supervision
and to compute confusion matrices, we used as true label the galaxy
classification from GalaxyZoo. With a 48145 objects dataset after preprocessing
(44760 galaxies labeled as S and 3385 as E), we performed experiments with
Support Vector Machine (SVM) and Decision Tree (DT). Whit a 1962 objects
balanced dataset, we applied K- means and Agglomerative Hierarchical
Clustering. All experiments with supervision reached an Overall Accuracy OA >=
97%.
",0,1,0,0,0,0
1997,Time-Resolved High Spectral Resolution Observation of 2MASSW J0746425+200032AB,"  Many brown dwarfs exhibit photometric variability at levels from tenths to
tens of percents. The photometric variability is related to magnetic activity
or patchy cloud coverage, characteristic of brown dwarfs near the L-T
transition. Time-resolved spectral monitoring of brown dwarfs provides
diagnostics of cloud distribution and condensate properties. However, current
time-resolved spectral studies of brown dwarfs are limited to low spectral
resolution (R$\sim$100) with the exception of the study of Luhman 16 AB at
resolution of 100,000 using the VLT$+$CRIRES. This work yielded the first map
of brown dwarf surface inhomogeneity, highlighting the importance and unique
contribution of high spectral resolution observations. Here, we report on the
time-resolved high spectral resolution observations of a nearby brown dwarf
binary, 2MASSW J0746425+200032AB. We find no coherent spectral variability that
is modulated with rotation. Based on simulations we conclude that the coverage
of a single spot on 2MASSW J0746425+200032AB is smaller than 1\% or 6.25\% if
spot contrast is 50\% or 80\% of its surrounding flux, respectively. Future
high spectral resolution observations aided by adaptive optics systems can put
tighter constraints on the spectral variability of 2MASSW J0746425+200032AB and
other nearby brown dwarfs.
",0,1,0,0,0,0
479,Dimension-free Wasserstein contraction of nonlinear filters,"  For a class of partially observed diffusions, sufficient conditions are given
for the map from initial condition of the signal to filtering distribution to
be contractive with respect to Wasserstein distances, with rate which has no
dependence on the dimension of the state-space and is stable under tensor
products of the model. The main assumptions are that the signal has affine
drift and constant diffusion coefficient, and that the likelihood functions are
log-concave. Contraction estimates are obtained from an $h$-process
representation of the transition probabilities of the signal reweighted so as
to condition on the observations.
",0,0,1,1,0,0
16361,A Decentralized Mobile Computing Network for Multi-Robot Systems Operations,"  Collective animal behaviors are paradigmatic examples of fully decentralized
operations involving complex collective computations such as collective turns
in flocks of birds or collective harvesting by ants. These systems offer a
unique source of inspiration for the development of fault-tolerant and
self-healing multi-robot systems capable of operating in dynamic environments.
Specifically, swarm robotics emerged and is significantly growing on these
premises. However, to date, most swarm robotics systems reported in the
literature involve basic computational tasks---averages and other algebraic
operations. In this paper, we introduce a novel Collective computing framework
based on the swarming paradigm, which exhibits the key innate features of
swarms: robustness, scalability and flexibility. Unlike Edge computing, the
proposed Collective computing framework is truly decentralized and does not
require user intervention or additional servers to sustain its operations. This
Collective computing framework is applied to the complex task of collective
mapping, in which multiple robots aim at cooperatively map a large area. Our
results confirm the effectiveness of the cooperative strategy, its robustness
to the loss of multiple units, as well as its scalability. Furthermore, the
topology of the interconnecting network is found to greatly influence the
performance of the collective action.
",1,0,0,0,0,0
9797,A quantum phase transition induced by a microscopic boundary condition,"  Quantum phase transitions are sudden changes in the ground-state wavefunction
of a many-body system that can occur as a control parameter such as a
concentration or a field strength is varied. They are driven purely by the
competition between quantum fluctuations and mutual interactions among
constituents of the system, not by thermal fluctuations; hence they can occur
even at zero temperature. Examples of quantum phase transitions in many-body
physics may be found in systems ranging from high-temperature superconductors
to topological insulators. A quantum phase transition usually can be
characterized by nonanalyticity/discontinuity in certain order parameters or
divergence of the ground state energy eigenvalue and/or its derivatives with
respect to certain physical quantities. Here in a circular one-dimensional spin
model with Heisenberg XY interaction and no magnetic field, we observe critical
phenomena for the $n_0=1/N\rightarrow0$ Mott insulator caused by a qualitative
change of the boundary condition. We demonstrate in the vicinity of the
transition point a sudden change in ground-state properties accompanied by an
avoided level-crossing between the ground and the first excited states.
Notably, our result links conventional quantum phase transitions to microscopic
boundary conditions, with significant implications for quantum information,
quantum control, and quantum computing.
",0,1,0,0,0,0
489,Clustering of Gamma-Ray bursts through kernel principal component analysis,"  We consider the problem related to clustering of gamma-ray bursts (from
""BATSE"" catalogue) through kernel principal component analysis in which our
proposed kernel outperforms results of other competent kernels in terms of
clustering accuracy and we obtain three physically interpretable groups of
gamma-ray bursts. The effectivity of the suggested kernel in combination with
kernel principal component analysis in revealing natural clusters in noisy and
nonlinear data while reducing the dimension of the data is also explored in two
simulated data sets.
",0,1,0,1,0,0
16842,Adaptive Representation Selection in Contextual Bandit,"  We consider an extension of the contextual bandit setting, motivated by
several practical applications, where an unlabeled history of contexts can
become available for pre-training before the online decision-making begins. We
propose an approach for improving the performance of contextual bandit in such
setting, via adaptive, dynamic representation learning, which combines offline
pre-training on unlabeled history of contexts with online selection and
modification of embedding functions. Our experiments on a variety of datasets
and in different nonstationary environments demonstrate clear advantages of our
approach over the standard contextual bandit.
",0,0,0,1,0,0
14869,Optimal Allocation of Static Var Compensator via Mixed Integer Conic Programming,"  Shunt FACTS devices, such as, a Static Var Compensator (SVC), are capable of
providing local reactive power compensation. They are widely used in the
network to reduce the real power loss and improve the voltage profile. This
paper proposes a planning model based on mixed integer conic programming (MICP)
to optimally allocate SVCs in the transmission network considering load
uncertainty. The load uncertainties are represented by a number of scenarios.
Reformulation and linearization techniques are utilized to transform the
original non-convex model into a convex second order cone programming (SOCP)
model. Numerical case studies based on the IEEE 30-bus system demonstrate the
effectiveness of the proposed planning model.
",0,0,1,0,0,0
3349,Combinatorial and Asymptotical Results on the Neighborhood Grid,"  In 2009, Joselli et al introduced the Neighborhood Grid data structure for
fast computation of neighborhood estimates in point clouds. Even though the
data structure has been used in several applications and shown to be
practically relevant, it is theoretically not yet well understood. The purpose
of this paper is to present a polynomial-time algorithm to build the data
structure. Furthermore, it is investigated whether the presented algorithm is
optimal. This investigations leads to several combinatorial questions for which
partial results are given.
",1,0,0,0,0,0
11702,"Alternating Double Euler Sums, Hypergeometric Identities and a Theorem of Zagier","  In this work, we derive relations between generating functions of double
stuffle relations and double shuffle relations to express the alternating
double Euler sums $\zeta\left(\overline{r}, s\right)$, $\zeta\left(r,
\overline{s}\right)$ and $\zeta\left(\overline{r}, \overline{s}\right)$ with
$r+s$ odd in terms of zeta values. We also give a direct proof of a
hypergeometric identity which is a limiting case of a basic hypergeometric
identity of Andrews. Finally, we gave another proof for the formula of Zagier
on the multiple zeta values $\zeta(2,\ldots,2,3,2,\ldots,2)$.
",0,0,1,0,0,0
16872,The Steinberg linkage class for a reductive algebraic group,"  Let G be a reductive algebraic group over a field of positive characteristic
and denote by C(G) the category of rational G-modules. In this note we
investigate the subcategory of C(G) consisting of those modules whose
composition factors all have highest weights linked to the Steinberg weight.
This subcategory is denoted ST and called the Steinberg component. We give an
explicit equivalence between ST and C(G) and we derive some consequences. In
particular, our result allows us to relate the Frobenius contracting functor to
the projection functor from C(G) onto ST .
",0,0,1,0,0,0
13565,Writer Independent Offline Signature Recognition Using Ensemble Learning,"  The area of Handwritten Signature Verification has been broadly researched in
the last decades, but remains an open research problem. In offline (static)
signature verification, the dynamic information of the signature writing
process is lost, and it is difficult to design good feature extractors that can
distinguish genuine signatures and skilled forgeries. This verification task is
even harder in writer independent scenarios which is undeniably fiscal for
realistic cases. In this paper, we have proposed an Ensemble model for offline
writer, independent signature verification task with Deep learning. We have
used two CNNs for feature extraction, after that RGBT for classification &
Stacking to generate final prediction vector. We have done extensive
experiments on various datasets from various sources to maintain a variance in
the dataset. We have achieved the state of the art performance on various
datasets.
",1,0,0,1,0,0
12593,"Projectors separating spectra for $L^2$ on symmetric spaces $GL(n,\C)/GL(n,\R)$","  The Plancherel decomposition of $L^2$ on a pseudo-Riemannian symmetric space
$GL(n,C)/GL(n,R)$ has spectrum of $[n/2]$ types. We write explicitly orthogonal
projectors separating spectrum into uniform pieces
",0,0,1,0,0,0
1016,Two-photon exchange correction to the hyperfine splitting in muonic hydrogen,"  We reevaluate the Zemach, recoil and polarizability corrections to the
hyperfine splitting in muonic hydrogen expressing them through the low-energy
proton structure constants and obtain the precise values of the Zemach radius
and two-photon exchange (TPE) contribution. The uncertainty of TPE correction
to S energy levels in muonic hydrogen of 105 ppm exceeds the ppm accuracy level
of the forthcoming 1S hyperfine splitting measurements at PSI, J-PARC and
RIKEN-RAL.
",0,1,0,0,0,0
2748,A Formal Approach to Exploiting Multi-Stage Attacks based on File-System Vulnerabilities of Web Applications (Extended Version),"  Web applications require access to the file-system for many different tasks.
When analyzing the security of a web application, secu- rity analysts should
thus consider the impact that file-system operations have on the security of
the whole application. Moreover, the analysis should take into consideration
how file-system vulnerabilities might in- teract with other vulnerabilities
leading an attacker to breach into the web application. In this paper, we first
propose a classification of file- system vulnerabilities, and then, based on
this classification, we present a formal approach that allows one to exploit
file-system vulnerabilities. We give a formal representation of web
applications, databases and file- systems, and show how to reason about
file-system vulnerabilities. We also show how to combine file-system
vulnerabilities and SQL-Injection vulnerabilities for the identification of
complex, multi-stage attacks. We have developed an automatic tool that
implements our approach and we show its efficiency by discussing several
real-world case studies, which are witness to the fact that our tool can
generate, and exploit, complex attacks that, to the best of our knowledge, no
other state-of-the-art-tool for the security of web applications can find.
",1,0,0,0,0,0
18445,Indices in XML Databases,"  With XML becoming a standard for business information representation and
exchange, stor-ing, indexing, and querying XML documents have rapidly become
major issues in database research. In this context, query processing and
optimization are primordial, native-XML data-bases not being mature yet. Data
structures such as indices, which help enhance performances substantially, are
extensively researched, especially since XML data bear numerous specifici-ties
with respect to relational data. In this paper, we survey state-of-the-art XML
indices and discuss the main issues, tradeoffs and future trends in XML
indexing. We also present an in-dex that we specifically designed for the
particular architecture of XML data warehouses.
",1,0,0,0,0,0
9379,A simple and efficient feedback control strategy for wastewater denitrification,"  Due to severe mathematical modeling and calibration difficulties open-loop
feedforward control is mainly employed today for wastewater denitrification,
which is a key ecological issue. In order to improve the resulting poor
performances a new model-free control setting and its corresponding
""intelligent"" controller are introduced. The pitfall of regulating two output
variables via a single input variable is overcome by introducing also an
open-loop knowledge-based control deduced from the plant behavior. Several
convincing computer simulations are presented and discussed.
",1,0,1,0,0,0
293,Quantum Charge Pumps with Topological Phases in Creutz Ladder,"  Quantum charge pumping phenomenon connects band topology through the dynamics
of a one-dimensional quantum system. In terms of a microscopic model, the
Su-Schrieffer-Heeger/Rice-Mele quantum pump continues to serve as a fruitful
starting point for many considerations of topological physics. Here we present
a generalized Creutz scheme as a distinct two-band quantum pump model. By
noting that it undergoes two kinds of topological band transitions accompanying
with a Zak-phase-difference of $\pi$ and $2\pi$, respectively, various charge
pumping schemes are studied by applying an elaborate Peierl's phase
substitution. Translating into real space, the transportation of quantized
charges is a result of cooperative quantum interference effect. In particular,
an all-flux quantum pump emerges which operates with time-varying fluxes only
and transports two charge units. This puts cold atoms with artificial gauge
fields as an unique system where this kind of phenomena can be realized.
",0,1,0,0,0,0
20256,Security Analysis of Cache Replacement Policies,"  Modern computer architectures share physical resources between different
programs in order to increase area-, energy-, and cost-efficiency.
Unfortunately, sharing often gives rise to side channels that can be exploited
for extracting or transmitting sensitive information. We currently lack
techniques for systematic reasoning about this interplay between security and
efficiency. In particular, there is no established way for quantifying security
properties of shared caches.
In this paper, we propose a novel model that enables us to characterize
important security properties of caches. Our model encompasses two aspects: (1)
The amount of information that can be absorbed by a cache, and (2) the amount
of information that can effectively be extracted from the cache by an
adversary. We use our model to compute both quantities for common cache
replacement policies (FIFO, LRU, and PLRU) and to compare their isolation
properties. We further show how our model for information extraction leads to
an algorithm that can be used to improve the bounds delivered by the CacheAudit
static analyzer.
",1,0,0,0,0,0
14171,Nonlinear Kalman Filtering for Censored Observations,"  The use of Kalman filtering, as well as its nonlinear extensions, for the
estimation of system variables and parameters has played a pivotal role in many
fields of scientific inquiry where observations of the system are restricted to
a subset of variables. However in the case of censored observations, where
measurements of the system beyond a certain detection point are impossible, the
estimation problem is complicated. Without appropriate consideration, censored
observations can lead to inaccurate estimates. Motivated by the work of [1], we
develop a modified version of the extended Kalman filter to handle the case of
censored observations in nonlinear systems. We validate this methodology in a
simple oscillator system first, showing its ability to accurately reconstruct
state variables and track system parameters when observations are censored.
Finally, we utilize the nonlinear censored filter to analyze censored datasets
from patients with hepatitis C and human immunodeficiency virus.
",0,0,1,1,0,0
9689,A new proof of Kirchberg's $\mathcal O_2$-stable classification,"  I present a new proof of Kirchberg's $\mathcal O_2$-stable classification
theorem: two separable, nuclear, stable/unital, $\mathcal O_2$-stable
$C^\ast$-algebras are isomorphic if and only if their ideal lattices are order
isomorphic, or equivalently, their primitive ideal spaces are homeomorphic.
Many intermediate results do not depend on pure infiniteness of any sort.
",0,0,1,0,0,0
7844,Shutting down or powering up a (U)LIRG? Merger components in distinctly different evolutionary states in IRAS 19115-2124 (The Bird),"  We present new SINFONI near-infrared integral field unit (IFU) spectroscopy
and SALT optical long-slit spectroscopy characterising the history of a nearby
merging luminous infrared galaxy, dubbed the Bird (IRAS19115-2114). The NIR
line-ratio maps of the IFU data-cubes and stellar population fitting of the
SALT spectra now allow dating of the star formation (SF) over the triple system
uncovered from our previous adaptive optics data. The distinct components
separate very clearly in a line-ratio diagnostic diagram. An off-nuclear pure
starburst dominates the current SF of the Bird with 60-70% of the total, with a
4-7 Myr age, and signs of a fairly constant long-term star formation of the
underlying stellar population. The most massive nucleus, in contrast, is
quenched with a starburst age of >40 Myr and shows hints of budding AGN
activity. The secondary massive nucleus is at an intermediate stage. The two
major components have a population of older stars, consistent with a starburst
triggered 1 Gyr ago in a first encounter. The simplest explanation of the
history is that of a triple merger, where the strongly star forming component
has joined later. We detect multiple gas flows in different phases. The Bird
offers an opportunity to witness multiple stages of galaxy evolution in the
same system; triggering as well as quenching of SF, and the early appearance of
AGN activity. It also serves as a cautionary note on interpretations of
observations with lower spatial resolution and/or without infrared data. At
high-redshift the system would look like a clumpy starburst with crucial pieces
of its puzzle hidden, in danger of misinterpretations.
",0,1,0,0,0,0
6345,Large odd order character sums and improvements of the Pólya-Vinogradov inequality,"  For a primitive Dirichlet character $\chi$ modulo $q$, we define
$M(\chi)=\max_{t } |\sum_{n \leq t} \chi(n)|$. In this paper, we study this
quantity for characters of a fixed odd order $g\geq 3$. Our main result
provides a further improvement of the classical Pólya-Vinogradov inequality
in this case. More specifically, we show that for any such character $\chi$ we
have $$M(\chi)\ll_{\varepsilon} \sqrt{q}(\log q)^{1-\delta_g}(\log\log
q)^{-1/4+\varepsilon},$$ where $\delta_g := 1-\frac{g}{\pi}\sin(\pi/g)$. This
improves upon the works of Granville and Soundararajan and of Goldmakher.
Furthermore, assuming the Generalized Riemann hypothesis (GRH) we prove that $$
M(\chi) \ll \sqrt{q} \left(\log_2 q\right)^{1-\delta_g} \left(\log_3
q\right)^{-\frac{1}{4}}\left(\log_4 q\right)^{O(1)}, $$ where $\log_j$ is the
$j$-th iterated logarithm. We also show unconditionally that this bound is best
possible (up to a power of $\log_4 q$). One of the key ingredients in the proof
of the upper bounds is a new Halász-type inequality for logarithmic mean
values of completely multiplicative functions, which might be of independent
interest.
",0,0,1,0,0,0
8679,Characterization of Multi-scale Invariant Random Fields,"  Applying certain flexible geometric sampling of a multi-scale invariant (MSI)
field we provide a multi-dimensional multi-selfsimilar field which has a one to
one correspondence with such sampled MSI field. This sampling enables us to
characterize harmonic-like representation and spectral density function of the
sampled MSI field. Imposing Markov property for the MSI field, we find that the
covariance function and spectral density matrix of such sampled Markov MSI
field are characterized by the covariance functions of samples of the first
scale rectangle. We present an example of MSI field as two-dimensional simple
fractional Brownian motion. We consider a real data example of the
precipitation in some area of Brisbane in Australia for some special period. We
show that precipitation on this area has MSI property and estimate time
dependent scale and Hurst parameters of this MSI field in three dimension as
latitude, longitude and time. Our method enables one to predict precipitation
in time and place.
",0,0,0,1,0,0
1515,Total variation regularization with variable Lebesgue prior,"  This work proposes the variable exponent Lebesgue modular as a replacement
for the 1-norm in total variation (TV) regularization. It allows the exponent
to vary with spatial location and thus enables users to locally select whether
to preserve edges or smooth intensity variations. In contrast to earlier work
using TV-like methods with variable exponents, the exponent function is here
computed offline as a fixed parameter of the final optimization problem,
resulting in a convex goal functional. The obtained formulas for the convex
conjugate and the proximal operators are simple in structure and can be
evaluated very efficiently, an important property for practical usability.
Numerical results with variable $L^p$ TV prior in denoising and tomography
problems on synthetic data compare favorably to total generalized variation
(TGV) and TV.
",0,0,1,0,0,0
75,Pattern-forming fronts in a Swift-Hohenberg equation with directional quenching - parallel and oblique stripes,"  We study the effect of domain growth on the orientation of striped phases in
a Swift-Hohenberg equation. Domain growth is encoded in a step-like parameter
dependence that allows stripe formation in a half plane, and suppresses
patterns in the complement, while the boundary of the pattern-forming region is
propagating with fixed normal velocity. We construct front solutions that leave
behind stripes in the pattern-forming region that are parallel to or at a small
oblique angle to the boundary.
Technically, the construction of stripe formation parallel to the boundary
relies on ill-posed, infinite-dimensional spatial dynamics. Stripes forming at
a small oblique angle are constructed using a functional-analytic, perturbative
approach. Here, the main difficulties are the presence of continuous spectrum
and the fact that small oblique angles appear as a singular perturbation in a
traveling-wave problem. We resolve the former difficulty using a farfield-core
decomposition and Fredholm theory in weighted spaces. The singular perturbation
problem is resolved using preconditioners and boot-strapping.
",0,1,0,0,0,0
18491,On free Gelfand--Dorfman--Novikov superalgebras and a PBW type theorem,"  We construct a linear basis of a free GDN superalgebra over a field of
characteristic $\neq 2$.
As applications, we prove a PBW theorem, that is, any GDN superalgebra can be
embedded into its universal enveloping commutative associative differential
superalgebra. An Engel theorem under some assumptions is given.
",0,0,1,0,0,0
533,Identification of microRNA clusters cooperatively acting on Epithelial to Mesenchymal Transition in Triple Negative Breast Cancer,"  MicroRNAs play important roles in many biological processes. Their aberrant
expression can have oncogenic or tumor suppressor function directly
participating to carcinogenesis, malignant transformation, invasiveness and
metastasis. Indeed, miRNA profiles can distinguish not only between normal and
cancerous tissue but they can also successfully classify different subtypes of
a particular cancer. Here, we focus on a particular class of transcripts
encoding polycistronic miRNA genes that yields multiple miRNA components. We
describe clustered MiRNA Master Regulator Analysis (ClustMMRA), a fully
redesigned release of the MMRA computational pipeline (MiRNA Master Regulator
Analysis), developed to search for clustered miRNAs potentially driving cancer
molecular subtyping. Genomically clustered miRNAs are frequently co-expressed
to target different components of pro-tumorigenic signalling pathways. By
applying ClustMMRA to breast cancer patient data, we identified key miRNA
clusters driving the phenotype of different tumor subgroups. The pipeline was
applied to two independent breast cancer datasets, providing statistically
concordant results between the two analysis. We validated in cell lines the
miR-199/miR-214 as a novel cluster of miRNAs promoting the triple negative
subtype phenotype through its control of proliferation and EMT.
",0,0,0,0,1,0
13243,Thermal conductivity changes across a structural phase transition: the case of high-pressure silica,"  By means of first-principles calculations, we investigate the thermal
properties of silica as it evolves, under hydrostatic compression, from a
stishovite phase into a CaCl$_2$-type structure. We compute the thermal
conductivity tensor by solving the linearized Boltzmann transport equation
iteratively in a wide temperature range, using for this the pressure-dependent
harmonic and anharmonic interatomic couplings obtained from first principles.
Most remarkably, we find that, at low temperatures, SiO$_2$ displays a large
peak in the in-plane thermal conductivity and a highly anisotropic behavior
close to the structural transformation. We trace back the origin of these
features by analyzing the phonon contributions to the conductivity. We discuss
the implications of our results in the general context of continuous structural
transformations in solids, as well as the potential geological interest of our
results for silica.
",0,1,0,0,0,0
5718,Adaptive Exploration-Exploitation Tradeoff for Opportunistic Bandits,"  In this paper, we propose and study opportunistic bandits - a new variant of
bandits where the regret of pulling a suboptimal arm varies under different
environmental conditions, such as network load or produce price. When the
load/price is low, so is the cost/regret of pulling a suboptimal arm (e.g.,
trying a suboptimal network configuration). Therefore, intuitively, we could
explore more when the load/price is low and exploit more when the load/price is
high. Inspired by this intuition, we propose an Adaptive Upper-Confidence-Bound
(AdaUCB) algorithm to adaptively balance the exploration-exploitation tradeoff
for opportunistic bandits. We prove that AdaUCB achieves $O(\log T)$ regret
with a smaller coefficient than the traditional UCB algorithm. Furthermore,
AdaUCB achieves $O(1)$ regret with respect to $T$ if the exploration cost is
zero when the load level is below a certain threshold. Last, based on both
synthetic data and real-world traces, experimental results show that AdaUCB
significantly outperforms other bandit algorithms, such as UCB and TS (Thompson
Sampling), under large load/price fluctuations.
",1,0,0,1,0,0
14916,A Dynamic Programming Solution to Bounded Dejittering Problems,"  We propose a dynamic programming solution to image dejittering problems with
bounded displacements and obtain efficient algorithms for the removal of line
jitter, line pixel jitter, and pixel jitter.
",1,0,1,0,0,0
18287,Efficiently Learning Mixtures of Mallows Models,"  Mixtures of Mallows models are a popular generative model for ranking data
coming from a heterogeneous population. They have a variety of applications
including social choice, recommendation systems and natural language
processing. Here we give the first polynomial time algorithm for provably
learning the parameters of a mixture of Mallows models with any constant number
of components. Prior to our work, only the two component case had been settled.
Our analysis revolves around a determinantal identity of Zagier which was
proven in the context of mathematical physics, which we use to show polynomial
identifiability and ultimately to construct test functions to peel off one
component at a time.
To complement our upper bounds, we show information-theoretic lower bounds on
the sample complexity as well as lower bounds against restricted families of
algorithms that make only local queries. Together, these results demonstrate
various impediments to improving the dependence on the number of components.
They also motivate the study of learning mixtures of Mallows models from the
perspective of beyond worst-case analysis. In this direction, we show that when
the scaling parameters of the Mallows models have separation, there are much
faster learning algorithms.
",0,0,0,1,0,0
11662,Minimally-Supervised Attribute Fusion for Data Lakes,"  Aggregate analysis, such as comparing country-wise sales versus global market
share across product categories, is often complicated by the unavailability of
common join attributes, e.g., category, across diverse datasets from different
geographies or retail chains, even after disparate data is technically ingested
into a common data lake. Sometimes this is a missing data issue, while in other
cases it may be inherent, e.g., the records in different geographical databases
may actually describe different product 'SKUs', or follow different norms for
categorization. Record linkage techniques can be used to automatically map
products in different data sources to a common set of global attributes,
thereby enabling federated aggregation joins to be performed. Traditional
record-linkage techniques are typically unsupervised, relying textual
similarity features across attributes to estimate matches. In this paper, we
present an ensemble model combining minimal supervision using Bayesian network
models together with unsupervised textual matching for automating such
'attribute fusion'. We present results of our approach on a large volume of
real-life data from a market-research scenario and compare with a standard
record matching algorithm. Finally we illustrate how attribute fusion using
machine learning could be included as a data-lake management feature,
especially as our approach also provides confidence values for matches,
enabling human intervention, if required.
",1,0,0,0,0,0
9715,Recurrent Additive Networks,"  We introduce recurrent additive networks (RANs), a new gated RNN which is
distinguished by the use of purely additive latent state updates. At every time
step, the new state is computed as a gated component-wise sum of the input and
the previous state, without any of the non-linearities commonly used in RNN
transition dynamics. We formally show that RAN states are weighted sums of the
input vectors, and that the gates only contribute to computing the weights of
these sums. Despite this relatively simple functional form, experiments
demonstrate that RANs perform on par with LSTMs on benchmark language modeling
problems. This result shows that many of the non-linear computations in LSTMs
and related networks are not essential, at least for the problems we consider,
and suggests that the gates are doing more of the computational work than
previously understood.
",1,0,0,0,0,0
3594,PSYM-WIDE: a survey for large-separation planetary-mass companions to late spectral type members of young moving groups,"  We present the results of a direct-imaging survey for very large separation
($>$100 au), companions around 95 nearby young K5-L5 stars and brown dwarfs.
They are high-likelihood candidates or confirmed members of the young
($\lessapprox$150 Myr) $\beta$ Pictoris and AB Doradus moving groups (ABDMG)
and the TW Hya, Tucana-Horologium, Columba, Carina, and Argus associations.
Images in $i'$ and $z'$ filters were obtained with the Gemini Multi-Object
Spectrograph (GMOS) on Gemini South to search for companions down to an
apparent magnitude of $z'\sim$22-24 at separations $\gtrapprox$20"" from the
targets and in the remainder of the wide 5.5' $\times$ 5.5' GMOS field of view.
This allowed us to probe the most distant region where planetary-mass
companions could be gravitationally bound to the targets. This region was left
largely unstudied by past high-contrast imaging surveys, which probed much
closer-in separations. This survey led to the discovery of a planetary-mass
(9-13 $\,M_{\rm{Jup}}$) companion at 2000 au from the M3V star GU Psc, a highly
probable member of ABDMG. No other substellar companions were identified. These
results allowed us to constrain the frequency of distant planetary-mass
companions (5-13 $\,M_{\rm{Jup}}$) to 0.84$_{-0.66}^{+6.73}$% (95% confidence)
at semimajor axes between 500 and 5000 au around young K5-L5 stars and brown
dwarfs. This is consistent with other studies suggesting that gravitationally
bound planetary-mass companions at wide separations from low-mass stars are
relatively rare.
",0,1,0,0,0,0
13358,Good Arm Identification via Bandit Feedback,"  We consider a novel stochastic multi-armed bandit problem called {\em good
arm identification} (GAI), where a good arm is defined as an arm with expected
reward greater than or equal to a given threshold. GAI is a pure-exploration
problem that a single agent repeats a process of outputting an arm as soon as
it is identified as a good one before confirming the other arms are actually
not good. The objective of GAI is to minimize the number of samples for each
process. We find that GAI faces a new kind of dilemma, the {\em
exploration-exploitation dilemma of confidence}, which is different difficulty
from the best arm identification. As a result, an efficient design of
algorithms for GAI is quite different from that for the best arm
identification. We derive a lower bound on the sample complexity of GAI that is
tight up to the logarithmic factor $\mathrm{O}(\log \frac{1}{\delta})$ for
acceptance error rate $\delta$. We also develop an algorithm whose sample
complexity almost matches the lower bound. We also confirm experimentally that
our proposed algorithm outperforms naive algorithms in synthetic settings based
on a conventional bandit problem and clinical trial researches for rheumatoid
arthritis.
",0,0,0,1,0,0
19160,Hubble Frontier Fields: systematic errors in strong lensing models of galaxy clusters - Implications for cosmography,"  Strong gravitational lensing by galaxy clusters is a fundamental tool to
study dark matter and constrain the geometry of the Universe. Recently, the
Hubble Space Telescope Frontier Fields programme has allowed a significant
improvement of mass and magnification measurements but lensing models still
have a residual root mean square between 0.2 arcsec and few arcsec- onds, not
yet completely understood. Systematic errors have to be better understood and
treated in order to use strong lensing clusters as reliable cosmological
probes. We have analysed two simulated Hubble-Frontier-Fields-like clusters
from the Hubble Frontier Fields Comparison Challenge, Ares and Hera. We use
several estimators (relative bias on magnification, den- sity profiles,
ellipticity and orientation) to quantify the goodness of our reconstructions by
comparing our multiple models, optimized with the parametric software LENSTOOL
, with the input models. We have quantified the impact of systematic errors
arising, first, from the choice of different density profiles and
configurations and, secondly, from the availability of con- straints
(spectroscopic or photometric redshifts, redshift ranges of the background
sources) in the parametric modelling of strong lensing galaxy clusters and
therefore on the retrieval of cosmological parameters. We find that
substructures in the outskirts have a significant im- pact on the position of
the multiple images, yielding tighter cosmological contours. The need for
wide-field imaging around massive clusters is thus reinforced. We show that
competitive cosmological constraints can be obtained also with complex
multimodal clusters and that photometric redshifts improve the constraints on
cosmological parameters when considering a narrow range of (spectroscopic)
redshifts for the sources.
",0,1,0,0,0,0
18076,"Water, High-Altitude Condensates, and Possible Methane Depletion in the Atmosphere of the Warm Super-Neptune WASP-107b","  The super-Neptune exoplanet WASP-107b is an exciting target for atmosphere
characterization. It has an unusually large atmospheric scale height and a
small, bright host star, raising the possibility of precise constraints on its
current nature and formation history. We report the first atmospheric study of
WASP-107b, a Hubble Space Telescope measurement of its near-infrared
transmission spectrum. We determined the planet's composition with two
techniques: atmospheric retrieval based on the transmission spectrum and
interior structure modeling based on the observed mass and radius. The interior
structure models set a $3\,\sigma$ upper limit on the atmospheric metallicity
of $30\times$ solar. The transmission spectrum shows strong evidence for water
absorption ($6.5\,\sigma$ confidence), and the retrieved water abundance is
consistent with expectations for a solar abundance pattern. The inferred
carbon-to-oxygen ratio is subsolar at $2.7\,\sigma$ confidence, which we
attribute to possible methane depletion in the atmosphere. The spectral
features are smaller than predicted for a cloud-free composition, crossing less
than one scale height. A thick condensate layer at high altitudes (0.1 - 3
mbar) is needed to match the observations. We find that physically motivated
cloud models with moderate sedimentation efficiency ($f_\mathrm{sed} = 0.3$) or
hazes with a particle size of 0.3 $\mu$m reproduce the observed spectral
feature amplitude. Taken together, these findings serve as an illustration of
the diversity and complexity of exoplanet atmospheres. The community can look
forward to more such results with the high precision and wide spectral coverage
afforded by future observing facilities.
",0,1,0,0,0,0
13265,Nonlocal Cauchy problems for wave equations and applications,"  In this paper, the existence, the uniqueness and estimates of solution to the
integral Cauchy problem for linear and nonlinear abstract wave equations are
proved. The equation includes a linear operator A defined in a Banach space E,
in which by choosing E and A we can obtain numerous classis of nonlocal initial
value problems for wave equations which occur in a wide variety of physical
systems.
",0,0,1,0,0,0
20464,Generalized Index Coding Problem and Discrete Polymatroids,"  The index coding problem has been generalized recently to accommodate
receivers which demand functions of messages and which possess functions of
messages. The connections between index coding and matroid theory have been
well studied in the recent past. Index coding solutions were first connected to
multi linear representation of matroids. For vector linear index codes discrete
polymatroids which can be viewed as a generalization of the matroids was used.
It was shown that a vector linear solution to an index coding problem exists if
and only if there exists a representable discrete polymatroid satisfying
certain conditions. In this work we explore the connections between generalized
index coding and discrete polymatroids. The conditions that needs to be
satisfied by a representable discrete polymatroid for a generalized index
coding problem to have a vector linear solution is established. From a discrete
polymatroid we construct an index coding problem with coded side information
and shows that if the index coding problem has a certain optimal length
solution then the discrete polymatroid satisfies certain properties. From a
matroid we construct a similar generalized index coding problem and shows that
the index coding problem has a binary scalar linear solution of optimal length
if and only if the matroid is binary representable.
",1,0,1,0,0,0
18266,A Note on Some Approximation Kernels on the Sphere,"  We produce precise estimates for the Kogbetliantz kernel for the
approximation of functions on the sphere. Furthermore, we propose and study a
new approximation kernel, which has slightly better properties.
",0,0,1,0,0,0
568,An Extended Low Fat Allocator API and Applications,"  The primary function of memory allocators is to allocate and deallocate
chunks of memory primarily through the malloc API. Many memory allocators also
implement other API extensions, such as deriving the size of an allocated
object from the object's pointer, or calculating the base address of an
allocation from an interior pointer. In this paper, we propose a general
purpose extended allocator API built around these common extensions. We argue
that such extended APIs have many applications and demonstrate several use
cases, such as (manual) memory error detection, meta data storage, typed
pointers and compact data-structures. Because most existing allocators were not
designed for the extended API, traditional implementations are expensive or not
possible.
Recently, the LowFat allocator for heap and stack objects has been developed.
The LowFat allocator is an implementation of the idea of low-fat pointers,
where object bounds information (size and base) are encoded into the native
machine pointer representation itself. The ""killer app"" for low-fat pointers is
automated bounds check instrumentation for program hardening and bug detection.
However, the LowFat allocator can also be used to implement highly optimized
version of the extended allocator API, which makes the new applications (listed
above) possible. In this paper, we implement and evaluate several applications
based efficient memory allocator API extensions using low-fat pointers. We also
extend the LowFat allocator to cover global objects for the first time.
",1,0,0,0,0,0
4383,Monaural Singing Voice Separation with Skip-Filtering Connections and Recurrent Inference of Time-Frequency Mask,"  Singing voice separation based on deep learning relies on the usage of
time-frequency masking. In many cases the masking process is not a learnable
function or is not encapsulated into the deep learning optimization.
Consequently, most of the existing methods rely on a post processing step using
the generalized Wiener filtering. This work proposes a method that learns and
optimizes (during training) a source-dependent mask and does not need the
aforementioned post processing step. We introduce a recurrent inference
algorithm, a sparse transformation step to improve the mask generation process,
and a learned denoising filter. Obtained results show an increase of 0.49 dB
for the signal to distortion ratio and 0.30 dB for the signal to interference
ratio, compared to previous state-of-the-art approaches for monaural singing
voice separation.
",1,0,0,0,0,0
18968,Space-Time Geostatistical Models with both Linear and Seasonal Structures in the Temporal Components,"  We provide a novel approach to model space-time random fields where the
temporal argument is decomposed into two parts. The former captures the linear
argument, which is related, for instance, to the annual evolution of the field.
The latter is instead a circular variable describing, for instance, monthly
observations. The basic intuition behind this construction is to consider a
random field defined over space (a compact set of the $d$-dimensional Euclidean
space) across time, which is considered as the product space $\mathbb{R} \times
\mathbb{S}^1$, with $\mathbb{S}^1$ being the unit circle. Under such framework,
we derive new parametric families of covariance functions. In particular, we
focus on two classes of parametric families. The former being parenthetical to
the Gneiting class of covariance functions. The latter is instead obtained by
proposing a new Lagrangian framework for the space-time domain considered in
the manuscript. Our findings are illustrated through a real dataset of surface
air temperatures. We show that the incorporation of both temporal variables can
produce significant improvements in the predictive performances of the model.
We also discuss the extension of this approach for fields defined spatially on
a sphere, which allows to model space-time phenomena over large portions of
planet Earth.
",0,0,1,1,0,0
5174,A New Pseudo-color Technique Based on Intensity Information Protection for Passive Sensor Imagery,"  Remote sensing image processing is so important in geo-sciences. Images which
are obtained by different types of sensors might initially be unrecognizable.
To make an acceptable visual perception in the images, some pre-processing
steps (for removing noises and etc) are preformed which they affect the
analysis of images. There are different types of processing according to the
types of remote sensing images. The method that we are going to introduce in
this paper is to use virtual colors to colorize the gray-scale images of
satellite sensors. This approach helps us to have a better analysis on a sample
single-band image which has been taken by Landsat-8 (OLI) sensor (as a
multi-band sensor with natural color bands, its images' natural color can be
compared to synthetic color by our approach). A good feature of this method is
the original image reversibility in order to keep the suitable resolution of
output images.
",1,0,0,0,0,0
8329,Rethinking Reprojection: Closing the Loop for Pose-aware ShapeReconstruction from a Single Image,"  An emerging problem in computer vision is the reconstruction of 3D shape and
pose of an object from a single image. Hitherto, the problem has been addressed
through the application of canonical deep learning methods to regress from the
image directly to the 3D shape and pose labels. These approaches, however, are
problematic from two perspectives. First, they are minimizing the error between
3D shapes and pose labels - with little thought about the nature of this label
error when reprojecting the shape back onto the image. Second, they rely on the
onerous and ill-posed task of hand labeling natural images with respect to 3D
shape and pose. In this paper we define the new task of pose-aware shape
reconstruction from a single image, and we advocate that cheaper 2D annotations
of objects silhouettes in natural images can be utilized. We design
architectures of pose-aware shape reconstruction which re-project the predicted
shape back on to the image using the predicted pose. Our evaluation on several
object categories demonstrates the superiority of our method for predicting
pose-aware 3D shapes from natural images.
",1,0,0,0,0,0
5142,Run-Wise Simulations for Imaging Atmospheric Cherenkov Telescope Arrays,"  We present a new paradigm for the simulation of arrays of Imaging Atmospheric
Cherenkov Telescopes (IACTs) which overcomes limitations of current approaches.
Up to now, all major IACT experiments rely on the same Monte-Carlo simulation
strategy, using predefined observation and instrument settings. Simulations
with varying parameters are generated to provide better estimates of the
Instrument Response Functions (IRFs) of different observations. However, a
large fraction of the simulation configuration remains preserved, leading to
complete negligence of all related influences. Additionally, the simulation
scheme relies on interpolations between different array configurations, which
are never fully reproducing the actual configuration for a given observation.
Interpolations are usually performed on zenith angles, off-axis angles, array
multiplicity, and the optical response of the instrument. With the advent of
hybrid systems consisting of a large number of IACTs with different sizes,
types, and camera configurations, the complexity of the interpolation and the
size of the phase space becomes increasingly prohibitive. Going beyond the
existing approaches, we introduce a new simulation and analysis concept which
takes into account the actual observation conditions as well as individual
telescope configurations of each observation run of a given data set. These
run-wise simulations (RWS) thus exhibit considerably reduced systematic
uncertainties compared to the existing approach, and are also more
computationally efficient and simple. The RWS framework has been implemented in
the H.E.S.S. software and tested, and is already being exploited in science
analysis.
",0,1,0,0,0,0
10183,Charge reconstruction study of the DAMPE Silicon-Tungsten Tracker with ion beams,"  The DArk Matter Particle Explorer (DAMPE) is one of the four satellites
within Strategic Pioneer Research Program in Space Science of the Chinese
Academy of Science (CAS). DAMPE can detect electrons, photons in a wide energy
range (5 GeV to 10 TeV) and ions up to iron (100GeV to 100 TeV).
Silicon-Tungsten Tracker (STK) is one of the four subdetectors in DAMPE,
providing photon-electron conversion, track reconstruction and charge
identification for ions. Ion beam test was carried out in CERN with 60GeV/u
Lead primary beams. Charge reconstruction and charge resolution of STK
detectors were investigated.
",0,1,0,0,0,0
18071,A Matrix Variate Skew-t Distribution,"  Although there is ample work in the literature dealing with skewness in the
multivariate setting, there is a relative paucity of work in the matrix variate
paradigm. Such work is, for example, useful for modelling three-way data. A
matrix variate skew-t distribution is derived based on a mean-variance matrix
normal mixture. An expectation-conditional maximization algorithm is developed
for parameter estimation. Simulated data are used for illustration.
",0,0,1,1,0,0
19165,"Notes on the replica symmetric solution of the classical and quantum SK model, including the matrix of second derivatives and the spin glass susceptibility","  A review of the replica symmetric solution of the classical and quantum,
infinite-range, Sherrington-Kirkpatrick spin glass is presented.
",0,1,0,0,0,0
13406,A simple anisotropic three-dimensional quantum spin liquid with fracton topological order,"  We present a three-dimensional cubic lattice spin model, anisotropic in the
$\hat{z}$ direction, that exhibits fracton topological order. The latter is a
novel type of topological order characterized by the presence of immobile
pointlike excitations, named fractons, residing at the corners of an operator
with two-dimensional support. As other recent fracton models, ours exhibits a
subextensive ground state degeneracy: On an $L_x\times L_y\times L_z$
three-torus, it has a $2^{2L_z}$ topological degeneracy, and an additional
non-topological degeneracy equal to $2^{L_xL_y-2}$. The fractons can be
combined into composite excitations that move either in a straight line along
the $\hat{z}$ direction, or freely in the $xy$ plane at a given height $z$.
While our model draws inspiration from the toric code, we demonstrate that it
cannot be adiabatically connected to a layered toric code construction.
Additionally, we investigate the effects of imposing open boundary conditions
on our system. We find zero energy modes on the surfaces perpendicular to
either the $\hat{x}$ or $\hat{y}$ directions, and their absence on the surfaces
normal to $\hat{z}$. This result can be explained using the properties of the
two kinds of composite two-fracton mobile excitations.
",0,1,0,0,0,0
12954,Characterising exo-ringsystems around fast-rotating stars using the Rossiter-McLaughlin effect,"  Planetary rings produce a distinct shape distortion in transit lightcurves.
However, to accurately model such lightcurves the observations need to cover
the entire transit, especially ingress and egress, as well as an out-of-transit
baseline. Such observations can be challenging for long period planets, where
the transits may last for over a day. Planetary rings will also impact the
shape of absorption lines in the stellar spectrum, as the planet and rings
cover different parts of the rotating star (the Rossiter-McLaughlin effect).
These line-profile distortions depend on the size, structure, opacity,
obliquity and sky projected angle of the ring system. For slow rotating stars,
this mainly impacts the amplitude of the induced velocity shift, however, for
fast rotating stars the large velocity gradient across the star allows the line
distortion to be resolved, enabling direct determination of the ring
parameters. We demonstrate that by modeling these distortions we can recover
ring system parameters (sky-projected angle, obliquity and size) using only a
small part of the transit. Substructure in the rings, e.g. gaps, can be
recovered if the width of the features ($\delta W$) relative to the size of the
star is similar to the intrinsic velocity resolution (set by the width of the
local stellar profile, $\gamma$) relative to the stellar rotation velocity ($v$
sin$i$, i.e. $\delta W / R_* \gtrsim v$sin$i$/$\gamma$). This opens up a new
way to study the ring systems around planets with long orbital periods, where
observations of the full transit, covering the ingress and egress, are not
always feasible.
",0,1,0,0,0,0
8383,Achievable Rate Region of Non-Orthogonal Multiple Access Systems with Wireless Powered Decoder,"  Non-orthogonal multiple access (NOMA) is a candidate multiple access scheme
in 5G systems for the simultaneous access of tremendous number of wireless
nodes. On the other hand, RF-enabled wireless energy harvesting is a promising
technology for self-sustainable wireless nodes. In this paper, we consider a
NOMA system where the near user harvests energy from the strong radio signal to
power-on the information decoder. A generalized energy harvesting scheme is
proposed by combining the conventional time switching and power splitting
scheme. The achievable rate region of the proposed scheme is characterized
under both constant and dynamic decoding power consumption models. If the
decoding power is constant, the achievable rate region can be found by solving
two convex optimization subproblems, and the regions for two special cases:
time switching and power splitting, are characterized in closed-form. If the
decoding power is proportional to data rate, the achievable rate region can be
found by exhaustive search algorithm. Numerical results show that the
achievable rate region of the proposed generalized scheme is larger than those
of time switching scheme and power splitting scheme, and rate-dependent decoder
design helps to enlarge the achievable rate region.
",1,0,0,0,0,0
1002,Near-field coupling of gold plasmonic antennas for sub-100 nm magneto-thermal microscopy,"  The development of spintronic technology with increasingly dense, high-speed,
and complex devices will be accelerated by accessible microscopy techniques
capable of probing magnetic phenomena on picosecond time scales and at deeply
sub-micron length scales. A recently developed time-resolved magneto-thermal
microscope provides a path towards this goal if it is augmented with a
picosecond, nanoscale heat source. We theoretically study adiabatic
nanofocusing and near-field heat induction using conical gold plasmonic
antennas to generate sub-100 nm thermal gradients for time-resolved
magneto-thermal imaging. Finite element calculations of antenna-sample
interactions reveal focused electromagnetic loss profiles that are either
peaked directly under the antenna or are annular, depending on the sample's
conductivity, the antenna's apex radius, and the tip-sample separation. We find
that the thermal gradient is confined to 40 nm to 60 nm full width at half
maximum for realistic ranges of sample conductivity and apex radius. To
mitigate this variation, which is undesirable for microscopy, we investigate
the use of a platinum capping layer on top of the sample as a thermal
transduction layer to produce heat uniformly across different sample materials.
After determining the optimal capping layer thickness, we simulate the
evolution of the thermal gradient in the underlying sample layer, and find that
the temporal width is below 10 ps. These results lay a theoretical foundation
for nanoscale, time-resolved magneto-thermal imaging.
",0,1,0,0,0,0
5143,Multi-party Poisoning through Generalized $p$-Tampering,"  In a poisoning attack against a learning algorithm, an adversary tampers with
a fraction of the training data $T$ with the goal of increasing the
classification error of the constructed hypothesis/model over the final test
distribution. In the distributed setting, $T$ might be gathered gradually from
$m$ data providers $P_1,\dots,P_m$ who generate and submit their shares of $T$
in an online way.
In this work, we initiate a formal study of $(k,p)$-poisoning attacks in
which an adversary controls $k\in[n]$ of the parties, and even for each
corrupted party $P_i$, the adversary submits some poisoned data $T'_i$ on
behalf of $P_i$ that is still ""$(1-p)$-close"" to the correct data $T_i$ (e.g.,
$1-p$ fraction of $T'_i$ is still honestly generated). For $k=m$, this model
becomes the traditional notion of poisoning, and for $p=1$ it coincides with
the standard notion of corruption in multi-party computation.
We prove that if there is an initial constant error for the generated
hypothesis $h$, there is always a $(k,p)$-poisoning attacker who can decrease
the confidence of $h$ (to have a small error), or alternatively increase the
error of $h$, by $\Omega(p \cdot k/m)$. Our attacks can be implemented in
polynomial time given samples from the correct data, and they use no wrong
labels if the original distributions are not noisy.
At a technical level, we prove a general lemma about biasing bounded
functions $f(x_1,\dots,x_n)\in[0,1]$ through an attack model in which each
block $x_i$ might be controlled by an adversary with marginal probability $p$
in an online way. When the probabilities are independent, this coincides with
the model of $p$-tampering attacks, thus we call our model generalized
$p$-tampering. We prove the power of such attacks by incorporating ideas from
the context of coin-flipping attacks into the $p$-tampering model and
generalize the results in both of these areas.
",0,0,0,1,0,0
268,Scaling Law for Three-body Collisions in Identical Fermions with $p$-wave Interactions,"  We experimentally confirmed the threshold behavior and scattering length
scaling law of the three-body loss coefficients in an ultracold spin-polarized
gas of $^6$Li atoms near a $p$-wave Feshbach resonance. We measured the
three-body loss coefficients as functions of temperature and scattering volume,
and found that the threshold law and the scattering length scaling law hold in
limited temperature and magnetic field regions. We also found that the
breakdown of the scaling laws is due to the emergence of the effective-range
term. This work is an important first step toward full understanding of the
loss of identical fermions with $p$-wave interactions.
",0,1,0,0,0,0
11084,A Systematic Evaluation of Static API-Misuse Detectors,"  Application Programming Interfaces (APIs) often have usage constraints, such
as restrictions on call order or call conditions. API misuses, i.e., violations
of these constraints, may lead to software crashes, bugs, and vulnerabilities.
Though researchers developed many API-misuse detectors over the last two
decades, recent studies show that API misuses are still prevalent. Therefore,
we need to understand the capabilities and limitations of existing detectors in
order to advance the state of the art. In this paper, we present the first-ever
qualitative and quantitative evaluation that compares static API-misuse
detectors along the same dimensions, and with original author validation. To
accomplish this, we develop MUC, a classification of API misuses, and
MUBenchPipe, an automated benchmark for detector comparison, on top of our
misuse dataset, MUBench. Our results show that the capabilities of existing
detectors vary greatly and that existing detectors, though capable of detecting
misuses, suffer from extremely low precision and recall. A systematic
root-cause analysis reveals that, most importantly, detectors need to go beyond
the naive assumption that a deviation from the most-frequent usage corresponds
to a misuse and need to obtain additional usage examples to train their models.
We present possible directions towards more-powerful API-misuse detectors.
",1,0,0,0,0,0
2756,Recurrent Deep Embedding Networks for Genotype Clustering and Ethnicity Prediction,"  The understanding of variations in genome sequences assists us in identifying
people who are predisposed to common diseases, solving rare diseases, and
finding the corresponding population group of the individuals from a larger
population group. Although classical machine learning techniques allow
researchers to identify groups (i.e. clusters) of related variables, the
accuracy, and effectiveness of these methods diminish for large and
high-dimensional datasets such as the whole human genome. On the other hand,
deep neural network architectures (the core of deep learning) can better
exploit large-scale datasets to build complex models. In this paper, we use the
K-means clustering approach for scalable genomic data analysis aiming towards
clustering genotypic variants at the population scale. Finally, we train a deep
belief network (DBN) for predicting the geographic ethnicity. We used the
genotype data from the 1000 Genomes Project, which covers the result of genome
sequencing for 2504 individuals from 26 different ethnic origins and comprises
84 million variants. Our experimental results, with a focus on accuracy and
scalability, show the effectiveness and superiority compared to the
state-of-the-art.
",0,0,0,0,1,0
9063,Decentralized DC MicroGrid Monitoring and Optimization via Primary Control Perturbations,"  We treat the emerging power systems with direct current (DC) MicroGrids,
characterized with high penetration of power electronic converters. We rely on
the power electronics to propose a decentralized solution for autonomous
learning of and adaptation to the operating conditions of the DC Mirogrids; the
goal is to eliminate the need to rely on an external communication system for
such purpose. The solution works within the primary droop control loops and
uses only local bus voltage measurements. Each controller is able to estimate
(i) the generation capacities of power sources, (ii) the load demands, and
(iii) the conductances of the distribution lines. To define a well-conditioned
estimation problem, we employ decentralized strategy where the primary droop
controllers temporarily switch between operating points in a coordinated
manner, following amplitude-modulated training sequences. We study the use of
the estimator in a decentralized solution of the Optimal Economic Dispatch
problem. The evaluations confirm the usefulness of the proposed solution for
autonomous MicroGrid operation.
",1,0,0,0,0,0
8750,Divergence-free positive symmetric tensors and fluid dynamics,"  We consider $d\times d$ tensors $A(x)$ that are symmetric, positive
semi-definite, and whose row-divergence vanishes identically. We establish
sharp inequalities for the integral of $(\det A)^{\frac1{d-1}}$. We apply them
to models of compressible inviscid fluids: Euler equations, Euler--Fourier,
relativistic Euler, Boltzman, BGK, etc... We deduce an {\em a priori} estimate
for a new quantity, namely the space-time integral of $\rho^{\frac1n}p$, where
$\rho$ is the mass density, $p$ the pressure and $n$ the space dimension. For
kinetic models, the corresponding quantity generalizes Bony's functional.
",0,1,1,0,0,0
13609,Robust Stackelberg controllability for the Navier--Stokes equations,"  In this paper we deal with a robust Stackelberg strategy for the
Navier--Stokes system. The scheme is based in considering a robust control
problem for the ""follower control"" and its associated disturbance function.
Afterwards, we consider the notion of Stackelberg optimization (which is
associated to the ""leader control"") in order to deduce a local null
controllability result for the Navier--Stokes system.
",0,0,1,0,0,0
4085,Interpretable LSTMs For Whole-Brain Neuroimaging Analyses,"  The analysis of neuroimaging data poses several strong challenges, in
particular, due to its high dimensionality, its strong spatio-temporal
correlation and the comparably small sample sizes of the respective datasets.
To address these challenges, conventional decoding approaches such as the
searchlight reduce the complexity of the decoding problem by considering local
clusters of voxels only. Thereby, neglecting the distributed spatial patterns
of brain activity underlying many cognitive states. In this work, we introduce
the DLight framework, which overcomes these challenges by utilizing a long
short-term memory unit (LSTM) based deep neural network architecture to analyze
the spatial dependency structure of whole-brain fMRI data. In order to maintain
interpretability of the neuroimaging data, we adapt the layer-wise relevance
propagation (LRP) method. Thereby, we enable the neuroscientist user to study
the learned association of the LSTM between the data and the cognitive state of
the individual. We demonstrate the versatility of DLight by applying it to a
large fMRI dataset of the Human Connectome Project. We show that the decoding
performance of our method scales better with large datasets, and moreover
outperforms conventional decoding approaches, while still detecting
physiologically appropriate brain areas for the cognitive states classified. We
also demonstrate that DLight is able to detect these areas on several levels of
data granularity (i.e., group, subject, trial, time point).
",0,0,0,0,1,0
3587,Absence of long range order in the frustrated magnet SrDy$_2$O$_4$ due to trapped defects from a dimensionality crossover,"  Magnetic frustration and low dimensionality can prevent long range magnetic
order and lead to exotic correlated ground states. SrDy$_2$O$_4$ consists of
magnetic Dy$^{3+}$ ions forming magnetically frustrated zig-zag chains along
the c-axis and shows no long range order to temperatures as low as $T=60$ mK.
We carried out neutron scattering and AC magnetic susceptibility measurements
using powder and single crystals of SrDy$_2$O$_4$. Diffuse neutron scattering
indicates strong one-dimensional (1D) magnetic correlations along the chain
direction that can be qualitatively accounted for by the axial next-nearest
neighbour Ising (ANNNI) model with nearest-neighbor and next-nearest-neighbor
exchange $J_1=0.3$ meV and $J_2=0.2$ meV, respectively. Three-dimensional (3D)
correlations become important below $T^*\approx0.7$ K. At $T=60$ mK, the short
range correlations are characterized by a putative propagation vector
$\textbf{k}_{1/2}=(0,\frac{1}{2},\frac{1}{2})$. We argue that the absence of
long range order arises from the presence of slowly decaying 1D domain walls
that are trapped due to 3D correlations. This stabilizes a low-temperature
phase without long range magnetic order, but with well-ordered chain segments
separated by slowly-moving domain walls.
",0,1,0,0,0,0
11369,Sliced Wasserstein Distance for Learning Gaussian Mixture Models,"  Gaussian mixture models (GMM) are powerful parametric tools with many
applications in machine learning and computer vision. Expectation maximization
(EM) is the most popular algorithm for estimating the GMM parameters. However,
EM guarantees only convergence to a stationary point of the log-likelihood
function, which could be arbitrarily worse than the optimal solution. Inspired
by the relationship between the negative log-likelihood function and the
Kullback-Leibler (KL) divergence, we propose an alternative formulation for
estimating the GMM parameters using the sliced Wasserstein distance, which
gives rise to a new algorithm. Specifically, we propose minimizing the
sliced-Wasserstein distance between the mixture model and the data distribution
with respect to the GMM parameters. In contrast to the KL-divergence, the
energy landscape for the sliced-Wasserstein distance is more well-behaved and
therefore more suitable for a stochastic gradient descent scheme to obtain the
optimal GMM parameters. We show that our formulation results in parameter
estimates that are more robust to random initializations and demonstrate that
it can estimate high-dimensional data distributions more faithfully than the EM
algorithm.
",1,0,0,1,0,0
2727,Bagged Empirical Null p-values: A Method to Account for Model Uncertainty in Large Scale Inference,"  When conducting large scale inference, such as genome-wide association
studies or image analysis, nominal $p$-values are often adjusted to improve
control over the family-wise error rate (FWER). When the majority of tests are
null, procedures controlling the False discovery rate (Fdr) can be improved by
replacing the theoretical global null with its empirical estimate. However,
these other adjustment procedures remain sensitive to the working model
assumption. Here we propose two key ideas to improve inference in this space.
First, we propose $p$-values that are standardized to the empirical null
distribution (instead of the theoretical null). Second, we propose model
averaging $p$-values by bootstrap aggregation (Bagging) to account for model
uncertainty and selection procedures. The combination of these two key ideas
yields bagged empirical null $p$-values (BEN $p$-values) that often
dramatically alter the rank ordering of significant findings. Moreover, we find
that a multidimensional selection criteria based on BEN $p$-values and bagged
model fit statistics is more likely to yield reproducible findings. A
re-analysis of the famous Golub Leukemia data is presented to illustrate these
ideas. We uncovered new findings in these data, not detected previously, that
are backed by published bench work pre-dating the Gloub experiment. A
pseudo-simulation using the leukemia data is also presented to explore the
stability of this approach under broader conditions, and illustrates the
superiority of the BEN $p$-values compared to the other approaches.
",0,0,0,1,0,0
3803,Dimensionality reduction for acoustic vehicle classification with spectral embedding,"  We propose a method for recognizing moving vehicles, using data from roadside
audio sensors. This problem has applications ranging widely, from traffic
analysis to surveillance. We extract a frequency signature from the audio
signal using a short-time Fourier transform, and treat each time window as an
individual data point to be classified. By applying a spectral embedding, we
decrease the dimensionality of the data sufficiently for K-nearest neighbors to
provide accurate vehicle identification.
",1,0,0,1,0,0
18075,Categoricity and Universal Classes,"  Let $(\mathcal{K} ,\subseteq )$ be a universal class with
$LS(\mathcal{K})=\lambda$ categorical in regular $\kappa >\lambda^+$ with
arbitrarily large models, and let $\mathcal{K}^*$ be the class of all
$\mathcal{A}\in\mathcal{K}_{>\lambda}$ for which there is $\mathcal{B} \in
\mathcal{K}_{\ge\kappa}$ such that $\mathcal{A}\subseteq\mathcal{B}$. We prove
that $\mathcal{K}^*$ is categorical in every $\xi >\lambda^+$,
$\mathcal{K}_{\ge\beth_{(2^{\lambda^+})^+}} \subseteq \mathcal{K}^{*}$, and the
models of $\mathcal{K}^*_{>\lambda^+}$ are essentially vector spaces (or
trivial i.e. disintegrated).
",0,0,1,0,0,0
5190,A Fast Noniterative Algorithm for Compressive Sensing Using Binary Measurement Matrices,"  In this paper we present a new algorithm for compressive sensing that makes
use of binary measurement matrices and achieves exact recovery of ultra sparse
vectors, in a single pass and without any iterations. Due to its noniterative
nature, our algorithm is hundreds of times faster than $\ell_1$-norm
minimization, and methods based on expander graphs, both of which require
multiple iterations. Our algorithm can accommodate nearly sparse vectors, in
which case it recovers index set of the largest components, and can also
accommodate burst noise measurements. Compared to compressive sensing methods
that are guaranteed to achieve exact recovery of all sparse vectors, our method
requires fewer measurements However, methods that achieve statistical recovery,
that is, recovery of almost all but not all sparse vectors, can require fewer
measurements than our method.
",1,0,0,0,0,0
861,Common Knowledge in a Logic of Gossips,"  Gossip protocols aim at arriving, by means of point-to-point or group
communications, at a situation in which all the agents know each other secrets.
Recently a number of authors studied distributed epistemic gossip protocols.
These protocols use as guards formulas from a simple epistemic logic, which
makes their analysis and verification substantially easier.
We study here common knowledge in the context of such a logic. First, we
analyze when it can be reduced to iterated knowledge. Then we show that the
semantics and truth for formulas without nested common knowledge operator are
decidable. This implies that implementability, partial correctness and
termination of distributed epistemic gossip protocols that use non-nested
common knowledge operator is decidable, as well. Given that common knowledge is
equivalent to an infinite conjunction of nested knowledge, these results are
non-trivial generalizations of the corresponding decidability results for the
original epistemic logic, established in (Apt & Wojtczak, 2016).
K. R. Apt & D. Wojtczak (2016): On Decidability of a Logic of Gossips. In
Proc. of JELIA 2016, pp. 18-33, doi:10.1007/ 978-3-319-48758-8_2.
",1,0,0,0,0,0
13294,Fast Incremental SVDD Learning Algorithm with the Gaussian Kernel,"  Support vector data description (SVDD) is a machine learning technique that
is used for single-class classification and outlier detection. The idea of SVDD
is to find a set of support vectors that defines a boundary around data. When
dealing with online or large data, existing batch SVDD methods have to be rerun
in each iteration. We propose an incremental learning algorithm for SVDD that
uses the Gaussian kernel. This algorithm builds on the observation that all
support vectors on the boundary have the same distance to the center of sphere
in a higher-dimensional feature space as mapped by the Gaussian kernel
function. Each iteration involves only the existing support vectors and the new
data point. Moreover, the algorithm is based solely on matrix manipulations;
the support vectors and their corresponding Lagrange multiplier $\alpha_i$'s
are automatically selected and determined in each iteration. It can be seen
that the complexity of our algorithm in each iteration is only $O(k^2)$, where
$k$ is the number of support vectors. Experimental results on some real data
sets indicate that FISVDD demonstrates significant gains in efficiency with
almost no loss in either outlier detection accuracy or objective function
value.
",0,0,0,1,0,0
16116,Volumetric parametrization from a level set boundary representation with PHT Splines,"  A challenge in isogeometric analysis is constructing analysis-suitable
volumetric meshes which can accurately represent the geometry of a given
physical domain. In this paper, we propose a method to derive a spline-based
representation of a domain of interest from voxel-based data. We show an
efficient way to obtain a boundary representation of the domain by a level-set
function. Then, we use the geometric information from the boundary (the normal
vectors and curvature) to construct a matching C1 representation with
hierarchical cubic splines. The approximation is done by a single template and
linear transformations (scaling, translations and rotations) without the need
for solving an optimization problem. We illustrate our method with several
examples in two and three dimensions, and show good performance on some
standard benchmark test problems.
",1,0,0,0,0,0
10864,Parsimonious Data: How a single Facebook like predicts voting behaviour in multiparty systems,"  Recently, two influential PNAS papers have shown how our preferences for
'Hello Kitty' and 'Harley Davidson', obtained through Facebook likes, can
accurately predict details about our personality, religiosity, political
attitude and sexual orientation (Konsinski et al. 2013; Youyou et al 2015). In
this paper, we make the claim that though the wide variety of Facebook likes
might predict such personal traits, even more accurate and generalizable
results can be reached through applying a contexts-specific, parsimonious data
strategy. We built this claim by predicting present day voter intention based
solely on likes directed toward posts from political actors. Combining the
online and offline, we join a subsample of surveyed respondents to their public
Facebook activity and apply machine learning classifiers to explore the link
between their political liking behaviour and actual voting intention. Through
this work, we show how even a single well-chosen Facebook like, can reveal as
much about our political voter intention as hundreds of random likes. Further,
by including the entire political like history of the respondents, our model
reaches prediction accuracies above previous multiparty studies (60-70%). We
conclude the paper by discussing how a parsimonious data strategy applied, with
some limitations, allow us to generalize our findings to the 1,4 million Danes
with at least one political like and even to other political multiparty
systems.
",1,0,0,0,0,0
6887,Marginally compact fractal trees with semiflexibility,"  We study marginally compact macromolecular trees that are created by means of
two different fractal generators. In doing so, we assume Gaussian statistics
for the vectors connecting nodes of the trees. Moreover, we introduce bond-bond
correlations that make the trees locally semiflexible. The symmetry of the
structures allows an iterative construction of full sets of eigenmodes
(notwithstanding the additional interactions that are present due to
semiflexibility constraints), enabling us to get physical insights about the
trees' behavior and to consider larger structures. Due to the local stiffness
the self-contact density gets drastically reduced.
",0,1,0,0,0,0
4573,Magnetic Actuation and Feedback Cooling of a Cavity Optomechanical Torque Sensor,"  We demonstrate the integration of a mesoscopic ferromagnetic needle with a
cavity optomechanical torsional resonator, and its use for quantitative
determination of the needle's magnetic properties, as well as amplification and
cooling of the resonator motion. With this system we measure torques as small
as 32 zNm, corresponding to sensing an external magnetic field of 0.12 A/m (150
nT). Furthermore, we are able to extract the magnetization (1710 kA/m) of the
magnetic sample, not known a priori, demonstrating this system's potential for
studies of nanomagnetism. Finally, we show that we can magnetically drive the
torsional resonator into regenerative oscillations, and dampen its mechanical
mode temperature from room temperature to 11.6 K, without sacrificing torque
sensitivity.
",0,1,0,0,0,0
17257,Hubble PanCET: An isothermal day-side atmosphere for the bloated gas-giant HAT-P-32Ab,"  We present a thermal emission spectrum of the bloated hot Jupiter HAT-P-32Ab
from a single eclipse observation made in spatial scan mode with the Wide Field
Camera 3 (WFC3) aboard the Hubble Space Telescope (HST). The spectrum covers
the wavelength regime from 1.123 to 1.644 microns which is binned into 14
eclipse depths measured to an averaged precision of 104 parts-per million. The
spectrum is unaffected by a dilution from the close M-dwarf companion
HAT-P-32B, which was fully resolved. We complemented our spectrum with
literature results and performed a comparative forward and retrieval analysis
with the 1D radiative-convective ATMO model. Assuming solar abundance of the
planet atmosphere, we find that the measured spectrum can best be explained by
the spectrum of a blackbody isothermal atmosphere with Tp = 1995 +/- 17K, but
can equally-well be described by a spectrum with modest thermal inversion. The
retrieved spectrum suggests emission from VO at the WFC3 wavelengths and no
evidence of the 1.4 micron water feature. The emission models with temperature
profiles decreasing with height are rejected at a high confidence. An
isothermal or inverted spectrum can imply a clear atmosphere with an absorber,
a dusty cloud deck or a combination of both. We find that the planet can have
continuum of values for the albedo and recirculation, ranging from high albedo
and poor recirculation to low albedo and efficient recirculation. Optical
spectroscopy of the planet's day-side or thermal emission phase curves can
potentially resolve the current albedo with recirculation degeneracy.
",0,1,0,0,0,0
9532,A likely detection of a local interplanetary dust cloud passing near the Earth in the AKARI mid-infrared all-sky map,"  Context. We are creating the AKARI mid-infrared all-sky diffuse maps. Through
a foreground removal of the zodiacal emission, we serendipitously detected a
bright residual component whose angular size is about 50 x 20 deg. at a
wavelength of 9 micron. Aims. We investigate the origin and the physical
properties of the residual component. Methods. We measured the surface
brightness of the residual component in the AKARI mid-infrared all-sky maps.
Results. The residual component was significantly detected only in 2007
January, even though the same region was observed in 2006 July and 2007 July,
which shows that it is not due to the Galactic emission. We suggest that this
may be a small cloud passing near the Earth. By comparing the observed
intensity ratio of I_9um/I_18um with the expected intensity ratio assuming
thermal equilibrium of dust grains at 1 AU for various dust compositions and
sizes, we find that dust grains in the moving cloud are likely to be much
smaller than typical grains that produce the bulk of the zodiacal light.
Conclusions. Considering the observed date and position, it is likely that it
originates in the solar coronal mass ejection (CME) which took place on 2007
January 25.
",0,1,0,0,0,0
6585,From jamming to collective cell migration through a boundary induced transition,"  Cell monolayers provide an interesting example of active matter, exhibiting a
phase transition from a flowing to jammed state as they age. Here we report
experiments and numerical simulations illustrating how a jammed cellular layer
rapidly reverts to a flowing state after a wound. Quantitative comparison
between experiments and simulations shows that cells change their
self-propulsion and alignement strength so that the system crosses a phase
transition line, which we characterize by finite-size scaling in an active
particle model. This wound-induced unjamming transition is found to occur
generically in epithelial, endothelial and cancer cells.
",0,0,0,0,1,0
18790,Herschel-PACS photometry of faint stars,"  Our aims are to determine flux densities and their photometric accuracy for a
set of seventeen stars that range in flux from intermediately bright (<2.5 Jy)
to faint (>5 mJy) in the far-infrared (FIR). We also aim to derive
signal-to-noise dependence with flux and time, and compare the results with
predictions from the Herschel exposure-time calculation tool. The PACS faint
star sample has allowed a comprehensive sensitivity assessment of the PACS
photometer. Accurate photometry allows us to establish a set of five FIR
primary standard candidates, namely alpha Ari, epsilon Lep, omega,Cap, HD41047
and 42Dra, which are 2 -- 20 times fainter than the faintest PACS fiducial
standard (gamma Dra) with absolute accuracy of <6%. For three of these primary
standard candidates, essential stellar parameters are known, meaning that a
dedicated flux model code may be run.
",0,1,0,0,0,0
20640,Liveness Verification and Synthesis: New Algorithms for Recursive Programs,"  We consider the problems of liveness verification and liveness synthesis for
recursive programs. The liveness verification problem (LVP) is to decide
whether a given omega-context-free language is contained in a given
omega-regular language. The liveness synthesis problem (LSP) is to compute a
strategy so that a given omega-context-free game, when played along the
strategy, is guaranteed to derive a word in a given omega-regular language. The
problems are known to be EXPTIME-complete and EXPTIME-complete, respectively.
Our contributions are new algorithms with optimal time complexity. For LVP, we
generalize recent lasso-finding algorithms (also known as Ramsey-based
algorithms) from finite to recursive programs. For LSP, we generalize a recent
summary-based algorithm from finite to infinite words. Lasso finding and
summaries have proven to be efficient in a number of implementations for the
finite state and finite word setting.
",1,0,0,0,0,0
7543,"COLOSSUS: A python toolkit for cosmology, large-scale structure, and dark matter halos","  This paper introduces Colossus, a public, open-source python package for
calculations related to cosmology, the large-scale structure (LSS) of matter in
the universe, and the properties of dark matter halos. The code is designed to
be fast and easy to use, with a coherent, well-documented user interface. The
cosmology module implements Friedman-Lemaitre-Robertson-Walker cosmologies
including curvature, relativistic species, and different dark energy equations
of state, and provides fast computations of the linear matter power spectrum,
variance, and correlation function. The LSS module is concerned with the
properties of peaks in Gaussian random fields and halos in a statistical sense,
including their peak height, peak curvature, halo bias, and mass function. The
halo module deals with spherical overdensity radii and masses, density
profiles, concentration, and the splashback radius. To facilitate the rapid
exploration of these quantities, Colossus implements more than 40 different
fitting functions from the literature. I discuss the core routines in detail,
with particular emphasis on their accuracy. Colossus is available at
bitbucket.org/bdiemer/colossus.
",0,1,0,0,0,0
17840,Subset Synchronization in Monotonic Automata,"  We study extremal and algorithmic questions of subset and careful
synchronization in monotonic automata. We show that several synchronization
problems that are hard in general automata can be solved in polynomial time in
monotonic automata, even without knowing a linear order of the states preserved
by the transitions. We provide asymptotically tight bounds on the maximum
length of a shortest word synchronizing a subset of states in a monotonic
automaton and a shortest word carefully synchronizing a partial monotonic
automaton. We provide a complexity framework for dealing with problems for
monotonic weakly acyclic automata over a three-letter alphabet, and use it to
prove NP-completeness and inapproximability of problems such as {\sc Finite
Automata Intersection} and the problem of computing the rank of a subset of
states in this class. We also show that checking whether a monotonic partial
automaton over a four-letter alphabet is carefully synchronizing is NP-hard.
Finally, we give a simple necessary and sufficient condition when a strongly
connected digraph with a selected subset of vertices can be transformed into a
deterministic automaton where the corresponding subset of states is
synchronizing.
",1,0,0,0,0,0
2354,The Muon g-2 experiment at Fermilab,"  The upcoming Fermilab E989 experiment will measure the muon anomalous
magnetic moment $a_{\mu}$ . This measurement is motivated by the previous
measurement performed in 2001 by the BNL E821 experiment that reported a 3-4
standard deviation discrepancy between the measured value and the Standard
Model prediction. The new measurement at Fermilab aims to improve the precision
by a factor of four reducing the total uncertainty from 540 parts per billion
(BNL E821) to 140 parts per billion (Fermilab E989). This paper gives the
status of the experiment.
",0,1,0,0,0,0
10446,Intra-Cluster Autonomous Coverage Optimization For Dense LTE-A Networks,"  Self Organizing Networks (SONs) are considered as vital deployments towards
upcoming dense cellular networks. From a mobile carrier point of view,
continuous coverage optimization is critical for better user perceptions. The
majority of SON contributions introduce novel algorithms that optimize specific
performance metrics. However, they require extensive processing delays and
advanced knowledge of network statistics that may not be available. In this
work, a progressive Autonomous Coverage Optimization (ACO) method combined with
adaptive cell dimensioning is proposed. The proposed method emphasizes the fact
that the effective cell coverage is a variant on actual user distributions. ACO
algorithm builds a generic Space-Time virtual coverage map per cell to detect
coverage holes in addition to limited or extended coverage conditions.
Progressive levels of optimization are followed to timely resolve coverage
issues with maintaining optimization stability. Proposed ACO is verified under
both simulations and practical deployment in a pilot cluster for a worldwide
mobile carrier. Key Performance Indicators show that proposed ACO method
significantly enhances system coverage and performance.
",1,0,0,0,0,0
4608,Autonomous Urban Localization and Navigation with Limited Information,"  Urban environments offer a challenging scenario for autonomous driving.
Globally localizing information, such as a GPS signal, can be unreliable due to
signal shadowing and multipath errors. Detailed a priori maps of the
environment with sufficient information for autonomous navigation typically
require driving the area multiple times to collect large amounts of data,
substantial post-processing on that data to obtain the map, and then
maintaining updates on the map as the environment changes. This paper addresses
the issue of autonomous driving in an urban environment by investigating
algorithms and an architecture to enable fully functional autonomous driving
with limited information. An algorithm to autonomously navigate urban roadways
with little to no reliance on an a priori map or GPS is developed. Localization
is performed with an extended Kalman filter with odometry, compass, and sparse
landmark measurement updates. Navigation is accomplished by a compass-based
navigation control law. Key results from Monte Carlo studies show success rates
of urban navigation under different environmental conditions. Experiments
validate the simulated results and demonstrate that, for given test conditions,
an expected range can be found for a given success rate.
",1,0,0,0,0,0
3381,Strichartz estimates for non-degenerate Schrödinger equations,"  We consider Schrödinger equation with a non-degenerate metric on the
Euclidean space. We study local in time Strichartz estimates for the
Schrödinger equation without loss of derivatives including the endpoint case.
In contrast to the Riemannian metric case, we need the additional assumptions
for the well-posedness of our Schrödinger equation and for proving Strichartz
estimates without loss.
",0,0,1,0,0,0
8690,Online Learning Without Prior Information,"  The vast majority of optimization and online learning algorithms today
require some prior information about the data (often in the form of bounds on
gradients or on the optimal parameter value). When this information is not
available, these algorithms require laborious manual tuning of various
hyperparameters, motivating the search for algorithms that can adapt to the
data with no prior information. We describe a frontier of new lower bounds on
the performance of such algorithms, reflecting a tradeoff between a term that
depends on the optimal parameter value and a term that depends on the
gradients' rate of growth. Further, we construct a family of algorithms whose
performance matches any desired point on this frontier, which no previous
algorithm reaches.
",1,0,0,1,0,0
19063,Complete DFM Model for High-Performance Computing SoCs with Guard Ring and Dummy Fill Effect,"  For nanotechnology, the semiconductor device is scaled down dramatically with
additional strain engineering for device enhancement, the overall device
characteristic is no longer dominated by the device size but also circuit
layout. The higher order layout effects, such as well proximity effect (WPE),
oxide spacing effect (OSE) and poly spacing effect (PSE), play an important
role for the device performance, it is critical to understand Design for
Manufacturability (DFM) impacts with various layout topology toward the overall
circuit performance. Currently, the layout effects (WPE, OSE and PSE) are
validated through digital standard cell and analog differential pair test
structure. However, two analog layout structures: the guard ring and dummy fill
impact are not well studied yet, then, this paper describes the current mirror
test circuit to examine the guard ring and dummy fills DFM impacts using TSMC
28nm HPM process.
",1,0,0,0,0,0
13917,Topological orders of strongly interacting particles,"  We investigate the self-organization of strongly interacting particles
confined in 1D and 2D. We consider hardcore bosons in spinless Hubbard lattice
models with short range interactions. We show that, many-body orders with
topological characteristics emerge, at different energy bands separated by
large gaps. These topological orders manifest in the way the particles organize
in real space to form states with different energy. Each of these states
contains topological defects/condensations whose Euler characteristic can be
used as a topological number to categorize states belonging to the same energy
band. We provide analytical formulas for this topological number and the full
energy spectrum of the system for both sparsely and densely filled systems.
Furthermore, we discuss the connection with the Gauss-Bonnet theorem of
differential geometry, by using the curvature generated in real space by the
particle structures. Our result is a demonstration of how topological orders
can arise in strongly interacting many-body systems with simple underlying
rules, without considering the spin, long-range microscopic interactions, or
external fields.
",0,1,0,0,0,0
18736,On utility maximization without passing by the dual problem,"  We treat utility maximization from terminal wealth for an agent with utility
function $U:\mathbb{R}\to\mathbb{R}$ who dynamically invests in a
continuous-time financial market and receives a possibly unbounded random
endowment. We prove the existence of an optimal investment without introducing
the associated dual problem. We rely on a recent result of Orlicz space theory,
due to Delbaen and Owari which leads to a simple and transparent proof.
Our results apply to non-smooth utilities and even strict concavity can be
relaxed. We can handle certain random endowments with non-hedgeable risks,
complementing earlier papers. Constraints on the terminal wealth can also be
incorporated.
As examples, we treat frictionless markets with finitely many assets and
large financial markets.
",0,0,1,0,0,0
13010,Learning retrosynthetic planning through self-play,"  The problem of retrosynthetic planning can be framed as one player game, in
which the chemist (or a computer program) works backwards from a molecular
target to simpler starting materials though a series of choices regarding which
reactions to perform. This game is challenging as the combinatorial space of
possible choices is astronomical, and the value of each choice remains
uncertain until the synthesis plan is completed and its cost evaluated. Here,
we address this problem using deep reinforcement learning to identify policies
that make (near) optimal reaction choices during each step of retrosynthetic
planning. Using simulated experience or self-play, we train neural networks to
estimate the expected synthesis cost or value of any given molecule based on a
representation of its molecular structure. We show that learned policies based
on this value network outperform heuristic approaches in synthesizing
unfamiliar molecules from available starting materials using the fewest number
of reactions. We discuss how the learned policies described here can be
incorporated into existing synthesis planning tools and how they can be adapted
to changes in the synthesis cost objective or material availability.
",1,0,0,1,0,0
14594,Feldman-Katok pseudometric and the GIKN construction of nonhyperbolic ergodic measures,"  The GIKN construction was introduced by Gorodetski, Ilyashenko, Kleptsyn, and
Nalsky in [Functional Analysis and its Applications, 39 (2005), 21--30]. It
gives a nonhyperbolic ergodic measure which is a weak$^*$ limit of a special
sequence of measures supported on periodic orbits. This method was later
adapted by numerous authors and provided examples of nonhyperbolic invariant
measures in various settings. We prove that the result of the GIKN construction
is always a loosely Kronecker measure in the sense of Ornstein, Rudolph, and
Weiss (equivalently, standard measure in the sense of Katok, another name is
loosely Bernoulli measure with zero entropy). For a proof we introduce and
study the Feldman-Katok pseudometric $\bar{F_{K}}$. The pseudodistance
$\bar{F_{K}}$ is a topological counterpart of the $\bar f$ metric for
finite-state stationary stochastic processes introduced by Feldman and,
independently, by Katok, later developed by Ornstein, Rudolph, and Weiss. We
show that every measure given by the GIKN construction is the
$\bar{F_{K}}$-limit of a sequence of periodic measures. On the other hand we
prove that a measure which is the $\bar{F_{K}}$-limit of a sequence of ergodic
measures is ergodic and its entropy is smaller or equal than the lower limit of
entropies of measures in the sequence. Furthermore we demonstrate that
$\bar{F_{K}}$-Cauchy sequence of periodic measures tends in the weak$^*$
topology either to a periodic measure or to a loosely Kronecker measure.
",0,0,1,0,0,0
1506,"Tick: a Python library for statistical learning, with a particular emphasis on time-dependent modelling","  Tick is a statistical learning library for Python~3, with a particular
emphasis on time-dependent models, such as point processes, and tools for
generalized linear models and survival analysis. The core of the library is an
optimization module providing model computational classes, solvers and proximal
operators for regularization. tick relies on a C++ implementation and
state-of-the-art optimization algorithms to provide very fast computations in a
single node multi-core setting. Source code and documentation can be downloaded
from this https URL
",0,0,0,1,0,0
10449,Embedded tori with prescribed mean curvature,"  We construct a sequence of compact, oriented, embedded, two-dimensional
surfaces of genus one into Euclidean 3-space with prescribed, almost constant,
mean curvature of the form $H(X)=1+{A}{|X|^{-\gamma}}$ for $|X|$ large, when
$A<0$ and $\gamma\in(0,2)$. Such surfaces are close to sections of unduloids
with small necksize, folded along circumferences centered at the origin and
with larger and larger radii. The construction involves a deep study of the
corresponding Jacobi operators, an application of the Lyapunov-Schmidt
reduction method and some variational argument.
",0,0,1,0,0,0
18543,On Nontrivial Zeros of Riemann Zeta Function,"  Let {\Xi} be a function relating to the Riemann zeta function with . In this
paper, we construct a function containing and {\Xi} , and prove that satisfies
a nonadjoint boundary value problem to a nonsingular differential equation if
is any nontrivial zero of {\Xi} . Inspecting properties of and using known
results of nontrivial zeros of , we derive that nontrivial zeros of all have
real part equal to , which concludes that Riemann Hypothesis is true.
",0,0,1,0,0,0
10487,The PSLQ Algorithm for Empirical Data,"  The celebrated integer relation finding algorithm PSLQ has been successfully
used in many applications. PSLQ was only analyzed theoretically for exact input
data, however, when the input data are irrational numbers, they must be
approximate ones due to the finite precision of the computer. When the
algorithm takes empirical data (inexact data with error bounded) instead of
exact real numbers as its input, how do we theoretically ensure the output of
the algorithm to be an exact integer relation?
In this paper, we investigate the PSLQ algorithm for empirical data as its
input. Firstly, we give a termination condition for this case. Secondly, we
analyze a perturbation on the hyperplane matrix constructed from the input data
and hence disclose a relationship between the accuracy of the input data and
the output quality (an upper bound on the absolute value of the inner product
of the exact data and the computed integer relation), which naturally leads to
an error control strategy for PSLQ. Further, we analyze the complexity bound of
the PSLQ algorithm for empirical data. Examples on transcendental numbers and
algebraic numbers show the meaningfulness of our error control strategy.
",1,0,1,0,0,0
13975,Entanglement and entropy production in coupled single-mode Bose-Einstein condensates,"  We investigate the time evolution of the entanglement entropy of coupled
single-mode Bose-Einstein condensates in a double well potential at $T=0$
temperature, by combining numerical results with analytical approximations. We
find that the coherent oscillations of the condensates result in entropy
oscillations on the top of a linear entropy generation at short time scales.
Due to dephasing, the entropy eventually saturates to a stationary value, in
spite of the lack of equilibration. We show that this long time limit of the
entropy reflects the semiclassical dynamics of the system, revealing the
self-trapping phase transition of the condensates at large interaction strength
by a sudden entropy jump. We compare the stationary limit of the entropy to the
prediction of a classical microcanonical ensemble, and find surprisingly good
agreement in spite of the non-equilibrium state of the system. Our predictions
should be experimentally observable on a Bose-Einstein condensate in a double
well potential or on a two-component condensate with inter-state coupling.
",0,1,0,0,0,0
4247,DNA translocation through alpha-haemolysin nano-pores with potential application to macromolecular data storage,"  Digital information can be encoded in the building-block sequence of
macromolecules, such as RNA and single-stranded DNA. Methods of ""writing"" and
""reading"" macromolecular strands are currently available, but they are slow and
expensive. In an ideal molecular data storage system, routine operations such
as write, read, erase, store, and transfer must be done reliably and at high
speed within an integrated chip. As a first step toward demonstrating the
feasibility of this concept, we report preliminary results of DNA readout
experiments conducted in miniaturized chambers that are scalable to even
smaller dimensions. We show that translocation of a single-stranded DNA
molecule (consisting of 50 adenosine bases followed by 100 cytosine bases)
through an ion-channel yields a characteristic signal that is attributable to
the 2-segment structure of the molecule. We also examine the dependence of the
translocation rate and speed on the adjustable parameters of the experiment.
",1,1,0,0,0,0
13065,Parameter and State Estimation in Queues and Related Stochastic Models: A Bibliography,"  This is an annotated bibliography on estimation and inference results for
queues and related stochastic models. The purpose of this document is to
collect and categorise works in the field, allowing for researchers and
practitioners to explore the various types of results that exist. This
bibliography attempts to include all known works that satisfy both of these
requirements: -Works that deal with queueing models. -Works that contain
contributions related to the methodology of parameter estimation, state
estimation, hypothesis testing, confidence interval and/or actual datasets of
application areas. Our attempt is to make this bibliography exhaustive, yet
there are possibly some papers that we have missed. As it is updated
continuously, additions and comments are welcomed. The sections below
categorise the works based on several categories. A single paper may appear in
several categories simultaneously. The final section lists all works in
chronological order along with short descriptions of the contributions. This
bibliography is maintained at
this http URL and may be cited as such.
We welcome additions and corrections.
",1,0,1,1,0,0
13048,A latent spatial factor approach for synthesizing opioid associated deaths and treatment admissions in Ohio counties,"  Background: Opioid misuse is a major public health issue in the United States
and in particular Ohio. However, the burden of the epidemic is challenging to
quantify as public health surveillance measures capture different aspects of
the problem. Here we synthesize county-level death and treatment counts to
compare the relative burden across counties and assess associations with social
environmental covariates. Methods: We construct a generalized spatial factor
model to jointly model death and treatment rates for each county. For each
outcome, we specify a spatial rates parameterization for a Poisson regression
model with spatially varying factor loadings. We use a conditional
autoregressive model to account for spatial dependence within a Bayesian
framework. Results: The estimated spatial factor was highest in the southern
and southwestern counties of the state, representing a higher burden of the
opioid epidemic. We found that relatively high rates of treatment contributed
to the factor in the southern part of the state; whereas, relatively higher
rates of death contributed in the southwest. The estimated factor was also
positively associated with the proportion of residents aged 18-64 on disability
and negatively associated with the proportion of residents reporting white
race. Conclusions: We synthesized the information in the opioid associated
death and treatment counts through a spatial factor model to estimate a latent
factor representing the consensus between the two surveillance measures. We
believe this framework provides a coherent approach to describe the epidemic
while leveraging information from multiple surveillance measures.
",0,0,0,1,0,0
16845,Some Ageing Properties of Dynamic Additive Mean Residual Life Model,"  Although proportional hazard rate model is a very popular model to analyze
failure time data, sometimes it becomes important to study the additive hazard
rate model. Again, sometimes the concept of the hazard rate function is
abstract, in comparison to the concept of mean residual life function. A new
model called `dynamic additive mean residual life model' where the covariates
are time-dependent has been defined in the literature. Here we study the
closure properties of the model for different positive and negative ageing
classes under certain condition(s). Quite a few examples are presented to
illustrate different properties of the model.
",0,0,1,1,0,0
4867,High quality factor manganese-doped aluminum lumped-element kinetic inductance detectors sensitive to frequencies below 100 GHz,"  Aluminum lumped-element kinetic inductance detectors (LEKIDs) sensitive to
millimeter-wave photons have been shown to exhibit high quality factors, making
them highly sensitive and multiplexable. The superconducting gap of aluminum
limits aluminum LEKIDs to photon frequencies above 100 GHz. Manganese-doped
aluminum (Al-Mn) has a tunable critical temperature and could therefore be an
attractive material for LEKIDs sensitive to frequencies below 100 GHz if the
internal quality factor remains sufficiently high when manganese is added to
the film. To investigate, we measured some of the key properties of Al-Mn
LEKIDs. A prototype eight-element LEKID array was fabricated using a 40 nm
thick film of Al-Mn deposited on a 500 {\mu}m thick high-resistivity,
float-zone silicon substrate. The manganese content was 900 ppm, the measured
$T_c = 694\pm1$ mK, and the resonance frequencies were near 150 MHz. Using
measurements of the forward scattering parameter $S_{21}$ at various bath
temperatures between 65 and 250 mK, we determined that the Al-Mn LEKIDs we
fabricated have internal quality factors greater than $2 \times 10^5$, which is
high enough for millimeter-wave astrophysical observations. In the dark
conditions under which these devices were measured, the fractional frequency
noise spectrum shows a shallow slope that depends on bath temperature and probe
tone amplitude, which could be two-level system noise. The anticipated white
photon noise should dominate this level of low-frequency noise when the
detectors are illuminated with millimeter-waves in future measurements. The
LEKIDs responded to light pulses from a 1550 nm light-emitting diode, and we
used these light pulses to determine that the quasiparticle lifetime is 60
{\mu}s.
",0,1,0,0,0,0
14373,Simultaneous Modeling of Multiple Complications for Risk Profiling in Diabetes Care,"  Type 2 diabetes mellitus (T2DM) is a chronic disease that often results in
multiple complications. Risk prediction and profiling of T2DM complications is
critical for healthcare professionals to design personalized treatment plans
for patients in diabetes care for improved outcomes. In this paper, we study
the risk of developing complications after the initial T2DM diagnosis from
longitudinal patient records. We propose a novel multi-task learning approach
to simultaneously model multiple complications where each task corresponds to
the risk modeling of one complication. Specifically, the proposed method
strategically captures the relationships (1) between the risks of multiple T2DM
complications, (2) between the different risk factors, and (3) between the risk
factor selection patterns. The method uses coefficient shrinkage to identify an
informative subset of risk factors from high-dimensional data, and uses a
hierarchical Bayesian framework to allow domain knowledge to be incorporated as
priors. The proposed method is favorable for healthcare applications because in
additional to improved prediction performance, relationships among the
different risks and risk factors are also identified. Extensive experimental
results on a large electronic medical claims database show that the proposed
method outperforms state-of-the-art models by a significant margin.
Furthermore, we show that the risk associations learned and the risk factors
identified lead to meaningful clinical insights.
",0,0,0,1,0,0
2632,"HPD-invariance of the Tate, Beilinson and Parshin conjectures","  We prove that the Tate, Beilinson and Parshin conjectures are invariant under
Homological Projective Duality (=HPD). As an application, we obtain a proof of
these celebrated conjectures (as well as of the strong form of the Tate
conjecture) in the new cases of linear sections of determinantal varieties and
complete intersections of quadrics. Furthermore, we extend the original
conjectures of Tate, Beilinson and Parshin from schemes to stacks and prove
these extended conjectures for certain low-dimensional global orbifolds.
",0,0,1,0,0,0
4537,Landau Damping of Beam Instabilities by Electron Lenses,"  Modern and future particle accelerators employ increasingly higher intensity
and brighter beams of charged particles and become operationally limited by
coherent beam instabilities. Usual methods to control the instabilities, such
as octupole magnets, beam feedback dampers and use of chromatic effects, become
less effective and insufficient. We show that, in contrast, Lorentz forces of a
low-energy, a magnetically stabilized electron beam, or ""electron lens"", easily
introduces transverse nonlinear focusing sufficient for Landau damping of
transverse beam instabilities in accelerators. It is also important that,
unlike other nonlinear elements, the electron lens provides the frequency
spread mainly at the beam core, thus allowing much higher frequency spread
without lifetime degradation. For the parameters of the Future Circular
Collider, a single conventional electron lens a few meters long would provide
stabilization superior to tens of thousands of superconducting octupole
magnets.
",0,1,0,0,0,0
7532,Bayesian Network Regularized Regression for Modeling Urban Crime Occurrences,"  This paper considers the problem of statistical inference and prediction for
processes defined on networks. We assume that the network is known and measures
similarity, and our goal is to learn about an attribute associated with its
vertices. Classical regression methods are not immediately applicable to this
setting, as we would like our model to incorporate information from both
network structure and pertinent covariates. Our proposed model consists of a
generalized linear model with vertex indexed predictors and a basis expansion
of their coefficients, allowing the coefficients to vary over the network. We
employ a regularization procedure, cast as a prior distribution on the
regression coefficients under a Bayesian setup, so that the predicted responses
vary smoothly according to the topology of the network. We motivate the need
for this model by examining occurrences of residential burglary in Boston,
Massachusetts. Noting that crime rates are not spatially homogeneous, and that
the rates appear to vary sharply across regions in the city, we construct a
hierarchical model that addresses these issues and gives insight into spatial
patterns of crime occurrences. Furthermore, we examine efficient
expectation-maximization fitting algorithms and provide
computationally-friendly methods for eliciting hyper-prior parameters.
",0,0,0,1,0,0
5716,Using Big Data Technologies for HEP Analysis,"  The HEP community is approaching an era were the excellent performances of
the particle accelerators in delivering collision at high rate will force the
experiments to record a large amount of information. The growing size of the
datasets could potentially become a limiting factor in the capability to
produce scientific results timely and efficiently. Recently, new technologies
and new approaches have been developed in industry to answer to the necessity
to retrieve information as quickly as possible to analyze PB and EB datasets.
Providing the scientists with these modern computing tools will lead to
rethinking the principles of data analysis in HEP, making the overall
scientific process faster and smoother.
In this paper, we are presenting the latest developments and the most recent
results on the usage of Apache Spark for HEP analysis. The study aims at
evaluating the efficiency of the application of the new tools both
quantitatively, by measuring the performances, and qualitatively, focusing on
the user experience. The first goal is achieved by developing a data reduction
facility: working together with CERN Openlab and Intel, CMS replicates a real
physics search using Spark-based technologies, with the ambition of reducing 1
PB of public data in 5 hours, collected by the CMS experiment, to 1 TB of data
in a format suitable for physics analysis.
The second goal is achieved by implementing multiple physics use-cases in
Apache Spark using as input preprocessed datasets derived from official CMS
data and simulation. By performing different end-analyses up to the publication
plots on different hardware, feasibility, usability and portability are
compared to the ones of a traditional ROOT-based workflow.
",1,0,0,0,0,0
11838,Sorting Phenomena in a Mathematical Model For Two Mutually Attracting/Repelling Species,"  Macroscopic models for systems involving diffusion, short-range repulsion,
and long-range attraction have been studied extensively in the last decades. In
this paper we extend the analysis to a system for two species interacting with
each other according to different inner- and intra-species attractions. Under
suitable conditions on this self- and crosswise attraction an interesting
effect can be observed, namely phase separation into neighbouring regions, each
of which contains only one of the species. We prove that the intersection of
the support of the stationary solutions of the continuum model for the two
species has zero Lebesgue measure, while the support of the sum of the two
densities is simply connected.
Preliminary results indicate the existence of phase separation, i.e. spatial
sorting of the different species. A detailed analysis in one spatial dimension
follows. The existence and shape of segregated stationary solutions is shown
via the Krein-Rutman theorem. Moreover, for small repulsion/nonlinear
diffusion, also uniqueness of these stationary states is proved.
",0,0,1,0,0,0
13839,"Nearest-Neighbor Sample Compression: Efficiency, Consistency, Infinite Dimensions","  We examine the Bayes-consistency of a recently proposed
1-nearest-neighbor-based multiclass learning algorithm. This algorithm is
derived from sample compression bounds and enjoys the statistical advantages of
tight, fully empirical generalization bounds, as well as the algorithmic
advantages of a faster runtime and memory savings. We prove that this algorithm
is strongly Bayes-consistent in metric spaces with finite doubling dimension
--- the first consistency result for an efficient nearest-neighbor sample
compression scheme. Rather surprisingly, we discover that this algorithm
continues to be Bayes-consistent even in a certain infinite-dimensional
setting, in which the basic measure-theoretic conditions on which classic
consistency proofs hinge are violated. This is all the more surprising, since
it is known that $k$-NN is not Bayes-consistent in this setting. We pose
several challenging open problems for future research.
",1,0,1,1,0,0
7172,Early Experiences with Crowdsourcing Airway Annotations in Chest CT,"  Measuring airways in chest computed tomography (CT) images is important for
characterizing diseases such as cystic fibrosis, yet very time-consuming to
perform manually. Machine learning algorithms offer an alternative, but need
large sets of annotated data to perform well. We investigate whether
crowdsourcing can be used to gather airway annotations which can serve directly
for measuring the airways, or as training data for the algorithms. We generate
image slices at known locations of airways and request untrained crowd workers
to outline the airway lumen and airway wall. Our results show that the workers
are able to interpret the images, but that the instructions are too complex,
leading to many unusable annotations. After excluding unusable annotations,
quantitative results show medium to high correlations with expert measurements
of the airways. Based on this positive experience, we describe a number of
further research directions and provide insight into the challenges of
crowdsourcing in medical images from the perspective of first-time users.
",1,0,0,0,0,0
1153,A probabilistic approach to the leader problem in random graphs,"  Consider the classical Erdos-Renyi random graph process wherein one starts
with an empty graph on $n$ vertices at time $t=0$. At each stage, an edge is
chosen uniformly at random and placed in the graph. After the original
fundamental work in [19], Erdős suggested that one should view the original
random graph process as a ""race of components"". This suggested understanding
functionals such as the time for fixation of the identity of the maximal
component, sometimes referred to as the ""leader problem"". Using refined
combinatorial techniques, {\L}uczak [25] provided a complete analysis of this
question including the close relationship to the critical scaling window of the
Erdos-Renyi process. In this paper, we abstract this problem to the context of
the multiplicative coalescent which by the work of Aldous in [3] describes the
evolution of the Erdos-Renyi random graph in the critical regime. Further,
different entrance boundaries of this process have arisen in the study of heavy
tailed network models in the critical regime with degree exponent $\tau \in
(3,4)$. The leader problem in the context of the Erdos-Renyi random graph also
played an important role in the study of the scaling limit of the minimal
spanning tree on the complete graph [2]. In this paper we provide a
probabilistic analysis of the leader problem for the multiplicative coalescent
in the context of entrance boundaries of relevance to critical random graphs.
As a special case we recover {\L}uczak's result in [25] for the Erdos-Renyi
random graph.
",0,0,1,0,0,0
13158,Risk quantification for the thresholding rule for multiple testing using Gaussian scale mixtures,"  In this paper we study the asymptotic properties of Bayesian multiple testing
procedures for a large class of Gaussian scale mixture pri- ors. We study two
types of multiple testing risks: a Bayesian risk proposed in Bogdan et al.
(2011) where the data are assume to come from a mixture of normal, and a
frequentist risk similar to the one proposed by Arias-Castro and Chen (2017).
Following the work of van der Pas et al. (2016), we give general conditions on
the prior such that both risks can be bounded. For the Bayesian risk, the bound
is almost sharp. This result show that under these conditions, the considered
class of continuous prior can be competitive with the usual two-group model
(e.g. spike and slab priors). We also show that if the non-zeros component of
the parameter are large enough, the minimax risk can be made asymptotically
null. The separation rates obtained are consistent with the one that could be
guessed from the existing literature (see van der Pas et al., 2017b). For both
problems, we then give conditions under which an adaptive version of the result
can be obtained.
",0,0,1,1,0,0
13369,New descriptions of the weighted Reed-Muller codes and the homogeneous Reed-Muller codes,"  We give a description of the weighted Reed-Muller codes over a prime field in
a modular algebra. A description of the homogeneous Reed-Muller codes in the
same ambient space is presented for the binary case. A decoding procedure using
the Landrock-Manz method is developed.
",1,0,1,0,0,0
3284,Incommensurately modulated twin structure of nyerereite Na1.64K0.36Ca(CO3)2,"  Incommensurately modulated twin structure of nyerereite Na1.64K0.36Ca(CO3)2
has been first determined in the (3+1)D symmetry group Cmcm({\alpha}00)00s with
modulation vector q = 0.383a*. Unit-cell values are a = 5.062(1), b = 8.790(1),
c = 12.744(1) {\AA}. Three orthorhombic components are related by threefold
rotation about [001]. Discontinuous crenel functions are used to describe
occupation modulation of Ca and some CO3 groups. Strong displacive modulation
of the oxygen atoms in vertexes of such CO3 groups is described using
x-harmonics in crenel intervals. The Na, K atoms occupy mixed sites whose
occupation modulation is described by two ways using either complementary
harmonic functions or crenels. The nyerereite structure has been compared both
with commensurately modulated structure of K-free Na2Ca(CO3)2 and with widely
known incommensurately modulated structure of {\gamma}-Na2CO3.
",0,1,0,0,0,0
15620,Energy Trading between microgrids Individual Cost Minimization and Social Welfare Maximization,"  High penetration of renewable energy source makes microgrid (MGs) be
environment friendly. However, the stochastic input from renewable energy
resource brings difficulty in balancing the energy supply and demand.
Purchasing extra energy from macrogrid to deal with energy shortage will
increase MG energy cost. To mitigate intermittent nature of renewable energy,
energy trading and energy storage which can exploit diversity of renewable
energy generation across space and time are efficient and cost-effective
methods. But current energy storage control action will impact the future
control action which brings challenge to energy management. In addition, due to
MG participating energy trading as prosumer, it calls for an efficient trading
mechanism. Therefore, this paper focuses on the problem of MG energy management
and trading. Energy trading problem is formulated as a stochastic optimization
one with both individual profit and social welfare maximization. Firstly a
Lyapunov optimization based algorithm is developed to solve the stochastic
problem. Secondly the double-auction based mechanism is provided to attract MG
truthful bidding for buying and selling energy. Through theoretical analysis,
we demonstrate that individual MG can achieve a time average energy cost close
to offline optimum with tradeoff between storage capacity and energy trading
cost. Meanwhile the social welfare is also asymptotically maximized under
double auction. Simulation results based on real world data show the
effectiveness of our algorithm.
",1,0,0,0,0,0
17131,Finite numbers of initial ideals in non-Noetherian polynomial rings,"  In this article, we generalize the well-known result that ideals of
Noetherian polynomial rings have only finitely many initial ideals to the
situation of ascending ideal chains in non-Noetherian polynomial rings. More
precisely, we study ideal chains in the polynomial ring $R=K[x_{i,j}\,|\,1\leq
i\leq c,j\in N]$ that are invariant under the action of the monoid $Inc(N)$ of
strictly increasing functions on $N$, which acts on $R$ by shifting the second
variable index. We show that for every such ideal chain, the number of initial
ideal chains with respect to term orders on $R$ that are compatible with the
action of $Inc(N)$ is finite. As a consequence of this, we will see that
$Inc(N)$-invariant ideals of $R$ have only finitely many initial ideals with
respect to $Inc(N)$-compatible term orders. The article also addresses the
question of how many such term orders exist. We give a complete list of the
$Inc(N)$-compatible term orders for the case $c=1$ and show that there are
infinitely many for $c >1$. This answers a question by Hillar, Kroner, Leykin.
",0,0,1,0,0,0
19592,Putting a Face to the Voice: Fusing Audio and Visual Signals Across a Video to Determine Speakers,"  In this paper, we present a system that associates faces with voices in a
video by fusing information from the audio and visual signals. The thesis
underlying our work is that an extremely simple approach to generating (weak)
speech clusters can be combined with visual signals to effectively associate
faces and voices by aggregating statistics across a video. This approach does
not need any training data specific to this task and leverages the natural
coherence of information in the audio and visual streams. It is particularly
applicable to tracking speakers in videos on the web where a priori information
about the environment (e.g., number of speakers, spatial signals for
beamforming) is not available. We performed experiments on a real-world dataset
using this analysis framework to determine the speaker in a video. Given a
ground truth labeling determined by human rater consensus, our approach had
~71% accuracy.
",1,0,0,0,0,0
3457,Baryonic impact on the dark matter orbital properties of Milky Way-sized haloes,"  We study the orbital properties of dark matter haloes by combining a spectral
method and cosmological simulations of Milky Way-sized galaxies. We compare the
dynamics and orbits of individual dark matter particles from both hydrodynamic
and $N$-body simulations, and find that the fraction of box, tube and resonant
orbits of the dark matter halo decreases significantly due to the effects of
baryons. In particular, the central region of the dark matter halo in the
hydrodynamic simulation is dominated by regular, short-axis tube orbits, in
contrast to the chaotic, box and thin orbits dominant in the $N$-body run. This
leads to a more spherical dark matter halo in the hydrodynamic run compared to
a prolate one as commonly seen in the $N$-body simulations. Furthermore, by
using a kernel based density estimator, we compare the coarse-grained
phase-space densities of dark matter haloes in both simulations and find that
it is lower by $\sim0.5$ dex in the hydrodynamic run due to changes in the
angular momentum distribution, which indicates that the baryonic process that
affects the dark matter is irreversible. Our results imply that baryons play an
important role in determining the shape, kinematics and phase-space density of
dark matter haloes in galaxies.
",0,1,0,0,0,0
2607,Bayesian Compression for Deep Learning,"  Compression and computational efficiency in deep learning have become a
problem of great significance. In this work, we argue that the most principled
and effective way to attack this problem is by adopting a Bayesian point of
view, where through sparsity inducing priors we prune large parts of the
network. We introduce two novelties in this paper: 1) we use hierarchical
priors to prune nodes instead of individual weights, and 2) we use the
posterior uncertainties to determine the optimal fixed point precision to
encode the weights. Both factors significantly contribute to achieving the
state of the art in terms of compression rates, while still staying competitive
with methods designed to optimize for speed or energy efficiency.
",1,0,0,1,0,0
15583,On predictive density estimation with additional information,"  Based on independently distributed $X_1 \sim N_p(\theta_1, \sigma^2_1 I_p)$
and $X_2 \sim N_p(\theta_2, \sigma^2_2 I_p)$, we consider the efficiency of
various predictive density estimators for $Y_1 \sim N_p(\theta_1, \sigma^2_Y
I_p)$, with the additional information $\theta_1 - \theta_2 \in A$ and known
$\sigma^2_1, \sigma^2_2, \sigma^2_Y$. We provide improvements on benchmark
predictive densities such as plug-in, the maximum likelihood, and the minimum
risk equivariant predictive densities. Dominance results are obtained for
$\alpha-$divergence losses and include Bayesian improvements for reverse
Kullback-Leibler loss, and Kullback-Leibler (KL) loss in the univariate case
($p=1$). An ensemble of techniques are exploited, including variance expansion
(for KL loss), point estimation duality, and concave inequalities.
Representations for Bayesian predictive densities, and in particular for
$\hat{q}_{\pi_{U,A}}$ associated with a uniform prior for $\theta=(\theta_1,
\theta_2)$ truncated to $\{\theta \in \mathbb{R}^{2p}: \theta_1 - \theta_2 \in
A \}$, are established and are used for the Bayesian dominance findings.
Finally and interestingly, these Bayesian predictive densities also relate to
skew-normal distributions, as well as new forms of such distributions.
",0,0,1,1,0,0
15917,PEN as self-vetoing structural Material,"  Polyethylene Naphtalate (PEN) is a mechanically very favorable polymer.
Earlier it was found that thin foils made from PEN can have very high
radio-purity compared to other commercially available foils. In fact, PEN is
already in use for low background signal transmission applications (cables).
Recently it has been realized that PEN also has favorable scintillating
properties. In combination, this makes PEN a very promising candidate as a
self-vetoing structural material in low background experiments. Components
instrumented with light detectors could be built from PEN. This includes
detector holders, detector containments, signal transmission links, etc. The
current R\&D towards qualification of PEN as a self-vetoing low background
structural material is be presented.
",0,1,0,0,0,0
20536,The Deep Underground Neutrino Experiment -- DUNE: the precision era of neutrino physics,"  The last decade was remarkable for neutrino physics. In particular, the
phenomenon of neutrino flavor oscillations has been firmly established by a
series of independent measurements. All parameters of the neutrino mixing are
now known and we have elements to plan a judicious exploration of new scenarios
that are opened by these recent advances. With precise measurements, we can
test the 3-neutrino paradigm, neutrino mass hierarchy and CP asymmetry in the
lepton sector. The future long-baseline experiments are considered to be a
fundamental tool to deepen our knowledge of electroweak interactions. The Deep
Underground Neutrino Experiment -- DUNE will detect a broad-band neutrino beam
from Fermilab in an underground massive Liquid Argon Time-Projection Chamber at
an L/E of about $10^3$ km / GeV to reach good sensitivity for CP-phase
measurements and the determination of the mass hierarchy. The dimensions and
the depth of the Far Detector also create an excellent opportunity to look for
rare signals like proton decay to study violation of baryonic number, as well
as supernova neutrino bursts, broadening the scope of the experiment to
astrophysics and associated impacts in cosmology. In this presentation, we will
discuss the physics motivations and the main experimental features of the DUNE
project required to reach its scientific goals.
",0,1,0,0,0,0
6891,Tensor tomography in periodic slabs,"  The X-ray transform on the periodic slab $[0,1]\times\mathbb T^n$, $n\geq0$,
has a non-trivial kernel due to the symmetry of the manifold and presence of
trapped geodesics. For tensor fields gauge freedom increases the kernel
further, and the X-ray transform is not solenoidally injective unless $n=0$. We
characterize the kernel of the geodesic X-ray transform for $L^2$-regular
$m$-tensors for any $m\geq0$. The characterization extends to more general
manifolds, twisted slabs, including the Möbius strip as the simplest example.
",0,0,1,0,0,0
5104,Development of Si-CMOS hybrid detectors towards electron tracking based Compton imaging in semiconductor detectors,"  Electron tracking based Compton imaging is a key technique to improve the
sensitivity of Compton cameras by measuring the initial direction of recoiled
electrons. To realize this technique in semiconductor Compton cameras, we
propose a new detector concept, Si-CMOS hybrid detector. It is a Si detector
bump-bonded to a CMOS readout integrated circuit to obtain electron trajectory
images. To acquire the energy and the event timing, signals from N-side are
also read out in this concept. By using an ASIC for the N-side readout, the
timing resolution of few us is achieved. In this paper, we present the results
of two prototypes with 20 um pitch pixels. The images of the recoiled electron
trajectories are obtained with them successfully. The energy resolutions (FWHM)
are 4.1 keV (CMOS) and 1.4 keV (N-side) at 59.5 keV. In addition, we confirmed
that the initial direction of the electron is determined using the
reconstruction algorithm based on the graph theory approach. These results show
that Si-CMOS hybrid detectors can be used for electron tracking based Compton
imaging.
",0,1,0,0,0,0
14524,Optimal Weighting for Exam Composition,"  A problem faced by many instructors is that of designing exams that
accurately assess the abilities of the students. Typically these exams are
prepared several days in advance, and generic question scores are used based on
rough approximation of the question difficulty and length. For example, for a
recent class taught by the author, there were 30 multiple choice questions
worth 3 points, 15 true/false with explanation questions worth 4 points, and 5
analytical exercises worth 10 points. We describe a novel framework where
algorithms from machine learning are used to modify the exam question weights
in order to optimize the exam scores, using the overall class grade as a proxy
for a student's true ability. We show that significant error reduction can be
obtained by our approach over standard weighting schemes, and we make several
new observations regarding the properties of the ""good"" and ""bad"" exam
questions that can have impact on the design of improved future evaluation
methods.
",0,0,0,1,0,0
15902,Flow-GAN: Combining Maximum Likelihood and Adversarial Learning in Generative Models,"  Adversarial learning of probabilistic models has recently emerged as a
promising alternative to maximum likelihood. Implicit models such as generative
adversarial networks (GAN) often generate better samples compared to explicit
models trained by maximum likelihood. Yet, GANs sidestep the characterization
of an explicit density which makes quantitative evaluations challenging. To
bridge this gap, we propose Flow-GANs, a generative adversarial network for
which we can perform exact likelihood evaluation, thus supporting both
adversarial and maximum likelihood training. When trained adversarially,
Flow-GANs generate high-quality samples but attain extremely poor
log-likelihood scores, inferior even to a mixture model memorizing the training
data; the opposite is true when trained by maximum likelihood. Results on MNIST
and CIFAR-10 demonstrate that hybrid training can attain high held-out
likelihoods while retaining visual fidelity in the generated samples.
",1,0,0,1,0,0
5310,Deep Temporal-Recurrent-Replicated-Softmax for Topical Trends over Time,"  Dynamic topic modeling facilitates the identification of topical trends over
time in temporal collections of unstructured documents. We introduce a novel
unsupervised neural dynamic topic model named as Recurrent Neural
Network-Replicated Softmax Model (RNNRSM), where the discovered topics at each
time influence the topic discovery in the subsequent time steps. We account for
the temporal ordering of documents by explicitly modeling a joint distribution
of latent topical dependencies over time, using distributional estimators with
temporal recurrent connections. Applying RNN-RSM to 19 years of articles on NLP
research, we demonstrate that compared to state-of-the art topic models, RNNRSM
shows better generalization, topic interpretation, evolution and trends. We
also introduce a metric (named as SPAN) to quantify the capability of dynamic
topic model to capture word evolution in topics over time.
",1,0,0,0,0,0
2659,"The Observable Properties of Cool Winds from Galaxies, AGN, and Star Clusters. I. Theoretical Framework","  Winds arising from galaxies, star clusters, and active galactic nuclei are
crucial players in star and galaxy formation, but it has proven remarkably
difficult to use observations of them to determine physical properties of
interest, particularly mass fluxes. Much of the difficulty stems from a lack of
a theory that links a physically-realistic model for winds' density, velocity,
and covering factors to calculations of light emission and absorption. In this
paper we provide such a model. We consider a wind launched from a turbulent
region with a range of column densities, derive the differential acceleration
of gas as a function of column density, and use this result to compute winds'
absorption profiles, emission profiles, and emission intensity maps in both
optically thin and optically thick species. The model is sufficiently simple
that all required computations can be done analytically up to straightforward
numerical integrals, rendering it suitable for the problem of deriving physical
parameters by fitting models to observed data. We show that our model produces
realistic absorption and emission profiles for some example cases, and argue
that the most promising methods of deducing mass fluxes are based on
combinations of absorption lines of different optical depths, or on combining
absorption with measurements of molecular line emission. In the second paper in
this series, we expand on these ideas by introducing a set of observational
diagnostics that are significantly more robust that those commonly in use, and
that can be used to obtain improved estimates of wind properties.
",0,1,0,0,0,0
19733,Kinetic inhibition of MHD-shocks in the vicinity of a parallel magnetic field,"  According to magnetohydrodynamics (MHD), the encounter of two collisional
magnetized plasmas at high velocity gives rise to shock waves. Investigations
conducted so far have found that the same conclusion still holds in the case of
collisionless plasmas. For the case of a flow-aligned field, MHD stipulates
that the field and the fluid are disconnected, so that the shock produced is
independent of the field. We present a violation of this MHD prediction when
considering the encounter of two cold pair plasmas along a flow-aligned
magnetic field. As the guiding magnetic field grows, isotropization is
progressively suppressed, resulting in a strong influence of the field on the
resulting structure. A micro-physics analysis allows to understand the
mechanisms at work. Particle-in-cell simulations also support our conclusions
and show that the results are not restricted to a strictly parallel field.
",0,1,0,0,0,0
308,Seismic fragility curves for structures using non-parametric representations,"  Fragility curves are commonly used in civil engineering to assess the
vulnerability of structures to earthquakes. The probability of failure
associated with a prescribed criterion (e.g. the maximal inter-storey drift of
a building exceeding a certain threshold) is represented as a function of the
intensity of the earthquake ground motion (e.g. peak ground acceleration or
spectral acceleration). The classical approach relies on assuming a lognormal
shape of the fragility curves; it is thus parametric. In this paper, we
introduce two non-parametric approaches to establish the fragility curves
without employing the above assumption, namely binned Monte Carlo simulation
and kernel density estimation. As an illustration, we compute the fragility
curves for a three-storey steel frame using a large number of synthetic ground
motions. The curves obtained with the non-parametric approaches are compared
with respective curves based on the lognormal assumption. A similar comparison
is presented for a case when a limited number of recorded ground motions is
available. It is found that the accuracy of the lognormal curves depends on the
ground motion intensity measure, the failure criterion and most importantly, on
the employed method for estimating the parameters of the lognormal shape.
",0,0,0,1,0,0
12381,Modulated magnetic structure of Fe3PO7 as seen by 57Fe Mössbauer spectroscopy,"  The paper reports new results of the 57Fe Mössbauer measurements on
Fe3PO4O3 powder sample recorded at various temperatures including the point of
magnetic phase transition TN ~ 163K. The spectra measured above TN consist of
quadrupole doublet with high quadrupole splitting of D300K ~ 1.10 mm/s,
emphasizing that Fe3+ ions are located in crystal positions with a strong
electric field gradient (EFG). In order to predict the sign and orientation of
the main components of the EFG tensor we calculated monopole lattice
contributions to the EFG. In the temperature range T < TN, the experimental
spectra were fitted assuming that the electric hyperfine interactions are
modulated when the Fe3+ spin (S) rotates with respect to the EFG axis and
emergence of spatial anisotropy of the hyperfine field Hhf = SÃI at 57Fe
nuclei. These data were analyzed to estimate the components of the anisotropic
hyperfine coupling tensor (Ã). The large anharmonicity parameter, m ~ 0.94,
of the spiral spin structure results from easy-axis anisotropy in the plane of
the iron spin rotation. The temperature evolution of the hyperfine field Hhf(T)
was described by Bean-Rodbell model that takes into account that the exchange
magnetic interactions are strong function of the lattice spacing. The obtained
Mössbauer data are in qualitative agreement with previous neutron diffraction
data for a modulated helical magnetic structure in strongly frustrated
Fe3PO4O3.
",0,1,0,0,0,0
11295,Low-dose cryo electron ptychography via non-convex Bayesian optimization,"  Electron ptychography has seen a recent surge of interest for phase sensitive
imaging at atomic or near-atomic resolution. However, applications are so far
mainly limited to radiation-hard samples because the required doses are too
high for imaging biological samples at high resolution. We propose the use of
non-convex, Bayesian optimization to overcome this problem and reduce the dose
required for successful reconstruction by two orders of magnitude compared to
previous experiments. We suggest to use this method for imaging single
biological macromolecules at cryogenic temperatures and demonstrate 2D
single-particle reconstructions from simulated data with a resolution of 7.9
\AA$\,$ at a dose of 20 $e^- / \AA^2$. When averaging over only 15 low-dose
datasets, a resolution of 4 \AA$\,$ is possible for large macromolecular
complexes. With its independence from microscope transfer function, direct
recovery of phase contrast and better scaling of signal-to-noise ratio,
cryo-electron ptychography may become a promising alternative to Zernike
phase-contrast microscopy.
",0,1,1,1,0,0
769,FluxMarker: Enhancing Tactile Graphics with Dynamic Tactile Markers,"  For people with visual impairments, tactile graphics are an important means
to learn and explore information. However, raised line tactile graphics created
with traditional materials such as embossing are static. While available
refreshable displays can dynamically change the content, they are still too
expensive for many users, and are limited in size. These factors limit
wide-spread adoption and the representation of large graphics or data sets. In
this paper, we present FluxMaker, an inexpensive scalable system that renders
dynamic information on top of static tactile graphics with movable tactile
markers. These dynamic tactile markers can be easily reconfigured and used to
annotate static raised line tactile graphics, including maps, graphs, and
diagrams. We developed a hardware prototype that actuates magnetic tactile
markers driven by low-cost and scalable electromagnetic coil arrays, which can
be fabricated with standard printed circuit board manufacturing. We evaluate
our prototype with six participants with visual impairments and found positive
results across four application areas: location finding or navigating on
tactile maps, data analysis, and physicalization, feature identification for
tactile graphics, and drawing support. The user study confirms advantages in
application domains such as education and data exploration.
",1,0,0,0,0,0
2691,Smooth Pinball Neural Network for Probabilistic Forecasting of Wind Power,"  Uncertainty analysis in the form of probabilistic forecasting can
significantly improve decision making processes in the smart power grid for
better integrating renewable energy sources such as wind. Whereas point
forecasting provides a single expected value, probabilistic forecasts provide
more information in the form of quantiles, prediction intervals, or full
predictive densities. This paper analyzes the effectiveness of a novel approach
for nonparametric probabilistic forecasting of wind power that combines a
smooth approximation of the pinball loss function with a neural network
architecture and a weighting initialization scheme to prevent the quantile
cross over problem. A numerical case study is conducted using publicly
available wind data from the Global Energy Forecasting Competition 2014.
Multiple quantiles are estimated to form 10%, to 90% prediction intervals which
are evaluated using a quantile score and reliability measures. Benchmark models
such as the persistence and climatology distributions, multiple quantile
regression, and support vector quantile regression are used for comparison
where results demonstrate the proposed approach leads to improved performance
while preventing the problem of overlapping quantile estimates.
",0,0,0,1,0,0
6993,The exit time finite state projection scheme: bounding exit distributions and occupation measures of continuous-time Markov chains,"  We introduce the exit time finite state projection (ETFSP) scheme, a
truncation-based method that yields approximations to the exit distribution and
occupation measure associated with the time of exit from a domain (i.e., the
time of first passage to the complement of the domain) of time-homogeneous
continuous-time Markov chains. We prove that: (i) the computed approximations
bound the measures from below; (ii) the total variation distances between the
approximations and the measures decrease monotonically as states are added to
the truncation; and (iii) the scheme converges, in the sense that, as the
truncation tends to the entire state space, the total variation distances tend
to zero. Furthermore, we give a computable bound on the total variation
distance between the exit distribution and its approximation, and we delineate
the cases in which the bound is sharp. We also revisit the related finite state
projection scheme and give a comprehensive account of its theoretical
properties. We demonstrate the use of the ETFSP scheme by applying it to two
biological examples: the computation of the first passage time associated with
the expression of a gene, and the fixation times of competing species subject
to demographic noise.
",0,0,0,0,1,0
14588,Constraints on Quenching of $z\lesssim2$ Massive Galaxies from the Evolution of the average Sizes of Star-Forming and Quenched Populations in COSMOS,"  We use $>$9400 $\log(m/M_{\odot})>10$ quiescent and star-forming galaxies at
$z\lesssim2$ in COSMOS/UltraVISTA to study the average size evolution of these
systems, with focus on the rare, ultra-massive population at
$\log(m/M_{\odot})>11.4$. The large 2-square degree survey area delivers a
sample of $\sim400$ such ultra-massive systems. Accurate sizes are derived
using a calibration based on high-resolution images from the Hubble Space
Telescope. We find that, at these very high masses, the size evolution of
star-forming and quiescent galaxies is almost indistinguishable in terms of
normalization and power-law slope. We use this result to investigate possible
pathways of quenching massive $m>M^*$ galaxies at $z<2$. We consistently model
the size evolution of quiescent galaxies from the star-forming population by
assuming different simple models for the suppression of star-formation. These
models include an instantaneous and delayed quenching without altering the
structure of galaxies and a central starburst followed by compaction. We find
that instantaneous quenching reproduces well the observed mass-size relation of
massive galaxies at $z>1$. Our starburst$+$compaction model followed by
individual growth of the galaxies by minor mergers is preferred over other
models without structural change for $\log(m/M_{\odot})>11.0$ galaxies at
$z>0.5$. None of our models is able to meet the observations at $m>M^*$ and
$z<1$ with out significant contribution of post-quenching growth of individual
galaxies via mergers. We conclude that quenching is a fast process in galaxies
with $ m \ge 10^{11} M_\odot$, and that major mergers likely play a major role
in the final steps of their evolution.
",0,1,0,0,0,0
16917,E-learning Information Technology Based on an Ontology Driven Learning Engine,"  In the article, proposed is a new e-learning information technology based on
an ontology driven learning engine, which is matched with modern pedagogical
technologies. With the help of proposed engine and developed question database
we have conducted an experiment, where students were tested. The developed
ontology driven system of e-learning facilitates the creation of favorable
conditions for the development of personal qualities and creation of a holistic
understanding of the subject area among students throughout the educational
process.
",1,0,0,0,0,0
14024,The Noether numbers and the Davenport constants of the groups of order less than 32,"  The computation of the Noether numbers of all groups of order less than
thirty-two is completed. It turns out that for these groups in non-modular
characteristic the Noether number is attained on a multiplicity free
representation, it is strictly monotone on subgroups and factor groups, and it
does not depend on the characteristic. Algorithms are developed and used to
determine the small and large Davenport constants of these groups. For each of
these groups the Noether number is greater than the small Davenport constant,
whereas the first example of a group whose Noether number exceeds the large
Davenport constant is found, answering partially a question posed by
Geroldinger and Grynkiewicz.
",0,0,1,0,0,0
2103,"Non-Euclidean geometry, nontrivial topology and quantum vacuum effects","  Space out of a topological defect of the Abrikosov-Nielsen-Olesen vortex type
is locally flat but non-Euclidean. If a spinor field is quantized in such a
space, then a variety of quantum effects is induced in the vacuum. Basing on
the continuum model for long-wavelength electronic excitations, originating in
the tight-binding approximation for the nearest neighbor interaction of atoms
in the crystal lattice, we consider quantum ground state effects in monolayer
structures warped into nanocones by a disclination; the nonzero size of the
disclination is taken into account, and a boundary condition at the edge of the
disclination is chosen to ensure self-adjointness of the Dirac-Weyl Hamiltonian
operator. In the case of carbon nanocones, we find circumstances when the
quantum ground state effects are independent of the boundary parameter and the
disclination size.
",0,1,0,0,0,0
15585,Bases of standard modules for affine Lie algebras of type $C_\ell^{(1)}$,"  Feigin-Stoyanovsky's type subspaces for affine Lie algebras of type
$C_\ell^{(1)}$ have monomial bases with a nice combinatorial description. We
describe bases of whole standard modules in terms of semi-infinite monomials
obtained as ""a limit of translations"" of bases for Feigin-Stoyanovsky's type
subspaces.
",0,0,1,0,0,0
3444,Classical Music Clustering Based on Acoustic Features,"  In this paper we cluster 330 classical music pieces collected from MusicNet
database based on their musical note sequence. We use shingling and chord
trajectory matrices to create signature for each music piece and performed
spectral clustering to find the clusters. Based on different resolution, the
output clusters distinctively indicate composition from different classical
music era and different composing style of the musicians.
",1,0,0,0,0,0
6055,Basis Adaptive Sample Efficient Polynomial Chaos (BASE-PC),"  For a large class of orthogonal basis functions, there has been a recent
identification of expansion methods for computing accurate, stable
approximations of a quantity of interest. This paper presents, within the
context of uncertainty quantification, a practical implementation using basis
adaptation, and coherence motivated sampling, which under assumptions has
satisfying guarantees. This implementation is referred to as Basis Adaptive
Sample Efficient Polynomial Chaos (BASE-PC). A key component of this is the use
of anisotropic polynomial order which admits evolving global bases for
approximation in an efficient manner, leading to consistently stable
approximation for a practical class of smooth functionals. This fully adaptive,
non-intrusive method, requires no a priori information of the solution, and has
satisfying theoretical guarantees of recovery. A key contribution to stability
is the use of a presented correction sampling for coherence-optimal sampling in
order to improve stability and accuracy within the adaptive basis scheme.
Theoretically, the method may dramatically reduce the impact of dimensionality
in function approximation, and numerically the method is demonstrated to
perform well on problems with dimension up to 1000.
",0,0,1,1,0,0
20452,A compilation of LEGO Technic parts to support learning experiments on linkages,"  We present a compilation of LEGO Technic parts to provide easy-to-build
constructions of basic planar linkages. Some technical issues and their
possible solutions are discussed. To solve questions on fine details---like
deciding whether the motion is an exactly straight line or not---we refer to
the dynamic mathematics software tool GeoGebra.
",0,0,1,0,0,0
4337,RNN-based Early Cyber-Attack Detection for the Tennessee Eastman Process,"  An RNN-based forecasting approach is used to early detect anomalies in
industrial multivariate time series data from a simulated Tennessee Eastman
Process (TEP) with many cyber-attacks. This work continues a previously
proposed LSTM-based approach to the fault detection in simpler data. It is
considered necessary to adapt the RNN network to deal with data containing
stochastic, stationary, transitive and a rich variety of anomalous behaviours.
There is particular focus on early detection with special NAB-metric. A
comparison with the DPCA approach is provided. The generated data set is made
publicly available.
",1,0,0,0,0,0
5163,Gaussian Parsimonious Clustering Models with Covariates,"  We consider model-based clustering methods for continuous, correlated data
that account for external information available in the presence of mixed-type
fixed covariates by proposing the MoEClust suite of models. These allow
covariates influence the component weights and/or component densities by
modelling the parameters of the mixture as functions of the covariates. A
familiar range of constrained eigen-decomposition parameterisations of the
component covariance matrices are also accommodated. This paper thus addresses
the equivalent aims of including covariates in Gaussian Parsimonious Clustering
Models and incorporating parsimonious covariance structures into the Gaussian
mixture of experts framework. The MoEClust models demonstrate significant
improvement from both perspectives in applications to univariate and
multivariate data sets.
",0,0,0,1,0,0
6619,Generation High resolution 3D model from natural language by Generative Adversarial Network,"  We present a method of generating high resolution 3D shapes from natural
language descriptions. To achieve this goal, we propose two steps that
generating low resolution shapes which roughly reflect texts and generating
high resolution shapes which reflect the detail of texts. In a previous paper,
the authors have shown a method of generating low resolution shapes. We improve
it to generate 3D shapes more faithful to natural language and test the
effectiveness of the method. To generate high resolution 3D shapes, we use the
framework of Conditional Wasserstein GAN. We propose two roles of Critic
separately, which calculate the Wasserstein distance between two probability
distribution, so that we achieve generating high quality shapes or acceleration
of learning speed of model. To evaluate our approach, we performed quantitive
evaluation with several numerical metrics for Critic models. Our method is
first to realize the generation of high quality model by propagating text
embedding information to high resolution task when generating 3D model.
",1,0,0,1,0,0
7628,Dynamic anisotropy in MHD turbulence induced by mean magnetic field,"  In this paper, we study the development of anisotropy in strong MHD
turbulence in the presence of a large scale magnetic field B 0 by analyzing the
results of direct numerical simulations. Our results show that the developed
anisotropy among the different components of the velocity and magnetic field is
a direct outcome of the inverse cascade of energy of the perpendicular velocity
components u? and a forward cascade of the energy of the parallel component u k
. The inverse cascade develops for a strong B0, where the flow exhibits a
strong vortical structure by the suppression of fluctuations along the magnetic
field. Both the inverse and the forward cascade are examined in detail by
investigating the anisotropic energy spectra, the energy fluxes, and the shell
to shell energy transfers among different scales.
",0,1,0,0,0,0
20275,Computational Eco-Systems for Handwritten Digits Recognition,"  Inspired by the importance of diversity in biological system, we built an
heterogeneous system that could achieve this goal. Our architecture could be
summarized in two basic steps. First, we generate a diverse set of
classification hypothesis using both Convolutional Neural Networks, currently
the state-of-the-art technique for this task, among with other traditional and
innovative machine learning techniques. Then, we optimally combine them through
Meta-Nets, a family of recently developed and performing ensemble methods.
",0,0,0,1,0,0
536,Motion Planning for a Humanoid Mobile Manipulator System,"  A high redundant non-holonomic humanoid mobile dual-arm manipulator system is
presented in this paper where the motion planning to realize ""human-like""
autonomous navigation and manipulation tasks is studied. Firstly, an improved
MaxiMin NSGA-II algorithm, which optimizes five objective functions to solve
the problems of singularity, redundancy, and coupling between mobile base and
manipulator simultaneously, is proposed to design the optimal pose to
manipulate the target object. Then, in order to link the initial pose and that
optimal pose, an off-line motion planning algorithm is designed. In detail, an
efficient direct-connect bidirectional RRT and gradient descent algorithm is
proposed to reduce the sampled nodes largely, and a geometric optimization
method is proposed for path pruning. Besides, head forward behaviors are
realized by calculating the reasonable orientations and assigning them to the
mobile base to improve the quality of human-robot interaction. Thirdly, the
extension to on-line planning is done by introducing real-time sensing,
collision-test and control cycles to update robotic motion in dynamic
environments. Fourthly, an EEs' via-point-based multi-objective genetic
algorithm is proposed to design the ""human-like"" via-poses by optimizing four
objective functions. Finally, numerous simulations are presented to validate
the effectiveness of proposed algorithms.
",1,0,0,0,0,0
5367,A Deep Network Model for Paraphrase Detection in Short Text Messages,"  This paper is concerned with paraphrase detection. The ability to detect
similar sentences written in natural language is crucial for several
applications, such as text mining, text summarization, plagiarism detection,
authorship authentication and question answering. Given two sentences, the
objective is to detect whether they are semantically identical. An important
insight from this work is that existing paraphrase systems perform well when
applied on clean texts, but they do not necessarily deliver good performance
against noisy texts. Challenges with paraphrase detection on user generated
short texts, such as Twitter, include language irregularity and noise. To cope
with these challenges, we propose a novel deep neural network-based approach
that relies on coarse-grained sentence modeling using a convolutional neural
network and a long short-term memory model, combined with a specific
fine-grained word-level similarity matching model. Our experimental results
show that the proposed approach outperforms existing state-of-the-art
approaches on user-generated noisy social media data, such as Twitter texts,
and achieves highly competitive performance on a cleaner corpus.
",1,0,0,0,0,0
14909,Approximation of full-boundary data from partial-boundary electrode measurements,"  Measurements on a subset of the boundary are common in electrical impedance
tomography, especially any electrode model can be interpreted as a partial
boundary problem. The information obtained is different to full-boundary
measurements as modeled by the ideal continuum model. In this study we discuss
an approach to approximate full-boundary data from partial-boundary
measurements that is based on the knowledge of the involved projections. The
approximate full-boundary data can then be obtained as the solution of a
suitable optimization problem on the coefficients of the Neumann-to-Dirichlet
map. By this procedure we are able to improve the reconstruction quality of
continuum model based algorithms, in particular we present the effectiveness
with a D-bar method. Reconstructions are presented for noisy simulated and real
measurement data.
",0,0,1,0,0,0
6240,Dynamical transport measurement of the Luttinger parameter in helical edges states of 2D topological insulators,"  One-dimensional (1D) electron systems in the presence of Coulomb interaction
are described by Luttinger liquid theory. The strength of Coulomb interaction
in the Luttinger liquid, as parameterized by the Luttinger parameter K, is in
general difficult to measure. This is because K is usually hidden in powerlaw
dependencies of observables as a function of temperature or applied bias. We
propose a dynamical way to measure K on the basis of an electronic
time-of-flight experiment. We argue that the helical Luttinger liquid at the
edge of a 2D topological insulator constitutes a preeminently suited
realization of a 1D system to test our proposal. This is based on the
robustness of helical liquids against elastic backscattering in the presence of
time reversal symmetry.
",0,1,0,0,0,0
989,Characterizing complex networks using Entropy-degree diagrams: unveiling changes in functional brain connectivity induced by Ayahuasca,"  Open problems abound in the theory of complex networks, which has found
successful application to diverse fields of science. With the aim of further
advancing the understanding of the brain's functional connectivity, we propose
to evaluate a network metric which we term the geodesic entropy. This entropy,
in a way that can be made precise, quantifies the Shannon entropy of the
distance distribution to a specific node from all other nodes. Measurements of
geodesic entropy allow for the characterization of the structural information
of a network that takes into account the distinct role of each node into the
network topology. The measurement and characterization of this structural
information has the potential to greatly improve our understanding of sustained
activity and other emergent behaviors in networks, such as self-organized
criticality sometimes seen in such contexts. We apply these concepts and
methods to study the effects of how the psychedelic Ayahuasca affects the
functional connectivity of the human brain. We show that the geodesic entropy
is able to differentiate the functional networks of the human brain in two
different states of consciousness in the resting state: (i) the ordinary waking
state and (ii) a state altered by ingestion of the Ayahuasca. The entropy of
the nodes of brain networks from subjects under the influence of Ayahuasca
diverge significantly from those of the ordinary waking state. The functional
brain networks from subjects in the altered state have, on average, a larger
geodesic entropy compared to the ordinary state. We conclude that geodesic
entropy is a useful tool for analyzing complex networks and discuss how and why
it may bring even further valuable insights into the study of the human brain
and other empirical networks.
",0,0,0,0,1,0
11277,Blind Source Separation Using Mixtures of Alpha-Stable Distributions,"  We propose a new blind source separation algorithm based on mixtures of
alpha-stable distributions. Complex symmetric alpha-stable distributions have
been recently showed to better model audio signals in the time-frequency domain
than classical Gaussian distributions thanks to their larger dynamic range.
However, inference of these models is notoriously hard to perform because their
probability density functions do not have a closed-form expression in general.
Here, we introduce a novel method for estimating mixture of alpha-stable
distributions based on characteristic function matching. We apply this to the
blind estimation of binary masks in individual frequency bands from
multichannel convolutive audio mixes. We show that the proposed method yields
better separation performance than Gaussian-based binary-masking methods.
",1,0,0,1,0,0
2826,World Literature According to Wikipedia: Introduction to a DBpedia-Based Framework,"  Among the manifold takes on world literature, it is our goal to contribute to
the discussion from a digital point of view by analyzing the representation of
world literature in Wikipedia with its millions of articles in hundreds of
languages. As a preliminary, we introduce and compare three different
approaches to identify writers on Wikipedia using data from DBpedia, a
community project with the goal of extracting and providing structured
information from Wikipedia. Equipped with our basic set of writers, we analyze
how they are represented throughout the 15 biggest Wikipedia language versions.
We combine intrinsic measures (mostly examining the connectedness of articles)
with extrinsic ones (analyzing how often articles are frequented by readers)
and develop methods to evaluate our results. The better part of our findings
seems to convey a rather conservative, old-fashioned version of world
literature, but a version derived from reproducible facts revealing an implicit
literary canon based on the editing and reading behavior of millions of people.
While still having to solve some known issues, the introduced methods will help
us build an observatory of world literature to further investigate its
representativeness and biases.
",1,0,0,0,0,0
9411,Click-based porous cationic polymers for enhanced carbon dioxide capture,"  Imidazolium based porous cationic polymers were synthesized using an
innovative and facile approach, which takes advantage of the Debus Radziszewski
reaction to obtain meso- and microporous polymers following click chemistry
principles. In the obtained set of materials, click based porous cationic
polymers have the same cationic backbone, whereas they bear the commonly used
anions of imidazolium poly(ionic liquid)s. These materials show hierarchical
porosity and a good specific surface area. Furthermore, their chemical
structure was extensively characterized using ATR FTIR and SS NMR
spectroscopies, and HR MS. These polymers show good performance towards carbon
dioxide sorption, especially those possessing the acetate anion. This polymer
has an uptake of 2 mmol per g of CO2 at 1 bar and 273 K, a value which is among
the highest recorded for imidazolium poly(ionic liquid)s. These polymers were
also modified in order to introduce N-heterocyclic carbenes along the backbone.
Carbon dioxide loading in the carbene-containing polymer is in the same range
as that of the non-modified versions, but the nature of the interaction is
substantially different. The combined use of in situ FTIR spectroscopy and
microcalorimetry evidenced a chemisorption phenomenon that brings about the
formation of an imidazolium carboxylate zwitterion.
",0,1,0,0,0,0
11063,Chiral and Topological Orbital Magnetism of Spin Textures,"  Using a semiclassical Green's function formalism, we discover the emergence
of chiral and topological orbital magnetism in two-dimensional chiral spin
textures by explicitly finding the corrections to the orbital magnetization,
proportional to the powers of the gradients of the texture. We show that in the
absence of spin-orbit coupling, the resulting orbital moment can be understood
as the electronic response to the emergent magnetic field associated with the
real-space Berry curvature. By referring to the Rashba model, we demonstrate
that by tuning the parameters of surface systems the engineering of emergent
orbital magnetism in spin textures can pave the way to novel concepts in
orbitronics.
",0,1,0,0,0,0
92,The effects of subdiffusion on the NTA size measurements of extracellular vesicles in biological samples,"  The interest in the extracellular vesicles (EVs) is rapidly growing as they
became reliable biomarkers for many diseases. For this reason, fast and
accurate techniques of EVs size characterization are the matter of utmost
importance. One increasingly popular technique is the Nanoparticle Tracking
Analysis (NTA), in which the diameters of EVs are calculated from their
diffusion constants. The crucial assumption here is that the diffusion in NTA
follows the Stokes-Einstein relation, i.e. that the Mean Square Displacement
(MSD) of a particle grows linearly in time (MSD $\propto t$). However, we show
that NTA violates this assumption in both artificial and biological samples,
i.e. a large population of particles show a strongly sub-diffusive behaviour
(MSD $\propto t^\alpha$, $0<\alpha<1$). To support this observation we present
a range of experimental results for both polystyrene beads and EVs. This is
also related to another problem: for the same samples there exists a huge
discrepancy (by the factor of 2-4) between the sizes measured with NTA and with
the direct imaging methods, such as AFM. This can be remedied by e.g. the
Finite Track Length Adjustment (FTLA) method in NTA, but its applicability is
limited in the biological and poly-disperse samples. On the other hand, the
models of sub-diffusion rarely provide the direct relation between the size of
a particle and the generalized diffusion constant. However, we solve this last
problem by introducing the logarithmic model of sub-diffusion, aimed at
retrieving the size data. In result, we propose a novel protocol of NTA data
analysis. The accuracy of our method is on par with FTLA for small
($\simeq$200nm) particles. We apply our method to study the EVs samples and
corroborate the results with AFM.
",0,1,0,0,0,0
10484,Medoids in almost linear time via multi-armed bandits,"  Computing the medoid of a large number of points in high-dimensional space is
an increasingly common operation in many data science problems. We present an
algorithm Med-dit which uses O(n log n) distance evaluations to compute the
medoid with high probability. Med-dit is based on a connection with the
multi-armed bandit problem. We evaluate the performance of Med-dit empirically
on the Netflix-prize and the single-cell RNA-Seq datasets, containing hundreds
of thousands of points living in tens of thousands of dimensions, and observe a
5-10x improvement in performance over the current state of the art. Med-dit is
available at this https URL
",1,0,0,1,0,0
20028,Parylene-C microfibrous thin films as phononic crystals,"  Phononic bandgaps of Parylene-C microfibrous thin films (muFTFs) were
computationally determined by treating them as phononic crystals comprising
identical microfibers arranged either on a square or a hexagonal lattice. The
microfibers could be columnar,chevronic, or helical in shape, and the host
medium could be either water or air. All bandgaps were observed to lie in the
0.01-to-162.9-MHz regime, for microfibers of realistically chosen dimensions.
The upper limit of the frequency of bandgaps was the highest for the columnar
muFTF and the lowest for the chiral muFTF. More bandgaps exist when the host
medium is water than air. Complete bandgaps were observed for the columnar
muFTF with microfibers arranged on a hexagonal lattice in air, the chevronic
muFTF with microfibers arranged on a square lattice in water, and the chiral
muFTF with microfibers arranged on a hexagonal lattice in either air or water.
The softness of the Parylene-C muFTFs makes them mechanically tunable, and
their bandgaps can be exploited in multiband ultrasonic filters.
",0,1,0,0,0,0
14624,A Lichnerowicz estimate for the spectral gap of the sub-Laplacian,"  For a second order operator on a compact manifold satisfying the strong
Hörmander condition, we give a bound for the spectral gap analogous to the
Lichnerowicz estimate for the Laplacian of a Riemannian manifold. We consider a
wide class of such operators which includes horizontal lifts of the Laplacian
on Riemannian submersions with minimal leaves.
",0,0,1,0,0,0
16044,A Fuzzy Community-Based Recommender System Using PageRank,"  Recommendation systems are widely used by different user service providers
specially those who have interactions with the large community of users. This
paper introduces a recommender system based on community detection. The
recommendation is provided using the local and global similarities between
users. The local information is obtained from communities, and the global ones
are based on the ratings. Here, a new fuzzy community detection using the
personalized PageRank metaphor is introduced. The fuzzy membership values of
the users to the communities are utilized to define a similarity measure. The
method is evaluated by using two well-known datasets: MovieLens and FilmTrust.
The results show that our method outperforms recent recommender systems.
",1,0,0,0,0,0
8430,Optimal Threshold Design for Quanta Image Sensor,"  Quanta Image Sensor (QIS) is a binary imaging device envisioned to be the
next generation image sensor after CCD and CMOS. Equipped with a massive number
of single photon detectors, the sensor has a threshold $q$ above which the
number of arriving photons will trigger a binary response ""1"", or ""0""
otherwise. Existing methods in the device literature typically assume that
$q=1$ uniformly. We argue that a spatially varying threshold can significantly
improve the signal-to-noise ratio of the reconstructed image. In this paper, we
present an optimal threshold design framework. We make two contributions.
First, we derive a set of oracle results to theoretically inform the maximally
achievable performance. We show that the oracle threshold should match exactly
with the underlying pixel intensity. Second, we show that around the oracle
threshold there exists a set of thresholds that give asymptotically unbiased
reconstructions. The asymptotic unbiasedness has a phase transition behavior
which allows us to develop a practical threshold update scheme using a
bisection method. Experimentally, the new threshold design method achieves
better rate of convergence than existing methods.
",1,0,0,0,0,0
15764,Combining Probabilistic Load Forecasts,"  Probabilistic load forecasts provide comprehensive information about future
load uncertainties. In recent years, many methodologies and techniques have
been proposed for probabilistic load forecasting. Forecast combination, a
widely recognized best practice in point forecasting literature, has never been
formally adopted to combine probabilistic load forecasts. This paper proposes a
constrained quantile regression averaging (CQRA) method to create an improved
ensemble from several individual probabilistic forecasts. We formulate the CQRA
parameter estimation problem as a linear program with the objective of
minimizing the pinball loss, with the constraints that the parameters are
nonnegative and summing up to one. We demonstrate the effectiveness of the
proposed method using two publicly available datasets, the ISO New England data
and Irish smart meter data. Comparing with the best individual probabilistic
forecast, the ensemble can reduce the pinball score by 4.39% on average. The
proposed ensemble also demonstrates superior performance over nine other
benchmark ensembles.
",0,0,0,1,0,0
18217,TimeNet: Pre-trained deep recurrent neural network for time series classification,"  Inspired by the tremendous success of deep Convolutional Neural Networks as
generic feature extractors for images, we propose TimeNet: a deep recurrent
neural network (RNN) trained on diverse time series in an unsupervised manner
using sequence to sequence (seq2seq) models to extract features from time
series. Rather than relying on data from the problem domain, TimeNet attempts
to generalize time series representation across domains by ingesting time
series from several domains simultaneously. Once trained, TimeNet can be used
as a generic off-the-shelf feature extractor for time series. The
representations or embeddings given by a pre-trained TimeNet are found to be
useful for time series classification (TSC). For several publicly available
datasets from UCR TSC Archive and an industrial telematics sensor data from
vehicles, we observe that a classifier learned over the TimeNet embeddings
yields significantly better performance compared to (i) a classifier learned
over the embeddings given by a domain-specific RNN, as well as (ii) a nearest
neighbor classifier based on Dynamic Time Warping.
",1,0,0,0,0,0
4298,Lower bounds on the Noether number,"  The best known method to give a lower bound for the Noether number of a given
finite group is to use the fact that it is greater than or equal to the Noether
number of any of the subgroups or factor groups. The results of the present
paper show in particular that these inequalities are strict for proper
subgroups or factor groups. This is established by studying the algebra of
coinvariants of a representation induced from a representation of a subgroup.
",0,0,1,0,0,0
19431,Zero-cycles of degree one on Skorobogatov's bielliptic surface,"  Skorobogatov constructed a bielliptic surface which is a counterexample to
the Hasse principle not explained by the Brauer-Manin obstruction. We show that
this surface has a $0$-cycle of degree 1, as predicted by a conjecture of
Colliot-Thélène.
",0,0,1,0,0,0
20115,Towards Robust Neural Networks via Random Self-ensemble,"  Recent studies have revealed the vulnerability of deep neural networks: A
small adversarial perturbation that is imperceptible to human can easily make a
well-trained deep neural network misclassify. This makes it unsafe to apply
neural networks in security-critical applications. In this paper, we propose a
new defense algorithm called Random Self-Ensemble (RSE) by combining two
important concepts: {\bf randomness} and {\bf ensemble}. To protect a targeted
model, RSE adds random noise layers to the neural network to prevent the strong
gradient-based attacks, and ensembles the prediction over random noises to
stabilize the performance. We show that our algorithm is equivalent to ensemble
an infinite number of noisy models $f_\epsilon$ without any additional memory
overhead, and the proposed training procedure based on noisy stochastic
gradient descent can ensure the ensemble model has a good predictive
capability. Our algorithm significantly outperforms previous defense techniques
on real data sets. For instance, on CIFAR-10 with VGG network (which has 92\%
accuracy without any attack), under the strong C\&W attack within a certain
distortion tolerance, the accuracy of unprotected model drops to less than
10\%, the best previous defense technique has $48\%$ accuracy, while our method
still has $86\%$ prediction accuracy under the same level of attack. Finally,
our method is simple and easy to integrate into any neural network.
",1,0,0,1,0,0
10297,Activation Maximization Generative Adversarial Nets,"  Class labels have been empirically shown useful in improving the sample
quality of generative adversarial nets (GANs). In this paper, we mathematically
study the properties of the current variants of GANs that make use of class
label information. With class aware gradient and cross-entropy decomposition,
we reveal how class labels and associated losses influence GAN's training.
Based on that, we propose Activation Maximization Generative Adversarial
Networks (AM-GAN) as an advanced solution. Comprehensive experiments have been
conducted to validate our analysis and evaluate the effectiveness of our
solution, where AM-GAN outperforms other strong baselines and achieves
state-of-the-art Inception Score (8.91) on CIFAR-10. In addition, we
demonstrate that, with the Inception ImageNet classifier, Inception Score
mainly tracks the diversity of the generator, and there is, however, no
reliable evidence that it can reflect the true sample quality. We thus propose
a new metric, called AM Score, to provide a more accurate estimation of the
sample quality. Our proposed model also outperforms the baseline methods in the
new metric.
",1,0,0,1,0,0
180,On the Deployment of Distributed Antennas for Wireless Power Transfer with Safety Electromagnetic Radiation Level Requirement,"  The extremely low efficiency is regarded as the bottleneck of Wireless Power
Transfer (WPT) technology. To tackle this problem, either enlarging the
transfer power or changing the infrastructure of WPT system could be an
intuitively proposed way. However, the drastically important issue on the user
exposure of electromagnetic radiation is rarely considered while we try to
improve the efficiency of WPT. In this paper, a Distributed Antenna Power
Beacon (DA-PB) based WPT system where these antennas are uniformly distributed
on a circle is analyzed and optimized with the safety electromagnetic radiation
level (SERL) requirement. In this model, three key questions are intended to be
answered: 1) With the SERL, what is the performance of the harvested power at
the users ? 2) How do we configure the parameters to maximize the efficiency of
WPT? 3) Under the same constraints, does the DA-PB still have performance gain
than the Co-located Antenna PB (CA-PB)? First, the minimum antenna height of
DA-PB is derived to make the radio frequency (RF) electromagnetic radiation
power density at any location of the charging cell lower than the SERL
published by the Federal Communications Commission (FCC). Second, the
closed-form expressions of average harvested Direct Current (DC) power per user
in the charging cell for pass-loss exponent 2 and 4 are also provided. In order
to maximize the average efficiency of WPT, the optimal radii for distributed
antennas elements (DAEs) are derived when the pass-loss exponent takes the
typical value $2$ and $4$. For comparison, the CA-PB is also analyzed as a
benchmark. Simulation results verify our derived theoretical results. And it is
shown that the proposed DA-PB indeed achieves larger average harvested DC power
than CA-PB and can improve the efficiency of WPT.
",1,0,0,0,0,0
14262,A Cloud-based Service for Real-Time Performance Evaluation of NoSQL Databases,"  We have created a cloud-based service that allows the end users to run tests
on multiple different databases to find which databases are most suitable for
their project. From our research, we could not find another application that
enables the user to test several databases to gauge the difference between
them. This application allows the user to choose which type of test to perform
and which databases to target. The application also displays the results of
different tests that were run by other users previously. There is also a map to
show the location where all the tests are run to give the user an estimate of
the location. Unlike the orthodox static tests and reports conducted to
evaluate NoSQL databases, we have created a web application to run and analyze
these tests in real time. This web application evaluates the performance of
several NoSQL databases. The databases covered are MongoDB, DynamoDB, CouchDB,
and Firebase. The web service is accessible from: nosqldb.nextproject.ca.
",1,0,0,0,0,0
7009,Inverse of a Special Matrix and Application,"  The matrix inversion is an interesting topic in algebra mathematics. However,
to determine an inverse matrix from a given matrix is required many computation
tools and time resource if the size of matrix is huge. In this paper, we have
shown an inverse closed form for an interesting matrix which has much
applications in communication system. Base on this inverse closed form, the
channel capacity closed form of a communication system can be determined via
the error rate parameter alpha
",1,0,0,0,0,0
2,Rotation Invariance Neural Network,"  Rotation invariance and translation invariance have great values in image
recognition tasks. In this paper, we bring a new architecture in convolutional
neural network (CNN) named cyclic convolutional layer to achieve rotation
invariance in 2-D symbol recognition. We can also get the position and
orientation of the 2-D symbol by the network to achieve detection purpose for
multiple non-overlap target. Last but not least, this architecture can achieve
one-shot learning in some cases using those invariance.
",1,0,0,0,0,0
14585,Stable explicit schemes for simulation of nonlinear moisture transfer in porous materials,"  Implicit schemes have been extensively used in building physics to compute
the solution of moisture diffusion problems in porous materials for improving
stability conditions. Nevertheless, these schemes require important
sub-iterations when treating non-linear problems. To overcome this
disadvantage, this paper explores the use of improved explicit schemes, such as
Dufort-Frankel, Crank-Nicolson and hyperbolisation approaches. A first case
study has been considered with the hypothesis of linear transfer. The
Dufort-Frankel, Crank-Nicolson and hyperbolisation schemes were compared to the
classical Euler explicit scheme and to a reference solution. Results have shown
that the hyperbolisation scheme has a stability condition higher than the
standard Courant-Friedrichs-Lewy (CFL) condition. The error of this schemes
depends on the parameter \tau representing the hyperbolicity magnitude added
into the equation. The Dufort-Frankel scheme has the advantages of being
unconditionally stable and is preferable for non-linear transfer, which is the
second case study. Results have shown the error is proportional to O(\Delta t).
A modified Crank-Nicolson scheme has been proposed in order to avoid
sub-iterations to treat the non-linearities at each time step. The main
advantages of the Dufort-Frankel scheme are (i) to be twice faster than the
Crank-Nicolson approach; (ii) to compute explicitly the solution at each time
step; (iii) to be unconditionally stable and (iv) easier to parallelise on
high-performance computer systems. Although the approach is unconditionally
stable, the choice of the time discretisation $\Delta t$ remains an important
issue to accurately represent the physical phenomena.
",1,1,0,0,0,0
7233,Statics and dynamics of a self-bound dipolar matter-wave droplet,"  We study the statics and dynamics of a stable, mobile, self-bound
three-dimensional dipolar matter-wave droplet created in the presence of a tiny
repulsive three-body interaction. In frontal collision with an impact parameter
and in angular collision at large velocities {along all directions} two
droplets behave like quantum solitons. Such collision is found to be quasi
elastic and the droplets emerge undeformed after collision without any change
of velocity. However, in a collision at small velocities the axisymmeric
dipolar interaction plays a significant role and the collision dynamics is
sensitive to the direction of motion. For an encounter along the $z$ direction
at small velocities, two droplets, polarized along the $z$ direction, coalesce
to form a larger droplet $-$ a droplet molecule. For an encounter along the $x$
direction at small velocities, the same droplets stay apart and never meet each
other due to the dipolar repulsion. The present study is based on an analytic
variational approximation and a numerical solution of the mean-field
Gross-Pitaevskii equation using the parameters of $^{52}$Cr atoms.
",0,1,0,0,0,0
7349,"How far are we from solving the 2D & 3D Face Alignment problem? (and a dataset of 230,000 3D facial landmarks)","  This paper investigates how far a very deep neural network is from attaining
close to saturating performance on existing 2D and 3D face alignment datasets.
To this end, we make the following 5 contributions: (a) we construct, for the
first time, a very strong baseline by combining a state-of-the-art architecture
for landmark localization with a state-of-the-art residual block, train it on a
very large yet synthetically expanded 2D facial landmark dataset and finally
evaluate it on all other 2D facial landmark datasets. (b) We create a guided by
2D landmarks network which converts 2D landmark annotations to 3D and unifies
all existing datasets, leading to the creation of LS3D-W, the largest and most
challenging 3D facial landmark dataset to date ~230,000 images. (c) Following
that, we train a neural network for 3D face alignment and evaluate it on the
newly introduced LS3D-W. (d) We further look into the effect of all
""traditional"" factors affecting face alignment performance like large pose,
initialization and resolution, and introduce a ""new"" one, namely the size of
the network. (e) We show that both 2D and 3D face alignment networks achieve
performance of remarkable accuracy which is probably close to saturating the
datasets used. Training and testing code as well as the dataset can be
downloaded from this https URL
",1,0,0,0,0,0
9268,A Web of Hate: Tackling Hateful Speech in Online Social Spaces,"  Online social platforms are beset with hateful speech - content that
expresses hatred for a person or group of people. Such content can frighten,
intimidate, or silence platform users, and some of it can inspire other users
to commit violence. Despite widespread recognition of the problems posed by
such content, reliable solutions even for detecting hateful speech are lacking.
In the present work, we establish why keyword-based methods are insufficient
for detection. We then propose an approach to detecting hateful speech that
uses content produced by self-identifying hateful communities as training data.
Our approach bypasses the expensive annotation process often required to train
keyword systems and performs well across several established platforms, making
substantial improvements over current state-of-the-art approaches.
",1,0,0,0,0,0
9084,Public Evidence from Secret Ballots,"  Elections seem simple---aren't they just counting? But they have a unique,
challenging combination of security and privacy requirements. The stakes are
high; the context is adversarial; the electorate needs to be convinced that the
results are correct; and the secrecy of the ballot must be ensured. And they
have practical constraints: time is of the essence, and voting systems need to
be affordable and maintainable, and usable by voters, election officials, and
pollworkers. It is thus not surprising that voting is a rich research area
spanning theory, applied cryptography, practical systems analysis, usable
security, and statistics. Election integrity involves two key concepts:
convincing evidence that outcomes are correct and privacy, which amounts to
convincing assurance that there is no evidence about how any given person
voted. These are obviously in tension. We examine how current systems walk this
tightrope.
",1,0,0,0,0,0
20883,Deep Learning for Semantic Segmentation on Minimal Hardware,"  Deep learning has revolutionised many fields, but it is still challenging to
transfer its success to small mobile robots with minimal hardware.
Specifically, some work has been done to this effect in the RoboCup humanoid
football domain, but results that are performant and efficient and still
generally applicable outside of this domain are lacking. We propose an approach
conceptually different from those taken previously. It is based on semantic
segmentation and does achieve these desired properties. In detail, it is being
able to process full VGA images in real-time on a low-power mobile processor.
It can further handle multiple image dimensions without retraining, it does not
require specific domain knowledge for achieving a high frame rate and it is
applicable on a minimal mobile hardware.
",1,0,0,1,0,0
8285,A Cluster Elastic Net for Multivariate Regression,"  We propose a method for estimating coefficients in multivariate regression
when there is a clustering structure to the response variables. The proposed
method includes a fusion penalty, to shrink the difference in fitted values
from responses in the same cluster, and an L1 penalty for simultaneous variable
selection and estimation. The method can be used when the grouping structure of
the response variables is known or unknown. When the clustering structure is
unknown the method will simultaneously estimate the clusters of the response
and the regression coefficients. Theoretical results are presented for the
penalized least squares case, including asymptotic results allowing for p >> n.
We extend our method to the setting where the responses are binomial variables.
We propose a coordinate descent algorithm for both the normal and binomial
likelihood, which can easily be extended to other generalized linear model
(GLM) settings. Simulations and data examples from business operations and
genomics are presented to show the merits of both the least squares and
binomial methods.
",0,0,0,1,0,0
13548,Resolving API Mentions in Informal Documents,"  Developer forums contain opinions and information related to the usage of
APIs. API names in forum posts are often not explicitly linked to their
official resources. Automatic linking of an API mention to its official
resources can be challenging for various reasons, such as, name overloading. We
present a technique, ANACE, to automatically resolve API mentions in the
textual contents of forum posts. Given a database of APIs, we first detect all
words in a forum post that are potential references to an API. We then use a
combination of heuristics and machine learning to eliminate false positives and
to link true positives to the actual APIs and their resources.
",1,0,0,0,0,0
7396,Multiscale mixing patterns in networks,"  Assortative mixing in networks is the tendency for nodes with the same
attributes, or metadata, to link to each other. It is a property often found in
social networks manifesting as a higher tendency of links occurring between
people with the same age, race, or political belief. Quantifying the level of
assortativity or disassortativity (the preference of linking to nodes with
different attributes) can shed light on the factors involved in the formation
of links and contagion processes in complex networks. It is common practice to
measure the level of assortativity according to the assortativity coefficient,
or modularity in the case of discrete-valued metadata. This global value is the
average level of assortativity across the network and may not be a
representative statistic when mixing patterns are heterogeneous. For example, a
social network spanning the globe may exhibit local differences in mixing
patterns as a consequence of differences in cultural norms. Here, we introduce
an approach to localise this global measure so that we can describe the
assortativity, across multiple scales, at the node level. Consequently we are
able to capture and qualitatively evaluate the distribution of mixing patterns
in the network. We find that for many real-world networks the distribution of
assortativity is skewed, overdispersed and multimodal. Our method provides a
clearer lens through which we can more closely examine mixing patterns in
networks.
",1,0,0,0,0,0
11802,"Voyager 1 Measurements Beyond the Heliopause of Galactic Cosmic Ray Helium, Boron, Carbon, Oxygen, Magnesium, Silicon and Iron Nuclei with Energies 0.5 to >1.5 GeV/nuc","  We have obtained the energy spectra of cosmic ray He, B, C, O, Mg, S and Fe
nuclei in the range 0.5-1.5 GeV/nuc and above using the penetrating particle
mode of the High Energy Telescope, part of the Cosmic Ray Science (CRS)
experiment on Voyagers 1 and 2. The data analysis procedures are the same as
those used to obtain similar spectra from the identical V2 HET telescope while
it was in the heliosphere between about 23 and 54 AU. The time period of
analysis includes 4 years of data beyond the heliopause (HP). These new
interstellar spectra are compared with various earlier experiments at the same
energies at the Earth to determine the solar modulation parameter, phi. These
new spectra are also compared with recent measurements of the spectra of the
same nuclei measured by the same telescope at low energies. It is found that
the ratio of intensities at 100 MeV/nuc to those at 1.0 GeV/nuc are
significantly Z dependent. Some of this Z dependence can be explained by the Z2
dependence of energy loss by ionization in the 7-10 g/cm2 of interstellar H and
He traversed by cosmic rays of these energies in the galaxy; some by the Z
dependent loss due to nuclear interactions in this same material; some by
possible differences in the source spectra of these nuclei and some by the
non-uniformity of the source distribution and propagation conditions. The
observed features of the spectra, also including a Z dependence of the peak
intensities of the various nuclei, pose interesting problems related to the
propagation and source distribution of these cosmic rays.
",0,1,0,0,0,0
16123,A Probability Monad as the Colimit of Finite Powers,"  We define and study a probability monad on the category of complete metric
spaces and short maps. It assigns to each space the space of Radon probability
measures on it with finite first moment, equipped with the
Kantorovich-Wasserstein distance. This monad is analogous to the Giry monad on
the category of Polish spaces, and it extends a construction due to van Breugel
for compact and for 1-bounded complete metric spaces.
We prove that this Kantorovich monad arises from a colimit construction on
finite powers, which formalizes the intuition that probability measures are
limits of finite samples. The proof relies on a criterion for when an ordinary
left Kan extension of lax monoidal functors is a monoidal Kan extension. The
colimit characterization allows the development of integration theory and the
treatment of measures on spaces of measures, without measure theory.
We also show that the category of algebras of the Kantorovich monad is
equivalent to the category of closed convex subsets of Banach spaces with short
affine maps as morphisms.
",1,0,0,0,0,0
670,Robust Detection of Covariate-Treatment Interactions in Clinical Trials,"  Detection of interactions between treatment effects and patient descriptors
in clinical trials is critical for optimizing the drug development process. The
increasing volume of data accumulated in clinical trials provides a unique
opportunity to discover new biomarkers and further the goal of personalized
medicine, but it also requires innovative robust biomarker detection methods
capable of detecting non-linear, and sometimes weak, signals. We propose a set
of novel univariate statistical tests, based on the theory of random walks,
which are able to capture non-linear and non-monotonic covariate-treatment
interactions. We also propose a novel combined test, which leverages the power
of all of our proposed univariate tests into a single general-case tool. We
present results for both synthetic trials as well as real-world clinical
trials, where we compare our method with state-of-the-art techniques and
demonstrate the utility and robustness of our approach.
",0,0,0,1,0,0
9374,Multiple Scaled Contaminated Normal Distribution and Its Application in Clustering,"  The multivariate contaminated normal (MCN) distribution represents a simple
heavy-tailed generalization of the multivariate normal (MN) distribution to
model elliptical contoured scatters in the presence of mild outliers, referred
to as ""bad"" points. The MCN can also automatically detect bad points. The price
of these advantages is two additional parameters, both with specific and useful
interpretations: proportion of good observations and degree of contamination.
However, points may be bad in some dimensions but good in others. The use of an
overall proportion of good observations and of an overall degree of
contamination is limiting. To overcome this limitation, we propose a multiple
scaled contaminated normal (MSCN) distribution with a proportion of good
observations and a degree of contamination for each dimension. Once the model
is fitted, each observation has a posterior probability of being good with
respect to each dimension. Thanks to this probability, we have a method for
simultaneous directional robust estimation of the parameters of the MN
distribution based on down-weighting and for the automatic directional
detection of bad points by means of maximum a posteriori probabilities. The
term ""directional"" is added to specify that the method works separately for
each dimension. Mixtures of MSCN distributions are also proposed as an
application of the proposed model for robust clustering. An extension of the EM
algorithm is used for parameter estimation based on the maximum likelihood
approach. Real and simulated data are used to show the usefulness of our
mixture with respect to well-established mixtures of symmetric distributions
with heavy tails.
",0,0,0,1,0,0
8632,Pi Visits Manhattan,"  Is it possible to draw a circle in Manhattan, using only its discrete network
of streets and boulevards? In this study, we will explore the construction and
properties of circular paths on an integer lattice, a discrete space where the
distance between two points is not governed by the familiar Euclidean metric,
but the Manhattan or taxicab distance, a metric linear in its coordinates. In
order to achieve consistency with the continuous ideal, we need to abandon
Euclid's very original definition of the circle in favour of a parametric
construction. Somewhat unexpectedly, we find that the Euclidean circle's
defining constant $\pi$ can be recovered in such a discrete setting.
",0,0,1,0,0,0
18093,T-matrix evaluation of acoustic radiation forces on nonspherical objects in Bessel beams,"  Acoustical radiation force (ARF) induced by a single Bessel beam with
arbitrary order and location on a nonspherical shape is studied with the
emphasis on the physical mechanism and parameter conditions of negative
(pulling) forces. Numerical experiments are conducted to verify the T-matrix
method (TMM) for axial ARFs. This study may guide the experimental set-up to
find negative axial ARF quickly and effectively based on the predicted
parameters with TMM, and could be extended for lateral forces. The present work
could help to design acoustic tweezers numerical toolbox, which provides an
alternate to the optic tweezers.
",0,1,0,0,0,0
19449,Social learning in a simple task allocation game,"  We investigate the effects of social interactions in task al- location using
Evolutionary Game Theory (EGT). We propose a simple task-allocation game and
study how different learning mechanisms can give rise to specialised and non-
specialised colonies under different ecological conditions. By combining
agent-based simulations and adaptive dynamics we show that social learning can
result in colonies of generalists or specialists, depending on ecological
parameters. Agent-based simulations further show that learning dynamics play a
crucial role in task allocation. In particular, introspective individual
learning readily favours the emergence of specialists, while a process
resembling task recruitment favours the emergence of generalists.
",1,0,0,0,0,0
13189,Floquet prethermalization in the resonantly driven Hubbard model,"  We demonstrate the existence of long-lived prethermalized states in the Mott
insulating Hubbard model driven by periodic electric fields. These states,
which also exist in the resonantly driven case with a large density of
photo-induced doublons and holons, are characterized by a nonzero current and
an effective temperature of the doublons and holons which depends sensitively
on the driving condition. Focusing on the specific case of resonantly driven
models whose effective time-independent Hamiltonian in the high-frequency
driving limit corresponds to noninteracting fermions, we show that the time
evolution of the double occupation can be reproduced by the effective
Hamiltonian, and that the prethermalization plateaus at finite driving
frequency are controlled by the next-to-leading order correction in the
high-frequency expansion of the effective Hamiltonian. We propose a numerical
procedure to determine an effective Hubbard interaction that mimics the
correlation effects induced by these higher order terms.
",0,1,0,0,0,0
9179,"$({\mathfrak{gl}}_M, {\mathfrak{gl}}_N)$-Dualities in Gaudin Models with Irregular Singularities","  We establish $({\mathfrak{gl}}_M, {\mathfrak{gl}}_N)$-dualities between
quantum Gaudin models with irregular singularities. Specifically, for any $M, N
\in {\mathbb Z}_{\geq 1}$ we consider two Gaudin models: the one associated
with the Lie algebra ${\mathfrak{gl}}_M$ which has a double pole at infinity
and $N$ poles, counting multiplicities, in the complex plane, and the same
model but with the roles of $M$ and $N$ interchanged. Both models can be
realized in terms of Weyl algebras, i.e., free bosons; we establish that, in
this realization, the algebras of integrals of motion of the two models
coincide. At the classical level we establish two further generalizations of
the duality. First, we show that there is also a duality for realizations in
terms of free fermions. Second, in the bosonic realization we consider the
classical cyclotomic Gaudin model associated with the Lie algebra
${\mathfrak{gl}}_M$ and its diagram automorphism, with a double pole at
infinity and $2N$ poles, counting multiplicities, in the complex plane. We
prove that it is dual to a non-cyclotomic Gaudin model associated with the Lie
algebra ${\mathfrak{sp}}_{2N}$, with a double pole at infinity and $M$ simple
poles in the complex plane. In the special case $N=1$ we recover the well-known
self-duality in the Neumann model.
",0,0,1,0,0,0
17806,Depicting urban boundaries from a mobility network of spatial interactions: A case study of Great Britain with geo-located Twitter data,"  Existing urban boundaries are usually defined by government agencies for
administrative, economic, and political purposes. Defining urban boundaries
that consider socio-economic relationships and citizen commute patterns is
important for many aspects of urban and regional planning. In this paper, we
describe a method to delineate urban boundaries based upon human interactions
with physical space inferred from social media. Specifically, we depicted the
urban boundaries of Great Britain using a mobility network of Twitter user
spatial interactions, which was inferred from over 69 million geo-located
tweets. We define the non-administrative anthropographic boundaries in a
hierarchical fashion based on different physical movement ranges of users
derived from the collective mobility patterns of Twitter users in Great
Britain. The results of strongly connected urban regions in the form of
communities in the network space yield geographically cohesive, non-overlapping
urban areas, which provide a clear delineation of the non-administrative
anthropographic urban boundaries of Great Britain. The method was applied to
both national (Great Britain) and municipal scales (the London metropolis).
While our results corresponded well with the administrative boundaries, many
unexpected and interesting boundaries were identified. Importantly, as the
depicted urban boundaries exhibited a strong instance of spatial proximity, we
employed a gravity model to understand the distance decay effects in shaping
the delineated urban boundaries. The model explains how geographical distances
found in the mobility patterns affect the interaction intensity among different
non-administrative anthropographic urban areas, which provides new insights
into human spatial interactions with urban space.
",1,1,0,0,0,0
6553,Polar codes with a stepped boundary,"  We consider explicit polar constructions of blocklength $n\rightarrow\infty$
for the two extreme cases of code rates $R\rightarrow1$ and $R\rightarrow0.$
For code rates $R\rightarrow1,$ we design codes with complexity order of $n\log
n$ in code construction, encoding, and decoding. These codes achieve the
vanishing output bit error rates on the binary symmetric channels with any
transition error probability $p\rightarrow 0$ and perform this task with a
substantially smaller redundancy $(1-R)n$ than do other known high-rate codes,
such as BCH codes or Reed-Muller (RM). We then extend our design to the
low-rate codes that achieve the vanishing output error rates with the same
complexity order of $n\log n$ and an asymptotically optimal code rate
$R\rightarrow0$ for the case of $p\rightarrow1/2.$
",1,0,0,0,0,0
41,"Covariances, Robustness, and Variational Bayes","  Mean-field Variational Bayes (MFVB) is an approximate Bayesian posterior
inference technique that is increasingly popular due to its fast runtimes on
large-scale datasets. However, even when MFVB provides accurate posterior means
for certain parameters, it often mis-estimates variances and covariances.
Furthermore, prior robustness measures have remained undeveloped for MFVB. By
deriving a simple formula for the effect of infinitesimal model perturbations
on MFVB posterior means, we provide both improved covariance estimates and
local robustness measures for MFVB, thus greatly expanding the practical
usefulness of MFVB posterior approximations. The estimates for MFVB posterior
covariances rely on a result from the classical Bayesian robustness literature
relating derivatives of posterior expectations to posterior covariances and
include the Laplace approximation as a special case. Our key condition is that
the MFVB approximation provides good estimates of a select subset of posterior
means---an assumption that has been shown to hold in many practical settings.
In our experiments, we demonstrate that our methods are simple, general, and
fast, providing accurate posterior uncertainty estimates and robustness
measures with runtimes that can be an order of magnitude faster than MCMC.
",0,0,0,1,0,0
2564,Quasiconvex elastodynamics: weak-strong uniqueness for measure-valued solutions,"  A weak-strong uniqueness result is proved for measure-valued solutions to the
system of conservation laws arising in elastodynamics. The main novelty brought
forward by the present work is that the underlying stored-energy function of
the material is assumed strongly quasiconvex. The proof employs tools from the
calculus of variations to establish general convexity-type bounds on
quasiconvex functions and recasts them in order to adapt the relative entropy
method to quasiconvex elastodynamics.
",0,0,1,0,0,0
17255,Nearly Instance Optimal Sample Complexity Bounds for Top-k Arm Selection,"  In the Best-$k$-Arm problem, we are given $n$ stochastic bandit arms, each
associated with an unknown reward distribution. We are required to identify the
$k$ arms with the largest means by taking as few samples as possible. In this
paper, we make progress towards a complete characterization of the
instance-wise sample complexity bounds for the Best-$k$-Arm problem. On the
lower bound side, we obtain a novel complexity term to measure the sample
complexity that every Best-$k$-Arm instance requires. This is derived by an
interesting and nontrivial reduction from the Best-$1$-Arm problem. We also
provide an elimination-based algorithm that matches the instance-wise lower
bound within doubly-logarithmic factors. The sample complexity of our algorithm
strictly dominates the state-of-the-art for Best-$k$-Arm (module constant
factors).
",1,0,0,1,0,0
14312,Shot noise and biased tracers: a new look at the halo model,"  Shot noise is an important ingredient to any measurement or theoretical
modeling of discrete tracers of the large scale structure. Recent work has
shown that the shot noise in the halo power spectrum becomes increasingly
sub-Poissonian at high mass. Interestingly, while the halo model predicts a
shot noise power spectrum in qualitative agreement with the data, it leads to
an unphysical white noise in the cross halo-matter and matter power spectrum.
In this work, we show that absorbing all the halo model sources of shot noise
into the halo fluctuation field leads to meaningful predictions for the shot
noise contributions to halo clustering statistics and remove the unphysical
white noise from the cross halo-matter statistics. Our prescription
straightforwardly maps onto the general bias expansion, so that the
renormalized shot noise terms can be expressed as combinations of the halo
model shot noises. Furthermore, we demonstrate that non-Poissonian
contributions are related to volume integrals over correlation functions and
their response to long-wavelength density perturbations. This leads to a new
class of consistency relations for discrete tracers, which appear to be
satisfied by our reformulation of the halo model. We test our theoretical
predictions against measurements of halo shot noise bispectra extracted from a
large suite of numerical simulations. Our model reproduces qualitatively the
observed sub-Poissonian noise, although it underestimates the magnitude of this
effect.
",0,1,0,0,0,0
10326,A note on a two-weight estimate for the dyadic square function,"  We show that the two-weight estimate for the dyadic square function proved by
Lacey--Li in [2] is sharp.
",0,0,1,0,0,0
19241,The barocaloric effect: A Spin-off of the Discovery of High-Temperature Superconductivity,"  Some key results obtained in joint research projects with Alex Müller are
summarized, concentrating on the invention of the barocaloric effect and its
application for cooling as well as on important findings in the field of
high-temperature superconductivity resulting from neutron scattering
experiments.
",0,1,0,0,0,0
5416,Spin pumping into superconductors: A new probe of spin dynamics in a superconducting thin film,"  Spin pumping refers to the microwave-driven spin current injection from a
ferromagnet into the adjacent target material. We theoretically investigate the
spin pumping into superconductors by fully taking account of impurity
spin-orbit scattering that is indispensable to describe diffusive spin
transport with finite spin diffusion length. We calculate temperature
dependence of the spin pumping signal and show that a pronounced coherence peak
appears immediately below the superconducting transition temperature Tc, which
survives even in the presence of the spin-orbit scattering. The phenomenon
provides us with a new way of studying the dynamic spin susceptibility in a
superconducting thin film. This is contrasted with the nuclear magnetic
resonance technique used to study a bulk superconductor.
",0,1,0,0,0,0
16466,Analysis of bacterial population growth using extended logistic growth model with distributed delay,"  In the present work, we develop a delayed Logistic growth model to study the
effects of decontamination on the bacterial population in the ambient
environment. Using the linear stability analysis, we study different case
scenarios, where bacterial population may establish at the positive equilibrium
or go extinct due to increased decontamination. The results are verified using
numerical simulation of the model.
",0,0,0,0,1,0
4555,Proton fire hose instabilities in the expanding solar wind,"  Using two-dimensional hybrid expanding box simulations we study the
competition between the continuously driven parallel proton temperature
anisotropy and fire hose instabilities in collisionless homogeneous plasmas.
For quasi radial ambient magnetic field the expansion drives
$T_{\mathrm{p}\|}>T_{\mathrm{p}\perp}$ and the system becomes eventually
unstable with respect to the dominant parallel fire hose instability. This
instability is generally unable to counteract the induced anisotropization and
the system typically becomes unstable with respect to the oblique fire hose
instability later on. The oblique instability efficiently reduces the
anisotropy and the system rapidly stabilizes while a significant part of the
generated electromagnetic fluctuations is damped to protons. As long as the
magnetic field is in the quasi radial direction, this evolution repeats itself
and the electromagnetic fluctuations accumulate. For sufficiently oblique
magnetic field the expansion drives $T_{\mathrm{p}\perp}>T_{\mathrm{p}\|}$ and
brings the system to the stable region with respect to the fire hose
instabilities.
",0,1,0,0,0,0
6521,Graph isomorphisms in quasi-polynomial time,"  Let us be given two graphs $\Gamma_1$, $\Gamma_2$ of $n$ vertices. Are they
isomorphic? If they are, the set of isomorphisms from $\Gamma_1$ to $\Gamma_2$
can be identified with a coset $H\cdot\pi$ inside the symmetric group on $n$
elements. How do we find $\pi$ and a set of generators of $H$?
The challenge of giving an always efficient algorithm answering these
questions remained open for a long time. Babai has recently shown how to solve
these problems -- and others linked to them -- in quasi-polynomial time, i.e.
in time $\exp\left(O(\log n)^{O(1)}\right)$. His strategy is based in part on
the algorithm by Luks (1980/82), who solved the case of graphs of bounded
degree.
",0,0,1,0,0,0
13733,Probabilistic Causal Analysis of Social Influence,"  Mastering the dynamics of social influence requires separating, in a database
of information propagation traces, the genuine causal processes from temporal
correlation, i.e., homophily and other spurious causes. However, most studies
to characterize social influence, and, in general, most data-science analyses
focus on correlations, statistical independence, or conditional independence.
Only recently, there has been a resurgence of interest in ""causal data
science"", e.g., grounded on causality theories. In this paper we adopt a
principled causal approach to the analysis of social influence from
information-propagation data, rooted in the theory of probabilistic causation.
Our approach consists of two phases. In the first one, in order to avoid the
pitfalls of misinterpreting causation when the data spans a mixture of several
subtypes (""Simpson's paradox""), we partition the set of propagation traces into
groups, in such a way that each group is as less contradictory as possible in
terms of the hierarchical structure of information propagation. To achieve this
goal, we borrow the notion of ""agony"" and define the Agony-bounded Partitioning
problem, which we prove being hard, and for which we develop two efficient
algorithms with approximation guarantees. In the second phase, for each group
from the first phase, we apply a constrained MLE approach to ultimately learn a
minimal causal topology. Experiments on synthetic data show that our method is
able to retrieve the genuine causal arcs w.r.t. a ground-truth generative
model. Experiments on real data show that, by focusing only on the extracted
causal structures instead of the whole social graph, the effectiveness of
predicting influence spread is significantly improved.
",1,0,0,1,0,0
2928,MATMPC - A MATLAB Based Toolbox for Real-time Nonlinear Model Predictive Control,"  In this paper we introduce MATMPC, an open source software built in MATLAB
for nonlinear model predictive control (NMPC). It is designed to facilitate
modelling, controller design and simulation for a wide class of NMPC
applications. MATMPC has a number of algorithmic modules, including automatic
differentiation, direct multiple shooting, condensing, linear quadratic program
(QP) solver and globalization. It also supports a unique Curvature-like Measure
of Nonlinearity (CMoN) MPC algorithm. MATMPC has been designed to provide
state-of-the-art performance while making the prototyping easy, also with
limited programming knowledge. This is achieved by writing each module directly
in MATLAB API for C. As a result, MATMPC modules can be compiled into MEX
functions with performance comparable to plain C/C++ solvers. MATMPC has been
successfully used in operating systems including WINDOWS, LINUX AND OS X.
Selected examples are shown to highlight the effectiveness of MATMPC.
",1,0,0,0,0,0
1595,Quantifying and suppressing ranking bias in a large citation network,"  It is widely recognized that citation counts for papers from different fields
cannot be directly compared because different scientific fields adopt different
citation practices. Citation counts are also strongly biased by paper age since
older papers had more time to attract citations. Various procedures aim at
suppressing these biases and give rise to new normalized indicators, such as
the relative citation count. We use a large citation dataset from Microsoft
Academic Graph and a new statistical framework based on the Mahalanobis
distance to show that the rankings by well known indicators, including the
relative citation count and Google's PageRank score, are significantly biased
by paper field and age. We propose a general normalization procedure motivated
by the $z$-score which produces much less biased rankings when applied to
citation count and PageRank score.
",1,1,0,1,0,0
16091,Autocomplete 3D Sculpting,"  Digital sculpting is a popular means to create 3D models but remains a
challenging task for many users. This can be alleviated by recent advances in
data-driven and procedural modeling, albeit bounded by the underlying data and
procedures. We propose a 3D sculpting system that assists users in freely
creating models without predefined scope. With a brushing interface similar to
common sculpting tools, our system silently records and analyzes users'
workflows, and predicts what they might or should do in the future to reduce
input labor or enhance output quality. Users can accept, ignore, or modify the
suggestions and thus maintain full control and individual style. They can also
explicitly select and clone past workflows over output model regions. Our key
idea is to consider how a model is authored via dynamic workflows in addition
to what it is shaped in static geometry, for more accurate analysis of user
intentions and more general synthesis of shape structures. The workflows
contain potential repetitions for analysis and synthesis, including user inputs
(e.g. pen strokes on a pressure sensing tablet), model outputs (e.g. extrusions
on an object surface), and camera viewpoints. We evaluate our method via user
feedbacks and authored models.
",1,0,0,0,0,0
13573,Locally-adaptive Bayesian nonparametric inference for phylodynamics,"  Phylodynamics is an area of population genetics that uses genetic sequence
data to estimate past population dynamics. Modern state-of-the-art Bayesian
nonparametric methods for phylodynamics use either change-point models or
Gaussian process priors to recover population size trajectories of unknown
form. Change-point models suffer from computational issues when the number of
change-points is unknown and needs to be estimated. Gaussian process-based
methods lack local adaptivity and cannot accurately recover trajectories that
exhibit features such as abrupt changes in trend or varying levels of
smoothness. We propose a novel, locally-adaptive approach to Bayesian
nonparametric phylodynamic inference that has the flexibility to accommodate a
large class of functional behaviors. Local adaptivity results from modeling the
log-transformed effective population size a priori as a horseshoe Markov random
field, a recently proposed statistical model that blends together the best
properties of the change-point and Gaussian process modeling paradigms. We use
simulated data to assess model performance, and find that our proposed method
results in reduced bias and increased precision when compared to contemporary
methods. We also use our models to reconstruct past changes in genetic
diversity of human hepatitis C virus in Egypt and to estimate population size
changes of ancient and modern steppe bison. These analyses show that our new
method captures features of the population size trajectories that were missed
by the state-of-the-art phylodynamic methods.
",0,0,0,0,1,0
4017,Estimating the sensitivity of centrality measures w.r.t. measurement errors,"  Most network studies rely on an observed network that differs from the
underlying network which is obfuscated by measurement errors. It is well known
that such errors can have a severe impact on the reliability of network
metrics, especially on centrality measures: a more central node in the observed
network might be less central in the underlying network.
We introduce a metric for the reliability of centrality measures -- called
sensitivity. Given two randomly chosen nodes, the sensitivity means the
probability that the more central node in the observed network is also more
central in the underlying network. The sensitivity concept relies on the
underlying network which is usually not accessible. Therefore, we propose two
methods to approximate the sensitivity. The iterative method, which simulates
possible underlying networks for the estimation and the imputation method,
which uses the sensitivity of the observed network for the estimation. Both
methods rely on the observed network and assumptions about the underlying type
of measurement error (e.g., the percentage of missing edges or nodes).
Our experiments on real-world networks and random graphs show that the
iterative method performs well in many cases. In contrast, the imputation
method does not yield useful estimations for networks other than
Erdős-Rényi graphs.
",1,1,0,0,0,0
15339,Exploiting Negative Curvature in Deterministic and Stochastic Optimization,"  This paper addresses the question of whether it can be beneficial for an
optimization algorithm to follow directions of negative curvature. Although
prior work has established convergence results for algorithms that integrate
both descent and negative curvature steps, there has not yet been extensive
numerical evidence showing that such methods offer consistent performance
improvements. In this paper, we present new frameworks for combining descent
and negative curvature directions: alternating two-step approaches and dynamic
step approaches. The aspect that distinguishes our approaches from ones
previously proposed is that they make algorithmic decisions based on
(estimated) upper-bounding models of the objective function. A consequence of
this aspect is that our frameworks can, in theory, employ fixed stepsizes,
which makes the methods readily translatable from deterministic to stochastic
settings. For deterministic problems, we show that instances of our dynamic
framework yield gains in performance compared to related methods that only
follow descent steps. We also show that gains can be made in a stochastic
setting in cases when a standard stochastic-gradient-type method might make
slow progress.
",0,0,1,0,0,0
12317,Finite Temperature Phase Diagrams of a Two-band Model of Superconductivity,"  We explore the temperature effects in the superconducting phases of a
hybridized two-band system. We show that for zero hybridization between the
bands, there are two different critical temperatures. However, for any finite
hybridization there are only one critical temperature at which the two gaps
vanish simultaneously. We construct the phase diagrams of the critical
temperature versus hybridization parameter $\alpha$ and critical temperature
versus critical chemical potential asymmetry $\delta \mu$ between the bands,
identifying the superconductor and normal phases in the system. We find an
interesting reentrant behavior in the superconducting phase as the parameters
$\alpha$ or $\delta \mu$, which drive the phase transitions, increase. We also
find that for optimal values of both $\alpha$ and $\delta \mu$ there is a
significant enhancement of the critical temperature of the model.
",0,1,0,0,0,0
1905,Making up for the deficit in a marathon run,"  To predict the final result of an athlete in a marathon run thoroughly is the
eternal desire of each trainer. Usually, the achieved result is weaker than the
predicted one due to the objective (e.g., environmental conditions) as well as
subjective factors (e.g., athlete's malaise). Therefore, making up for the
deficit between predicted and achieved results is the main ingredient of the
analysis performed by trainers after the competition. In the analysis, they
search for parts of a marathon course where the athlete lost time. This paper
proposes an automatic making up for the deficit by using a Differential
Evolution algorithm. In this case study, the results that were obtained by a
wearable sports-watch by an athlete in a real marathon are analyzed. The first
experiments with Differential Evolution show the possibility of using this
method in the future.
",1,0,0,0,0,0
3548,Audio to Body Dynamics,"  We present a method that gets as input an audio of violin or piano playing,
and outputs a video of skeleton predictions which are further used to animate
an avatar. The key idea is to create an animation of an avatar that moves their
hands similarly to how a pianist or violinist would do, just from audio. Aiming
for a fully detailed correct arms and fingers motion is a goal, however, it's
not clear if body movement can be predicted from music at all. In this paper,
we present the first result that shows that natural body dynamics can be
predicted at all. We built an LSTM network that is trained on violin and piano
recital videos uploaded to the Internet. The predicted points are applied onto
a rigged avatar to create the animation.
",1,0,0,0,0,0
13050,Optimal Multi-Object Segmentation with Novel Gradient Vector Flow Based Shape Priors,"  Shape priors have been widely utilized in medical image segmentation to
improve segmentation accuracy and robustness. A major way to encode such a
prior shape model is to use a mesh representation, which is prone to causing
self-intersection or mesh folding. Those problems require complex and expensive
algorithms to mitigate. In this paper, we propose a novel shape prior directly
embedded in the voxel grid space, based on gradient vector flows of a
pre-segmentation. The flexible and powerful prior shape representation is ready
to be extended to simultaneously segmenting multiple interacting objects with
minimum separation distance constraint. The problem is formulated as a Markov
random field problem whose exact solution can be efficiently computed with a
single minimum s-t cut in an appropriately constructed graph. The proposed
algorithm is validated on two multi-object segmentation applications: the brain
tissue segmentation in MRI images, and the bladder/prostate segmentation in CT
images. Both sets of experiments show superior or competitive performance of
the proposed method to other state-of-the-art methods.
",1,0,0,0,0,0
3717,The Autonomic Architecture of the Licas System,"  Licas (lightweight internet-based communication for autonomic services) is a
distributed framework for building service-based systems. The framework
provides a p2p server and more intelligent processing of information through
its AI algorithms. Distributed communication includes XML-RPC, REST, HTTP and
Web Services. It can now provide a robust platform for building different types
of system, where Microservices or SOA would be possible. However, the system
may be equally suited for the IoT, as it provides classes to connect with
external sources and has an optional Autonomic Manager with a MAPE control loop
integrated into the communication process. The system is also mobile-compatible
with Android. This paper focuses in particular on the autonomic setup and how
that might be used. A novel linking mechanism has been described previously [5]
that can be used to dynamically link sources and this is also considered, as
part of the autonomous framework.
",1,0,0,0,0,0
12766,Spectral Clustering Methods for Multiplex Networks,"  Multiplex networks offer an important tool for the study of complex systems
and extending techniques originally designed for single--layer networks is an
important area of study. One of the most important methods for analyzing
networks is clustering the nodes into communities that represent common
connectivity patterns. In this paper we extend spectral clustering to multiplex
structures and discuss some of the difficulties that arise in attempting to
define a natural generalization. In order to analyze our approach, we describe
three simple, synthetic multiplex networks and compare the performance of
different multiplex models. Our results suggest that a dynamically motivated
model is more successful than a structurally motivated model in discovering the
appropriate communities.
",1,1,0,0,0,0
10530,An Integer Programming Formulation of the Key Management Problem in Wireless Sensor Networks,"  With the advent of modern communications systems, much attention has been put
on developing methods for securely transferring information between
constituents of wireless sensor networks. To this effect, we introduce a
mathematical programming formulation for the key management problem, which
broadly serves as a mechanism for encrypting communications. In particular, an
integer programming model of the q-Composite scheme is proposed and utilized to
distribute keys among nodes of a network whose topology is known. Numerical
experiments demonstrating the effectiveness of the proposed model are conducted
using using a well-known optimization solver package. An illustrative example
depicting an optimal encryption for a small-scale network is also presented.
",1,0,0,0,0,0
7369,DADAM: A Consensus-based Distributed Adaptive Gradient Method for Online Optimization,"  Adaptive gradient-based optimization methods such as ADAGRAD, RMSPROP, and
ADAM are widely used in solving large-scale machine learning problems including
deep learning. A number of schemes have been proposed in the literature aiming
at parallelizing them, based on communications of peripheral nodes with a
central node, but incur high communications cost. To address this issue, we
develop a novel consensus-based distributed adaptive moment estimation method
(DADAM) for online optimization over a decentralized network that enables data
parallelization, as well as decentralized computation. The method is
particularly useful, since it can accommodate settings where access to local
data is allowed. Further, as established theoretically in this work, it can
outperform centralized adaptive algorithms, for certain classes of loss
functions used in applications. We analyze the convergence properties of the
proposed algorithm and provide a dynamic regret bound on the convergence rate
of adaptive moment estimation methods in both stochastic and deterministic
settings. Empirical results demonstrate that DADAM works also well in practice
and compares favorably to competing online optimization methods.
",1,0,0,1,0,0
5748,Coupling of multiscale and multi-continuum approaches,"  Simulating complex processes in fractured media requires some type of model
reduction. Well-known approaches include multi-continuum techniques, which have
been commonly used in approximating subgrid effects for flow and transport in
fractured media. Our goal in this paper is to (1) show a relation between
multi-continuum approaches and Generalized Multiscale Finite Element Method
(GMsFEM) and (2) to discuss coupling these approaches for solving problems in
complex multiscale fractured media. The GMsFEM, a systematic approach,
constructs multiscale basis functions via local spectral decomposition in
pre-computed snapshot spaces. We show that GMsFEM can automatically identify
separate fracture networks via local spectral problems. We discuss the relation
between these basis functions and continuums in multi-continuum methods. The
GMsFEM can automatically detect each continuum and represent the interaction
between the continuum and its surrounding (matrix). For problems with
simplified fracture networks, we propose a simplified basis construction with
the GMsFEM. This simplified approach is effective when the fracture networks
are known and have simplified geometries. We show that this approach can
achieve a similar result compared to the results using the GMsFEM with spectral
basis functions. Further, we discuss the coupling between the GMsFEM and
multi-continuum approaches. In this case, many fractures are resolved while for
unresolved fractures, we use a multi-continuum approach with local
Representative Volume Element (RVE) information. As a result, the method deals
with a system of equations on a coarse grid, where each equation represents one
of the continua on the fine grid. We present various basis construction
mechanisms and numerical results.
",0,0,1,0,0,0
8714,Deep Learning Super-Resolution Enables Rapid Simultaneous Morphological and Quantitative Magnetic Resonance Imaging,"  Obtaining magnetic resonance images (MRI) with high resolution and generating
quantitative image-based biomarkers for assessing tissue biochemistry is
crucial in clinical and research applications. How- ever, acquiring
quantitative biomarkers requires high signal-to-noise ratio (SNR), which is at
odds with high-resolution in MRI, especially in a single rapid sequence. In
this paper, we demonstrate how super-resolution can be utilized to maintain
adequate SNR for accurate quantification of the T2 relaxation time biomarker,
while simultaneously generating high- resolution images. We compare the
efficacy of resolution enhancement using metrics such as peak SNR and
structural similarity. We assess accuracy of cartilage T2 relaxation times by
comparing against a standard reference method. Our evaluation suggests that SR
can successfully maintain high-resolution and generate accurate biomarkers for
accelerating MRI scans and enhancing the value of clinical and research MRI.
",0,0,0,1,0,0
9173,Signal Recovery from Unlabeled Samples,"  In this paper, we study the recovery of a signal from a set of noisy linear
projections (measurements), when such projections are unlabeled, that is, the
correspondence between the measurements and the set of projection vectors
(i.e., the rows of the measurement matrix) is not known a priori. We consider a
special case of unlabeled sensing referred to as Unlabeled Ordered Sampling
(UOS) where the ordering of the measurements is preserved. We identify a
natural duality between this problem and classical Compressed Sensing (CS),
where we show that the unknown support (location of nonzero elements) of a
sparse signal in CS corresponds to the unknown indices of the measurements in
UOS. While in CS it is possible to recover a sparse signal from an
under-determined set of linear equations (less equations than the signal
dimension), successful recovery in UOS requires taking more samples than the
dimension of the signal. Motivated by this duality, we develop a Restricted
Isometry Property (RIP) similar to that in CS. We also design a low-complexity
Alternating Minimization algorithm that achieves a stable signal recovery under
the established RIP. We analyze our proposed algorithm for different signal
dimensions and number of measurements theoretically and investigate its
performance empirically via simulations. The results are reminiscent of
phase-transition similar to that occurring in CS.
",1,0,0,1,0,0
13042,A new precision measurement of the α-decay half-life of 190Pt,"  A laboratory measurement of the $\alpha$-decay half-life of $^{190}$Pt has
been performed using a low background Frisch grid ionisation chamber. A total
amount of 216.60(17) mg of natural platinum has been measured for 75.9 days.
The resulting half-life is $(4.97\pm0.16)\times 10^{11}$ years, with a total
uncertainty of 3.2%. This number is in good agreement with the half-life
obtained using the geological comparison method.
",0,1,0,0,0,0
18712,Realizing an optimization approach inspired from Piagets theory on cognitive development,"  The objective of this paper is to introduce an artificial intelligence based
optimization approach, which is inspired from Piagets theory on cognitive
development. The approach has been designed according to essential processes
that an individual may experience while learning something new or improving his
/ her knowledge. These processes are associated with the Piagets ideas on an
individuals cognitive development. The approach expressed in this paper is a
simple algorithm employing swarm intelligence oriented tasks in order to
overcome single-objective optimization problems. For evaluating effectiveness
of this early version of the algorithm, test operations have been done via some
benchmark functions. The obtained results show that the approach / algorithm
can be an alternative to the literature in terms of single-objective
optimization. The authors have suggested the name: Cognitive Development
Optimization Algorithm (CoDOA) for the related intelligent optimization
approach.
",1,0,1,0,0,0
8581,Record statistics of a strongly correlated time series: random walks and Lévy flights,"  We review recent advances on the record statistics of strongly correlated
time series, whose entries denote the positions of a random walk or a Lévy
flight on a line. After a brief survey of the theory of records for independent
and identically distributed random variables, we focus on random walks. During
the last few years, it was indeed realized that random walks are a very useful
""laboratory"" to test the effects of correlations on the record statistics. We
start with the simple one-dimensional random walk with symmetric jumps (both
continuous and discrete) and discuss in detail the statistics of the number of
records, as well as of the ages of the records, i.e., the lapses of time
between two successive record breaking events. Then we review the results that
were obtained for a wide variety of random walk models, including random walks
with a linear drift, continuous time random walks, constrained random walks
(like the random walk bridge) and the case of multiple independent random
walkers. Finally, we discuss further observables related to records, like the
record increments, as well as some questions raised by physical applications of
record statistics, like the effects of measurement error and noise.
",0,1,1,0,0,0
3407,Winds and radiation in unison: a new semi-analytic feedback model for cloud dissolution,"  Star clusters interact with the interstellar medium (ISM) in various ways,
most importantly in the destruction of molecular star-forming clouds, resulting
in inefficient star formation on galactic scales. On cloud scales, ionizing
radiation creates \hii regions, while stellar winds and supernovae drive the
ISM into thin shells. These shells are accelerated by the combined effect of
winds, radiation pressure and supernova explosions, and slowed down by gravity.
Since radiative and mechanical feedback is highly interconnected, they must be
taken into account in a self-consistent and combined manner, including the
coupling of radiation and matter. We present a new semi-analytic
one-dimensional feedback model for isolated massive clouds ($\geq
10^5\,M_{\odot}$) to calculate shell dynamics and shell structure
simultaneously. It allows us to scan a large range of physical parameters (gas
density, star formation efficiency, metallicity) and to estimate escape
fractions of ionizing radiation $f_{\rm{esc,i}}$, the minimum star formation
efficiency $\epsilon_{\rm{min}}$ required to drive an outflow, and recollapse
time scales for clouds that are not destroyed by feedback. Our results show
that there is no simple answer to the question of what dominates cloud
dynamics, and that each feedback process significantly influences the
efficiency of the others. We find that variations in natal cloud density can
very easily explain differences between dense-bound and diffuse-open star
clusters. We also predict, as a consequence of feedback, a $4-6$ Myr age
difference for massive clusters with multiple generations.
",0,1,0,0,0,0
16439,Multiphase Aluminum A356 Foam Formation Process Simulation Using Lattice Boltzmann Method,"  Shan-Chen model is a numerical scheme to simulate multiphase fluid flows
using Lattice Boltzmann approach. The original Shan-Chen model suffers from
inability to accurately predict behavior of air bubbles interacting in a
non-aqueous fluid. In the present study, we extended the Shan-Chen model to
take the effect of the attraction-repulsion barriers among bubbles in to
account. The proposed model corrects the interaction and coalescence criterion
of the original Shan-Chen scheme in order to have a more accurate simulation of
bubbles morphology in a metal foam. The model is based on forming a thin film
(narrow channel) between merging bubbles during growth. Rupturing of the film
occurs when an oscillation in velocity and pressure arises inside the channel
followed by merging of the bubbles. Comparing numerical results obtained from
proposed model with mettallorgraphy images for aluminum A356 demonstrated a
good consistency in mean bubble size and bubbles distribution
",1,1,0,0,0,0
13173,Remarks on Liouville Type Theorems for Steady-State Navier-Stokes Equations,"  Liouville type theorems for the stationary Navier-Stokes equations are proven
under certain assumptions. These assumptions are motivated by conditions that
appear in Liouvile type theorems for the heat equations with a given divergence
free drift.
",0,0,1,0,0,0
8509,Quantum Graphs: $ \mathcal{PT}$-symmetry and reflection symmetry of the spectrum,"  Not necessarily self-adjoint quantum graphs -- differential operators on
metric graphs -- are considered. Assume in addition that the underlying metric
graph possesses an automorphism (symmetry) $ \mathcal P $. If the differential
operator is $ \mathcal P \mathcal T$-symmetric, then its spectrum has
reflection symmetry with respect to the real line. Our goal is to understand
whether the opposite statement holds, namely whether the reflection symmetry of
the spectrum of a quantum graph implies that the underlying metric graph
possesses a non-trivial automorphism and the differential operator is $
\mathcal P \mathcal T$-symmetric. We give partial answer to this question by
considering equilateral star-graphs. The corresponding Laplace operator with
Robin vertex conditions possesses reflection-symmetric spectrum if and only if
the operator is $ \mathcal P \mathcal T$-symmetric with $ \mathcal P $ being an
automorphism of the metric graph.
",0,0,1,0,0,0
11002,Bursting dynamics of viscous film without circular symmetry: the effect of confinement,"  We experimentally investigate the bursting dynamics of confined liquid film
suspended in air and find a viscous dynamics distinctly different from the
non-confined counterpart, due to lack of circular symmetry in the shape of
expanding hole: the novel confined-viscous bursting proceeds at a constant
speed and a rim formed at the bursting tip does not grow. We find a
confined-viscous to confined-inertial crossover, as well as a
nonconfined-inertial to confined-inertial crossover, at which bursting speed
does not change although the circular symmetry in the hole shape breaks
dynamically.
",0,1,0,0,0,0
14581,Group actions and a multi-parameter Falconer distance problem,"  In this paper we study the following multi-parameter variant of the
celebrated Falconer distance problem. Given ${\textbf{d}}=(d_1,d_2, \dots,
d_{\ell})\in \mathbb{N}^{\ell}$ with $d_1+d_2+\dots+d_{\ell}=d$ and $E
\subseteq \mathbb{R}^d$, we define $$ \Delta_{\textbf{d}}(E) = \left\{
\left(|x^{(1)}-y^{(1)}|,\ldots,|x^{(\ell)}-y^{(\ell)}|\right) : x,y \in E
\right\} \subseteq \mathbb{R}^{\ell}, $$ where for $x\in \mathbb{R}^d$ we write
$x=\left( x^{(1)},\dots, x^{(\ell)} \right)$ with $x^{(i)} \in
\mathbb{R}^{d_i}$.
We ask how large does the Hausdorff dimension of $E$ need to be to ensure
that the $\ell$-dimensional Lebesgue measure of $\Delta_{\textbf{d}}(E)$ is
positive? We prove that if $2 \leq d_i$ for $1 \leq i \leq \ell$, then the
conclusion holds provided $$ \dim(E)>d-\frac{\min d_i}{2}+\frac{1}{3}.$$ We
also note that, by previous constructions, the conclusion does not in general
hold if $$\dim(E)<d-\frac{\min d_i}{2}.$$ A group action derivation of a
suitable Mattila integral plays an important role in the argument.
",0,0,1,0,0,0
2246,A note on the asymptotics of the modified Bessel functions on the Stokes lines,"  We employ the exponentially improved asymptotic expansions of the confluent
hypergeometric functions on the Stokes lines discussed by the author [Appl.
Math. Sci. {\bf 7} (2013) 6601--6609] to give the analogous expansions of the
modified Bessel functions $I_\nu(z)$ and $K_\nu(z)$ for large $z$ and finite
$\nu$ on $\arg\,z=\pm\pi$ (and, in the case of $I_\nu(z)$, also on
$\arg\,z=0$). Numerical results are presented to illustrate the accuracy of
these expansions.
",0,0,1,0,0,0
14159,Recursive computation of the invariant distribution of Markov and Feller processes,"  This paper provides a general and abstract approach to approximate ergodic
regimes of Markov and Feller processes. More precisely, we show that the
recursive algorithm presented in Lamberton & Pages (2002) and based on
simulation algorithms of stochastic schemes with decreasing step can be used to
build invariant measures for general Markov and Feller processes. We also
propose applications in three different configurations: Approximation of Markov
switching Brownian diffusion ergodic regimes using Euler scheme, approximation
of Markov Brownian diffusion ergodic regimes with Milstein scheme and
approximation of general diffusions with jump components ergodic regimes.
",0,0,1,0,0,0
12403,Generation of attosecond electron beams in relativistic ionization by short laser pulses,"  Ionization by relativistically intense short laser pulses is studied in the
framework of strong-field quantum electrodynamics. Distinctive patterns are
found in the energy probability distributions of photoelectrons. Except of the
already observed patterns, which were studied in Phys. Rev. A {\bf 94}, 013402
(2016), we discover an additional interference-free smooth supercontinuum in
the high-energy portion of the spectrum, reaching tens of kiloelectronovolts.
As we show, the latter is sensitive to the driving field intensity and it can
be detected in a narrow polar-angular window. Once these high-energy electrons
are collected, they can form solitary attosecond pulses. This is particularly
important in light of various applications of attosecond electron beams such as
in ultrafast electron diffraction and crystallography, or in time-resolved
electron microscopy of physical, chemical, and biological processes.
",0,1,0,0,0,0
10117,Topological dimension tunes activity patterns in hierarchical modular network models,"  Connectivity patterns of relevance in neuroscience and systems biology can be
encoded in hierarchical modular networks (HMNs). Moreover, recent studies
highlight the role of hierarchical modular organization in shaping brain
activity patterns, providing an excellent substrate to promote both the
segregation and integration of neural information. Here we propose an extensive
numerical analysis of the critical spreading rate (or ""epidemic"" threshold)
--separating a phase with endemic persistent activity from one in which
activity ceases-- on diverse HMNs. By employing analytical and computational
techniques we determine the nature of such a threshold and scrutinize how it
depends on general structural features of the underlying HMN. We critically
discuss the extent to which current graph-spectral methods can be applied to
predict the onset of spreading in HMNs, and we propose the network topological
dimension as a relevant and unifying structural parameter, controlling the
epidemic threshold.
",0,1,0,0,0,0
7767,On the lateral instability analysis of MEMS comb-drive electrostatic transducers,"  This paper investigates the lateral pull-in effect of an in-plane
overlap-varying transducer. The instability is induced by the translational and
rotational displacements. Based on the principle of virtual work, the
equilibrium conditions of force and moment in lateral directions are derived.
The analytical solutions of the critical voltage, at which the pull-in
phenomenon occurs, are developed when considering only the translational
stiffness or only the rotational stiffness of the mechanical spring. The
critical voltage in general case is numerically determined by using nonlinear
optimization techniques, taking into account the combined effect of translation
and rotation. The effects of possible translational offsets and angular
deviations to the critical voltage are modeled and numerically analyzed. The
investigation is then the first time expanded to anti-phase operation mode and
Bennet's doubler configuration of the two transducers.
",0,1,0,0,0,0
4835,Bayesian Bootstraps for Massive Data,"  Recently, two scalable adaptations of the bootstrap have been proposed: the
bag of little bootstraps (BLB; Kleiner et al., 2014) and the subsampled double
bootstrap (SDB; Sengupta et al., 2016). In this paper, we introduce Bayesian
bootstrap analogues to the BLB and SDB that have similar theoretical and
computational properties, a strategy to perform lossless inference for a class
of functionals of the Bayesian bootstrap, and briefly discuss extensions for
Dirichlet Processes.
",0,0,0,1,0,0
2333,Shape Convergence for Aggregate Tiles in Conformal Tilings,"  Given a substitution tiling $T$ of the plane with subdivision operator
$\tau$, we study the conformal tilings $\mathcal{T}_n$ associated with $\tau^n
T$. We prove that aggregate tiles within $\mathcal{T}_n$ converge in shape as
$n\rightarrow \infty$ to their associated Euclidean tiles in $T$.
",0,0,1,0,0,0
15272,BayesVP: a Bayesian Voigt profile fitting package,"  We introduce a Bayesian approach for modeling Voigt profiles in absorption
spectroscopy and its implementation in the python package, BayesVP, publicly
available at this https URL. The code fits the
absorption line profiles within specified wavelength ranges and generates
posterior distributions for the column density, Doppler parameter, and
redshifts of the corresponding absorbers. The code uses publicly available
efficient parallel sampling packages to sample posterior and thus can be run on
parallel platforms. BayesVP supports simultaneous fitting for multiple
absorption components in high-dimensional parameter space. We provide other
useful utilities in the package, such as explicit specification of priors of
model parameters, continuum model, Bayesian model comparison criteria, and
posterior sampling convergence check.
",0,1,0,0,0,0
18306,A dynamic stochastic blockmodel for interaction lengths,"  We propose a new dynamic stochastic blockmodel that focuses on the analysis
of interaction lengths in networks. The model does not rely on a discretization
of the time dimension and may be used to analyze networks that evolve
continuously over time. The framework relies on a clustering structure on the
nodes, whereby two nodes belonging to the same latent group tend to create
interactions and non-interactions of similar lengths. We introduce a fast
variational expectation-maximization algorithm to perform inference, and adapt
a widely used clustering criterion to perform model choice. Finally, we test
our methodology on artificial data, and propose a demonstration on a dataset
concerning face-to-face interactions between students in a high-school.
",0,0,0,1,0,0
7703,Fixed points of diffeomorphisms on nilmanifolds with a free nilpotent fundamental group,"  Let $M$ be a nilmanifold with a fundamental group which is free $2$-step
nilpotent on at least 4 generators. We will show that for any nonnegative
integer $n$ there exists a self-diffeomorphism $h_n$ of $M$ such that $h_n$ has
exactly $n$ fixed points and any self-map $f$ of $M$ which is homotopic to
$h_n$ has at least $n$ fixed points. We will also shed some light on the
situation for less generators and also for higher nilpotency classes.
",0,0,1,0,0,0
17908,The duration of load effect in lumber as stochastic degradation,"  This paper proposes a gamma process for modelling the damage that accumulates
over time in the lumber used in structural engineering applications when stress
is applied. The model separates the stochastic processes representing features
internal to the piece of lumber on the one hand, from those representing
external forces due to applied dead and live loads. The model applies those
external forces through a time-varying population level function designed for
time-varying loads. The application of this type of model, which is standard in
reliability analysis, is novel in this context, which has been dominated by
accumulated damage models (ADMs) over more than half a century. The proposed
model is compared with one of the traditional ADMs. Our statistical results
based on a Bayesian analysis of experimental data highlight the limitations of
using accelerated testing data to assess long-term reliability, as seen in the
wide posterior intervals. This suggests the need for more comprehensive testing
in future applications, or to encode appropriate expert knowledge in the priors
used for Bayesian analysis.
",0,0,0,1,0,0
2398,Production of Entanglement Entropy by Decoherence,"  We examine the dynamics of entanglement entropy of all parts in an open
system consisting of a two-level dimer interacting with an environment of
oscillators. The dimer-environment interaction is almost energy conserving. We
find the precise link between decoherence and production of entanglement
entropy. We show that not all environment oscillators carry significant
entanglement entropy and we identify the oscillator frequency regions which
contribute to the production of entanglement entropy. Our results hold for
arbitrary strengths of the dimer-environment interaction, and they are
mathematically rigorous.
",0,1,0,0,0,0
8835,Non-Oscillatory Pattern Learning for Non-Stationary Signals,"  This paper proposes a novel non-oscillatory pattern (NOP) learning scheme for
several oscillatory data analysis problems including signal decomposition,
super-resolution, and signal sub-sampling. To the best of our knowledge, the
proposed NOP is the first algorithm for these problems with fully
non-stationary oscillatory data with close and crossover frequencies, and
general oscillatory patterns. NOP is capable of handling complicated situations
while existing algorithms fail; even in simple cases, e.g., stationary cases
with trigonometric patterns, numerical examples show that NOP admits
competitive or better performance in terms of accuracy and robustness than
several state-of-the-art algorithms.
",0,0,0,1,0,0
15188,"On Synchronous, Asynchronous, and Randomized Best-Response schemes for computing equilibria in Stochastic Nash games","  This work considers a stochastic Nash game in which each player solves a
parameterized stochastic optimization problem. In deterministic regimes,
best-response schemes have been shown to be convergent under a suitable
spectral property associated with the proximal best-response map. However, a
direct application of this scheme to stochastic settings requires obtaining
exact solutions to stochastic optimization at each iteration. Instead, we
propose an inexact generalization in which an inexact solution is computed via
an increasing number of projected stochastic gradient steps. Based on this
framework, we present three inexact best-response schemes: (i) First, we
propose a synchronous scheme where all players simultaneously update their
strategies; (ii) Subsequently, we extend this to a randomized setting where a
subset of players is randomly chosen to their update strategies while the
others keep their strategies invariant; (iii) Finally, we propose an
asynchronous scheme, where each player determines its own update frequency and
may use outdated rival-specific data in updating its strategy. Under a suitable
contractive property of the proximal best-response map, we derive a.s.
convergence of the iterates for (i) and (ii) and mean-convergence for (i) --
(iii). In addition, we show that for (i) -- (iii), the iterates converge to the
unique equilibrium in mean at a prescribed linear rate. Finally, we establish
the overall iteration complexity in terms of projected stochastic gradient
steps for computing an $\epsilon-$Nash equilibrium and in all settings, the
iteration complexity is ${\cal O}(1/\epsilon^{2(1+c) + \delta})$ where $c = 0$
in the context of (i) and represents the positive cost of randomization (in
(ii)) and asynchronicity and delay (in (iii)). The schemes are further extended
to linear and quadratic recourse-based stochastic Nash games.
",0,0,1,0,0,0
8789,Bi-Lagrangian structures and Teichmüller theory,"  This paper has two purposes: the first is to study several structures on
manifolds in the general setting of real and complex differential geometry; the
second is to apply this study to Teichmüller theory. We primarily focus on
bi-Lagrangian structures, which are the data of a symplectic structure and a
pair of transverse Lagrangian foliations, and are equivalent to para-Kähler
structures. First we carefully study real and complex bi-Lagrangian structures
and discuss other closely related structures and their interrelationships. Next
we prove the existence of a canonical complex bi-Lagrangian structure in the
complexification of any real-analytic Kähler manifold and showcase its
properties. We later use this bi-Lagrangian structure to construct a natural
almost hyper-Hermitian structure. We then specialize our study to moduli spaces
of geometric structures on closed surfaces, which tend to have a rich
symplectic structure. We show that some of the recognized geometric features of
these moduli spaces are formal consequences of the general theory, while
revealing other new geometric features. We also gain clarity on several
well-known results of Teichmüller theory by deriving them from pure
differential geometric machinery.
",0,0,1,0,0,0
12799,Planar Drawings of Fixed-Mobile Bigraphs,"  A fixed-mobile bigraph G is a bipartite graph such that the vertices of one
partition set are given with fixed positions in the plane and the mobile
vertices of the other part, together with the edges, must be added to the
drawing. We assume that G is planar and study the problem of finding, for a
given k >= 0, a planar poly-line drawing of G with at most k bends per edge. In
the most general case, we show NP-hardness. For k=0 and under additional
constraints on the positions of the fixed or mobile vertices, we either prove
that the problem is polynomial-time solvable or prove that it belongs to NP.
Finally, we present a polynomial-time testing algorithm for a certain type of
""layered"" 1-bend drawings.
",1,0,0,0,0,0
7590,Construction of curve pairs and their applications,"  In this study, we introduce a new approach to curve pairs by using integral
curves. We consider the direction curve and donor curve to study curve couples
such as involute-evolute curves, Mannheim partner curves and Bertrand partner
curves. We obtain new methods to construct partner curves of a unit speed curve
and give some applications related to helices, slant helices and plane curves.
",0,0,1,0,0,0
2806,Fourth-order time-stepping for stiff PDEs on the sphere,"  We present in this paper algorithms for solving stiff PDEs on the unit sphere
with spectral accuracy in space and fourth-order accuracy in time. These are
based on a variant of the double Fourier sphere method in coefficient space
with multiplication matrices that differ from the usual ones, and
implicit-explicit time-stepping schemes. Operating in coefficient space with
these new matrices allows one to use a sparse direct solver, avoids the
coordinate singularity and maintains smoothness at the poles, while
implicit-explicit schemes circumvent severe restrictions on the time-steps due
to stiffness. A comparison is made against exponential integrators and it is
found that implicit-explicit schemes perform best. Implementations in MATLAB
and Chebfun make it possible to compute the solution of many PDEs to high
accuracy in a very convenient fashion.
",0,0,1,0,0,0
14234,Linear-Cost Covariance Functions for Gaussian Random Fields,"  Gaussian random fields (GRF) are a fundamental stochastic model for
spatiotemporal data analysis. An essential ingredient of GRF is the covariance
function that characterizes the joint Gaussian distribution of the field.
Commonly used covariance functions give rise to fully dense and unstructured
covariance matrices, for which required calculations are notoriously expensive
to carry out for large data. In this work, we propose a construction of
covariance functions that result in matrices with a hierarchical structure.
Empowered by matrix algorithms that scale linearly with the matrix dimension,
the hierarchical structure is proved to be efficient for a variety of random
field computations, including sampling, kriging, and likelihood evaluation.
Specifically, with $n$ scattered sites, sampling and likelihood evaluation has
an $O(n)$ cost and kriging has an $O(\log n)$ cost after preprocessing,
particularly favorable for the kriging of an extremely large number of sites
(e.g., predicting on more sites than observed). We demonstrate comprehensive
numerical experiments to show the use of the constructed covariance functions
and their appealing computation time. Numerical examples on a laptop include
simulated data of size up to one million, as well as a climate data product
with over two million observations.
",0,0,0,1,0,0
17532,Simulation assisted machine learning,"  Predicting how a proposed cancer treatment will affect a given tumor can be
cast as a machine learning problem, but the complexity of biological systems,
the number of potentially relevant genomic and clinical features, and the lack
of very large scale patient data repositories make this a unique challenge.
""Pure data"" approaches to this problem are underpowered to detect
combinatorially complex interactions and are bound to uncover false
correlations despite statistical precautions taken (1). To investigate this
setting, we propose a method to integrate simulations, a strong form of prior
knowledge, into machine learning, a combination which to date has been largely
unexplored. The results of multiple simulations (under various uncertainty
scenarios) are used to compute similarity measures between every pair of
samples: sample pairs are given a high similarity score if they behave
similarly under a wide range of simulation parameters. These similarity values,
rather than the original high dimensional feature data, are used to train
kernelized machine learning algorithms such as support vector machines, thus
handling the curse-of-dimensionality that typically affects genomic machine
learning. Using four synthetic datasets of complex systems--three biological
models and one network flow optimization model--we demonstrate that when the
number of training samples is small compared to the number of features, the
simulation kernel approach dominates over no-prior-knowledge methods. In
addition to biology and medicine, this approach should be applicable to other
disciplines, such as weather forecasting, financial markets, and agricultural
management, where predictive models are sought and informative yet approximate
simulations are available. The Python SimKern software, the models (in MATLAB,
Octave, and R), and the datasets are made freely available at
this https URL .
",0,0,0,1,1,0
10931,Exponentially Slow Heating in Short and Long-range Interacting Floquet Systems,"  We analyze the dynamics of periodically-driven (Floquet) Hamiltonians with
short- and long-range interactions, finding clear evidence for a thermalization
time, $\tau^*$, that increases exponentially with the drive frequency. We
observe this behavior, both in systems with short-ranged interactions, where
our results are consistent with rigorous bounds, and in systems with long-range
interactions, where such bounds do not exist at present. Using a combination of
heating and entanglement dynamics, we explicitly extract the effective energy
scale controlling the rate of thermalization. Finally, we demonstrate that for
times shorter than $\tau^*$, the dynamics of the system is well-approximated by
evolution under a time-independent Hamiltonian $D_{\mathrm{eff}}$, for both
short- and long-range interacting systems.
",0,1,0,0,0,0
9590,Testing High-dimensional Covariance Matrices under the Elliptical Distribution and Beyond,"  We study testing high-dimensional covariance matrices under a generalized
elliptical model. The model accommodates several stylized facts of real data
including heteroskedasticity, heavy-tailedness, asymmetry, etc. We consider the
high-dimensional setting where the dimension $p$ and the sample size $n$ grow
to infinity proportionally, and establish a central limit theorem for the
{linear spectral statistic} of the sample covariance matrix based on
self-normalized observations. The central limit theorem is different from the
existing ones for the linear spectral statistic of the usual sample covariance
matrix. Our tests based on the new central limit theorem neither assume a
specific parametric distribution nor involve the kurtosis of data. Simulation
studies show that our tests work well even when the fourth moment does not
exist. Empirically, we analyze the idiosyncratic returns under the Fama-French
three-factor model for S\&P 500 Financials sector stocks, and our tests reject
the hypothesis that the idiosyncratic returns are uncorrelated.
",0,0,1,1,0,0
19416,Realizability of tropical canonical divisors,"  We use recent results by Bainbridge-Chen-Gendron-Grushevsky-Moeller on
compactifications of strata of abelian differentials to give a comprehensive
solution to the realizability problem for effective tropical canonical divisors
in equicharacteristic zero. Given a pair $(\Gamma, D)$ consisting of a stable
tropical curve $\Gamma$ and a divisor $D$ in the canonical linear system on
$\Gamma$, we give a purely combinatorial condition to decide whether there is a
smooth curve $X$ over a non-Archimedean field whose stable reduction has
$\Gamma$ as its dual tropical curve together with a effective canonical divisor
$K_X$ that specializes to $D$. Along the way, we develop a moduli-theoretic
framework to understand Baker's specialization of divisors from algebraic to
tropical curves as a natural toroidal tropicalization map in the sense of
Abramovich-Caporaso-Payne.
",0,0,1,0,0,0
359,A one-dimensional model for water desalination by flow-through electrode capacitive deionization,"  Capacitive deionization (CDI) is a fast-emerging water desalination
technology in which a small cell voltage of ~1 V across porous carbon
electrodes removes salt from feedwaters via electrosorption. In flow-through
electrode (FTE) CDI cell architecture, feedwater is pumped through macropores
or laser perforated channels in porous electrodes, enabling highly compact
cells with parallel flow and electric field, as well as rapid salt removal. We
here present a one-dimensional model describing water desalination by FTE CDI,
and a comparison to data from a custom-built experimental cell. The model
employs simple cell boundary conditions derived via scaling arguments. We show
good model-to-data fits with reasonable values for fitting parameters such as
the Stern layer capacitance, micropore volume, and attraction energy. Thus, we
demonstrate that from an engineering modeling perspective, an FTE CDI cell may
be described with simpler one-dimensional models, unlike more typical
flow-between electrodes architecture where 2D models are required.
",1,1,0,0,0,0
232,On Scalable Inference with Stochastic Gradient Descent,"  In many applications involving large dataset or online updating, stochastic
gradient descent (SGD) provides a scalable way to compute parameter estimates
and has gained increasing popularity due to its numerical convenience and
memory efficiency. While the asymptotic properties of SGD-based estimators have
been established decades ago, statistical inference such as interval estimation
remains much unexplored. The traditional resampling method such as the
bootstrap is not computationally feasible since it requires to repeatedly draw
independent samples from the entire dataset. The plug-in method is not
applicable when there are no explicit formulas for the covariance matrix of the
estimator. In this paper, we propose a scalable inferential procedure for
stochastic gradient descent, which, upon the arrival of each observation,
updates the SGD estimate as well as a large number of randomly perturbed SGD
estimates. The proposed method is easy to implement in practice. We establish
its theoretical properties for a general class of models that includes
generalized linear models and quantile regression models as special cases. The
finite-sample performance and numerical utility is evaluated by simulation
studies and two real data applications.
",1,0,0,1,0,0
2602,Algebraic characterization of regular fractions under level permutations,"  In this paper we study the behavior of the fractions of a factorial design
under permutations of the factor levels. We focus on the notion of regular
fraction and we introduce methods to check whether a given symmetric orthogonal
array can or can not be transformed into a regular fraction by means of
suitable permutations of the factor levels. The proposed techniques take
advantage of the complex coding of the factor levels and of some tools from
polynomial algebra. Several examples are described, mainly involving factors
with five levels.
",0,0,1,1,0,0
9226,Minimal coloring number on minimal diagrams for $\mathbb{Z}$-colorable links,"  It was shown that any $\mathbb{Z}$-colorable link has a diagram which admits
a non-trivial $\mathbb{Z}$-coloring with at most four colors. In this paper, we
consider minimal numbers of colors for non-trivial $\mathbb{Z}$-colorings on
minimal diagrams of $\mathbb{Z}$-colorable links. We show, for any positive
integer $N$, there exists a minimal diagram of a $\mathbb{Z}$-colorable link
such that any $\mathbb{Z}$-coloring on the diagram has at least $N$ colors. On
the other hand, it is shown that certain $\mathbb{Z}$-colorable torus links
have minimal diagrams admitting $\mathbb{Z}$-colorings with only four colors.
",0,0,1,0,0,0
518,Topology optimization for transient response of structures subjected to dynamic loads,"  This paper presents a topology optimization framework for structural problems
subjected to transient loading. The mechanical model assumes a linear elastic
isotropic material, infinitesimal strains, and a dynamic response. The
optimization problem is solved using the gradient-based optimizer Method of
Moving Asymptotes (MMA) with time-dependent sensitivities provided via the
adjoint method. The stiffness of materials is interpolated using the Solid
Isotropic Material with Penalization (SIMP) method and the Heaviside Projection
Method (HPM) is used to stabilize the problem numerically and improve the
manufacturability of the topology-optimized designs. Both static and dynamic
optimization examples are considered here. The resulting optimized designs
demonstrate the ability of topology optimization to tailor the transient
response of structures.
",0,1,1,0,0,0
18065,Extracting and Exploiting Inherent Sparsity for Efficient IoT Support in 5G: Challenges and Potential Solutions,"  Besides enabling an enhanced mobile broadband, next generation of mobile
networks (5G) are envisioned for the support of massive connectivity of
heterogeneous Internet of Things (IoT)s. These IoTs are envisioned for a large
number of use-cases including smart cities, environment monitoring, smart
vehicles, etc. Unfortunately, most IoTs have very limited computing and storage
capabilities and need cloud services. Hence, connecting these devices through
5G systems requires huge spectrum resources in addition to handling the massive
connectivity and improved security. This article discusses the challenges
facing the support of IoTs through 5G systems. The focus is devoted to
discussing physical layer limitations in terms of spectrum resources and radio
access channel connectivity. We show how sparsity can be exploited for
addressing these challenges especially in terms of enabling wideband spectrum
management and handling the connectivity by exploiting device-to-device
communications and edge-cloud. Moreover, we identify major open problems and
research directions that need to be explored towards enabling the support of
massive heterogeneous IoTs through 5G systems.
",1,0,0,0,0,0
19834,On Generalized Gibbs Ensembles with an infinite set of conserved charges,"  We revisit the question of whether and how the steady states arising after
non-equilibrium time evolution in integrable models (and in particular in the
XXZ spin chain) can be described by the so-called Generalized Gibbs Ensemble
(GGE). It is known that the micro-canonical ensemble built on a complete set of
charges correctly describes the long-time limit of local observables, and
recently a canonical ensemble was built by Ilievski et. al. using particle
occupation number operators. Here we provide an alternative construction by
considering truncated GGE's (tGGE's) that only include a finite number of well
localized conserved operators. It is shown that the tGGE's can approximate the
steady states with arbitrary precision, i.e. all physical observables are
exactly reproduced in the infinite truncation limit. In addition, we show that
a complete canonical ensemble can in fact be built in terms of a new (discrete)
set of charges built as linear combinations of the standard ones.
Our general arguments are applied to concrete quench situations in the XXZ
chain, where the initial states are simple two-site or four-site product
states. Depending on the quench we find that numerical results for the local
correlators can be obtained with remarkable precision using truncated GGE's
with only 10-100 charges.
",0,1,0,0,0,0
5646,A Cofibration Category on Closure Spaces,"  We construct a cofibration category structure on the category of closure
spaces $\mathbf{Cl}$, the category whose objects are sets endowed with a
Čech closure operator and whose morphisms are the continuous maps between
them. We then study various closure structures on metric spaces, graphs, and
simplicial complexes, showing how each case gives rise to an interesting
homotopy theory. In particular, we show that there exists a natural family of
closure structures on metric spaces which produces a non-trivial homotopy
theory for finite metric spaces, i.e. point clouds, the spaces of interest in
topological data analysis. We then give a closure structure to graphs and
simplicial complexes which may be used to construct a new combinatorial (as
opposed to topological) homotopy theory for each skeleton of those spaces. We
show that there is a Seifert-van Kampen theorem for closure spaces, a
well-defined notion of persistent homotopy and an associated interleaving
distance, and, as an illustration of the difference with the topological
setting, we calculate the fundamental group for the circle and the wedge of
circles endowed with different closure structures.
",0,0,1,0,0,0
11444,On Optimal Weighted-Delay Scheduling in Input-Queued Switches,"  Motivated by relatively few delay-optimal scheduling results, in comparison
to results on throughput optimality, we investigate an input-queued switch
scheduling problem in which the objective is to minimize a linear function of
the queue-length vector. Theoretical properties of variants of the well-known
MaxWeight scheduling algorithm are established within this context, which
includes showing that these algorithms exhibit optimal heavy-traffic
queue-length scaling. For the case of $2 \times 2$ input-queued switches, we
derive an optimal scheduling policy and establish its theoretical properties,
demonstrating fundamental differences with the variants of MaxWeight
scheduling. Our theoretical results are expected to be of interest more broadly
than input-queued switches. Computational experiments demonstrate and quantify
the benefits of our optimal scheduling policy.
",0,0,1,0,0,0
19200,Training Shallow and Thin Networks for Acceleration via Knowledge Distillation with Conditional Adversarial Networks,"  There is an increasing interest on accelerating neural networks for real-time
applications. We study the student-teacher strategy, in which a small and fast
student network is trained with the auxiliary information learned from a large
and accurate teacher network. We propose to use conditional adversarial
networks to learn the loss function to transfer knowledge from teacher to
student. The proposed method is particularly effective for relatively small
student networks. Moreover, experimental results show the effect of network
size when the modern networks are used as student. We empirically study the
trade-off between inference time and classification accuracy, and provide
suggestions on choosing a proper student network.
",1,0,0,0,0,0
15117,Image-based Proof of Work Algorithm for the Incentivization of Blockchain Archival of Interesting Images,"  A new variation of blockchain proof of work algorithm is proposed to
incentivize the timely execution of image processing algorithms. A sample image
processing algorithm is proposed to determine interesting images using analysis
of the entropy of pixel subsets within images. The efficacy of the image
processing algorithm is examined using two small sets of training and test
data. The interesting image algorithm is then integrated into a simplified
blockchain mining proof of work algorithm based on Bitcoin. The incentive of
cryptocurrency mining is theorized to incentivize the execution of the
algorithm and thus the retrieval of images that satisfy a minimum requirement
set forth by the interesting image algorithm. The digital storage implications
of running an image- based blockchain are then examined mathematically.
",1,0,0,0,0,0
20818,Noise-synchronizability of opinion dynamics,"  With the analysis of noise-induced synchronization of opinion dynamics with
bounded confidence (BC), a natural and fundamental question is what opinion
structures can be synchronized by noise. In the traditional Hegselmann-Krause
(HK) model, each agent examines the opinion values of all the other ones and
then choose neighbors to update its own opinion according to the BC scheme. In
reality, people are more likely to interchange opinions with only some
individuals, resulting in a predetermined local discourse relationship as
introduced by the DeGroot model. In this paper, we consider an opinion dynamics
that combines the schemes of BC and local discourse topology and investigate
its synchronization induced by noise. The new model endows the heterogeneous HK
model with a time-varying discourse topology. With the proposed definition of
noise-synchronizability, it is shown that the compound noisy model is almost
surely noise-synchronizable if and only if the time-varying discourse graph is
uniformly jointly connected, taking the noise-induced synchronization of the
classical heterogeneous HK model as a special case. As a natural implication,
the result for the first time builds the equivalence between the connectivity
of discourse graph and the beneficial effect of noise for opinion consensus.
",1,0,0,0,0,0
6796,SRN: Side-output Residual Network for Object Symmetry Detection in the Wild,"  In this paper, we establish a baseline for object symmetry detection in
complex backgrounds by presenting a new benchmark and an end-to-end deep
learning approach, opening up a promising direction for symmetry detection in
the wild. The new benchmark, named Sym-PASCAL, spans challenges including
object diversity, multi-objects, part-invisibility, and various complex
backgrounds that are far beyond those in existing datasets. The proposed
symmetry detection approach, named Side-output Residual Network (SRN),
leverages output Residual Units (RUs) to fit the errors between the object
symmetry groundtruth and the outputs of RUs. By stacking RUs in a
deep-to-shallow manner, SRN exploits the 'flow' of errors among multiple scales
to ease the problems of fitting complex outputs with limited layers,
suppressing the complex backgrounds, and effectively matching object symmetry
of different scales. Experimental results validate both the benchmark and its
challenging aspects related to realworld images, and the state-of-the-art
performance of our symmetry detection approach. The benchmark and the code for
SRN are publicly available at this https URL.
",1,0,0,0,0,0
18871,Ab initio effective Hamiltonians for cuprate superconductors,"  Ab initio low-energy effective Hamiltonians of two typical high-temperature
copper-oxide superconductors, whose mother compounds are La$_2$CuO$_4$ and
HgBa$_2$CuO$_4$, are derived by utilizing the multi-scale ab initio scheme for
correlated electrons (MACE). The effective Hamiltonians obtained in the present
study serve as platforms of future studies to accurately solve the low-energy
effective Hamiltonians beyond the density functional theory. It allows further
study on the superconducting mechanism from the first principles and
quantitative basis without adjustable parameters not only for the available
cuprates but also for future design of higher Tc in general. More concretely,
we derive effective Hamiltonians for three variations, 1)one-band Hamiltonian
for the antibonding orbital generated from strongly hybridized Cu
$3d_{x^2-y^2}$ and O $2p_\sigma$ orbitals 2)two-band Hamiltonian constructed
from the antibonding orbital and Cu $3d_{3z^2-r^2}$ orbital hybridized mainly
with the apex oxygen $p_z$ orbital 3)three-band Hamiltonian consisting mainly
of Cu $3d_{x^2-y^2}$ orbitals and two O $2p_\sigma$ orbitals. Differences
between the Hamiltonians for La$_2$CuO$_4$ and HgBa$_2$CuO$_4$, which have
relatively low and high critical temperatures, respectively, at optimally doped
compounds, are elucidated. The main differences are summarized as i) the oxygen
$2p_\sigma$ orbitals are farther(~3.7eV) below from the Cu $d_{x^2-y^2}$
orbital for the La compound than the Hg compound(~2.4eV) in the three-band
Hamiltonian. This causes a substantial difference in the character of the
$d_{x^2-y^2}-2p_\sigma$ antibonding band at the Fermi level and makes the
effective onsite Coulomb interaction U larger for the La compound than the Hg
compound for the two- and one-band Hamiltonians. ii)The ratio of the
second-neighbor to the nearest transfer t'/t is also substantially different
(~0.26) for the Hg and ~0.15 for the La compound in the one-band Hamiltonian.
",0,1,0,0,0,0
17919,"CO~($J = 1-0$) Observations of a Filamentary Molecular Cloud in the Galactic Region Centered at $l = 150\arcdeg, b = 3.5\arcdeg$","  We present large-field (4.25~$\times$~3.75 deg$^2$) mapping observations
toward the Galactic region centered at $l = 150\arcdeg, b = 3.5\arcdeg$ in the
$J = 1-0$ emission line of CO isotopologues ($^{12}$CO, $^{13}$CO, and
C$^{18}$O), using the 13.7 m millimeter-wavelength telescope of the Purple
Mountain Observatory. Based on the $^{13}$CO observations, we reveal a
filamentary cloud in the Local Arm at a velocity range of $-$0.5 to
6.5~km~s$^{-1}$. This molecular cloud contains 1 main filament and 11
sub-filaments, showing the so-called ""ridge-nest"" structure. The main filament
and three sub-filaments are also detected in the C$^{18}$O line. The velocity
structures of most identified filaments display continuous distribution with
slight velocity gradients. The measured median excitation temperature, line
width, length, width, and linear mass of the filaments are $\sim$9.28~K,
0.85~km~s$^{-1}$, 7.30~pc, 0.79~pc, and 17.92~$M_\sun$~pc$^{-1}$, respectively,
assuming a distance of 400~pc. We find that the four filaments detected in the
C$^{18}$O line are thermally supercritical, and two of them are in the
virialized state, and thus tend to be gravitationally bound. We identify in
total 146 $^{13}$CO clumps in the cloud, about 77$\%$ of the clumps are
distributed along the filaments. About 56$\%$ of the virialized clumps are
found to be associated with the supercritical filaments. Three young stellar
object (YSO) candidates are also identified in the supercritical filaments,
based on the complementary infrared (IR) data. These results indicate that the
supercritical filaments, especially the virialized filaments, may contain
star-forming activities.
",0,1,0,0,0,0
802,Attention-Based Guided Structured Sparsity of Deep Neural Networks,"  Network pruning is aimed at imposing sparsity in a neural network
architecture by increasing the portion of zero-valued weights for reducing its
size regarding energy-efficiency consideration and increasing evaluation speed.
In most of the conducted research efforts, the sparsity is enforced for network
pruning without any attention to the internal network characteristics such as
unbalanced outputs of the neurons or more specifically the distribution of the
weights and outputs of the neurons. That may cause severe accuracy drop due to
uncontrolled sparsity. In this work, we propose an attention mechanism that
simultaneously controls the sparsity intensity and supervised network pruning
by keeping important information bottlenecks of the network to be active. On
CIFAR-10, the proposed method outperforms the best baseline method by 6% and
reduced the accuracy drop by 2.6x at the same level of sparsity.
",0,0,0,1,0,0
11910,Unsaturated deformable porous media flow with phase transition,"  In the present paper, a continuum model is introduced for fluid flow in a
deformable porous medium, where the fluid may undergo phase transitions.
Typically, such problems arise in modeling liquid-solid phase transformations
in groundwater flows. The system of equations is derived here from the
conservation principles for mass, momentum, and energy and from the
Clausius-Duhem inequality for entropy. It couples the evolution of the
displacement in the matrix material, of the capillary pressure, of the absolute
temperature, and of the phase fraction. Mathematical results are proved under
the additional hypothesis that inertia effects and shear stresses can be
neglected. For the resulting highly nonlinear system of two PDEs, one ODE and
one ordinary differential inclusion with natural initial and boundary
conditions, existence of global in time solutions is proved by means of cut-off
techniques and suitable Moser-type estimates.
",0,0,1,0,0,0
9624,Frank-Wolfe Optimization for Symmetric-NMF under Simplicial Constraint,"  Symmetric nonnegative matrix factorization has found abundant applications in
various domains by providing a symmetric low-rank decomposition of nonnegative
matrices. In this paper we propose a Frank-Wolfe (FW) solver to optimize the
symmetric nonnegative matrix factorization problem under a simplicial
constraint, which has recently been proposed for probabilistic clustering.
Compared with existing solutions, this algorithm is simple to implement, and
has no hyperparameters to be tuned. Building on the recent advances of FW
algorithms in nonconvex optimization, we prove an $O(1/\varepsilon^2)$
convergence rate to $\varepsilon$-approximate KKT points, via a tight bound
$\Theta(n^2)$ on the curvature constant, which matches the best known result in
unconstrained nonconvex setting using gradient methods. Numerical results
demonstrate the effectiveness of our algorithm. As a side contribution, we
construct a simple nonsmooth convex problem where the FW algorithm fails to
converge to the optimum. This result raises an interesting question about
necessary conditions of the success of the FW algorithm on convex problems.
",1,0,1,1,0,0
20953,Spatial Variational Auto-Encoding via Matrix-Variate Normal Distributions,"  The key idea of variational auto-encoders (VAEs) resembles that of
traditional auto-encoder models in which spatial information is supposed to be
explicitly encoded in the latent space. However, the latent variables in VAEs
are vectors, which can be interpreted as multiple feature maps of size 1x1.
Such representations can only convey spatial information implicitly when
coupled with powerful decoders. In this work, we propose spatial VAEs that use
feature maps of larger size as latent variables to explicitly capture spatial
information. This is achieved by allowing the latent variables to be sampled
from matrix-variate normal (MVN) distributions whose parameters are computed
from the encoder network. To increase dependencies among locations on latent
feature maps and reduce the number of parameters, we further propose spatial
VAEs via low-rank MVN distributions. Experimental results show that the
proposed spatial VAEs outperform original VAEs in capturing rich structural and
spatial information.
",1,0,0,1,0,0
877,Refracting Metasurfaces without Spurious Diffraction,"  Refraction represents one of the most fundamental operations that may be
performed by a metasurface. However, simple phasegradient metasurface designs
suffer from restricted angular deflection due to spurious diffraction orders.
It has been recently shown, using a circuit-based approach, that refraction
without spurious diffraction, or diffraction-free, can fortunately be achieved
by a transverse metasurface exhibiting either loss-gain or bianisotropy. Here,
we rederive these conditions using a medium-based - and hence more insightfull
- approach based on Generalized Sheet Transition Conditions (GSTCs) and surface
susceptibility tensors, and experimentally demonstrate two diffraction-free
refractive metasurfaces that are essentially lossless, passive, bianisotropic
and reciprocal.
",0,1,0,0,0,0
2651,Risk measure estimation for $β$-mixing time series and applications,"  In this paper, we discuss the application of extreme value theory in the
context of stationary $\beta$-mixing sequences that belong to the Fréchet
domain of attraction. In particular, we propose a methodology to construct
bias-corrected tail estimators. Our approach is based on the combination of two
estimators for the extreme value index to cancel the bias. The resulting
estimator is used to estimate an extreme quantile. In a simulation study, we
outline the performance of our proposals that we compare to alternative
estimators recently introduced in the literature. Also, we compute the
asymptotic variance in specific examples when possible. Our methodology is
applied to two datasets on finance and environment.
",0,0,1,1,0,0
6325,Partitioning the Outburst Energy of a Low Eddington Accretion Rate AGN at the Center of an Elliptical Galaxy: the Recent 12 Myr History of the Supermassive Black Hole in M87,"  M87, the active galaxy at the center of the Virgo cluster, is ideal for
studying the interaction of a supermassive black hole (SMBH) with a hot,
gas-rich environment. A deep Chandra observation of M87 exhibits an
approximately circular shock front (13 kpc radius, in projection) driven by the
expansion of the central cavity (filled by the SMBH with relativistic
radio-emitting plasma) with projected radius $\sim$1.9 kpc. We combine
constraints from X-ray and radio observations of M87 with a shock model to
derive the properties of the outburst that created the 13 kpc shock. Principal
constraints for the model are 1) the measured Mach number ($M$$\sim$1.2), 2)
the radius of the 13 kpc shock, and 3) the observed size of the central
cavity/bubble (the radio-bright cocoon) that serves as the piston to drive the
shock. We find an outburst of $\sim$5$\times$$10^{57}$ ergs that began about 12
Myr ago and lasted $\sim$2 Myr matches all the constraints. In this model,
$\sim$22% of the energy is carried by the shock as it expands. The remaining
$\sim$80% of the outburst energy is available to heat the core gas. More than
half the total outburst energy initially goes into the enthalpy of the central
bubble, the radio cocoon. As the buoyant bubble rises, much of its energy is
transferred to the ambient thermal gas. For an outburst repetition rate of
about 12 Myrs (the age of the outburst), 80% of the outburst energy is
sufficient to balance the radiative cooling.
",0,1,0,0,0,0
19876,Milnor and Tjurina numbers for a hypersurface germ with isolated singularity,"  Assume that $f:(\mathbb{C}^n,0) \to (\mathbb{C},0)$ is an analytic function
germ at the origin with only isolated singularity. Let $\mu$ and $\tau$ be the
corresponding Milnor and Tjurina numbers. We show that $\dfrac{\mu}{\tau} \leq
n$. As an application, we give a lower bound for the Tjurina number in terms of
$n$ and the multiplicity of $f$ at the origin.
",0,0,1,0,0,0
8044,Equivariant mirror symmetry for the weighted projective line,"  In this paper, we establish equivariant mirror symmetry for the weighted
projective line. This extends the results by B. Fang, C.C. Liu and Z. Zong,
where the projective line was considered [{\it Geometry \& Topology}
24:2049-2092, 2017]. More precisely, we prove the equivalence of the
$R$-matrices for A-model and B-model at large radius limit, and establish
isomorphism for $R$-matrices for general radius. We further demonstrate that
the graph sum of higher genus cases are the same for both models, hence
establish equivariant mirror symmetry for the weighted projective line.
",0,0,1,0,0,0
13102,Robust Bayesian Model Selection for Variable Clustering with the Gaussian Graphical Model,"  Variable clustering is important for explanatory analysis. However, only few
dedicated methods for variable clustering with the Gaussian graphical model
have been proposed. Even more severe, small insignificant partial correlations
due to noise can dramatically change the clustering result when evaluating for
example with the Bayesian Information Criteria (BIC). In this work, we try to
address this issue by proposing a Bayesian model that accounts for negligible
small, but not necessarily zero, partial correlations. Based on our model, we
propose to evaluate a variable clustering result using the marginal likelihood.
To address the intractable calculation of the marginal likelihood, we propose
two solutions: one based on a variational approximation, and another based on
MCMC. Experiments on simulated data shows that the proposed method is similarly
accurate as BIC in the no noise setting, but considerably more accurate when
there are noisy partial correlations. Furthermore, on real data the proposed
method provides clustering results that are intuitively sensible, which is not
always the case when using BIC or its extensions.
",0,0,0,1,0,0
15428,Particle-hole symmetry of charge excitation spectra in the paramagnetic phase of the Hubbard model,"  The Kotliar and Ruckenstein slave-boson representation of the Hubbard model
allows to obtain an approximation of the charge dynamical response function
resulting from the Gaussian fluctuations around the paramagnetic saddle-point
in analytical form. Numerical evaluation in the thermodynamical limit yields
charge excitation spectra consisting of a continuum, a gapless collective mode
with anisotropic zero-sound velocity, and a correlation induced high-frequency
mode at $\omega\approx U$. In this work we show that this analytical expression
obeys the particle-hole symmetry of the model on any bipartite lattice with one
atom in the unit cell. Other formal aspects of the approach are also addressed.
",0,1,0,0,0,0
7918,Solving 1ODEs with functions,"  Here we present a new approach to deal with first order ordinary differential
equations (1ODEs), presenting functions. This method is an alternative to the
one we have presented in [1]. In [2], we have establish the theoretical
background to deal, in the extended Prelle-Singer approach context, with
systems of 1ODEs. In this present paper, we will apply these results in order
to produce a method that is more efficient in a great number of cases.
Directly, the solving of 1ODEs is applicable to any problem presenting
parameters to which the rate of change is related to the parameter itself.
Apart from that, the solving of 1ODEs can be a part of larger mathematical
processes vital to dealing with many problems.
",0,1,1,0,0,0
9542,Multi-Dimensional Conservation Laws and Integrable Systems,"  In this paper we introduce a new property of two-dimensional integrable
systems -- existence of infinitely many local three-dimensional conservation
laws for pairs of integrable two-dimensional commuting flows. Infinitely many
three-dimensional local conservation laws for the Korteweg de Vries pair of
commuting flows and for the Benney commuting hydrodynamic chains are
constructed. As a by-product we established a new method for computation of
local conservation laws for three-dimensional integrable systems. The Mikhalev
equation and the dispersionless limit of the Kadomtsev--Petviashvili equation
are investigated. All known local and infinitely many new quasi-local
three-dimensional conservation laws are presented. Also four-dimensional
conservation laws are considered for couples of three-dimensional integrable
quasilinear systems and for triples of corresponding hydrodynamic chains.
",0,1,0,0,0,0
13505,Tilings of convex sets by mutually incongruent equilateral triangles contain arbitrarily small tiles,"  We show that every tiling of a convex set in the Euclidean plane
$\mathbb{R}^2$ by equilateral triangles of mutually different sizes contains
arbitrarily small tiles. The proof is purely elementary up to the discussion of
one family of tilings of the full plane $\mathbb{R}^2$, which is based on a
surprising connection to a random walk on a directed graph.
",0,0,1,0,0,0
18043,Constraints on the sum of neutrino masses using cosmological data including the latest extended Baryon Oscillation Spectroscopic Survey DR14 quasar sample,"  We investigate the constraints on the sum of neutrino masses ($\Sigma m_\nu$)
using the most recent cosmological data, which combines the distance
measurement from baryonic acoustic oscillation in the extended Baryon
Oscillation Spectroscopic Survey DR14 quasar sample with the power spectra of
temperature and polarization anisotropies in the cosmic microwave background
from the Planck 2015 data release. We also use other low-redshift observations
including the baryonic acoustic oscillation at relatively low redshifts, the
supernovae of type Ia and the local measurement of Hubble constant. In the
standard cosmological constant $\Lambda$ cold dark matter plus massive neutrino
model, we obtain the $95\%$ \acl{CL} upper limit to be $\Sigma
m_\nu<0.129~\mathrm{eV}$ for the degenerate mass hierarchy, $\Sigma
m_{\nu}<0.159~\mathrm{eV}$ for the normal mass hierarchy, and $\Sigma
m_{\nu}<0.189~\mathrm{eV}$ for the inverted mass hierarchy. Based on Bayesian
evidence, we find that the degenerate hierarchy is positively supported, and
the current data combination can not distinguish normal and inverted
hierarchies. Assuming the degenerate mass hierarchy, we extend our study to
non-standard cosmological models including the generic dark energy, the spatial
curvature, and the extra relativistic degrees of freedom, respectively, but
find these models not favored by the data.
",0,1,0,0,0,0
9596,Phase boundaries in alternating field quantum XY model with Dzyaloshinskii-Moriya interaction: Sustainable entanglement in dynamics,"  We report all phases and corresponding critical lines of the quantum
anisotropic transverse XY model with Dzyaloshinskii-Moriya (DM) interaction
along with uniform and alternating transverse magnetic fields (ATXY) by using
appropriately chosen order parameters. We prove that when DM interaction is
weaker than the anisotropy parameter, it has no effect at all on the
zero-temperature states of the XY model with uniform transverse magnetic field
which is not the case for the ATXY model. However, when DM interaction is
stronger than the anisotropy parameter, we show appearance of a new gapless
phase - a chiral phase - in the XY model with uniform as well as alternating
field. We further report that first derivatives of nearest neighbor two-site
entanglement with respect to magnetic fields can detect all the critical lines
present in the system. We also observe that the factorization surface at
zero-temperature present in this model without DM interaction becomes a volume
on the introduction of the later. We find that DM interaction can generate
bipartite entanglement sustainable at large times, leading to a proof of
ergodic nature of bipartite entanglement in this system, and can induce a
transition from non-monotonicity of entanglement with temperature to a
monotonic one.
",0,1,0,0,0,0
9954,Some remarks on Huisken's monotonicity formula for mean curvature flow,"  We discuss a monotone quantity related to Huisken's monotonicity formula and
some technical consequences for mean curvature flow.
",0,0,1,0,0,0
7425,Superfluidity and relaxation dynamics of a laser-stirred 2D Bose gas,"  We investigate the superfluid behavior of a two-dimensional (2D) Bose gas of
$^{87}$Rb atoms using classical field dynamics. In the experiment by R.
Desbuquois \textit{et al.}, Nat. Phys. \textbf{8}, 645 (2012), a 2D
quasicondensate in a trap is stirred by a blue-detuned laser beam along a
circular path around the trap center. Here, we study this experiment from a
theoretical perspective. The heating induced by stirring increases rapidly
above a velocity $v_c$, which we define as the critical velocity. We identify
the superfluid, the crossover, and the thermal regime by a finite, a sharply
decreasing, and a vanishing critical velocity, respectively. We demonstrate
that the onset of heating occurs due to the creation of vortex-antivortex
pairs. A direct comparison of our numerical results to the experimental ones
shows good agreement, if a systematic shift of the critical phase-space density
is included. We relate this shift to the absence of thermal equilibrium between
the condensate and the thermal wings, which were used in the experiment to
extract the temperature. We expand on this observation by studying the full
relaxation dynamics between the condensate and the thermal cloud.
",0,1,0,0,0,0
13693,Continuous cocycle superrigidity for coinduced actions and relative ends,"  We prove that certain coinduced actions for an inclusion of finitely
generated commensurated subgroups with relative one end are continuous cocycle
superrigid actions. We also show the necessity for the relative end assumption.
",0,0,1,0,0,0
2747,Galaxy Rotation and Supermassive Black Hole Binary Evolution,"  Supermassive black hole (SMBH) binaries residing at the core of merging
galaxies are recently found to be strongly affected by the rotation of their
host galaxies. The highly eccentric orbits that form when the host is
counterrotating emit strong bursts of gravitational waves that propel rapid
SMBH binary coalescence. Most prior work, however, focused on planar orbits and
a uniform rotation profile, an unlikely interaction configuration. However, the
coupling between rotation and SMBH binary evolution appears to be such a strong
dynamical process that it warrants further investigation. This study uses
direct N-body simulations to isolate the effect of galaxy rotation in more
realistic interactions. In particular, we systematically vary the SMBH orbital
plane with respect to the galaxy rotation axis, the radial extent of the
rotating component, and the initial eccentricity of the SMBH binary orbit. We
find that the initial orbital plane orientation and eccentricity alone can
change the inspiral time by an order of magnitude. Because SMBH binary inspiral
and merger is such a loud gravitational wave source, these studies are critical
for the future gravitational wave detector, LISA, an ESA/NASA mission currently
set to launch by 2034.
",0,1,0,0,0,0
2145,Ride Sharing and Dynamic Networks Analysis,"  The potential of an efficient ride-sharing scheme to significantly reduce
traffic congestion, lower emission level, as well as facilitating the
introduction of smart cities has been widely demonstrated. This positive thrust
however is faced with several delaying factors, one of which is the volatility
and unpredictability of the potential benefit (or utilization) of ride-sharing
at different times, and in different places. In this work the following
research questions are posed: (a) Is ride-sharing utilization stable over time
or does it undergo significant changes? (b) If ride-sharing utilization is
dynamic, can it be correlated with some traceable features of the traffic? and
(c) If ride-sharing utilization is dynamic, can it be predicted ahead of time?
We analyze a dataset of over 14 Million taxi trips taken in New York City. We
propose a dynamic travel network approach for modeling and forecasting the
potential ride-sharing utilization over time, showing it to be highly volatile.
In order to model the utilization's dynamics we propose a network-centric
approach, projecting the aggregated traffic taken from continuous time periods
into a feature space comprised of topological features of the network implied
by this traffic. This feature space is then used to model the dynamics of
ride-sharing utilization over time. The results of our analysis demonstrate the
significant volatility of ride-sharing utilization over time, indicating that
any policy, design or plan that would disregard this aspect and chose a static
paradigm would undoubtably be either highly inefficient or provide insufficient
resources. We show that using our suggested approach it is possible to model
the potential utilization of ride sharing based on the topological properties
of the rides network. We also show that using this method the potential
utilization can be forecasting a few hours ahead of time.
",1,1,0,0,0,0
10733,Capacity of the Aperture-Constrained AWGN Free-Space Communication Channel,"  In this paper, we derive upper and lower bounds as well as a simple
closed-form approximation for the capacity of the continuous-time, bandlimited,
additive white Gaussian noise channel in a three-dimensional free-space
electromagnetic propagation environment subject to constraints on the total
effective antenna aperture area of the link and a total transmitter power
constraint. We assume that the communication range is much larger than the
radius of the sphere containing the antennas at both ends of the link, and we
show that, in general, the capacity can only be achieved by transmitting
multiple spatially-multiplexed data streams simultaneously over the channel.
Furthermore, the lower bound on capacity can be approached asymptotically by
transmitting the data streams between a pair of physically-realizable
distributed antenna arrays at either end of the link. A consequence of this
result is that, in general, communication at close to the maximum achievable
data rate on a deep-space communication link can be achieved in practice if and
only if the communication system utilizes spatial multiplexing over a
distributed MIMO antenna array. Such an approach to deep-space communication
does not appear to be envisioned currently by any of the international space
agencies or any commercial space companies. A second consequence is that the
capacity of a long-range free-space communication link, if properly utilized,
grows asymptotically as a function of the square root of the received SNR
rather than only logarithmically in the received SNR.
",1,0,0,0,0,0
12483,Strong magnetic frustration in Y$_{3}$Cu$_{9}$(OH)$_{19}$Cl$_{8}$: a distorted kagome antiferromagnet,"  We present the crystal structure and magnetic properties of
Y$_{3}$Cu$_{9}$(OH)$_{19}$Cl$_{8}$, a stoichiometric frustrated quantum spin
system with slightly distorted kagome layers. Single crystals of
Y$_{3}$Cu$_{9}$(OH)$_{19}$Cl$_{8}$ were grown under hydrothermal conditions.
The structure was determined from single crystal X-ray diffraction and
confirmed by neutron powder diffraction. The observed structure reveals two
different Cu-positions leading to a slightly distored kagome layer in contrast
to the closely related YCu$_{3}$(OH)$_{6}$Cl$_{3}$. Curie-Weiss behavior at
high-temperatures with a Weiss-temperature $\theta_{W}$ of the order of $-100$
K, shows a large dominant antiferromagnetic coupling within the kagome planes.
Specific-heat and magnetization measurements on single crystals reveal an
antiferromagnetic transition at T$_{N}=2.2$ K indicating a pronounced
frustration parameter of $\theta_{W}/T_{N}\approx50$. Optical transmission
experiments on powder samples and single crystals confirm the structural
findings. Specific-heat measurements on YCu$_{3}$(OH)$_{6}$Cl$_{3}$ down to 0.4
K confirm the proposed quantum spin-liquid state of that system. Therefore, the
two Y-Cu-OH-Cl compounds present a unique setting to investigate closely
related structures with a spin-liquid state and a strongly frustrated AFM
ordered state, by slightly releasing the frustration in a kagome lattice.
",0,1,0,0,0,0
1928,"VB-Courant algebroids, E-Courant algebroids and generalized geometry","  In this paper, we first discuss the relation between VB-Courant algebroids
and E-Courant algebroids and construct some examples of E-Courant algebroids.
Then we introduce the notion of a generalized complex structure on an E-Courant
algebroid, unifying the usual generalized complex structures on
even-dimensional manifolds and generalized contact structures on
odd-dimensional manifolds. Moreover, we study generalized complex structures on
an omni-Lie algebroid in detail. In particular, we show that generalized
complex structures on an omni-Lie algebra $\gl(V)\oplus V$ correspond to
complex Lie algebra structures on V.
",0,0,1,0,0,0
12885,X-ray luminescence computed tomography using a focused X-ray beam,"  Due to the low X-ray photon utilization efficiency and low measurement
sensitivity of the electron multiplying charge coupled device (EMCCD) camera
setup, the collimator based narrow beam X-ray luminescence computed tomography
(XLCT) usually requires a long measurement time. In this paper, we, for the
first time, report a focused X-ray beam based XLCT imaging system with
measurements by a single optical fiber bundle and a photomultiplier tube (PMT).
An X-ray tube with a polycapillary lens was used to generate a focused X-ray
beam whose X-ray photon density is 1200 times larger than a collimated X-ray
beam. An optical fiber bundle was employed to collect and deliver the emitted
photons on the phantom surface to the PMT. The total measurement time was
reduced to 12.5 minutes. For numerical simulations of both single and six fiber
bundle cases, we were able to reconstruct six targets successfully. For the
phantom experiment, two targets with an edge-to-edge distance of 0.4 mm and a
center-to-center distance of 0.8 mm were successfully reconstructed by the
measurement setup with a single fiber bundle and a PMT.
",0,1,0,0,0,0
11368,"Unveiling the Role of Dopant Polarity on the Recombination, and Performance of Organic Light-Emitting Diodes","  The recombination of charges is an important process in organic photonic
devices because the process influences the device characteristics such as the
driving voltage, efficiency and lifetime. By combining the dipole trap theory
with the drift-diffusion model, we report that the stationary dipole moment
({\mu}0) of the dopant is a major factor determining the recombination
mechanism in the dye-doped organic light emitting diodes when the trap depth
({\Delta}Et) is larger than 0.3 eV where any de-trapping effect becomes
negligible. Dopants with large {\mu}0 (e.g., homoleptic Ir(III) dyes) induce
large charge trapping on them, resulting in high driving voltage and
trap-assisted-recombination dominated emission. On the other hand, dyes with
small {\mu}0 (e.g., heteroleptic Ir(III) dyes) show much less trapping on them
no matter what {\Delta}Et is, leading to lower driving voltage, higher
efficiencies and Langevin recombination dominated emission characteristics.
This finding will be useful in any organic photonic devices where trapping and
recombination sites play key roles.
",0,1,0,0,0,0
16224,A Zero-Shot Learning application in Deep Drawing process using Hyper-Process Model,"  One of the consequences of passing from mass production to mass customization
paradigm in the nowadays industrialized world is the need to increase
flexibility and responsiveness of manufacturing companies. The high-mix /
low-volume production forces constant accommodations of unknown product
variants, which ultimately leads to high periods of machine calibration. The
difficulty related with machine calibration is that experience is required
together with a set of experiments to meet the final product quality.
Unfortunately, all possible combinations of machine parameters is so high that
is difficult to build empirical knowledge. Due to this fact, normally trial and
error approaches are taken making one-of-a-kind products not viable. Therefore,
a Zero-Shot Learning (ZSL) based approach called hyper-process model (HPM) to
learn the relation among multiple tasks is used as a way to shorten the
calibration phase. Assuming each product variant is a task to solve, first, a
shape analysis on data to learn common modes of deformation between tasks is
made, and secondly, a mapping between these modes and task descriptions is
performed. Ultimately, the present work has two main contributions: 1)
Formulation of an industrial problem into a ZSL setting where new process
models can be generated for process optimization and 2) the definition of a
regression problem in the domain of ZSL. For that purpose, a 2-d deep drawing
simulated process was used based on data collected from the Abaqus simulator,
where a significant number of process models were collected to test the
effectiveness of the approach. The obtained results show that is possible to
learn new tasks without any available data (both labeled and unlabeled) by
leveraging information about already existing tasks, allowing to speed up the
calibration phase and make a quicker integration of new products into
manufacturing systems.
",1,0,0,1,0,0
15849,Data hiding in Fingerprint Minutiae Template for Privacy Protection,"  In this paper, we propose a novel scheme for data hiding in the fingerprint
minutiae template, which is the most popular in fingerprint recognition
systems. Various strategies are proposed in data embedding in order to maintain
the accuracy of fingerprint recognition as well as the undetectability of data
hiding. In bits replacement based data embedding, we replace the last few bits
of each element of the original minutiae template with the data to be hidden.
This strategy can be further improved using an optimized bits replacement based
data embedding, which is able to minimize the impact of data hiding on the
performance of fingerprint recognition. The third strategy is an order
preserving mechanism which is proposed to reduce the detectability of data
hiding. By using such a mechanism, it would be difficult for the attacker to
differentiate the minutiae template with hidden data from the original minutiae
templates. The experimental results show that the proposed data hiding scheme
achieves sufficient capacity for hiding common personal data, where the
accuracy of fingerprint recognition is acceptable after the data hiding.
",1,0,0,0,0,0
13045,Generalized magnetic mirrors,"  We propose generalized magnetic mirrors that can be achieved by excitations
of sole electric resonances. Conventional approaches to obtain magnetic mirrors
rely heavily on exciting the fundamental magnetic dipoles, whereas here we
reveal that besides magnetic resonances, electric resonances of higher orders
can be also employed to obtain highly efficient magnetic mirrors. Based on the
electromagnetic duality, it is also shown that electric mirrors can be achieved
by exciting magnetic resonances. We provide direct demonstrations of the
generalized mirrors proposed in a simple system of one-dimensional periodic
array of all-dielectric wires, which may shed new light to many advanced fields
of photonics related to resonant multipolar excitations and interferences.
",0,1,0,0,0,0
16424,Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step,"  Generative adversarial networks (GANs) are a family of generative models that
do not minimize a single training criterion. Unlike other generative models,
the data distribution is learned via a game between a generator (the generative
model) and a discriminator (a teacher providing training signal) that each
minimize their own cost. GANs are designed to reach a Nash equilibrium at which
each player cannot reduce their cost without changing the other players'
parameters. One useful approach for the theory of GANs is to show that a
divergence between the training distribution and the model distribution obtains
its minimum value at equilibrium. Several recent research directions have been
motivated by the idea that this divergence is the primary guide for the
learning process and that every step of learning should decrease the
divergence. We show that this view is overly restrictive. During GAN training,
the discriminator provides learning signal in situations where the gradients of
the divergences between distributions would not be useful. We provide empirical
counterexamples to the view of GAN training as divergence minimization.
Specifically, we demonstrate that GANs are able to learn distributions in
situations where the divergence minimization point of view predicts they would
fail. We also show that gradient penalties motivated from the divergence
minimization perspective are equally helpful when applied in other contexts in
which the divergence minimization perspective does not predict they would be
helpful. This contributes to a growing body of evidence that GAN training may
be more usefully viewed as approaching Nash equilibria via trajectories that do
not necessarily minimize a specific divergence at each step.
",1,0,0,1,0,0
20665,On separated solutions of logistic population equation with harvesting,"  We provide a surprising answer to a question raised in S. Ahmad and A.C.
Lazer [2], and extend the results of that paper.
",0,0,1,0,0,0
6557,Poisson brackets symmetry from the pentagon-wheel cocycle in the graph complex,"  Kontsevich designed a scheme to generate infinitesimal symmetries
$\dot{\mathcal{P}} = \mathcal{Q}(\mathcal{P})$ of Poisson brackets
$\mathcal{P}$ on all affine manifolds $M^r$; every such deformation is encoded
by oriented graphs on $n+2$ vertices and $2n$ edges. In particular, these
symmetries can be obtained by orienting sums of non-oriented graphs $\gamma$ on
$n$ vertices and $2n-2$ edges. The bi-vector flow $\dot{\mathcal{P}} =
\text{Or}(\gamma)(\mathcal{P})$ preserves the space of Poisson structures if
$\gamma$ is a cocycle with respect to the vertex-expanding differential in the
graph complex.
A class of such cocycles $\boldsymbol{\gamma}_{2\ell+1}$ is known to exist:
marked by $\ell \in \mathbb{N}$, each of them contains a $(2\ell+1)$-gon wheel
with a nonzero coefficient. At $\ell=1$ the tetrahedron $\boldsymbol{\gamma}_3$
itself is a cocycle; at $\ell=2$ the Kontsevich--Willwacher pentagon-wheel
cocycle $\boldsymbol{\gamma}_5$ consists of two graphs. We reconstruct the
symmetry $\mathcal{Q}_5(\mathcal{P}) =
\text{Or}(\boldsymbol{\gamma}_5)(\mathcal{P})$ and verify that $\mathcal{Q}_5$
is a Poisson cocycle indeed:
$[\![\mathcal{P},\mathcal{Q}_5(\mathcal{P})]\!]\doteq 0$ via
$[\![\mathcal{P},\mathcal{P}]\!]=0$.
",0,0,1,0,0,0
11854,pMR: A high-performance communication library,"  On many parallel machines, the time LQCD applications spent in communication
is a significant contribution to the total wall-clock time, especially in the
strong-scaling limit. We present a novel high-performance communication library
that can be used as a de facto drop-in replacement for MPI in existing
software. Its lightweight nature that avoids some of the unnecessary overhead
introduced by MPI allows us to improve the communication performance of
applications without any algorithmic or complicated implementation changes. As
a first real-world benchmark, we make use of the pMR library in the coarse-grid
solve of the Regensburg implementation of the DD-$\alpha$AMG algorithm. On
realistic lattices, we see an improvement of a factor 2x in pure communication
time and total execution time savings of up to 20%.
",1,1,0,0,0,0
6128,Demand-Independent Optimal Tolls,"  Wardrop equilibria in nonatomic congestion games are in general inefficient
as they do not induce an optimal flow that minimizes the total travel time.
Network tolls are a prominent and popular way to induce an optimum flow in
equilibrium. The classical approach to find such tolls is marginal cost pricing
which requires the exact knowledge of the demand on the network. In this paper,
we investigate under which conditions demand-independent optimum tolls exist
that induce the system optimum flow for any travel demand in the network. We
give several characterizations for the existence of such tolls both in terms of
the cost structure and the network structure of the game. Specifically we show
that demand-independent optimum tolls exist if and only if the edge cost
functions are shifted monomials as used by the Bureau of Public Roads.
Moreover, non-negative demand-independent optimum tolls exist when the network
is a directed acyclic multi-graph. Finally, we show that any network with a
single origin-destination pair admits demand-independent optimum tolls that,
although not necessarily non-negative, satisfy a budget constraint.
",1,0,0,0,0,0
4737,Gravitational instabilities in a protosolar-like disc II: continuum emission and mass estimates,"  Gravitational instabilities (GIs) are most likely a fundamental process
during the early stages of protoplanetary disc formation. Recently, there have
been detections of spiral features in young, embedded objects that appear
consistent with GI-driven structure. It is crucial to perform hydrodynamic and
radiative transfer simulations of gravitationally unstable discs in order to
assess the validity of GIs in such objects, and constrain optimal targets for
future observations. We utilise the radiative transfer code LIME to produce
continuum emission maps of a $0.17\,\mathrm{M}_{\odot}$ self-gravitating
protosolar-like disc. We note the limitations of using LIME as is and explore
methods to improve upon the default gridding. We use CASA to produce synthetic
observations of 270 continuum emission maps generated across different
frequencies, inclinations and dust opacities. We find that the spiral structure
of our protosolar-like disc model is distinguishable across the majority of our
parameter space after 1 hour of observation, and is especially prominent at
230$\,$GHz due to the favourable combination of angular resolution and
sensitivity. Disc mass derived from the observations is sensitive to the
assumed dust opacities and temperatures, and therefore can be underestimated by
a factor of at least 30 at 850$\,$GHz and 2.5 at 90$\,$GHz. As a result, this
effect could retrospectively validate GIs in discs previously thought not
massive enough to be gravitationally unstable, which could have a significant
impact on the understanding of the formation and evolution of protoplanetary
discs.
",0,1,0,0,0,0
16326,Dynamical inverse problem for Jacobi matrices,"  We consider the inverse dynamical problem for the dynamical system with
discrete time associated with the semi-infinite Jacobi matrix. We solve the
inverse problem for such a system and answer a question on the characterization
of the inverse data. As a by-product we give a necessary and sufficient
condition for the measure on the real line line to be the spectral measure of
semi-infinite discrete Schrodinger operator.
",0,0,1,0,0,0
2810,Erratum to: Medial axis and singularities,"  We correct one erroneous statement made in our recent paper ""Medial axis and
singularities"".
",0,0,1,0,0,0
19614,Renewal theorems and mixing for non Markov flows with infinite measure,"  We obtain results on mixing for a large class of (not necessarily Markov)
infinite measure semiflows and flows. Erickson proved, amongst other things, a
strong renewal theorem in the corresponding i.i.d. setting. Using operator
renewal theory, we extend Erickson's methods to the deterministic (i.e.
non-i.i.d.) continuous time setting and obtain results on mixing as a
consequence.
Our results apply to intermittent semiflows and flows of Pomeau-Manneville
type (both Markov and nonMarkov), and to semiflows and flows over
Collet-Eckmann maps with nonintegrable roof function.
",0,0,1,0,0,0
16096,New results of the search for hidden photons by means of a multicathode counter,"  New upper limit on a mixing parameter for hidden photons with a mass from 5
eV till 10 keV has been obtained from the results of measurements during 78
days in two configurations R1 and R2 of a multicathode counter. For a region of
a maximal sensitivity from 10 eV till 30 eV the upper limit obtained is less
than 4 x 10-11. The measurements have been performed at three temperatures:
26C, 31C and 36C. A positive effect for the spontaneous emission of single
electrons has been obtained at the level of more than 7{\sigma}. A falling
tendency of a temperature dependence of the spontaneous emission rate indicates
that the effect of thermal emission from a copper cathode can be neglected.
",0,1,0,0,0,0
11961,Error-Correcting Neural Sequence Prediction,"  In this paper we propose a novel neural language modelling (NLM) method based
on \textit{error-correcting output codes} (ECOC), abbreviated as ECOC-NLM. This
latent variable based approach provides a principled way to choose a varying
amount of latent output codes and avoids exact softmax normalization. Instead
of minimizing measures between the predicted probability distribution and true
distribution, we use error-correcting codes to represent both predictions and
outputs. Secondly, we propose multiple ways to improve accuracy and convergence
rates by maximizing the separability between codes that correspond to classes
proportional to word embedding similarities. Lastly, we introduce a novel
method called \textit{Latent Mixture Sampling}, a technique that is used to
mitigate exposure bias and can be integrated into training latent-based neural
language models. This involves mixing the latent codes (i.e variables) of past
predictions and past targets in one of two ways: (1) according to a predefined
sampling schedule or (2) a differentiable sampling procedure whereby the mixing
probability is learned throughout training by replacing the greedy argmax
operation with a smooth approximation. In evaluating Codeword Mixture Sampling
for ECOC-NLM, we also baseline it against CWMS in a closely related Hierarhical
Softmax-based NLM.
",1,0,0,1,0,0
633,Learning Models from Data with Measurement Error: Tackling Underreporting,"  Measurement error in observational datasets can lead to systematic bias in
inferences based on these datasets. As studies based on observational data are
increasingly used to inform decisions with real-world impact, it is critical
that we develop a robust set of techniques for analyzing and adjusting for
these biases. In this paper we present a method for estimating the distribution
of an outcome given a binary exposure that is subject to underreporting. Our
method is based on a missing data view of the measurement error problem, where
the true exposure is treated as a latent variable that is marginalized out of a
joint model. We prove three different conditions under which the outcome
distribution can still be identified from data containing only error-prone
observations of the exposure. We demonstrate this method on synthetic data and
analyze its sensitivity to near violations of the identifiability conditions.
Finally, we use this method to estimate the effects of maternal smoking and
opioid use during pregnancy on childhood obesity, two import problems from
public health. Using the proposed method, we estimate these effects using only
subject-reported drug use data and substantially refine the range of estimates
generated by a sensitivity analysis-based approach. Further, the estimates
produced by our method are consistent with existing literature on both the
effects of maternal smoking and the rate at which subjects underreport smoking.
",1,0,0,1,0,0
18194,Second-oder analysis in second-oder cone programming,"  The paper conducts a second-order variational analysis for an important class
of nonpolyhedral conic programs generated by the so-called
second-order/Lorentz/ice-cream cone $Q$. From one hand, we prove that the
indicator function of $Q$ is always twice epi-differentiable and apply this
result to characterizing the uniqueness of Lagrange multipliers at stationary
points together with an error bound estimate in the general second-order cone
setting involving ${\cal C}^2$-smooth data. On the other hand, we precisely
calculate the graphical derivative of the normal cone mapping to $Q$ under the
weakest metric subregularity constraint qualification and then give an
application of the latter result to a complete characterization of isolated
calmness for perturbed variational systems associated with second-order cone
programs. The obtained results seem to be the first in the literature in these
directions for nonpolyhedral problems without imposing any nondegeneracy
assumptions.
",0,0,1,0,0,0
8237,Two-dimensional Fermi gases near a p-wave resonance: effect of quantum fluctuations,"  We study the stability of p-wave superfluidity against quantum fluctuations
in two-dimensional Fermi gases near a p-wave Feshbach resonance . An analysis
is carried out in the limit when the interchannel coupling is strong. By
investigating the effective potential for the pairing field via the standard
loop expansion, we show that a homogeneous p-wave pairing state becomes
unstable when two-loop quantum fluctuations are taken into account. This is in
contrast to the previously predicted $p + ip$ supefluid in the weak-coupling
limit [V. Gurarie et al., Phys. Rev. Lett. 94, 230403 (2005)]. It implies a
possible onset of instability at certain intermediate interchannel coupling
strength. Alternatively, the instability can also be driven by lowering the
particle density. We also discuss the validity of our analysis.
",0,1,0,0,0,0
16884,"Dining Philosophers, Leader Election and Ring Size problems, in the quantum setting","  We provide the first quantum (exact) protocol for the Dining Philosophers
problem (DP), a central problem in distributed algorithms. It is well known
that the problem cannot be solved exactly in the classical setting. We then use
our DP protocol to provide a new quantum protocol for the tightly related
problem of exact leader election (LE) on a ring, improving significantly in
both time and memory complexity over the known LE protocol by Tani et. al. To
do this, we show that in some sense the exact DP and exact LE problems are
equivalent; interestingly, in the classical non-exact setting they are not.
Hopefully, the results will lead to exact quantum protocols for other important
distributed algorithmic questions; in particular, we discuss interesting
connections to the ring size problem, as well as to a physically motivated
question of breaking symmetry in 1D translationally invariant systems.
",1,0,0,0,0,0
7557,On the Genus of the Moonshine Module,"  We provide a novel and simple description of Schellekens' seventy-one affine
Kac-Moody structures of self-dual vertex operator algebras of central charge 24
by utilizing cyclic subgroups of the glue codes of the Niemeier lattices with
roots. We also discuss a possible uniform construction procedure of the
self-dual vertex operator algebras of central charge 24 starting from the Leech
lattice. This also allows us to consider the uniqueness question for all
non-trivial affine Kac-Moody structures. We finally discuss our description
from a Lorentzian viewpoint.
",0,0,1,0,0,0
2092,CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning,"  We propose CM3, a new deep reinforcement learning method for cooperative
multi-agent problems where agents must coordinate for joint success in
achieving different individual goals. We restructure multi-agent learning into
a two-stage curriculum, consisting of a single-agent stage for learning to
accomplish individual tasks, followed by a multi-agent stage for learning to
cooperate in the presence of other agents. These two stages are bridged by
modular augmentation of neural network policy and value functions. We further
adapt the actor-critic framework to this curriculum by formulating local and
global views of the policy gradient and learning via a double critic,
consisting of a decentralized value function and a centralized action-value
function. We evaluated CM3 on a new high-dimensional multi-agent environment
with sparse rewards: negotiating lane changes among multiple autonomous
vehicles in the Simulation of Urban Mobility (SUMO) traffic simulator. Detailed
ablation experiments show the positive contribution of each component in CM3,
and the overall synthesis converges significantly faster to higher performance
policies than existing cooperative multi-agent methods.
",0,0,0,1,0,0
20066,Nondegeneracy of the traveling lump solution to the $2+1$ Toda lattice,"  We consider the $2+1$ Toda system \[ \frac{1}{4}\Delta
q_{n}=e^{q_{n-1}-q_{n}}-e^{q_{n}-q_{n+1}}\text{ in }\mathbb{R}^{2},\
n\in\mathbb{Z}. \] It has a traveling wave type solution $\left\{ Q_{n}\right\}
$ satisfying $Q_{n+1}(x,y)=Q_{n}(x+\frac{1}{2\sqrt{2}},y)$, and is explicitly
given by \[ Q_{n}\left( x,y\right) =\ln\frac{\frac{1}{4}+\left(
n-1+2\sqrt{2}x\right) ^{2}+4y^{2}}{\frac{1}{4}+\left( n+2\sqrt{2}x\right)
^{2}+4y^{2}}. \] In this paper we prove that \{$Q_{n}$\} is nondegenerate.
",0,0,1,0,0,0
19497,The Lubin-Tate stack and Gross-Hopkins duality,"  Morava $E$-theory $E$ is an $E_\infty$-ring with an action of the Morava
stabilizer group $\Gamma$. We study the derived stack $\operatorname{Spf}
E/\Gamma$. Descent-theoretic techniques allow us to deduce a theorem of
Hopkins-Mahowald-Sadofsky on the $K(n)$-local Picard group, as well as a recent
result of Barthel-Beaudry-Stojanoska on the Anderson duals of higher real
$K$-theories.
",0,0,1,0,0,0
196,Blockchain and human episodic memory,"  We relate the concepts used in decentralized ledger technology to studies of
episodic memory in the mammalian brain. Specifically, we introduce the standard
concepts of linked list, hash functions, and sharding, from computer science.
We argue that these concepts may be more relevant to studies of the neural
mechanisms of memory than has been previously appreciated. In turn, we also
highlight that certain phenomena studied in the brain, namely metacognition,
reality monitoring, and how perceptual conscious experiences come about, may
inspire development in blockchain technology too, specifically regarding
probabilistic consensus protocols.
",0,0,0,0,1,0
14311,Improving OpenCL Performance by Specializing Compiler Phase Selection and Ordering,"  Automatic compiler phase selection/ordering has traditionally been focused on
CPUs and, to a lesser extent, FPGAs. We present experiments regarding compiler
phase ordering specialization of OpenCL kernels targeting a GPU. We use
iterative exploration to specialize LLVM phase orders on 15 OpenCL benchmarks
to an NVIDIA GPU. We analyze the generated NVIDIA PTX code for the various
versions to identify the main causes of the most significant improvements and
present results of a set of experiments that demonstrate the importance of
using specific phase orders. Using specialized compiler phase orders, we were
able to achieve geometric mean improvements of 1.54x (up to 5.48x) and 1.65x
(up to 5.7x) over PTX generated by the NVIDIA CUDA compiler from CUDA versions
of the same kernels, and over execution of the OpenCL kernels compiled from
source with the NVIDIA OpenCL driver, respectively. We also evaluate the use of
code-features in the OpenCL kernels. More specifically, we evaluate an approach
that achieves geometric mean improvements of 1.49x and 1.56x over the same
OpenCL baseline, by using the compiler sequences of the 1 or 3 most similar
benchmarks, respectively.
",1,0,0,0,0,0
2745,Measuring Item Similarity in Introductory Programming: Python and Robot Programming Case Studies,"  A personalized learning system needs a large pool of items for learners to
solve. When working with a large pool of items, it is useful to measure the
similarity of items. We outline a general approach to measuring the similarity
of items and discuss specific measures for items used in introductory
programming. Evaluation of quality of similarity measures is difficult. To this
end, we propose an evaluation approach utilizing three levels of abstraction.
We illustrate our approach to measuring similarity and provide evaluation using
items from three diverse programming environments.
",0,0,0,1,0,0
5504,Trust Region Value Optimization using Kalman Filtering,"  Policy evaluation is a key process in reinforcement learning. It assesses a
given policy using estimation of the corresponding value function. When using a
parameterized function to approximate the value, it is common to optimize the
set of parameters by minimizing the sum of squared Bellman Temporal Differences
errors. However, this approach ignores certain distributional properties of
both the errors and value parameters. Taking these distributions into account
in the optimization process can provide useful information on the amount of
confidence in value estimation. In this work we propose to optimize the value
by minimizing a regularized objective function which forms a trust region over
its parameters. We present a novel optimization method, the Kalman Optimization
for Value Approximation (KOVA), based on the Extended Kalman Filter. KOVA
minimizes the regularized objective function by adopting a Bayesian perspective
over both the value parameters and noisy observed returns. This distributional
property provides information on parameter uncertainty in addition to value
estimates. We provide theoretical results of our approach and analyze the
performance of our proposed optimizer on domains with large state and action
spaces.
",1,0,0,1,0,0
5852,Genetic Algorithms for Evolving Computer Chess Programs,"  This paper demonstrates the use of genetic algorithms for evolving: 1) a
grandmaster-level evaluation function, and 2) a search mechanism for a chess
program, the parameter values of which are initialized randomly. The evaluation
function of the program is evolved by learning from databases of (human)
grandmaster games. At first, the organisms are evolved to mimic the behavior of
human grandmasters, and then these organisms are further improved upon by means
of coevolution. The search mechanism is evolved by learning from tactical test
suites. Our results show that the evolved program outperforms a two-time world
computer chess champion and is at par with the other leading computer chess
programs.
",1,0,0,1,0,0
15801,Finite temperature disordered bosons in two dimensions,"  We study phase transitions in a two dimensional weakly interacting Bose gas
in a random potential at finite temperatures. We identify superfluid, normal
fluid, and insulator phases and construct the phase diagram. At T=0 one has a
tricritical point where the three phases coexist. The truncation of the energy
distribution at the trap barrier, which is a generic phenomenon in cold atom
systems, limits the growth of the localization length and in contrast to the
thermodynamic limit the insulator phase is present at any temperature.
",0,1,0,0,0,0
10217,Temporal Segment Networks for Action Recognition in Videos,"  Deep convolutional networks have achieved great success for image
recognition. However, for action recognition in videos, their advantage over
traditional methods is not so evident. We present a general and flexible
video-level framework for learning action models in videos. This method, called
temporal segment network (TSN), aims to model long-range temporal structures
with a new segment-based sampling and aggregation module. This unique design
enables our TSN to efficiently learn action models by using the whole action
videos. The learned models could be easily adapted for action recognition in
both trimmed and untrimmed videos with simple average pooling and multi-scale
temporal window integration, respectively. We also study a series of good
practices for the instantiation of TSN framework given limited training
samples. Our approach obtains the state-the-of-art performance on four
challenging action recognition benchmarks: HMDB51 (71.0%), UCF101 (94.9%),
THUMOS14 (80.1%), and ActivityNet v1.2 (89.6%). Using the proposed RGB
difference for motion models, our method can still achieve competitive accuracy
on UCF101 (91.0%) while running at 340 FPS. Furthermore, based on the temporal
segment networks, we won the video classification track at the ActivityNet
challenge 2016 among 24 teams, which demonstrates the effectiveness of TSN and
the proposed good practices.
",1,0,0,0,0,0
10322,Deep Learning for Design and Retrieval of Nano-photonic Structures,"  Our visual perception of our surroundings is ultimately limited by the
diffraction limit, which stipulates that optical information smaller than
roughly half the illumination wavelength is not retrievable. Over the past
decades, many breakthroughs have led to unprecedented imaging capabilities
beyond the diffraction-limit, with applications in biology and nanotechnology.
In this context, nano-photonics has revolutionized the field of optics in
recent years by enabling the manipulation of light-matter interaction with
subwavelength structures. However, despite the many advances in this field, its
impact and penetration in our daily life has been hindered by a convoluted and
iterative process, cycling through modeling, nanofabrication and
nano-characterization. The fundamental reason is the fact that not only the
prediction of the optical response is very time consuming and requires solving
Maxwell's equations with dedicated numerical packages. But, more significantly,
the inverse problem, i.e. designing a nanostructure with an on-demand optical
response, is currently a prohibitive task even with the most advanced numerical
tools due to the high non-linearity of the problem. Here, we harness the power
of Deep Learning, a new path in modern machine learning, and show its ability
to predict the geometry of nanostructures based solely on their far-field
response. This approach also addresses in a direct way the currently
inaccessible inverse problem breaking the ground for on-demand design of
optical response with applications such as sensing, imaging and also for
plasmon's mediated cancer thermotherapy.
",0,1,0,0,0,0
18892,Discretization error estimates for penalty formulations of a linearized Canham-Helfrich type energy,"  This paper is concerned with minimization of a fourth-order linearized
Canham-Helfrich energy subject to Dirichlet boundary conditions on curves
inside the domain. Such problems arise in the modeling of the mechanical
interaction of biomembranes with embedded particles. There, the curve
conditions result from the imposed particle--membrane coupling. We prove
almost-$H^{\frac{5}{2}}$ regularity of the solution and then consider two
possible penalty formulations. For the combination of these penalty
formulations with a Bogner-Fox-Schmit finite element discretization we prove
discretization error estimates which are optimal in view of the solution's
reduced regularity. The error estimates are based on a general estimate for
linear penalty problems in Hilbert spaces. Finally, we illustrate the
theoretical results by numerical computations. An important feature of the
presented discretization is that it does not require to resolve the particle
boundary. This is crucial in order to avoid re-meshing if the presented problem
arises as subproblem in a model where particles are allowed to move or rotate.
",0,0,1,0,0,0
17917,"Big Data, Data Science, and Civil Rights","  Advances in data analytics bring with them civil rights implications.
Data-driven and algorithmic decision making increasingly determine how
businesses target advertisements to consumers, how police departments monitor
individuals or groups, how banks decide who gets a loan and who does not, how
employers hire, how colleges and universities make admissions and financial aid
decisions, and much more. As data-driven decisions increasingly affect every
corner of our lives, there is an urgent need to ensure they do not become
instruments of discrimination, barriers to equality, threats to social justice,
and sources of unfairness. In this paper, we argue for a concrete research
agenda aimed at addressing these concerns, comprising five areas of emphasis:
(i) Determining if models and modeling procedures exhibit objectionable bias;
(ii) Building awareness of fairness into machine learning methods; (iii)
Improving the transparency and control of data- and model-driven decision
making; (iv) Looking beyond the algorithm(s) for sources of bias and
unfairness-in the myriad human decisions made during the problem formulation
and modeling process; and (v) Supporting the cross-disciplinary scholarship
necessary to do all of that well.
",1,0,0,0,0,0
8680,Interpretable 3D Human Action Analysis with Temporal Convolutional Networks,"  The discriminative power of modern deep learning models for 3D human action
recognition is growing ever so potent. In conjunction with the recent
resurgence of 3D human action representation with 3D skeletons, the quality and
the pace of recent progress have been significant. However, the inner workings
of state-of-the-art learning based methods in 3D human action recognition still
remain mostly black-box. In this work, we propose to use a new class of models
known as Temporal Convolutional Neural Networks (TCN) for 3D human action
recognition. Compared to popular LSTM-based Recurrent Neural Network models,
given interpretable input such as 3D skeletons, TCN provides us a way to
explicitly learn readily interpretable spatio-temporal representations for 3D
human action recognition. We provide our strategy in re-designing the TCN with
interpretability in mind and how such characteristics of the model is leveraged
to construct a powerful 3D activity recognition method. Through this work, we
wish to take a step towards a spatio-temporal model that is easier to
understand, explain and interpret. The resulting model, Res-TCN, achieves
state-of-the-art results on the largest 3D human action recognition dataset,
NTU-RGBD.
",1,0,0,0,0,0
16857,Power-Sum Denominators,"  The power sum $1^n + 2^n + \cdots + x^n$ has been of interest to
mathematicians since classical times. Johann Faulhaber, Jacob Bernoulli, and
others who followed expressed power sums as polynomials in $x$ of degree $n+1$
with rational coefficients. Here we consider the denominators of these
polynomials, and prove some of their properties. A remarkable one is that such
a denominator equals $n+1$ times the squarefree product of certain primes $p$
obeying the condition that the sum of the base-$p$ digits of $n+1$ is at least
$p$. As an application, we derive a squarefree product formula for the
denominators of the Bernoulli polynomials.
",0,0,1,0,0,0
443,Adelic point groups of elliptic curves,"  We show that for an elliptic curve E defined over a number field K, the group
E(A) of points of E over the adele ring A of K is a topological group that can
be analyzed in terms of the Galois representation associated to the torsion
points of E. An explicit description of E(A) is given, and we prove that for K
of degree n, almost all elliptic curves over K have an adelic point group
topologically isomorphic to a universal group depending on n. We also show that
there exist infinitely many elliptic curves over K having a different adelic
point group.
",0,0,1,0,0,0
9473,Teaching robots to imitate a human with no on-teacher sensors. What are the key challenges?,"  In this paper, we consider the problem of learning object manipulation tasks
from human demonstration using RGB or RGB-D cameras. We highlight the key
challenges in capturing sufficiently good data with no tracking devices -
starting from sensor selection and accurate 6DoF pose estimation to natural
language processing. In particular, we focus on two showcases: gluing task with
a glue gun and simple block-stacking with variable blocks. Furthermore, we
discuss how a linguistic description of the task could help to improve the
accuracy of task description. We also present the whole architecture of our
transfer of the imitated task to the simulated and real robot environment.
",1,0,0,0,0,0
4762,Multitask Learning for Fundamental Frequency Estimation in Music,"  Fundamental frequency (f0) estimation from polyphonic music includes the
tasks of multiple-f0, melody, vocal, and bass line estimation. Historically
these problems have been approached separately, and only recently, using
learning-based approaches. We present a multitask deep learning architecture
that jointly estimates outputs for various tasks including multiple-f0, melody,
vocal and bass line estimation, and is trained using a large,
semi-automatically annotated dataset. We show that the multitask model
outperforms its single-task counterparts, and explore the effect of various
design decisions in our approach, and show that it performs better or at least
competitively when compared against strong baseline methods.
",1,0,0,1,0,0
11423,"Ab initio study of magnetocrystalline anisotropy, magnetostriction, and Fermi surface of L10 FeNi (tetrataenite)","  The ordered L1$_0$ FeNi phase (tetrataenite) is recently considered as a
promising candidate for the rare-earth free permanent magnets applications. In
this work we calculate several characteristics of the L1$_0$ FeNi, where most
of the results come form the fully relativistic full potential FPLO method with
the generalized gradient approximation (GGA). A special attention deserves the
summary of the magnetocrystalline anisotropy energies (MAE's), the full
potential calculations of the anisotropy constant $K_3$, and the combined
analysis of the Fermi surface and three-dimensional $\mathbf{k}$-resolved MAE.
Other calculated parameters presented in this article are the magnetic moments
$m_{s}$ and $m_{l}$, magnetostrictive coefficient $\lambda_{001}$, bulk modulus
B$_0$, and lattice parameters. The MAE's summary shows rather big discrepancies
between the experimental MAE's from literature and also between the calculated
MAE's.
",0,1,0,0,0,0
9742,Quantum and thermal fluctuations in a Raman spin-orbit coupled Bose gas,"  We theoretically study a three-dimensional weakly-interacting Bose gas with
Raman-induced spin-orbit coupling at finite temperature. By employing a
generalized Hartree-Fock-Bogoliubov theory with Popov approximation, we
determine a complete finite-temperature phase diagram of three exotic
condensation phases (i.e., the stripe, plane-wave and zero-momentum phases),
against both quantum and thermal fluctuations. We find that the plane-wave
phase is significantly broadened by thermal fluctuations. The phonon mode and
sound velocity at the transition from the plane-wave phase to the zero-momentum
phase are thoughtfully analyzed. At zero temperature, we find that quantum
fluctuations open an unexpected gap in sound velocity at the phase transition,
in stark contrast to the previous theoretical prediction of a vanishing sound
velocity. At finite temperature, thermal fluctuations continue to significantly
enlarge the gap, and simultaneously shift the critical minimum. For a Bose gas
of $^{87}$Rb atoms at the typical experimental temperature, $T=0.3T_{0}$, where
$T_{0}$ is the critical temperature of an ideal Bose gas without spin-orbit
coupling, our results of gap opening and critical minimum shifting in the sound
velocity, are qualitatively consistent with the recent experimental observation
{[}S.-C. Ji \textit{et al.}, Phys. Rev. Lett. \textbf{114}, 105301 (2015){]}.
",0,1,0,0,0,0
305,Distinct evolutions of Weyl fermion quasiparticles and Fermi arcs with bulk band topology in Weyl semimetals,"  The Weyl semimetal phase is a recently discovered topological quantum state
of matter characterized by the presence of topologically protected degeneracies
near the Fermi level. These degeneracies are the source of exotic phenomena,
including the realization of chiral Weyl fermions as quasiparticles in the bulk
and the formation of Fermi arc states on the surfaces. Here, we demonstrate
that these two key signatures show distinct evolutions with the bulk band
topology by performing angle-resolved photoemission spectroscopy, supported by
first-principle calculations, on transition-metal monophosphides. While Weyl
fermion quasiparticles exist only when the chemical potential is located
between two saddle points of the Weyl cone features, the Fermi arc states
extend in a larger energy scale and are robust across the bulk Lifshitz
transitions associated with the recombination of two non-trivial Fermi surfaces
enclosing one Weyl point into a single trivial Fermi surface enclosing two Weyl
points of opposite chirality. Therefore, in some systems (e.g. NbP),
topological Fermi arc states are preserved even if Weyl fermion quasiparticles
are absent in the bulk. Our findings not only provide insight into the
relationship between the exotic physical phenomena and the intrinsic bulk band
topology in Weyl semimetals, but also resolve the apparent puzzle of the
different magneto-transport properties observed in TaAs, TaP and NbP, where the
Fermi arc states are similar.
",0,1,0,0,0,0
7297,Half-range lattice Boltzmann models for the simulation of Couette flow using the Shakhov collision term,"  The three-dimensional Couette flow between parallel plates is addressed using
mixed lattice Boltzmann models which implement the half-range and the
full-range Gauss-Hermite quadratures on the Cartesian axes perpendicular and
parallel to the walls, respectively. The ability of our models to simulate
rarefied flows are validated through comparison against previously reported
results obtained using the linearized Boltzmann-BGK equation for values of the
Knudsen number (Kn) up to $100$. We find that recovering the non-linear part of
the velocity profile (i.e., its deviation from a linear function) at ${\rm Kn}
\gtrsim 1$ requires high quadrature orders. We then employ the Shakhov model
for the collision term to obtain macroscopic profiles for Maxwell molecules
using the standard $\mu \sim T^\omega$ law, as well as for monatomic Helium and
Argon gases, modeled through ab-initio potentials, where the viscosity is
recovered using the Sutherland model. We validate our implementation by
comparison with DSMC results and find excellent match for all macroscopic
quantities for ${\rm Kn} \lesssim 0.1$. At ${\rm Kn} \gtrsim 0.1$, small
deviations can be seen in the profiles of the diagonal components of the
pressure tensor, the heat flux parallel to the plates, and the velocity
profile, as well as in the values of the velocity gradient at the channel
center. We attribute these deviations to the limited applicability of the
Shakhov collision model for highly out of equilibrium flows.
",0,1,0,0,0,0
975,More on products of Baire spaces,"  New results on the Baire product problem are presented. It is shown that an
arbitrary product of almost locally ccc Baire spaces is Baire; moreover, the
product of a Baire space and a 1st countable space which is $\beta$-unfavorable
in the strong Choquet game is Baire.
",0,0,1,0,0,0
11236,Learning with Bounded Instance- and Label-dependent Label Noise,"  Instance- and label-dependent label noise (ILN) is widely existed in
real-world datasets but has been rarely studied. In this paper, we focus on a
particular case of ILN where the label noise rates, representing the
probabilities that the true labels of examples flip into the corrupted labels,
have upper bounds. We propose to handle this bounded instance- and
label-dependent label noise under two different conditions. First,
theoretically, we prove that when the marginal distributions $P(X|Y=+1)$ and
$P(X|Y=-1)$ have non-overlapping supports, we can recover every noisy example's
true label and perform supervised learning directly on the cleansed examples.
Second, for the overlapping situation, we propose a novel approach to learn a
well-performing classifier which needs only a few noisy examples to be labeled
manually. Experimental results demonstrate that our method works well on both
synthetic and real-world datasets.
",0,0,0,1,0,0
5299,Blowup constructions for Lie groupoids and a Boutet de Monvel type calculus,"  We present natural and general ways of building Lie groupoids, by using the
classical procedures of blowups and of deformations to the normal cone. Our
constructions are seen to recover many known ones involved in index theory. The
deformation and blowup groupoids obtained give rise to several extensions of
$C^*$-algebras and to full index problems. We compute the corresponding
K-theory maps. Finally, the blowup of a manifold sitting in a transverse way in
the space of objects of a Lie groupoid leads to a calculus, quite similar to
the Boutet de Monvel calculus for manifolds with boundary.
",0,0,1,0,0,0
16794,Infinite monochromatic sumsets for colourings of the reals,"  N. Hindman, I. Leader and D. Strauss proved that it is consistent that there
is a finite colouring of $\mathbb R$ so that no infinite sumset
$X+X=\{x+y:x,y\in X\}$ is monochromatic. Our aim in this paper is to prove a
consistency result in the opposite direction: we show that, under certain
set-theoretic assumptions, for any $c:\mathbb R\to r$ with $r$ finite there is
an infinite $X\subseteq \mathbb R$ so that $c$ is constant on $X+X$.
",0,0,1,0,0,0
10571,Unoriented Cobordism Maps on Link Floer Homology,"  We study the problem of defining maps on link Floer homology induced by
unoriented link cobordisms. We provide a natural notion of link cobordism,
disoriented link cobordism, which tracks the motion of index zero and index
three critical points. Then we construct a map on unoriented link Floer
homology associated to a disoriented link cobordism. Furthermore, we give a
comparison with Oszváth-Stipsicz-Szabó's and Manolescu's constructions of
link cobordism maps for an unoriented band move.
",0,0,1,0,0,0
8388,Cloaking and anamorphism for light and mass diffusion,"  We first review classical results on cloaking and mirage effects for
electromagnetic waves. We then show that transformation optics allows the
masking of objects or produces mirages in diffusive regimes. In order to
achieve this, we consider the equation for diffusive photon density in
transformed coordinates, which is valid for diffusive light in scattering
media. More precisely, generalizing transformations for star domains introduced
in [Diatta and Guenneau, J. Opt. 13, 024012, 2011] for matter waves, we
numerically demonstrate that infinite conducting objects of different shapes
scatter diffusive light in exactly the same way. We also propose a design of
external light-diffusion cloak with spatially varying sign-shifting parameters
that hides a finite size scatterer outside the cloak. We next analyse
non-physical parameter in the transformed Fick's equation derived in [Guenneau
and Puvirajesinghe, R. Soc. Interface 10, 20130106, 2013], and propose to use a
non-linear transform that overcomes this problem. We finally investigate other
form invariant transformed diffusion-like equations in the time domain, and
touch upon conformal mappings and non-Euclidean cloaking applied to diffusion
processes.
",0,1,1,0,0,0
7163,"Discussion on ""Random-projection ensemble classification"" by T. Cannings and R. Samworth","  Discussion on ""Random-projection ensemble classification"" by T. Cannings and
R. Samworth. We believe that the proposed approach can find many applications
in economics such as credit scoring (e.g. Altman (1968)) and can be extended to
more general type of classifiers. In this discussion we would like to draw
authors attention to the copula-based discriminant analysis (Han et al. (2013)
and He et al. (2016)).
",0,0,1,1,0,0
11956,Mining Density Contrast Subgraphs,"  Dense subgraph discovery is a key primitive in many graph mining
applications, such as detecting communities in social networks and mining gene
correlation from biological data. Most studies on dense subgraph mining only
deal with one graph. However, in many applications, we have more than one graph
describing relations among a same group of entities. In this paper, given two
graphs sharing the same set of vertices, we investigate the problem of
detecting subgraphs that contrast the most with respect to density. We call
such subgraphs Density Contrast Subgraphs, or DCS in short. Two widely used
graph density measures, average degree and graph affinity, are considered. For
both density measures, mining DCS is equivalent to mining the densest subgraph
from a ""difference"" graph, which may have both positive and negative edge
weights. Due to the existence of negative edge weights, existing dense subgraph
detection algorithms cannot identify the subgraph we need. We prove the
computational hardness of mining DCS under the two graph density measures and
develop efficient algorithms to find DCS. We also conduct extensive experiments
on several real-world datasets to evaluate our algorithms. The experimental
results show that our algorithms are both effective and efficient.
",1,0,0,0,0,0
7013,Conical: an extended module for computing a numerically satisfactory pair of solutions of the differential equation for conical functions,"  Conical functions appear in a large number of applications in physics and
engineering. In this paper we describe an extension of our module CONICAL for
the computation of conical functions. Specifically, the module includes now a
routine for computing the function ${\rm R}^{m}_{-\frac{1}{2}+i\tau}(x)$, a
real-valued numerically satisfactory companion of the function ${\rm
P}^m_{-\tfrac12+i\tau}(x)$ for $x>1$. In this way, a natural basis for solving
Dirichlet problems bounded by conical domains is provided.
",1,0,1,0,0,0
15296,Regression approaches for Approximate Bayesian Computation,"  This book chapter introduces regression approaches and regression adjustment
for Approximate Bayesian Computation (ABC). Regression adjustment adjusts
parameter values after rejection sampling in order to account for the imperfect
match between simulations and observations. Imperfect match between simulations
and observations can be more pronounced when there are many summary statistics,
a phenomenon coined as the curse of dimensionality. Because of this imperfect
match, credibility intervals obtained with regression approaches can be
inflated compared to true credibility intervals. The chapter presents the main
concepts underlying regression adjustment. A theorem that compares theoretical
properties of posterior distributions obtained with and without regression
adjustment is presented. Last, a practical application of regression adjustment
in population genetics shows that regression adjustment shrinks posterior
distributions compared to rejection approaches, which is a solution to avoid
inflated credibility intervals.
",0,0,0,1,0,0
8760,How to Differentiate Collective Variables in Free Energy Codes: Computer-Algebra Code Generation and Automatic Differentiation,"  The proper choice of collective variables (CVs) is central to biased-sampling
free energy reconstruction methods in molecular dynamics simulations. The
PLUMED 2 library, for instance, provides several sophisticated CV choices,
implemented in a C++ framework; however, developing new CVs is still time
consuming due to the need to provide code for the analytical derivatives of all
functions with respect to atomic coordinates. We present two solutions to this
problem, namely (a) symbolic differentiation and code generation, and (b)
automatic code differentiation, in both cases leveraging open-source libraries
(SymPy and Stan Math respectively). The two approaches are demonstrated and
discussed in detail implementing a realistic example CV, the local radius of
curvature of a polymer. Users may use the code as a template to streamline the
implementation of their own CVs using high-level constructs and automatic
gradient computation.
",0,1,0,0,0,0
7409,An Extension of Proof Graphs for Disjunctive Parameterised Boolean Equation Systems,"  A parameterised Boolean equation system (PBES) is a set of equations that
defines sets as the least and/or greatest fixed-points that satisfy the
equations. This system is regarded as a declarative program defining functions
that take a datum and returns a Boolean value. The membership problem of PBESs
is a problem to decide whether a given element is in the defined set or not,
which corresponds to an execution of the program. This paper introduces reduced
proof graphs, and studies a technique to solve the membership problem of PBESs,
which is undecidable in general, by transforming it into a reduced proof graph.
A vertex X(v) in a proof graph represents that the data v is in the set X, if
the graph satisfies conditions induced from a given PBES. Proof graphs are,
however, infinite in general. Thus we introduce vertices each of which stands
for a set of vertices of the original ones, which possibly results in a finite
graph. For a subclass of disjunctive PBESs, we clarify some conditions which
reduced proof graphs should satisfy. We also show some examples having no
finite proof graph except for reduced one. We further propose a reduced
dependency space, which contains reduced proof graphs as sub-graphs if a proof
graph exists. We provide a procedure to construct finite reduced dependency
spaces, and show the soundness and completeness of the procedure.
",1,0,0,0,0,0
13663,Adversarial Imitation via Variational Inverse Reinforcement Learning,"  We consider a problem of learning the reward and policy from expert examples
under unknown dynamics in high-dimensional scenarios. Our proposed method
builds on the framework of generative adversarial networks and introduces the
empowerment-regularized maximum-entropy inverse reinforcement learning to learn
near-optimal rewards and policies. Empowerment-based regularization prevents
the policy from overfitting expert demonstration, thus leads to a generalized
behavior which results in learning near-optimal rewards. Our method
simultaneously learns empowerment through variational information maximization
along with the reward and policy under the adversarial learning formulation. We
evaluate our approach on various high-dimensional complex control tasks. We
also test our learned rewards in challenging transfer learning problems where
training and testing environments are made to be different from each other in
terms of dynamics or structure. The results show that our proposed method not
only learns near-optimal rewards and policies that are matching expert behavior
but also performs significantly better than state-of-the-art inverse
reinforcement learning algorithms.
",1,0,0,1,0,0
10589,Linearly-Recurrent Autoencoder Networks for Learning Dynamics,"  This paper describes a method for learning low-dimensional approximations of
nonlinear dynamical systems, based on neural-network approximations of the
underlying Koopman operator. Extended Dynamic Mode Decomposition (EDMD)
provides a useful data-driven approximation of the Koopman operator for
analyzing dynamical systems. This paper addresses a fundamental problem
associated with EDMD: a trade-off between representational capacity of the
dictionary and over-fitting due to insufficient data. A new neural network
architecture combining an autoencoder with linear recurrent dynamics in the
encoded state is used to learn a low-dimensional and highly informative
Koopman-invariant subspace of observables. A method is also presented for
balanced model reduction of over-specified EDMD systems in feature space.
Nonlinear reconstruction using partially linear multi-kernel regression aims to
improve reconstruction accuracy from the low-dimensional state when the data
has complex but intrinsically low-dimensional structure. The techniques
demonstrate the ability to identify Koopman eigenfunctions of the unforced
Duffing equation, create accurate low-dimensional models of an unstable
cylinder wake flow, and make short-time predictions of the chaotic
Kuramoto-Sivashinsky equation.
",1,0,0,1,0,0
2221,Collaborative Filtering using Denoising Auto-Encoders for Market Basket Data,"  Recommender systems (RS) help users navigate large sets of items in the
search for ""interesting"" ones. One approach to RS is Collaborative Filtering
(CF), which is based on the idea that similar users are interested in similar
items. Most model-based approaches to CF seek to train a
machine-learning/data-mining model based on sparse data; the model is then used
to provide recommendations. While most of the proposed approaches are effective
for small-size situations, the combinatorial nature of the problem makes it
impractical for medium-to-large instances. In this work we present a novel
approach to CF that works by training a Denoising Auto-Encoder (DAE) on
corrupted baskets, i.e., baskets from which one or more items have been
removed. The DAE is then forced to learn to reconstruct the original basket
given its corrupted input. Due to recent advancements in optimization and other
technologies for training neural-network models (such as DAE), the proposed
method results in a scalable and practical approach to CF. The contribution of
this work is twofold: (1) to identify missing items in observed baskets and,
thus, directly providing a CF model; and, (2) to construct a generative model
of baskets which may be used, for instance, in simulation analysis or as part
of a more complex analytical method.
",1,0,0,1,0,0
4570,Intrinsic pinning by naturally occurring correlated defects in FeSe$_\text{1-x}$Te$_\text{x}$ superconductors,"  We study the angular dependence of the dissipation in the superconducting
state of FeSe and Fe(Se$_\text{1-x}$Te$_\text{x}$) through electrical transport
measurements, using crystalline intergrown materials. We reveal the key role of
the inclusions of the non superconducting magnetic phase
Fe$_\text{1-y}$(Se$_\text{1-x}$Te$_\text{x}$), growing into the
Fe(Se$_\text{1-x}$Te$_\text{x}$) pure $\beta$-phase, in the development of a
correlated defect structure. The matching of both atomic structures defines the
growth habit of the crystalline material as well as the correlated planar
defects orientation.
",0,1,0,0,0,0
993,When a triangle is isosceles?,"  In 1840 Jacob Steiner on Christian Rudolf's request proved that a triangle
with two equal bisectors is isosceles. But what about changing the bisectors to
cevians? Cevian is any line segment in a triangle with one endpoint on a vertex
of the triangle and other endpoint on the opposite side. Not for any pairs of
equal cevians the triangle is isosceles. Theorem. If for a triangle ABC there
are equal cevians issuing from A and B, which intersect on the bisector or on
the median of the angle C, then AC=BC (so the triangle ABC is isosceles).
Proposition. Let ABC be an isosceles triangle. Define circle C to be the circle
symmetric relative to AB to the circumscribed circle of the triangle ABC. Then
the locus of intersection points of pairs of equal cevians is the union of the
base AB, the triangle's axis of symmetry, and the circle C.
",0,0,1,0,0,0
19517,Periodic solutions of semilinear Duffing equations with impulsive effects,"  In this paper we are concerned with the existence of periodic solutions for
semilinear Duffing equations with impulsive effects. Firstly for the autonomous
one, basing on Poincaré-Birkhoff twist theorem, we prove the existence of
infinitely many periodic solutions. Secondly, as for the nonautonomous case,
the impulse brings us great challenges for the study, and there are only
finitely many periodic solutions, which is quite different from the
corresponding equation without impulses. Here, taking the autonomous one as an
auxiliary equation, we find the relation between these two equations and then
obtain the result also by Poincaré-Birkhoff twist theorem.
",0,1,1,0,0,0
11145,The way to uncover community structure with core and diversity,"  Communities are ubiquitous in nature and society. Individuals that share
common properties often self-organize to form communities. Avoiding the
shortages of computation complexity, pre-given information and unstable results
in different run, in this paper, we propose a simple and effcient method to
deepen our understanding of the emergence and diversity of communities in
complex systems. By introducing the rational random selection, our method
reveals the hidden deterministic and normal diverse community states of
community structure. To demonstrate this method, we test it with real-world
systems. The results show that our method could not only detect community
structure with high sensitivity and reliability, but also provide instructional
information about the hidden deterministic community world and our normal
diverse community world by giving out the core-community, the real-community,
the tide and the diversity. This is of paramount importance in understanding,
predicting, and controlling a variety of collective behaviors in complex
systems.
",1,1,0,0,0,0
17529,PEORL: Integrating Symbolic Planning and Hierarchical Reinforcement Learning for Robust Decision-Making,"  Reinforcement learning and symbolic planning have both been used to build
intelligent autonomous agents. Reinforcement learning relies on learning from
interactions with real world, which often requires an unfeasibly large amount
of experience. Symbolic planning relies on manually crafted symbolic knowledge,
which may not be robust to domain uncertainties and changes. In this paper we
present a unified framework {\em PEORL} that integrates symbolic planning with
hierarchical reinforcement learning (HRL) to cope with decision-making in a
dynamic environment with uncertainties.
Symbolic plans are used to guide the agent's task execution and learning, and
the learned experience is fed back to symbolic knowledge to improve planning.
This method leads to rapid policy search and robust symbolic plans in complex
domains. The framework is tested on benchmark domains of HRL.
",0,0,0,1,0,0
12837,Lion and man in non-metric spaces,"  A lion and a man move continuously in a space $X$. The aim of the lion is to
capture his prey while the man wants to escape forever. Which of them has a
strategy? This question has been studied for different metric domains. In this
article we consider the case of general topological spaces.
",0,0,1,0,0,0
11306,Pumping Lemma for Higher-order Languages,"  We study a pumping lemma for the word/tree languages generated by
higher-order grammars. Pumping lemmas are known up to order-2 word languages
(i.e., for regular/context-free/indexed languages), and have been used to show
that a given language does not belong to the classes of
regular/context-free/indexed languages. We prove a pumping lemma for word/tree
languages of arbitrary orders, modulo a conjecture that a higher-order version
of Kruskal's tree theorem holds. We also show that the conjecture indeed holds
for the order-2 case, which yields a pumping lemma for order-2 tree languages
and order-3 word languages.
",1,0,0,0,0,0
19570,Control of Asynchronous Imitation Dynamics on Networks,"  Imitation is widely observed in populations of decision-making agents. Using
our recent convergence results for asynchronous imitation dynamics on networks,
we consider how such networks can be efficiently driven to a desired
equilibrium state by offering payoff incentives for using a certain strategy,
either uniformly or targeted to individuals. In particular, if for each
available strategy, agents playing that strategy receive maximum payoff when
their neighbors play that same strategy, we show that providing incentives to
agents in a network that is at equilibrium will result in convergence to a
unique new equilibrium. For the case when a uniform incentive can be offered to
all agents, this result allows the computation of the optimal incentive using a
binary search algorithm. When incentives can be targeted to individual agents,
we propose an algorithm to select which agents should be chosen based on
iteratively maximizing a ratio of the number of agents who adopt the desired
strategy to the payoff incentive required to get those agents to do so.
Simulations demonstrate that the proposed algorithm computes near-optimal
targeted payoff incentives for a range of networks and payoff distributions in
coordination games.
",1,1,0,0,0,0
5132,CTCF Degradation Causes Increased Usage of Upstream Exons in Mouse Embryonic Stem Cells,"  Transcriptional repressor CTCF is an important regulator of chromatin 3D
structure, facilitating the formation of topologically associating domains
(TADs). However, its direct effects on gene regulation is less well understood.
Here, we utilize previously published ChIP-seq and RNA-seq data to investigate
the effects of CTCF on alternative splicing of genes with CTCF sites. We
compared the amount of RNA-seq signals in exons upstream and downstream of
binding sites following auxin-induced degradation of CTCF in mouse embryonic
stem cells. We found that changes in gene expression following CTCF depletion
were significant, with a general increase in the presence of upstream exons. We
infer that a possible mechanism by which CTCF binding contributes to
alternative splicing is by causing pauses in the transcription mechanism during
which splicing elements are able to concurrently act on upstream exons already
transcribed into RNA.
",0,0,0,0,1,0
4000,Projection Free Rank-Drop Steps,"  The Frank-Wolfe (FW) algorithm has been widely used in solving nuclear norm
constrained problems, since it does not require projections. However, FW often
yields high rank intermediate iterates, which can be very expensive in time and
space costs for large problems. To address this issue, we propose a rank-drop
method for nuclear norm constrained problems. The goal is to generate descent
steps that lead to rank decreases, maintaining low-rank solutions throughout
the algorithm. Moreover, the optimization problems are constrained to ensure
that the rank-drop step is also feasible and can be readily incorporated into a
projection-free minimization method, e.g., Frank-Wolfe. We demonstrate that by
incorporating rank-drop steps into the Frank-Wolfe algorithm, the rank of the
solution is greatly reduced compared to the original Frank-Wolfe or its common
variants.
",0,0,0,1,0,0
13571,An In Vitro Vascularized Tumor Platform for Modeling Breast Tumor Stromal Interactions and Characterizing the Subsequent Response,"  Tumor stromal interactions have been shown to be the driving force behind the
poor prognosis associated with aggressive breast tumors. These interactions,
specifically between tumor and the surrounding ECM, and tumor and vascular
endothelium, promote tumor formation, angiogenesis, and metastasis. In this
study, we develop an in vitro vascularized tumor platform that allows for
investigation of tumor-stromal interactions in three breast tumor derived cell
lines of varying aggressiveness: MDA-IBC3, SUM149, and MDA-MB-231. The platform
recreates key features of breast tumors, including increased vascular
permeability, vessel sprouting, and ECM remodeling. Morphological and
quantitative analysis reveals differential effects from each tumor cell type on
endothelial coverage, permeability, expression of VEGF, and collagen
remodeling. Triple negative tumors, SUM149 and MDA-MB-321, resulted in a
significantly (p<0.05) higher endothelial permeability and decreased
endothelial coverage compared to the control TIME only platform. SUM149/TIME
platforms were 1.3 fold lower (p<0.05), and MDA-MB-231/TIME platforms were 1.5
fold lower (p<0.01) in endothelial coverage compared to the control TIME only
platform. HER2+ MDA-IBC3 tumor cells expressed high levels of VEGF (p<0.01) and
induced vessel sprouting. Vessels sprouting was tracked for 3 weeks and with
increasing time exhibited formation of multiple vessel sprouts that invaded
into the ECM and surrounded clusters of MDA-IBC3 cells. Both IBC cell lines,
SUM149 and MDA-IBC3, resulted in a collagen ECM with significantly greater
porosity with 1.6 and 1.1 fold higher compared to control, p<0.01. The breast
cancer in vitro vascularized platforms introduced in this paper are an
adaptable, high throughout tool for unearthing tumor-stromal mechanisms and
dynamics behind tumor progression and may prove essential in developing
effective targeted therapeutics.
",0,0,0,0,1,0
11659,Constraining the contribution of active galactic nuclei to reionisation,"  Recent results have suggested that active galactic nuclei (AGN) could provide
enough photons to reionise the Universe. We assess the viability of this
scenario using a semi-numerical framework for modeling reionisation, to which
we add a quasar contribution by constructing a Quasar Halo Occupation
Distribution (QHOD) based on Giallongo et al. observations. Assuming a constant
QHOD, we find that an AGN-only model cannot simultaneously match observations
of the optical depth $\tau_e$, neutral fraction, and ionising emissivity. Such
a model predicts $\tau_e$ too low by $\sim 2\sigma$ relative to Planck
constraints, and reionises the Universe at $z\lesssim 5$. Arbitrarily
increasing the AGN emissivity to match these results yields a strong mismatch
with the observed ionising emissivity at $z\sim 5$. If we instead assume a
redshift-independent AGN luminosity function yielding an emissivity evolution
like that assumed in Madau & Haardt model, then we can match $\tau_e$ albeit
with late reionisation, however such evolution is inconsistent with
observations at $z\sim 4-6$ and poorly motivated physically. These results
arise because AGN are more biased towards massive halos than typical reionising
galaxies, resulting in stronger clustering and later formation times.
AGN-dominated models produce larger ionising bubbles that are reflected in
$\sim\times 2$ more 21cm power on all scales. A model with equal parts galaxies
and AGN contribution is still (barely) consistent with observations, but could
be distinguished using next-generation 21cm experiments HERA and SKA-low. We
conclude that, even with recent claims of more faint AGN than previously
thought, AGN are highly unlikely to dominate the ionising photon budget for
reionisation.
",0,1,0,0,0,0
16226,The Internet as Quantitative Social Science Platform: Insights from a Trillion Observations,"  With the large-scale penetration of the internet, for the first time,
humanity has become linked by a single, open, communications platform.
Harnessing this fact, we report insights arising from a unified internet
activity and location dataset of an unparalleled scope and accuracy drawn from
over a trillion (1.5$\times 10^{12}$) observations of end-user internet
connections, with temporal resolution of just 15min over 2006-2012. We first
apply this dataset to the expansion of the internet itself over 1,647 urban
agglomerations globally. We find that unique IP per capita counts reach
saturation at approximately one IP per three people, and take, on average, 16.1
years to achieve; eclipsing the estimated 100- and 60- year saturation times
for steam-power and electrification respectively. Next, we use intra-diurnal
internet activity features to up-scale traditional over-night sleep
observations, producing the first global estimate of over-night sleep duration
in 645 cities over 7 years. We find statistically significant variation between
continental, national and regional sleep durations including some evidence of
global sleep duration convergence. Finally, we estimate the relationship
between internet concentration and economic outcomes in 411 OECD regions and
find that the internet's expansion is associated with negative or positive
productivity gains, depending strongly on sectoral considerations. To our
knowledge, our study is the first of its kind to use online/offline activity of
the entire internet to infer social science insights, demonstrating the
unparalleled potential of the internet as a social data-science platform.
",1,1,0,1,0,0
12871,A review of possible effects of cognitive biases on interpretation of rule-based machine learning models,"  This paper investigates to what extent cognitive biases may affect human
understanding of interpretable machine learning models, in particular of rules
discovered from data. Twenty cognitive biases are covered, as are possible
debiasing techniques that can be adopted by designers of machine learning
algorithms and software. Our review transfers results obtained in cognitive
psychology to the domain of machine learning, aiming to bridge the current gap
between these two areas. It needs to be followed by empirical studies
specifically aimed at the machine learning domain.
",0,0,0,1,0,0
10235,On the next-to-minimal weight of projective Reed-Muller codes,"  In this paper we present several values for the next-to-minimal weights of
projective Reed-Muller codes. We work over $\mathbb{F}_q$ with $q \geq 3$ since
in IEEE-IT 62(11) p. 6300-6303 (2016) we have determined the complete values
for the next-to-minimal weights of binary projective Reed-Muller codes. As in
loc. cit. here we also find examples of codewords with next-to-minimal weight
whose set of zeros is not in a hyperplane arrangement.
",1,0,1,0,0,0
10310,Compatibility of quasi-orderings and valuations; A Baer-Krull Theorem for quasi-ordered Rings,"  In his work of 1969, Merle E. Manis introduced valuations on commutative
rings. Recently, the class of totally quasi-ordered rings was developped by the
second author. In the present paper, we establish the notion of compatibility
between valuations and quasi-orders on rings, leading to a definition of the
rank of a quasi-ordered ring. Moreover, we prove a Baer-Krull Theorem for
quasi-ordered rings: fixing a Manis valuation v on R, we characterize all
v-compatible quasi-orders of R by lifting the quasi-orders from the residue
class ring to R itself.
",0,0,1,0,0,0
6719,A Heuristic Search Algorithm Using the Stability of Learning Algorithms in Certain Scenarios as the Fitness Function: An Artificial General Intelligence Engineering Approach,"  This paper presents a non-manual design engineering method based on heuristic
search algorithm to search for candidate agents in the solution space which
formed by artificial intelligence agents modeled on the base of
bionics.Compared with the artificial design method represented by meta-learning
and the bionics method represented by the neural architecture chip,this method
is more feasible for realizing artificial general intelligence,and it has a
much better interaction with cognitive neuroscience;at the same time,the
engineering method is based on the theoretical hypothesis that the final
learning algorithm is stable in certain scenarios,and has generalization
ability in various scenarios.The paper discusses the theory preliminarily and
proposes the possible correlation between the theory and the fixed-point
theorem in the field of mathematics.Limited by the author's knowledge
level,this correlation is proposed only as a kind of conjecture.
",1,0,0,0,0,0
6543,Local and global boundary rigidity and the geodesic X-ray transform in the normal gauge,"  In this paper we analyze the local and global boundary rigidity problem for
general Riemannian manifolds with boundary $(M,g)$ whose boundary is strictly
convex. We show that the boundary distance function, i.e., $d_g|_{\partial
M\times\partial M}$, known over suitable open sets of $\partial M$ determines
$g$ in suitable corresponding open subsets of $M$, up to the natural
diffeomorphism invariance of the problem. We also show that if there is a
function on $M$ with suitable convexity properties relative to $g$ then
$d_g|_{\partial M\times\partial M}$ determines $g$ globally in the sense that
if $d_g|_{\partial M\times\partial M}=d_{\tilde g}|_{\partial M\times \partial
M}$ then there is a diffeomorphism $\psi$ fixing $\partial M$ (pointwise) such
that $g=\psi^*\tilde g$. This global assumption is satisfied, for instance, for
the distance function from a given point if the manifold has no focal points
(from that point).
We also consider the lens rigidity problem. The lens relation measures the
point of exit from $M$ and the direction of exit of geodesics issued from the
boundary and the length of the geodesic. The lens rigidity problem is whether
we can determine the metric up to isometry from the lens relation. We solve the
lens rigidity problem under the same global assumption mentioned above. This
shows, for instance, that manifolds with a strictly convex boundary and
non-positive sectional curvature are lens rigid.
The key tool is the analysis of the geodesic X-ray transform on 2-tensors,
corresponding to a metric $g$, in the normal gauge, such as normal coordinates
relative to a hypersurface, where one also needs to allow microlocal weights.
This is handled by refining and extending our earlier results in the solenoidal
gauge.
",0,0,1,0,0,0
4627,The effect of stellar and AGN feedback on the low redshift Lyman-$α$ forest in the Sherwood simulation suite,"  We study the effect of different feedback prescriptions on the properties of
the low redshift ($z\leq1.6$) Ly$\alpha$ forest using a selection of
hydrodynamical simulations drawn from the Sherwood simulation suite. The
simulations incorporate stellar feedback, AGN feedback and a simplified scheme
for efficiently modelling the low column density Ly$\alpha$ forest. We confirm
a discrepancy remains between Cosmic Origins Spectrograph (COS) observations of
the Ly$\alpha$ forest column density distribution function (CDDF) at $z \simeq
0.1$ for high column density systems ($N_{\rm HI}>10^{14}\rm\,cm^{-2}$), as
well as Ly$\alpha$ velocity widths that are too narrow compared to the COS
data. Stellar or AGN feedback -- as currently implemented in our simulations --
have only a small effect on the CDDF and velocity width distribution. We
conclude that resolving the discrepancy between the COS data and simulations
requires an increase in the temperature of overdense gas with $\Delta=4$--$40$,
either through additional He$\,\rm \scriptstyle II\ $ photo-heating at $z>2$ or
fine-tuned feedback that ejects overdense gas into the IGM at just the right
temperature for it to still contribute significantly to the Ly$\alpha$ forest.
Alternatively a larger, currently unresolved turbulent component to the line
width could resolve the discrepancy.
",0,1,0,0,0,0
9142,Classification of Minimal Separating Sets in Low Genus Surfaces,"  Consider a surface $S$ and let $M\subset S$. If $S\setminus M$ is not
connected, then we say $M$ \emph{separates} $S$, and we refer to $M$ as a
\emph{separating set} of $S$. If $M$ separates $S$, and no proper subset of $M$
separates $S$, then we say $M$ is a \emph{minimal separating set} of $S$. In
this paper we use methods of computational combinatorial topology to classify
the minimal separating sets of the orientable surfaces of genus $g=2$ and
$g=3$. The classification for genus 0 and 1 was done in earlier work, using
methods of algebraic topology.
",0,0,1,0,0,0
5608,Edge Erasures and Chordal Graphs,"  We prove several results about chordal graphs and weighted chordal graphs by
focusing on exposed edges. These are edges that are properly contained in a
single maximal complete subgraph. This leads to a characterization of chordal
graphs via deletions of a sequence of exposed edges from a complete graph. Most
interesting is that in this context the connected components of the
edge-induced subgraph of exposed edges are 2-edge connected. We use this latter
fact in the weighted case to give a modified version of Kruskal's second
algorithm for finding a minimum spanning tree in a weighted chordal graph. This
modified algorithm benefits from being local in an important sense.
",1,0,1,0,0,0
19363,Vacancy-driven extended stability of cubic metastable Ta-Al-N and Nb-Al-N phases,"  Quantum mechanical calculations had been previously applied to predict phase
stability in many ternary and multinary nitride systems. While the predictions
were very accurate for the Ti-Al-N system, some discrepancies between theory
and experiment were obtained in the case of other systems. Namely, in the case
of Ta-Al-N, the calculations tend to overestimate the minimum Al content
necessary to obtain a metastable solid solution with a cubic structure. In this
work, we present a comprehensive study of the impact of vacancies on the phase
fields in quasi-binary TaN-AlN and NbN-AlN systems. Our calculations clearly
show that presence of point defects strongly enlarges the cubic phase field in
the TaN-AlN system, while the effect is less pronounced in the NbN-AlN case.
The present phase stability predictions agree better with experimental
observations of physical vapour deposited thin films reported in the literature
than that based on perfect, non-defected structures. This study shows that a
representative structural model is crucial for a meaningful comparison with
experimental data.
",0,1,0,0,0,0
18822,Front Propagation for Nonlocal KPP Reaction-Diffusion Equations in Periodic Media,"  We study front propagation phenomena for a large class of nonlocal KPP-type
reaction-diffusion equations in oscillatory environments, which model various
forms of population growth with periodic dependence. The nonlocal diffusion is
an anisotropic integro-differential operator of order $\alpha \in (0,2)$.
",0,0,1,0,0,0
13215,Frequency measurement of the clock transition of an indium ion sympathetically-cooled in a linear trap,"  We report frequency measurement of the clock transition in an 115In+ ion
sympathetically-cooled with Ca+ ions in a linear rf trap. The Ca+ ions are used
as a probe of the external electromagnetic field and as the coolant for
preparing the cold In+. The frequency is determined to be 1 267 402 452 901
049.9 (6.9) Hz by averaging 36 measurements using an optical frequency comb
referenced to the frequency standards located in the same site.
",0,1,0,0,0,0
12065,Reduced chemistry for butanol isomers at engine-relevant conditions,"  Butanol has received significant research attention as a second-generation
biofuel in the past few years. In the present study, skeletal mechanisms for
four butanol isomers were generated from two widely accepted, well-validated
detailed chemical kinetic models for the butanol isomers. The detailed models
were reduced using a two-stage approach consisting of the directed relation
graph with error propagation and sensitivity analysis. During the reduction
process, issues were encountered with pressure-dependent reactions formulated
using the logarithmic pressure interpolation approach; these issues are
discussed and recommendations made to avoid ambiguity in its future
implementation in mechanism development. The performance of the skeletal
mechanisms generated here was compared with that of detailed mechanisms in
simulations of autoignition delay times, laminar flame speeds, and perfectly
stirred reactor temperature response curves and extinction residence times,
over a wide range of pressures, temperatures, and equivalence ratios. The
detailed and skeletal mechanisms agreed well, demonstrating the adequacy of the
resulting reduced chemistry for all the butanol isomers in predicting global
combustion phenomena. In addition, the skeletal mechanisms closely predicted
the time-histories of fuel mass fractions in homogeneous compression-ignition
engine simulations. The performance of each butanol isomer was additionally
compared with that of a gasoline surrogate with an antiknock index of 87 in a
homogeneous compression-ignition engine simulation. The gasoline surrogate was
consumed faster than any of the butanol isomers, with tert-butanol exhibiting
the slowest fuel consumption rate. While n-butanol and isobutanol displayed the
most similar consumption profiles relative to the gasoline surrogate, the two
literature chemical kinetic models predicted different orderings.
",0,1,0,0,0,0
9601,Morphology of PbTe crystal surface sputtered by argon plasma under Secondary Neutral Mass Spectrometry conditions,"  We have investigated morphology of the lateral surfaces of PbTe crystal
samples grown from melt by the Bridgman method sputtered by Ar+ plasma with ion
energy of 50-550 eV for 5-50 minutes under Secondary Neutral Mass Spectrometry
(SNMS) conditions. The sputtered PbTe crystal surface was found to be
simultaneously both the source of sputtered material and the efficient
substrate for re-deposition of the sputtered material during the depth
profiling. During sputtering PbTe crystal surface is forming the dimple relief.
To be redeposited the sputtered Pb and Te form arrays of the microscopic
surface structures in the shapes of hillocks, pyramids, cones and others on the
PbTe crystal sputtered surface. Correlation between the density of re-deposited
microscopic surface structures, their shape, and average size, on the one hand,
and the energy and duration of sputtering, on the other, is revealed.
",0,1,0,0,0,0
1864,SAML-QC: a Stochastic Assessment and Machine Learning based QC technique for Industrial Printing,"  Recently, the advancement in industrial automation and high-speed printing
has raised numerous challenges related to the printing quality inspection of
final products. This paper proposes a machine vision based technique to assess
the printing quality of text on industrial objects. The assessment is based on
three quality defects such as text misalignment, varying printing shades, and
misprinted text. The proposed scheme performs the quality inspection through
stochastic assessment technique based on the second-order statistics of
printing. First: the text-containing area on printed product is identified
through image processing techniques. Second: the alignment testing of the
identified text-containing area is performed. Third: optical character
recognition is performed to divide the text into different small boxes and only
the intensity value of each text-containing box is taken as a random variable
and second-order statistics are estimated to determine the varying printing
defects in the text under one, two and three sigma thresholds. Fourth: the
K-Nearest Neighbors based supervised machine learning is performed to provide
the stochastic process for misprinted text detection. Finally, the technique is
deployed on an industrial image for the printing quality assessment with
varying values of n and m. The results have shown that the proposed SAML-QC
technique can perform real-time automated inspection for industrial printing.
",1,0,0,0,0,0
20455,Integral models of reductive groups and integral Mumford-Tate groups,"  Let $G$ be a reductive algebraic group over a $p$-adic field or number field
$K$, and let $V$ be a $K$-linear faithful representation of $G$. A lattice
$\Lambda$ in the vector space $V$ defines a model $\hat{G}_{\Lambda}$ of $G$
over $\mathscr{O}_K$. One may wonder to what extent $\Lambda$ is determined by
the group scheme $\hat{G}_{\Lambda}$. In this paper we prove that up to a
natural equivalence relation on the set of lattices there are only finitely
many $\Lambda$ corresponding to one model $\hat{G}_{\Lambda}$. Furthermore, we
relate this fact to moduli spaces of abelian varieties as follows: let
$\mathscr{A}_{g,n}$ be the moduli space of principally polarised abelian
varieties of dimension $g$ with level $n$ structure. We prove that there are at
most finitely many special subvarieties of $\mathscr{A}_{g,n}$ with a given
integral generic Mumford-Tate group.
",0,0,1,0,0,0
8701,FA*IR: A Fair Top-k Ranking Algorithm,"  In this work, we define and solve the Fair Top-k Ranking problem, in which we
want to determine a subset of k candidates from a large pool of n >> k
candidates, maximizing utility (i.e., select the ""best"" candidates) subject to
group fairness criteria. Our ranked group fairness definition extends group
fairness using the standard notion of protected groups and is based on ensuring
that the proportion of protected candidates in every prefix of the top-k
ranking remains statistically above or indistinguishable from a given minimum.
Utility is operationalized in two ways: (i) every candidate included in the
top-$k$ should be more qualified than every candidate not included; and (ii)
for every pair of candidates in the top-k, the more qualified candidate should
be ranked above. An efficient algorithm is presented for producing the Fair
Top-k Ranking, and tested experimentally on existing datasets as well as new
datasets released with this paper, showing that our approach yields small
distortions with respect to rankings that maximize utility without considering
fairness criteria.
To the best of our knowledge, this is the first algorithm grounded in
statistical tests that can mitigate biases in the representation of an
under-represented group along a ranked list.
",1,0,0,0,0,0
12001,Optimization and Testing in Linear Non-Gaussian Component Analysis,"  Independent component analysis (ICA) decomposes multivariate data into
mutually independent components (ICs). The ICA model is subject to a constraint
that at most one of these components is Gaussian, which is required for model
identifiability. Linear non-Gaussian component analysis (LNGCA) generalizes the
ICA model to a linear latent factor model with any number of both non-Gaussian
components (signals) and Gaussian components (noise), where observations are
linear combinations of independent components. Although the individual Gaussian
components are not identifiable, the Gaussian subspace is identifiable. We
introduce an estimator along with its optimization approach in which
non-Gaussian and Gaussian components are estimated simultaneously, maximizing
the discrepancy of each non-Gaussian component from Gaussianity while
minimizing the discrepancy of each Gaussian component from Gaussianity. When
the number of non-Gaussian components is unknown, we develop a statistical test
to determine it based on resampling and the discrepancy of estimated
components. Through a variety of simulation studies, we demonstrate the
improvements of our estimator over competing estimators, and we illustrate the
effectiveness of the test to determine the number of non-Gaussian components.
Further, we apply our method to real data examples and demonstrate its
practical value.
",0,0,1,1,0,0
6550,Correlation between clustering and degree in affiliation networks,"  We are interested in the probability that two randomly selected neighbors of
a random vertex of degree (at least) $k$ are adjacent. We evaluate this
probability for a power law random intersection graph, where each vertex is
prescribed a collection of attributes and two vertices are adjacent whenever
they share a common attribute. We show that the probability obeys the scaling
$k^{-\delta}$ as $k\to+\infty$. Our results are mathematically rigorous. The
parameter $0\le \delta\le 1$ is determined by the tail indices of power law
random weights defining the links between vertices and attributes.
",1,0,0,0,0,0
16340,A Theory of Solvability for Lossless Power Flow Equations -- Part II: Conditions for Radial Networks,"  This two-part paper details a theory of solvability for the power flow
equations in lossless power networks. In Part I, we derived a new formulation
of the lossless power flow equations, which we term the fixed-point power flow.
The model is parameterized by several graph-theoretic matrices -- the power
network stiffness matrices -- which quantify the internal coupling strength of
the network. In Part II, we leverage the fixed-point power flow to study power
flow solvability. For radial networks, we derive parametric conditions which
guarantee the existence and uniqueness of a high-voltage power flow solution,
and construct examples for which the conditions are also necessary. Our
conditions (i) imply convergence of the fixed-point power flow iteration, (ii)
unify and extend recent results on solvability of decoupled power flow, (iii)
directly generalize the textbook two-bus system results, and (iv) provide new
insights into how the structure and parameters of the grid influence power flow
solvability.
",0,0,1,0,0,0
1128,Lipschitz regularity of solutions to two-phase free boundary problems,"  We prove Lipschitz continuity of viscosity solutions to a class of two-phase
free boundary problems governed by fully nonlinear operators.
",0,0,1,0,0,0
4487,Humanoid Robots as Agents of Human Consciousness Expansion,"  The ""Loving AI"" project involves developing software enabling humanoid robots
to interact with people in loving and compassionate ways, and to promote
people' self-understanding and self-transcendence. Currently the project
centers on the Hanson Robotics robot ""Sophia"" -- specifically, on supplying
Sophia with personality content and cognitive, linguistic, perceptual and
behavioral content aimed at enabling loving interactions supportive of human
self-transcendence. In September 2017 a small pilot study was conducted,
involving the Sophia robot leading human subjects through dialogues and
exercises focused on meditation, visualization and relaxation. The pilot was an
apparent success, qualitatively demonstrating the viability of the approach and
the ability of appropriate human-robot interaction to increase human well-being
and advance human consciousness.
",1,0,0,0,0,0
3781,Environmental impact assessment for climate change policy with the simulation-based integrated assessment model E3ME-FTT-GENIE,"  A high degree of consensus exists in the climate sciences over the role that
human interference with the atmosphere is playing in changing the climate.
Following the Paris Agreement, a similar consensus exists in the policy
community over the urgency of policy solutions to the climate problem. The
context for climate policy is thus moving from agenda setting, which has now
been mostly established, to impact assessment, in which we identify policy
pathways to implement the Paris Agreement. Most integrated assessment models
currently used to address the economic and technical feasibility of avoiding
climate change are based on engineering perspectives with a normative systems
optimisation philosophy, suitable for agenda setting, but unsuitable to assess
the socio-economic impacts of a realistic baskets of climate policies. Here, we
introduce a fully descriptive, simulation-based integrated assessment model
designed specifically to assess policies, formed by the combination of (1) a
highly disaggregated macro-econometric simulation of the global economy based
on time series regressions (E3ME), (2) a family of bottom-up evolutionary
simulations of technology diffusion based on cross-sectional discrete choice
models (FTT), and (3) a carbon cycle and atmosphere circulation model of
intermediate complexity (GENIE-1). We use this combined model to create a
detailed global and sectoral policy map and scenario that sets the economy on a
pathway that achieves the goals of the Paris Agreement with >66% probability of
not exceeding 2$^\circ$C of global warming. We propose a blueprint for a new
role for integrated assessment models in this upcoming policy assessment
context.
",0,1,0,0,0,0
18049,Evidence for depletion of heavy silicon isotopes at comet 67P/Churyumov-Gerasimenko,"  Context. The Rosetta Orbiter Spectrometer for Ion and Neutral Analysis
(ROSINA) was designed to measure the composition of the gas in the coma of
comet 67P/Churyumov-Gerasimenko, the target of the European Space Agency's
Rosetta mission. In addition to the volatiles, ROSINA measured refractories
sputtered off the comet by the interaction of solar wind protons with the
surface of the comet.
Aims. The origin of different solar system materials is still heavily
debated. Isotopic ratios can be used to distinguish between different
reservoirs and investigate processes occurring during the formation of the
solar system.
Methods. ROSINA consisted of two mass spectrometers and a pressure sensor. In
the ROSINA Double Focusing Mass Spectrometer (DFMS), the neutral gas of
cometary origin was ionized and then deflected in an electric and a magnetic
field that separated the ions based on their mass-to-charge ratio. The DFMS had
a high mass resolution, dynamic range, and sensitivity that allowed detection
of rare species and the known major volatiles.
Results. We measured the relative abundance of all three stable silicon
isotopes with the ROSINA instrument on board the Rosetta spacecraft.
Furthermore, we measured $^{13}$C/$^{12}$C in C$_2$H$_4$, C$_2$H$_5$, and CO.
The DFMS in situ measurements indicate that the average silicon isotopic
composition shows depletion in the heavy isotopes $^{29}$Si and $^{30}$Si with
respect to $^{28}$Si and solar abundances, while $^{13}$C to $^{12}$C is
analytically indistinguishable from bulk planetary and meteorite compositions.
Although the origin of the deficiency of the heavy silicon isotopes cannot be
explained unambiguously, we discuss mechanisms that could have contributed to
the measured depletion of the isotopes $^{29}$Si and $^{30}$Si.
",0,1,0,0,0,0
18231,Novel approaches to spectral properties of correlated electron materials: From generalized Kohn-Sham theory to screened exchange dynamical mean field theory,"  The most intriguing properties of emergent materials are typically
consequences of highly correlated quantum states of their electronic degrees of
freedom. Describing those materials from first principles remains a challenge
for modern condensed matter theory. Here, we review, apply and discuss novel
approaches to spectral properties of correlated electron materials, assessing
current day predictive capabilities of electronic structure calculations. In
particular, we focus on the recent Screened Exchange Dynamical Mean-Field
Theory scheme and its relation to generalized Kohn-Sham theory. These concepts
are illustrated on the transition metal pnictide BaCo$_2$As$_2$ and elemental
zinc and cadmium.
",0,1,0,0,0,0
20828,On Properties of Nests: Some Answers and Questions,"  By considering nests on a given space, we explore order-theoretical and
topological properties that are closely related to the structure of a nest. In
particular, we see how subbases given by two dual nests can be an indicator of
how close or far are the properties of the space from the structure of a
linearly ordered space. Having in mind that the term interlocking nest is a key
tool to a general solution of the orderability problem, we give a
characterization of interlocking nest via closed sets in the Alexandroff
topology and via lower sets, respectively. We also characterize bounded subsets
of a given set in terms of nests and, finally, we explore the possibility of
characterizing topological groups via properties of nests. All sections are
followed by a number of open questions, which may give new directions to the
orderability problem.
",0,0,1,0,0,0
12209,"Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs","  The loss functions of deep neural networks are complex and their geometric
properties are not well understood. We show that the optima of these complex
loss functions are in fact connected by simple curves over which training and
test accuracy are nearly constant. We introduce a training procedure to
discover these high-accuracy pathways between modes. Inspired by this new
geometric insight, we also propose a new ensembling method entitled Fast
Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in
the time required to train a single model. We achieve improved performance
compared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10,
CIFAR-100, and ImageNet.
",0,0,0,1,0,0
746,Bounded height in families of dynamical systems,"  Let a and b be algebraic numbers such that exactly one of a and b is an
algebraic integer, and let f_t(z):=z^2+t be a family of polynomials
parametrized by t. We prove that the set of all algebraic numbers t for which
there exist positive integers m and n such that f_t^m(a)=f_t^n(b) has bounded
Weil height. This is a special case of a more general result supporting a new
bounded height conjecture in dynamics. Our results fit into the general setting
of the principle of unlikely intersections in arithmetic dynamics.
",0,0,1,0,0,0
20688,Combining Homotopy Methods and Numerical Optimal Control to Solve Motion Planning Problems,"  This paper presents a systematic approach for computing local solutions to
motion planning problems in non-convex environments using numerical optimal
control techniques. It extends the range of use of state-of-the-art numerical
optimal control tools to problem classes where these tools have previously not
been applicable. Today these problems are typically solved using motion
planners based on randomized or graph search. The general principle is to
define a homotopy that perturbs, or preferably relaxes, the original problem to
an easily solved problem. By combining a Sequential Quadratic Programming (SQP)
method with a homotopy approach that gradually transforms the problem from a
relaxed one to the original one, practically relevant locally optimal solutions
to the motion planning problem can be computed. The approach is demonstrated in
motion planning problems in challenging 2D and 3D environments, where the
presented method significantly outperforms a state-of-the-art open-source
optimizing sampled-based planner commonly used as benchmark.
",0,0,1,0,0,0
548,Semi-Analytical Perturbative Approaches to Third Body Resonant Trajectories,"  In the framework of multi-body dynamics, successive encounters with a third
body, even if well outside of its sphere of influence, can noticeably alter the
trajectory of a spacecraft. Examples of these effects have already been
exploited by past missions such as SMART-1, as well as are proposed to benefit
future missions to Jupiter, Saturn or Neptune, and disposal strategies from
Earth's High Eccentric or Libration Point Orbits. This paper revises three
totally different descriptions of the effects of the third body gravitational
perturbation. These are the averaged dynamics of the classical third body
perturbing function, the Opik's close encounter theory and the Keplerian map
approach. The first two techniques have respectively been applied to the cases
of a spacecraft either always remaining very far or occasionally experiencing
extremely close approaches to the third body. However, the paper also seeks
solutions for trajectories that undergo one or more close approaches at
distances in the order of the sphere of influence of the third body. The paper
attempts to gain insight into the accuracy of these different perturbative
techniques into each of these scenarios, as compared with the motion in the
Circular Restricted Three Body Problem.
",0,1,0,0,0,0
14498,Predicting and Discovering True Muonium,"  The recent observation of discrepancies in the muonic sector motivates
searches for the yet undiscovered atom true muonium $(\mu^+\mu^-)$. To leverage
potential experimental signals, precise theoretical calculations are required.
I will present the on-going work to compute higher-order corrections to the
hyperfine splitting and the Lamb shift. Further, possible detection in rare
meson decay experiments like REDTOP and using true muonium production to
constrain mesonic form factors will be discussed.
",0,1,0,0,0,0
10017,Deciding some Maltsev conditions in finite idempotent algebras,"  In this paper we investigate the computational complexity of deciding if a
given finite algebraic structure satisfies a fixed (strong) Maltsev condition
$\Sigma$. Our goal in this paper is to show that $\Sigma$-testing can be
accomplished in polynomial time when the algebras tested are idempotent and the
Maltsev condition $\Sigma$ can be described using paths. Examples of such path
conditions are having a Maltsev term, having a majority operation, and having a
chain of Jónsson (or Gumm) terms of fixed length.
",1,0,1,0,0,0
6058,Deep Learning Methods for Efficient Large Scale Video Labeling,"  We present a solution to ""Google Cloud and YouTube-8M Video Understanding
Challenge"" that ranked 5th place. The proposed model is an ensemble of three
model families, two frame level and one video level. The training was performed
on augmented dataset, with cross validation.
",1,0,0,1,0,0
19877,Analysis of the Gibbs Sampler for Gaussian hierarchical models via multigrid decomposition,"  We study the convergence properties of the Gibbs Sampler in the context of
posterior distributions arising from Bayesian analysis of Gaussian hierarchical
models. We consider centred and non-centred parameterizations as well as their
hybrids including the full family of partially non-centred parameterizations.
We develop a novel methodology based on multi-grid decompositions to derive
analytic expressions for the convergence rates of the algorithm for an
arbitrary number of layers in the hierarchy, while previous work was typically
limited to the two-level case. Our work gives a complete understanding for the
three-level symmetric case and this gives rise to approximations for the
non-symmetric case. We also give analogous, if less explicit, results for
models of arbitrary level. This theory gives rise to simple and
easy-to-implement guidelines for the practical implementation of Gibbs samplers
on conditionally Gaussian hierarchical models.
",0,0,0,1,0,0
19682,Electrical transient laws in neuronal microdomains based on electro-diffusion,"  The current-voltage (I-V) conversion characterizes the physiology of cellular
microdomains and reflects cellular communication, excitability, and electrical
transduction. Yet deriving such I-V laws remains a major challenge in most
cellular microdomains due to their small sizes and the difficulty of accessing
voltage with a high nanometer precision. We present here novel analytical
relations derived for different numbers of ionic species inside a neuronal
micro/nano-domains, such as dendritic spines. When a steady-state current is
injected, we find a large deviation from the classical Ohm's law, showing that
the spine neck resistance is insuficent to characterize electrical properties.
For a constricted spine neck, modeled by a hyperboloid, we obtain a new I-V law
that illustrates the consequences of narrow passages on electrical conduction.
Finally, during a fast current transient, the local voltage is modulated by the
distance between activated voltage-gated channels. To conclude,
electro-diffusion laws can now be used to interpret voltage distribution in
neuronal microdomains.
",0,0,0,0,1,0
2761,Born Again Neural Networks,"  Knowledge distillation (KD) consists of transferring knowledge from one
machine learning model (the teacher}) to another (the student). Commonly, the
teacher is a high-capacity model with formidable performance, while the student
is more compact. By transferring knowledge, one hopes to benefit from the
student's compactness. %we desire a compact model with performance close to the
teacher's. We study KD from a new perspective: rather than compressing models,
we train students parameterized identically to their teachers. Surprisingly,
these {Born-Again Networks (BANs), outperform their teachers significantly,
both on computer vision and language modeling tasks. Our experiments with BANs
based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10
(3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional
experiments explore two distillation objectives: (i) Confidence-Weighted by
Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP).
Both methods elucidate the essential components of KD, demonstrating a role of
the teacher outputs on both predicted and non-predicted classes. We present
experiments with students of various capacities, focusing on the under-explored
case where students overpower teachers. Our experiments show significant
advantages from transferring knowledge between DenseNets and ResNets in either
direction.
",0,0,0,1,0,0
10226,Learning Theory and Algorithms for Revenue Management in Sponsored Search,"  Online advertisement is the main source of revenue for Internet business.
Advertisers are typically ranked according to a score that takes into account
their bids and potential click-through rates(eCTR). Generally, the likelihood
that a user clicks on an ad is often modeled by optimizing for the click
through rates rather than the performance of the auction in which the click
through rates will be used. This paper attempts to eliminate this
dis-connection by proposing loss functions for click modeling that are based on
final auction performance.In this paper, we address two feasible metrics (AUC^R
and SAUC) to evaluate the on-line RPM (revenue per mille) directly rather than
the CTR. And then, we design an explicit ranking function by incorporating the
calibration fac-tor and price-squashed factor to maximize the revenue. Given
the power of deep networks, we also explore an implicit optimal ranking
function with deep model. Lastly, various experiments with two real world
datasets are presented. In particular, our proposed methods perform better than
the state-of-the-art methods with regard to the revenue of the platform.
",0,0,0,1,0,0
1285,Real-Time Model Predictive Control for Energy Management in Autonomous Underwater Vehicle,"  Improving endurance is crucial for extending the spatial and temporal
operation range of autonomous underwater vehicles (AUVs). Considering the
hardware constraints and the performance requirements, an intelligent energy
management system is required to extend the operation range of AUVs. This paper
presents a novel model predictive control (MPC) framework for energy-optimal
point-to-point motion control of an AUV. In this scheme, the energy management
problem of an AUV is reformulated as a surge motion optimization problem in two
stages. First, a system-level energy minimization problem is solved by managing
the trade-off between the energies required for overcoming the positive
buoyancy and surge drag force in static optimization. Next, an MPC with a
special cost function formulation is proposed to deal with transients and
system dynamics. A switching logic for handling the transition between the
static and dynamic stages is incorporated to reduce the computational efforts.
Simulation results show that the proposed method is able to achieve
near-optimal energy consumption with considerable lower computational
complexity.
",1,0,0,0,0,0
11539,Near Optimal Sketching of Low-Rank Tensor Regression,"  We study the least squares regression problem \begin{align*} \min_{\Theta \in
\mathcal{S}_{\odot D,R}} \|A\Theta-b\|_2, \end{align*} where
$\mathcal{S}_{\odot D,R}$ is the set of $\Theta$ for which $\Theta =
\sum_{r=1}^{R} \theta_1^{(r)} \circ \cdots \circ \theta_D^{(r)}$ for vectors
$\theta_d^{(r)} \in \mathbb{R}^{p_d}$ for all $r \in [R]$ and $d \in [D]$, and
$\circ$ denotes the outer product of vectors. That is, $\Theta$ is a
low-dimensional, low-rank tensor. This is motivated by the fact that the number
of parameters in $\Theta$ is only $R \cdot \sum_{d=1}^D p_d$, which is
significantly smaller than the $\prod_{d=1}^{D} p_d$ number of parameters in
ordinary least squares regression. We consider the above CP decomposition model
of tensors $\Theta$, as well as the Tucker decomposition. For both models we
show how to apply data dimensionality reduction techniques based on {\it
sparse} random projections $\Phi \in \mathbb{R}^{m \times n}$, with $m \ll n$,
to reduce the problem to a much smaller problem $\min_{\Theta} \|\Phi A \Theta
- \Phi b\|_2$, for which if $\Theta'$ is a near-optimum to the smaller problem,
then it is also a near optimum to the original problem. We obtain significantly
smaller dimension and sparsity in $\Phi$ than is possible for ordinary least
squares regression, and we also provide a number of numerical simulations
supporting our theory.
",1,0,0,1,0,0
14563,Influence des mécanismes dissociés de ludifications sur l'apprentissage en support numérique de la lecture en classe primaire,"  The introduction of serious games as pedagogical supports in the field of
education is a process gaining in popularity amongst the teaching community.
This article creates a link between the integration of new pedagogical
solutions in first-year primary class and the fundamental research on the
motivation of the players/learners, detailing an experiment based on a game
specifically developed, named QCM. QCM considers the learning worksheets issued
from the Freinet pedagogy using various gameplay mechanisms. The main
contribution of QCM in relation to more traditional games is the dissociation
of immersion mechanisms, in order to improve the understanding of the user
experience. This game also contains a system of gameplay metrics, the analysis
of which shows a relative increase in the motivation of students using QCM
instead of paper worksheets, while revealing large differences in students
behavior in conjunction with the mechanisms of gamification employed. Keywords
: Serious games, learning analytics, gamification, flow.
",1,0,0,0,0,0
5213,On the Binary Lossless Many-Help-One Problem with Independently Degraded Helpers,"  Although the rate region for the lossless many-help-one problem with
independently degraded helpers is already ""solved"", its solution is given in
terms of a convex closure over a set of auxiliary random variables. Thus, for
any such a problem in particular, an optimization over the set of auxiliary
random variables is required to truly solve the rate region. Providing the
solution is surprisingly difficult even for an example as basic as binary
sources. In this work, we derive a simple and tight inner bound on the rate
region's lower boundary for the lossless many-help-one problem with
independently degraded helpers when specialized to sources that are binary,
uniformly distributed, and interrelated through symmetric channels. This
scenario finds important applications in emerging cooperative communication
schemes in which the direct-link transmission is assisted via multiple lossy
relaying links. Numerical results indicate that the derived inner bound proves
increasingly tight as the helpers become more degraded.
",1,0,0,0,0,0
14252,Deformed Heisenberg Algebra with a minimal length: Application to some molecular potentials,"  We review the essentials of the formalism of quantum mechanics based on a
deformed Heisenbeg algebra, leading to the existence of a minimal length scale.
We compute in this context, the energy spectra of the pseudoharmonic oscillator
and Kratzer potentials by using a perturbative approach. We derive the
molecular constants, which characterize the vibration--rotation energy levels
of diatomic molecules, and investigate the effect of the minimal length on each
of these parameters for both potentials. We confront our result to experimental
data for the hydrogen molecule to estimate an order of magnitude of this
fundamental scale in molecular physics.
",0,1,0,0,0,0
15327,Role of Skin Friction Drag during Flow-Induced Reconfiguration of a Flexible Thin Plate,"  We investigate drag reduction due to the flow-induced reconfiguration of a
flexible thin plate in presence of skin friction drag at low Reynolds Number.
The plate is subjected to a uniform free stream and is tethered at one end. We
extend existing models in the literature to account for the skin friction drag.
The total drag on the plate with respect to a rigid upright plate decreases due
to flow-induced reconfiguration and further reconfiguration increases the total
drag due to increase in skin friction drag. A critical value of Cauchy number
($Ca$) exists at which the total drag on the plate with respect to a rigid
upright plate is minimum at a given Reynolds number. The reconfigured shape of
the plate for this condition is unique, beyond which the total drag increases
on the plate even with reconfiguration. The ratio of the form drag coefficient
for an upright rigid plate and skin drag coefficient for a horizontal rigid
plate ($\lambda$) determines the critical Cauchy number ($Ca_{cr}$). We propose
modification in the drag scaling with free stream velocity ($F_{x}$ ${\propto}$
$U^{n}$) in presence of the skin friction drag. The following expressions of
$n$ are found for $0.01 \leq Re \leq 1$, $n = 4/5 + {\lambda}/5$ for 1 $\leq$
$Ca$ $<$ $Ca_{cr}$ and $n = 1 + {\lambda}/5$ for $Ca_{cr} \leq Ca \leq 300$,
where $Re$ is Reynolds number. We briefly discuss the combined effect of the
skin friction drag and buoyancy on the drag reduction. An assessment of the
feasibility of experiments is presented in order to translate the present model
to physical systems.
",0,1,0,0,0,0
2012,Weighted density fields as improved probes of modified gravity models,"  When it comes to searches for extensions to general relativity, large efforts
are being dedicated to accurate predictions for the power spectrum of density
perturbations. While this observable is known to be sensitive to the
gravitational theory, its efficiency as a diagnostic for gravity is
significantly reduced when Solar System constraints are strictly adhered to. We
show that this problem can be overcome by studying weigthed density fields. We
propose a transformation of the density field for which the impact of modified
gravity on the power spectrum can be increased by more than a factor of three.
The signal is not only amplified, but the modified gravity features are shifted
to larger scales which are less affected by baryonic physics. Furthermore, the
overall signal-to-noise increases, which in principle makes identifying
signatures of modified gravity with future galaxy surveys more feasible. While
our analysis is focused on modified gravity, the technique can be applied to
other problems in cosmology, such as the detection of neutrinos, the effects of
baryons or baryon acoustic oscillations.
",0,1,0,0,0,0
8500,Quenching of supermassive black hole growth around the apparent maximum mass,"  Recent quasar surveys have revealed that supermassive black holes (SMBHs)
rarely exceed a mass of $M_{\rm BH} \sim {\rm a~few}\times10^{10}~M_{\odot}$
during the entire cosmic history. It has been argued that quenching of the BH
growth is caused by a transition of a nuclear accretion disk into an advection
dominated accretion flow, with which strong outflows and/or jets are likely to
be associated. We investigate a relation between the maximum mass of SMBHs and
the radio-loudness of quasars with a well-defined sample of $\sim 10^5$ quasars
at a redshift range of $0<z<2$, obtained from the Sloan Digital Sky Surveys DR7
catalog. We find that the number fraction of the radio-loud (RL) quasars
increases above a threshold of $M_{\rm BH} \simeq 10^{9.5}~M_{\odot}$,
independent of their redshifts. Moreover, the number fraction of RL quasars
with lower Eddington ratios (out of the whole RL quasars), indicating lower
accretion rates, increases above the critical BH mass. These observational
trends can be natural consequences of the proposed scenario of suppressing BH
growth around the apparent maximum mass of $\sim 10^{10}~M_{\odot}$. The
ongoing VLA Sky Survey in radio will allow us to estimate of the exact number
fraction of RL quasars more precisely, which gives further insights to
understand quenching processes for BH growth.
",0,1,0,0,0,0
14538,Efficient Online Timed Pattern Matching by Automata-Based Skipping,"  The timed pattern matching problem is an actively studied topic because of
its relevance in monitoring of real-time systems. There one is given a log $w$
and a specification $\mathcal{A}$ (given by a timed word and a timed automaton
in this paper), and one wishes to return the set of intervals for which the log
$w$, when restricted to the interval, satisfies the specification
$\mathcal{A}$. In our previous work we presented an efficient timed pattern
matching algorithm: it adopts a skipping mechanism inspired by the classic
Boyer--Moore (BM) string matching algorithm. In this work we tackle the problem
of online timed pattern matching, towards embedded applications where it is
vital to process a vast amount of incoming data in a timely manner.
Specifically, we start with the Franek-Jennings-Smyth (FJS) string matching
algorithm---a recent variant of the BM algorithm---and extend it to timed
pattern matching. Our experiments indicate the efficiency of our FJS-type
algorithm in online and offline timed pattern matching.
",1,0,0,0,0,0
9353,Burn-In Demonstrations for Multi-Modal Imitation Learning,"  Recent work on imitation learning has generated policies that reproduce
expert behavior from multi-modal data. However, past approaches have focused
only on recreating a small number of distinct, expert maneuvers, or have relied
on supervised learning techniques that produce unstable policies. This work
extends InfoGAIL, an algorithm for multi-modal imitation learning, to reproduce
behavior over an extended period of time. Our approach involves reformulating
the typical imitation learning setting to include ""burn-in demonstrations"" upon
which policies are conditioned at test time. We demonstrate that our approach
outperforms standard InfoGAIL in maximizing the mutual information between
predicted and unseen style labels in road scene simulations, and we show that
our method leads to policies that imitate expert autonomous driving systems
over long time horizons.
",1,0,0,1,0,0
1321,Time-lagged autoencoders: Deep learning of slow collective variables for molecular kinetics,"  Inspired by the success of deep learning techniques in the physical and
chemical sciences, we apply a modification of an autoencoder type deep neural
network to the task of dimension reduction of molecular dynamics data. We can
show that our time-lagged autoencoder reliably finds low-dimensional embeddings
for high-dimensional feature spaces which capture the slow dynamics of the
underlying stochastic processes - beyond the capabilities of linear dimension
reduction techniques.
",1,1,0,1,0,0
11664,Family-specific scaling laws in bacterial genomes,"  Among several quantitative invariants found in evolutionary genomics, one of
the most striking is the scaling of the overall abundance of proteins, or
protein domains, sharing a specific functional annotation across genomes of
given size. The size of these functional categories change, on average, as
power-laws in the total number of protein-coding genes. Here, we show that such
regularities are not restricted to the overall behavior of high-level
functional categories, but also exist systematically at the level of single
evolutionary families of protein domains. Specifically, the number of proteins
within each family follows family-specific scaling laws with genome size.
Functionally similar sets of families tend to follow similar scaling laws, but
this is not always the case. To understand this systematically, we provide a
comprehensive classification of families based on their scaling properties.
Additionally, we develop a quantitative score for the heterogeneity of the
scaling of families belonging to a given category or predefined group. Under
the common reasonable assumption that selection is driven solely or mainly by
biological function, these findings point to fine-tuned and interdependent
functional roles of specific protein domains, beyond our current functional
annotations. This analysis provides a deeper view on the links between
evolutionary expansion of protein families and the functional constraints
shaping the gene repertoire of bacterial genomes.
",0,1,0,0,0,0
907,Character Networks and Book Genre Classification,"  We compare the social character networks of biographical, legendary and
fictional texts, in search for marks of genre differentiation. We examine the
degree distribution of character appearance and find a power law that does not
depend on the literary genre or historical content. We also analyze local and
global complex networks measures, in particular, correlation plots between the
recently introduced Lobby (or Hirsh $H(1)$) index and Degree, Betweenness and
Closeness centralities. Assortativity plots, which previous literature claims
to separate fictional from real social networks, were also studied. We've found
no relevant differences in the books for these network measures and we give a
plausible explanation why the previous assortativity result is not correct.
",1,1,0,0,0,0
6873,Effect of stellar flares on the upper atmospheres of HD 189733b and HD 209458b,"  Stellar flares are a frequent occurrence on young low-mass stars around which
many detected exoplanets orbit. Flares are energetic, impulsive events, and
their impact on exoplanetary atmospheres needs to be taken into account when
interpreting transit observations. We have developed a model to describe the
upper atmosphere of Extrasolar Giant Planets (EGPs) orbiting flaring stars. The
model simulates thermal escape from the upper atmospheres of close-in EGPs.
Ionisation by solar radiation and electron impact is included and photochemical
and diffusive transport processes are simulated. This model is used to study
the effect of stellar flares from the solar-like G star HD209458 and the young
K star HD189733 on their respective planets. A hypothetical HD209458b-like
planet orbiting the active M star AU Mic is also simulated. We find that the
neutral upper atmosphere of EGPs is not significantly affected by typical
flares. Therefore, stellar flares alone would not cause large enough changes in
planetary mass loss to explain the variations in HD189733b transit depth seen
in previous studies, although we show that it may be possible that an extreme
stellar proton event could result in the required mass loss. Our simulations do
however reveal an enhancement in electron number density in the ionosphere of
these planets, the peak of which is located in the layer where stellar X-rays
are absorbed. Electron densities are found to reach 2.2 to 3.5 times pre-flare
levels and enhanced electron densities last from about 3 to 10 hours after the
onset of the flare. The strength of the flare and the width of its spectral
energy distribution affect the range of altitudes that see enhancements in
ionisation. A large broadband continuum component in the XUV portion of the
flaring spectrum in very young flare stars, such as AU Mic, results in a broad
range of altitudes affected in planets orbiting this star.
",0,1,0,0,0,0
5651,Diffusion along chains of normally hyperbolic cylinders,"  The present paper is part of a series of articles dedicated to the existence
of Arnold diffusion for cusp-residual perturbations of Tonelli Hamiltonians on
$\mathbb{A}^3$. Our goal here is to construct an abstract geometric framework
that can be used to prove the existence of diffusing orbits in the so-called a
priori stable setting, once the preliminary geometric reductions are preformed.
Our framework also applies, rather directly, to the a priori unstable setting.
The main geometric objects of interest are $3$-dimensional normally
hyperbolic invariant cylinders with boundary, which in particular admit
well-defined stable and unstable manifolds. These enable us to define, in our
setting, chains of cylinders, i.e., finite, ordered families of cylinders in
which each cylinder admits homoclinic connections, and any two consecutive
elements in the family admit heteroclinic connections.
Our main result is the existence of diffusing orbits drifting along such
chains, under precise conditions on the dynamics on the cylinders, and on their
homoclinic and heteroclinic structure.
",0,1,1,0,0,0
19382,Bayesian Model-Agnostic Meta-Learning,"  Learning to infer Bayesian posterior from a few-shot dataset is an important
step towards robust meta-learning due to the model uncertainty inherent in the
problem. In this paper, we propose a novel Bayesian model-agnostic
meta-learning method. The proposed method combines scalable gradient-based
meta-learning with nonparametric variational inference in a principled
probabilistic framework. During fast adaptation, the method is capable of
learning complex uncertainty structure beyond a point estimate or a simple
Gaussian approximation. In addition, a robust Bayesian meta-update mechanism
with a new meta-loss prevents overfitting during meta-update. Remaining an
efficient gradient-based meta-learner, the method is also model-agnostic and
simple to implement. Experiment results show the accuracy and robustness of the
proposed method in various tasks: sinusoidal regression, image classification,
active learning, and reinforcement learning.
",0,0,0,1,0,0
3640,Mesh Model (MeMo): A Systematic Approach to Agile System Engineering,"  Innovation and entrepreneurship have a very special role to play in creating
sustainable development in the world. Engineering design plays a major role in
innovation. These are not new facts. However this added to the fact that in
current time knowledge seem to increase at an exponential rate, growing twice
every few months. This creates a need to have newer methods to innovate with
very little scope to fall short of the expectations from customers. In terms of
reliable designing, system design tools and methodologies have been very
helpful and have been in use in most engineering industries for decades now.
But traditional system design is rigorous and rigid. As we can see, we need an
innovation system that should be rigorous and flexible at the same time. We
take our inspiration from biosphere, where some of the most rugged yet flexible
plants are creepers which grow to create mesh. In this thematic paper we shall
explain our approach to system engineering which we call the MeMo (Mesh Model)
that fuses the rigor of system engineering with the flexibility of agile
methods to create a scheme that can give rise to reliable innovation in the
high risk market of today.
",1,0,0,0,0,0
18174,Metric Reduction and Generalized Holomorphic Structures,"  In this paper, metric reduction in generalized geometry is investigated. We
show how the Bismut connections on the quotient manifold are obtained from
those on the original manifold. The result facilitates the analysis of
generalized K$\ddot{a}$hler reduction, which motivates the concept of metric
generalized principal bundles and our approach to construct a family of
generalized holomorphic line bundles over $\mathbb{C}P^2$ equipped with some
non-trivial generalized K$\ddot{a}$hler structures.
",0,0,1,0,0,0
13355,Quantitative CBA: Small and Comprehensible Association Rule Classification Models,"  Quantitative CBA is a postprocessing algorithm for association rule
classification algorithm CBA (Liu et al, 1998). QCBA uses original,
undiscretized numerical attributes to optimize the discovered association
rules, refining the boundaries of literals in the antecedent of the rules
produced by CBA. Some rules as well as literals from the rules can consequently
be removed, which makes the resulting classifier smaller. One-rule
classification and crisp rules make CBA classification models possibly most
comprehensible among all association rule classification algorithms. These
viable properties are retained by QCBA. The postprocessing is conceptually
fast, because it is performed on a relatively small number of rules that passed
data coverage pruning in CBA. Benchmark of our QCBA approach on 22 UCI datasets
shows average 53% decrease in the total size of the model as measured by the
total number of conditions in all rules. Model accuracy remains on the same
level as for CBA.
",1,0,0,1,0,0
14124,A Reinforcement Learning Approach to Jointly Adapt Vehicular Communications and Planning for Optimized Driving,"  Our premise is that autonomous vehicles must optimize communications and
motion planning jointly. Specifically, a vehicle must adapt its motion plan
staying cognizant of communications rate related constraints and adapt the use
of communications while being cognizant of motion planning related restrictions
that may be imposed by the on-road environment. To this end, we formulate a
reinforcement learning problem wherein an autonomous vehicle jointly chooses
(a) a motion planning action that executes on-road and (b) a communications
action of querying sensed information from the infrastructure. The goal is to
optimize the driving utility of the autonomous vehicle. We apply the Q-learning
algorithm to make the vehicle learn the optimal policy, which makes the optimal
choice of planning and communications actions at any given time. We demonstrate
the ability of the optimal policy to smartly adapt communications and planning
actions, while achieving large driving utilities, using simulations.
",1,0,0,0,0,0
6480,Kohn anomalies in momentum dependence of magnetic susceptibility of some three-dimensional systems,"  We study a question of presence of Kohn points, yielding at low temperatures
non-analytic momentum dependence of magnetic susceptibility near its maximum,
in electronic spectum of some three-dimensional systems. In particular, we
consider one-band model on face centered cubic lattice with hopping between
nearest and next-nearest neighbors, which models some aspects of the dispersion
of ZrZn$_2$, and the two-band model on body centered cubic lattice, modeling
the dispersion of chromium. For the former model it is shown that Kohn points
yielding maxima of susceptibility exist in a certain (sufficiently wide) region
of electronic concentrations; the dependence of the wave vectors, corresponding
to the maxima, on the chemical potential is investigated. For the two-band
model we show existence of the lines of Kohn points, yielding maximum of the
susceptibility, which position agrees with the results of band structure
calculations and experimental data on the wave vector of antiferromagnetism of
chromium.
",0,1,0,0,0,0
6038,ORSIm Detector: A Novel Object Detection Framework in Optical Remote Sensing Imagery Using Spatial-Frequency Channel Features,"  With the rapid development of spaceborne imaging techniques, object detection
in optical remote sensing imagery has drawn much attention in recent decades.
While many advanced works have been developed with powerful learning
algorithms, the incomplete feature representation still cannot meet the demand
for effectively and efficiently handling image deformations, particularly
objective scaling and rotation. To this end, we propose a novel object
detection framework, called optical remote sensing imagery detector (ORSIm
detector), integrating diverse channel features extraction, feature learning,
fast image pyramid matching, and boosting strategy. ORSIm detector adopts a
novel spatial-frequency channel feature (SFCF) by jointly considering the
rotation-invariant channel features constructed in frequency domain and the
original spatial channel features (e.g., color channel, gradient magnitude).
Subsequently, we refine SFCF using learning-based strategy in order to obtain
the high-level or semantically meaningful features. In the test phase, we
achieve a fast and coarsely-scaled channel computation by mathematically
estimating a scaling factor in the image domain. Extensive experimental results
conducted on the two different airborne datasets are performed to demonstrate
the superiority and effectiveness in comparison with previous state-of-the-art
methods.
",1,0,0,0,0,0
12975,Latent Constraints: Learning to Generate Conditionally from Unconditional Generative Models,"  Deep generative neural networks have proven effective at both conditional and
unconditional modeling of complex data distributions. Conditional generation
enables interactive control, but creating new controls often requires expensive
retraining. In this paper, we develop a method to condition generation without
retraining the model. By post-hoc learning latent constraints, value functions
that identify regions in latent space that generate outputs with desired
attributes, we can conditionally sample from these regions with gradient-based
optimization or amortized actor functions. Combining attribute constraints with
a universal ""realism"" constraint, which enforces similarity to the data
distribution, we generate realistic conditional images from an unconditional
variational autoencoder. Further, using gradient-based optimization, we
demonstrate identity-preserving transformations that make the minimal
adjustment in latent space to modify the attributes of an image. Finally, with
discrete sequences of musical notes, we demonstrate zero-shot conditional
generation, learning latent constraints in the absence of labeled data or a
differentiable reward function. Code with dedicated cloud instance has been
made publicly available (this https URL).
",1,0,0,1,0,0
3720,General multilevel Monte Carlo methods for pricing discretely monitored Asian options,"  We describe general multilevel Monte Carlo methods that estimate the price of
an Asian option monitored at $m$ fixed dates. Our approach yields unbiased
estimators with standard deviation $O(\epsilon)$ in $O(m + (1/\epsilon)^{2})$
expected time for a variety of processes including the Black-Scholes model,
Merton's jump-diffusion model, the Square-Root diffusion model, Kou's double
exponential jump-diffusion model, the variance gamma and NIG exponential Levy
processes and, via the Milstein scheme, processes driven by scalar stochastic
differential equations. Using the Euler scheme, our approach estimates the
Asian option price with root mean square error $O(\epsilon)$ in
$O(m+(\ln(\epsilon)/\epsilon)^{2})$ expected time for processes driven by
multidimensional stochastic differential equations. Numerical experiments
confirm that our approach outperforms the conventional Monte Carlo method by a
factor of order $m$.
",0,0,0,0,0,1
20273,Less Is More: A Comprehensive Framework for the Number of Components of Ensemble Classifiers,"  The number of component classifiers chosen for an ensemble greatly impacts
the prediction ability. In this paper, we use a geometric framework for a
priori determining the ensemble size, which is applicable to most of existing
batch and online ensemble classifiers. There are only a limited number of
studies on the ensemble size examining Majority Voting (MV) and Weighted
Majority Voting (WMV). Almost all of them are designed for batch-mode, hardly
addressing online environments. Big data dimensions and resource limitations,
in terms of time and memory, make determination of ensemble size crucial,
especially for online environments. For the MV aggregation rule, our framework
proves that the more strong components we add to the ensemble, the more
accurate predictions we can achieve. For the WMV aggregation rule, our
framework proves the existence of an ideal number of components, which is equal
to the number of class labels, with the premise that components are completely
independent of each other and strong enough. While giving the exact definition
for a strong and independent classifier in the context of an ensemble is a
challenging task, our proposed geometric framework provides a theoretical
explanation of diversity and its impact on the accuracy of predictions. We
conduct a series of experimental evaluations to show the practical value of our
theorems and existing challenges.
",1,0,0,1,0,0
10686,Network Transplanting (extended abstract),"  This paper focuses on a new task, i.e., transplanting a
category-and-task-specific neural network to a generic, modular network without
strong supervision. We design a functionally interpretable structure for the
generic network. Like building LEGO blocks, we teach the generic network a new
category by directly transplanting the module corresponding to the category
from a pre-trained network with a few or even without sample annotations. Our
method incrementally adds new categories to the generic network but does not
affect representations of existing categories. In this way, our method breaks
the typical bottleneck of learning a net for massive tasks and categories,
i.e., the requirement of collecting samples for all tasks and categories at the
same time before the learning begins. Thus, we use a new distillation
algorithm, namely back-distillation, to overcome specific challenges of network
transplanting. Our method without training samples even outperformed the
baseline with 100 training samples.
",1,0,0,1,0,0
12040,An Empirical Analysis of Approximation Algorithms for the Euclidean Traveling Salesman Problem,"  With applications to many disciplines, the traveling salesman problem (TSP)
is a classical computer science optimization problem with applications to
industrial engineering, theoretical computer science, bioinformatics, and
several other disciplines. In recent years, there have been a plethora of novel
approaches for approximate solutions ranging from simplistic greedy to
cooperative distributed algorithms derived from artificial intelligence. In
this paper, we perform an evaluation and analysis of cornerstone algorithms for
the Euclidean TSP. We evaluate greedy, 2-opt, and genetic algorithms. We use
several datasets as input for the algorithms including a small dataset, a
mediumsized dataset representing cities in the United States, and a synthetic
dataset consisting of 200 cities to test algorithm scalability. We discover
that the greedy and 2-opt algorithms efficiently calculate solutions for
smaller datasets. Genetic algorithm has the best performance for optimality for
medium to large datasets, but generally have longer runtime. Our
implementations is public available.
",1,0,0,0,0,0
5697,Measuring bot and human behavioral dynamics,"  Bots, social media accounts controlled by software rather than by humans,
have recently been under the spotlight for their association with various forms
of online manipulation. To date, much work has focused on social bot detection,
but little attention has been devoted to the characterization and measurement
of the behavior and activity of bots, as opposed to humans'. Over the course of
the years, bots have become more sophisticated, and capable to reflect some
short-term behavior, emulating that of human users. The goal of this paper is
to study the behavioral dynamics that bots exhibit over the course of one
activity session, and highlight if and how these differ from human activity
signatures. By using a large Twitter dataset associated with recent political
events, we first separate bots and humans, then isolate their activity
sessions. We compile a list of quantities to be measured, like the propensity
of users to engage in social interactions or to produce content. Our analysis
highlights the presence of short-term behavioral trends in humans, which can be
associated with a cognitive origin, that are absent in bots, intuitively due to
their automated activity. These findings are finally codified to create and
evaluate a machine learning algorithm to detect activity sessions produced by
bots and humans, to allow for more nuanced bot detection strategies.
",1,0,0,0,0,0
466,Identification and Off-Policy Learning of Multiple Objectives Using Adaptive Clustering,"  In this work, we present a methodology that enables an agent to make
efficient use of its exploratory actions by autonomously identifying possible
objectives in its environment and learning them in parallel. The identification
of objectives is achieved using an online and unsupervised adaptive clustering
algorithm. The identified objectives are learned (at least partially) in
parallel using Q-learning. Using a simulated agent and environment, it is shown
that the converged or partially converged value function weights resulting from
off-policy learning can be used to accumulate knowledge about multiple
objectives without any additional exploration. We claim that the proposed
approach could be useful in scenarios where the objectives are initially
unknown or in real world scenarios where exploration is typically a time and
energy intensive process. The implications and possible extensions of this work
are also briefly discussed.
",1,0,0,0,0,0
5333,LATTES: a novel detector concept for a gamma-ray experiment in the Southern hemisphere,"  The Large Array Telescope for Tracking Energetic Sources (LATTES), is a novel
concept for an array of hybrid EAS array detectors, composed of a Resistive
Plate Counter array coupled to a Water Cherenkov Detector, planned to cover
gamma rays from less than 100 GeV up to 100 TeVs. This experiment, to be
installed at high altitude in South America, could cover the existing gap in
sensitivity between satellite and ground arrays.
The low energy threshold, large duty cycle and wide field of view of LATTES
makes it a powerful tool to detect transient phenomena and perform long term
observations of variable sources. Moreover, given its characteristics, it would
be fully complementary to the planned Cherenkov Telescope Array (CTA) as it
would be able to issue alerts.
In this talk, a description of its main features and capabilities, as well as
results on its expected performance, and sensitivity, will be presented.
",0,1,0,0,0,0
8135,Design of a Time Delay Reservoir Using Stochastic Logic: A Feasibility Study,"  This paper presents a stochastic logic time delay reservoir design. The
reservoir is analyzed using a number of metrics, such as kernel quality,
generalization rank, performance on simple benchmarks, and is also compared to
a deterministic design. A novel re-seeding method is introduced to reduce the
adverse effects of stochastic noise, which may also be implemented in other
stochastic logic reservoir computing designs, such as echo state networks.
Benchmark results indicate that the proposed design performs well on
noise-tolerant classification problems, but more work needs to be done to
improve the stochastic logic time delay reservoir's robustness for regression
problems.
",1,0,0,1,0,0
9603,Proceedings 14th International Workshop on the ACL2 Theorem Prover and its Applications,"  This volume contains the proceedings of the Fourteenth International Workshop
on the ACL2 Theorem Prover and Its Applications, ACL2 2017, a two-day workshop
held in Austin, Texas, USA, on May 22-23, 2017. ACL2 workshops occur at
approximately 18-month intervals, and they provide a technical forum for
researchers to present and discuss improvements and extensions to the theorem
prover, comparisons of ACL2 with other systems, and applications of ACL2 in
formal verification.
ACL2 is a state-of-the-art automated reasoning system that has been
successfully applied in academia, government, and industry for specification
and verification of computing systems and in teaching computer science courses.
Boyer, Kaufmann, and Moore were awarded the 2005 ACM Software System Award for
their work on ACL2 and the other theorem provers in the Boyer-Moore
theorem-prover family.
The proceedings of ACL2 2017 include the seven technical papers and two
extended abstracts that were presented at the workshop. Each submission
received two or three reviews. The workshop also included three invited talks:
""Using Mechanized Mathematics in an Organization with a Simulation-Based
Mentality"", by Glenn Henry of Centaur Technology, Inc.; ""Formal Verification of
Financial Algorithms, Progress and Prospects"", by Grant Passmore of Aesthetic
Integration; and ""Verifying Oracle's SPARC Processors with ACL2"" by Greg
Grohoski of Oracle. The workshop also included several rump sessions discussing
ongoing research and the use of ACL2 within industry.
",1,0,0,0,0,0
8195,Local asymptotic properties for Cox-Ingersoll-Ross process with discrete observations,"  In this paper, we consider a one-dimensional Cox-Ingersoll-Ross (CIR) process
whose drift coefficient depends on unknown parameters. Considering the process
discretely observed at high frequency, we prove the local asymptotic normality
property in the subcritical case, the local asymptotic quadraticity in the
critical case, and the local asymptotic mixed normality property in the
supercritical case. To obtain these results, we use the Malliavin calculus
techniques developed recently for CIR process together with the $L^p$-norm
estimation for positive and negative moments of the CIR process. In this study,
we require the same conditions of high frequency $\Delta_n\rightarrow 0$ and
infinite horizon $n\Delta_n\rightarrow\infty$ as in the case of ergodic
diffusions with globally Lipschitz coefficients studied earlier by Gobet
\cite{G02}. However, in the non-ergodic cases, additional assumptions on the
decreasing rate of $\Delta_n$ are required due to the fact that the square root
diffusion coefficient of the CIR process is not regular enough. Indeed, we
assume $\frac{n\Delta_n^{\frac{3}{2}}}{\log(n\Delta_n)}\to 0$ for the critical
case and $n\Delta_n^2\to 0$ for the supercritical case.
",0,0,1,1,0,0
4026,Large-type Artin groups are systolic,"  We prove that Artin groups from a class containing all large-type Artin
groups are systolic. This provides a concise yet precise description of their
geometry. Immediate consequences are new results concerning large-type Artin
groups: biautomaticity; existence of $EZ$-boundaries; the Novikov conjecture;
descriptions of finitely presented subgroups, of virtually solvable subgroups,
and of centralizers for infinite order elements; the Burghelea conjecture and
the Bass conjecture; existence of low-dimensional models for classifying spaces
for some families of subgroups.
",0,0,1,0,0,0
11583,Disagreement-Based Combinatorial Pure Exploration: Sample Complexity Bounds and an Efficient Algorithm,"  We design new algorithms for the combinatorial pure exploration problem in
the multi-arm bandit framework. In this problem, we are given $K$ distributions
and a collection of subsets $\mathcal{V} \subset 2^{[K]}$ of these
distributions, and we would like to find the subset $v \in \mathcal{V}$ that
has largest mean, while collecting, in a sequential fashion, as few samples
from the distributions as possible. In both the fixed budget and fixed
confidence settings, our algorithms achieve new sample-complexity bounds that
provide polynomial improvements on previous results in some settings. Via an
information-theoretic lower bound, we show that no approach based on uniform
sampling can improve on ours in any regime, yielding the first interactive
algorithms for this problem with this basic property. Computationally, we show
how to efficiently implement our fixed confidence algorithm whenever
$\mathcal{V}$ supports efficient linear optimization. Our results involve
precise concentration-of-measure arguments and a new algorithm for linear
programming with exponentially many constraints.
",1,0,0,1,0,0
10136,Detecting in-plane tension induced crystal plasticity transition with nanoindentation,"  We present experimental data and simulations on the effects of in-plane
tension on nanoindentation hardness and pop-in noise. Nanoindentation
experiments using a Berkovich tip are performed on bulk polycrystaline Al
samples, under tension in a custom 4pt-bending fixture. The hardness displays a
transition, for indentation depths smaller than 10nm, as function of the
in-plane stress at a value consistent with the bulk tensile yield stress.
Displacement bursts appear insensitive to in-plane tension and this transition
disappears for larger indentation depths. Two dimensional discrete dislocation
dynamics simulations confirm that a regime exists where hardness is sensitive
to tension-induced pre-existing dislocations.
",0,1,0,0,0,0
17923,Asymptotically preserving particle-in-cell methods for inhomogenous strongly magnetized plasmas,"  We propose a class of Particle-In-Cell (PIC) methods for the Vlasov-Poisson
system with a strong and inhomogeneous external magnetic field with fixed
direction, where we focus on the motion of particles in the plane orthogonal to
the magnetic field (so-called poloidal directions). In this regime, the time
step can be subject to stability constraints related to the smallness of Larmor
radius and plasma frequency. To avoid this limitation, our approach is based on
first and higher-order semi-implicit numerical schemes already validated on
dissipative systems [3] and for homogeneous magnetic fields [10]. Thus, when
the magnitude of the external magnetic field becomes large, this method
provides a consistent PIC discretization of the guiding-center system taking
into account variations of the magnetic field. We carry out some theoretical
proofs and perform several numerical experiments that establish a solid
validation of the method and its underlying concepts.
",0,0,1,0,0,0
3970,Knowledge Discovery from Layered Neural Networks based on Non-negative Task Decomposition,"  Interpretability has become an important issue in the machine learning field,
along with the success of layered neural networks in various practical tasks.
Since a trained layered neural network consists of a complex nonlinear
relationship between large number of parameters, we failed to understand how
they could achieve input-output mappings with a given data set. In this paper,
we propose the non-negative task decomposition method, which applies
non-negative matrix factorization to a trained layered neural network. This
enables us to decompose the inference mechanism of a trained layered neural
network into multiple principal tasks of input-output mapping, and reveal the
roles of hidden units in terms of their contribution to each principal task.
",0,0,0,1,0,0
7141,A General Sequential Delay-Doppler Estimation Scheme for Sub-Nyquist Pulse-Doppler Radar,"  Sequential estimation of the delay and Doppler parameters for sub-Nyquist
radars by analog-to-information conversion (AIC) systems has received wide
attention recently. However, the estimation methods reported are AIC-dependent
and have poor performance for off-grid targets. This paper develops a general
estimation scheme in the sense that it is applicable to all AICs regardless
whether the targets are on or off the grids. The proposed scheme estimates the
delay and Doppler parameters sequentially, in which the delay estimation is
formulated into a beamspace direction-of- arrival problem and the Doppler
estimation is translated into a line spectrum estimation problem. Then the
well-known spatial and temporal spectrum estimation techniques are used to
provide efficient and high-resolution estimates of the delay and Doppler
parameters. In addition, sufficient conditions on the AIC to guarantee the
successful estimation of off-grid targets are provided, while the existing
conditions are mostly related to the on-grid targets. Theoretical analyses and
numerical experiments show the effectiveness and the correctness of the
proposed scheme.
",1,0,1,0,0,0
19951,Tree based weighted learning for estimating individualized treatment rules with censored data,"  Estimating individualized treatment rules is a central task for personalized
medicine. [zhao2012estimating] and [zhang2012robust] proposed outcome weighted
learning to estimate individualized treatment rules directly through maximizing
the expected outcome without modeling the response directly. In this paper, we
extend the outcome weighted learning to right censored survival data without
requiring either an inverse probability of censoring weighting or a
semiparametric modeling of the censoring and failure times as done in
[zhao2015doubly]. To accomplish this, we take advantage of the tree based
approach proposed in [zhu2012recursively] to nonparametrically impute the
survival time in two different ways. The first approach replaces the reward of
each individual by the expected survival time, while in the second approach
only the censored observations are imputed by their conditional expected
failure times. We establish consistency and convergence rates for both
estimators. In simulation studies, our estimators demonstrate improved
performance compared to existing methods. We also illustrate the proposed
method on a phase III clinical trial of non-small cell lung cancer.
",0,0,1,1,0,0
2730,An independent axiomatisation for free short-circuit logic,"  Short-circuit evaluation denotes the semantics of propositional connectives
in which the second argument is evaluated only if the first argument does not
suffice to determine the value of the expression. Free short-circuit logic is
the equational logic in which compound statements are evaluated from left to
right, while atomic evaluations are not memorised throughout the evaluation,
i.e., evaluations of distinct occurrences of an atom in a compound statement
may yield different truth values. We provide a simple semantics for free SCL
and an independent axiomatisation. Finally, we discuss evaluation strategies,
some other SCLs, and side effects.
",1,0,1,0,0,0
4495,Temporal Markov Processes for Transport in Porous Media: Random Lattice Networks,"  Monte Carlo (MC) simulations of transport in random porous networks indicate
that for high variances of the log-normal permeability distribution, the
transport of a passive tracer is non-Fickian. Here we model this non-Fickian
dispersion in random porous networks using discrete temporal Markov models. We
show that such temporal models capture the spreading behavior accurately. This
is true despite the fact that the slow velocities are strongly correlated in
time, and some studies have suggested that the persistence of low velocities
would render the temporal Markovian model inapplicable. Compared to previously
proposed temporal stochastic differential equations with case specific drift
and diffusion terms, the models presented here require fewer modeling
assumptions. Moreover, we show that discrete temporal Markov models can be used
to represent dispersion in unstructured networks, which are widely used to
model porous media. A new method is proposed to extend the state space of
temporal Markov models to improve the model predictions in the presence of
extremely low velocities in particle trajectories and extend the applicability
of the model to higher temporal resolutions. Finally, it is shown that by
combining multiple transitions, temporal models are more efficient for
computing particle evolution compared to correlated CTRW with spatial
increments that are equal to the lengths of the links in the network.
",1,1,0,0,0,0
19805,Exact relativistic Toda chain eigenfunctions from Separation of Variables and gauge theory,"  We provide a proposal, motivated by Separation of Variables and gauge theory
arguments, for constructing exact solutions to the quantum Baxter equation
associated to the $N$-particle relativistic Toda chain and test our proposal
against numerical results. Quantum Mechanical non-perturbative corrections,
essential in order to obtain a sensible solution, are taken into account in our
gauge theory approach by considering codimension two defects on curved
backgrounds (squashed $S^5$ and degenerate limits) rather than flat space; this
setting also naturally incorporates exact quantization conditions and energy
spectrum of the relativistic Toda chain as well as its modular dual structure.
",0,1,0,0,0,0
11967,Gamma-ray bursts and their relation to astroparticle physics and cosmology,"  This article gives an overview of gamma-ray bursts (GRBs) and their relation
to astroparticle physics and cosmology. GRBs are the most powerful explosions
in the universe that occur roughly once per day and are characterized by
flashes of gamma-rays typically lasting from a fraction of a second to
thousands of seconds. Even after more than four decades since their discovery
they still remain not fully understood. Two types of GRBs are observed:
spectrally harder short duration bursts and softer long duration bursts. The
long GRBs originate from the collapse of massive stars whereas the preferred
model for the short GRBs is coalescence of compact objects such as two neutron
stars or a neutron star and a black hole. There were suggestions that GRBs can
produce ultra-high energy cosmic rays and neutrinos. Also a certain sub-type of
GRBs may serve as a new standard candle that can help constrain and measure the
cosmological parameters to much higher redshift than what was possible so far.
I will review the recent experimental observations.
",0,1,0,0,0,0
6161,Khovanov-Rozansky homology and higher Catalan sequences,"  We give a simple recursion which computes the triply graded Khovanov-Rozansky
homology of several infinite families of knots and links, including the
$(n,nm\pm 1)$ and $(n,nm)$ torus links for $n,m\geq 1$. We interpret our
results in terms of Catalan combinatorics, proving a conjecture of Gorsky's.
Our computations agree with predictions coming from Hilbert schemes and
rational DAHA, which also proves the Gorsky-Oblomkov-Rasmussen-Shende
conjectures in these cases. Additionally, our results suggest a topological
interpretation of the symmetric functions which appear in the context of the
$m$-shuffle conjecture of Haglund-Haiman-Loehr-Remmel-Ulyanov.
",0,0,1,0,0,0
16002,Self-Supervised Vision-Based Detection of the Active Speaker as a Prerequisite for Socially-Aware Language Acquisition,"  This paper presents a self-supervised method for detecting the active speaker
in a multi-person spoken interaction scenario. We argue that this capability is
a fundamental prerequisite for any artificial cognitive system attempting to
acquire language in social settings. Our methods are able to detect an
arbitrary number of possibly overlapping active speakers based exclusively on
visual information about their face. Our methods do not rely on external
annotations, thus complying with cognitive development. Instead, they use
information from the auditory modality to support learning in the visual
domain. The methods have been extensively evaluated on a large multi-person
face-to-face interaction dataset. The results reach an accuracy of 80% on a
multi-speaker setting. We believe this system represents an essential component
of any artificial cognitive system or robotic platform engaging in social
interaction.
",1,0,0,1,0,0
12565,On the decay rate for the wave equation with viscoelastic boundary damping,"  We consider the wave equation with a boundary condition of memory type. Under
natural conditions on the acoustic impedance $\hat{k}$ of the boundary one can
define a corresponding semigroup of contractions (Desch, Fasangova, Milota,
Probst 2010). With the help of Tauberian theorems we establish energy decay
rates via resolvent estimates on the generator $-\mathcal{A}$ of the semigroup.
We reduce the problem of estimating the resolvent of $-\mathcal{A}$ to the
problem of estimating the resolvent of the corresponding stationary problem.
Under not too strict additional assumptions on $\hat{k}$ we establish an upper
bound on the resolvent. For the wave equation on the interval or the disk we
prove our estimates to be sharp.
",0,0,1,0,0,0
4749,In situ accretion of gaseous envelopes on to planetary cores embedded in evolving protoplanetary discs,"  The core accretion hypothesis posits that planets with significant gaseous
envelopes accreted them from their protoplanetary discs after the formation of
rocky/icy cores. Observations indicate that such exoplanets exist at a broad
range of orbital radii, but it is not known whether they accreted their
envelopes in situ, or originated elsewhere and migrated to their current
locations. We consider the evolution of solid cores embedded in evolving
viscous discs that undergo gaseous envelope accretion in situ with orbital
radii in the range $0.1-10\rm au$. Additionally, we determine the long-term
evolution of the planets that had no runaway gas accretion phase after disc
dispersal. We find: (i) Planets with $5 \rm M_{\oplus}$ cores never undergo
runaway accretion. The most massive envelope contained $2.8 \rm M_{\oplus}$
with the planet orbiting at $10 \rm au$. (ii) Accretion is more efficient onto
$10 \rm M_{\oplus}$ and $15 \rm M_{\oplus}$ cores. For orbital radii $a_{\rm p}
\ge 0.5 \rm au$, $15 \rm M_{\oplus}$ cores always experienced runaway gas
accretion. For $a_{\rm p} \ge 5 \rm au$, all but one of the $10 \rm M_{\oplus}$
cores experienced runaway gas accretion. No planets experienced runaway growth
at $a_{\rm p} = 0.1 \rm au$. (iii) We find that, after disc dispersal, planets
with significant gaseous envelopes cool and contract on Gyr time-scales, the
contraction time being sensitive to the opacity assumed. Our results indicate
that Hot Jupiters with core masses $\lesssim 15 \rm M_{\oplus}$ at $\lesssim
0.1 \rm au$ likely accreted their gaseous envelopes at larger distances and
migrated inwards. Consistently with the known exoplanet population,
Super-Earths and mini-Neptunes at small radii during the disc lifetime, accrete
only modest gaseous envelopes.
",0,1,0,0,0,0
8547,Goodness-of-fit tests for the functional linear model based on randomly projected empirical processes,"  We consider marked empirical processes indexed by a randomly projected
functional covariate to construct goodness-of-fit tests for the functional
linear model with scalar response. The test statistics are built from
continuous functionals over the projected process, resulting in computationally
efficient tests that exhibit root-n convergence rates and circumvent the curse
of dimensionality. The weak convergence of the empirical process is obtained
conditionally on a random direction, whilst the almost surely equivalence
between the testing for significance expressed on the original and on the
projected functional covariate is proved. The computation of the test in
practice involves calibration by wild bootstrap resampling and the combination
of several p-values, arising from different projections, by means of the false
discovery rate method. The finite sample properties of the tests are
illustrated in a simulation study for a variety of linear models, underlying
processes, and alternatives. The software provided implements the tests and
allows the replication of simulations and data applications.
",0,0,0,1,0,0
2518,Skoda's Ideal Generation from Vanishing Theorem for Semipositive Nakano Curvature and Cauchy-Schwarz Inequality for Tensors,"  Skoda's 1972 result on ideal generation is a crucial ingredient in the
analytic approach to the finite generation of the canonical ring and the
abundance conjecture. Special analytic techniques developed by Skoda, other
than applications of the usual vanishing theorems and L2 estimates for the
d-bar equation, are required for its proof. This note (which is part of a
lecture given in the 60th birthday conference for Lawrence Ein) gives a
simpler, more straightforward proof of Skoda's result, which makes it a natural
consequence of the standard techniques in vanishing theorems and solving d-bar
equation with L2 estimates. The proof involves the following three ingredients:
(i) one particular Cauchy-Schwarz inequality for tensors with a special factor
which accounts for the exponent of the denominator in the formulation of the
integral condition for Skoda's ideal generation, (ii) the nonnegativity of
Nakano curvature of the induced metric of a special co-rank-1 subbundle of a
trivial vector bundle twisted by a special scalar weight function, and (iii)
the vanishing theorem and solvability of d-bar equation with L2 estimates for
vector bundles of nonnegative Nakano curvature on a strictly pseudoconvex
domain. Our proof gives readily other similar results on ideal generation.
",0,0,1,0,0,0
4553,Weakly nonergodic dynamics in the Gross--Pitaevskii lattice,"  The microcanonical Gross--Pitaevskii (aka semiclassical Bose-Hubbard) lattice
model dynamics is characterized by a pair of energy and norm densities. The
grand canonical Gibbs distribution fails to describe a part of the density
space, due to the boundedness of its kinetic energy spectrum. We define
Poincare equilibrium manifolds and compute the statistics of microcanonical
excursion times off them. The tails of the distribution functions quantify the
proximity of the many-body dynamics to a weakly-nonergodic phase, which occurs
when the average excursion time is infinite. We find that a crossover to
weakly-nonergodic dynamics takes place inside the nonGibbs phase, being
unnoticed by the largest Lyapunov exponent. In the ergodic part of the
non-Gibbs phase, the Gibbs distribution should be replaced by an unknown
modified one. We relate our findings to the corresponding integrable limit,
close to which the actions are interacting through a short range coupling
network.
",0,1,0,0,0,0
16826,"Polarization, plasmon, and Debye screening in doped 3D ani-Weyl semimetal","  We compute the polarization function in a doped three-dimensional
anisotropic-Weyl semimetal, in which the fermion energy dispersion is linear in
two components of the momenta and quadratic in the third. Through detailed
calculations, we find that the long wavelength plasmon mode depends on the
fermion density $n_e$ in the form $\Omega_{p}^{\bot}\propto n_{e}^{3/10}$
within the basal plane and behaves as $\Omega_{p}^{z}\propto n_{e}^{1/2}$ along
the third direction. This unique characteristic of the plasmon mode can be
probed by various experimental techniques, such as electron energy-loss
spectroscopy. The Debye screening at finite chemical potential and finite
temperature is also analyzed based on the polarization function.
",0,1,0,0,0,0
6829,Partially chaotic orbits in a perturbed cubic force model,"  Three types of orbits are theoretically possible in autonomous Hamiltonian
systems with three degrees of freedom: fully chaotic (they only obey the energy
integral), partially chaotic (they obey an additional isolating integral
besides energy) and regular (they obey two isolating integrals besides energy).
The existence of partially chaotic orbits has been denied by several authors,
however, arguing either that there is a sudden transition from regularity to
full chaoticity, or that a long enough follow up of a supposedly partially
chaotic orbit would reveal a fully chaotic nature. This situation needs
clarification, because partially chaotic orbits might play a significant role
in the process of chaotic diffusion. Here we use numerically computed Lyapunov
exponents to explore the phase space of a perturbed three dimensional cubic
force toy model, and a generalization of the Poincaré maps to show that
partially chaotic orbits are actually present in that model. They turn out to
be double orbits joined by a bifurcation zone, which is the most likely source
of their chaos, and they are encapsulated in regions of phase space bounded by
regular orbits similar to each one of the components of the double orbit.
",0,1,0,0,0,0
10888,"Streaming kernel regression with provably adaptive mean, variance, and regularization","  We consider the problem of streaming kernel regression, when the observations
arrive sequentially and the goal is to recover the underlying mean function,
assumed to belong to an RKHS. The variance of the noise is not assumed to be
known. In this context, we tackle the problem of tuning the regularization
parameter adaptively at each time step, while maintaining tight confidence
bounds estimates on the value of the mean function at each point. To this end,
we first generalize existing results for finite-dimensional linear regression
with fixed regularization and known variance to the kernel setup with a
regularization parameter allowed to be a measurable function of past
observations. Then, using appropriate self-normalized inequalities we build
upper and lower bound estimates for the variance, leading to Bersntein-like
concentration bounds. The later is used in order to define the adaptive
regularization. The bounds resulting from our technique are valid uniformly
over all observation points and all time steps, and are compared against the
literature with numerical experiments. Finally, the potential of these tools is
illustrated by an application to kernelized bandits, where we revisit the
Kernel UCB and Kernel Thompson Sampling procedures, and show the benefits of
the novel adaptive kernel tuning strategy.
",1,0,0,1,0,0
1779,AACT: Application-Aware Cooperative Time Allocation for Internet of Things,"  As the number of Internet of Things (IoT) devices keeps increasing, data is
required to be communicated and processed by these devices at unprecedented
rates. Cooperation among wireless devices by exploiting Device-to-Device (D2D)
connections is promising, where aggregated resources in a cooperative setup can
be utilized by all devices, which would increase the total utility of the
setup. In this paper, we focus on the resource allocation problem for
cooperating IoT devices with multiple heterogeneous applications. In
particular, we develop Application-Aware Cooperative Time allocation (AACT)
framework, which optimizes the time that each application utilizes the
aggregated system resources by taking into account heterogeneous device
constraints and application requirements. AACT is grounded on the concept of
Rolling Horizon Control (RHC) where decisions are made by iteratively solving a
convex optimization problem over a moving control window of estimated system
parameters. The simulation results demonstrate significant performance gains.
",1,0,0,0,0,0
1004,High temperature thermodynamics of the honeycomb-lattice Kitaev-Heisenberg model: A high temperature series expansion study,"  We develop high temperature series expansions for the thermodynamic
properties of the honeycomb-lattice Kitaev-Heisenberg model. Numerical results
for uniform susceptibility, heat capacity and entropy as a function of
temperature for different values of the Kitaev coupling $K$ and Heisenberg
exachange coupling $J$ (with $|J|\le |K|$) are presented. These expansions show
good convergence down to a temperature of a fraction of $K$ and in some cases
down to $T=K/10$. In the Kitaev exchange dominated regime, the inverse
susceptibility has a nearly linear temperature dependence over a wide
temperature range. However, we show that already at temperatures $10$-times the
Curie-Weiss temperature, the effective Curie-Weiss constant estimated from the
data can be off by a factor of 2. We find that the magnitude of the heat
capacity maximum at the short-range order peak, is substantially smaller for
small $J/K$ than for $J$ of order or larger than $K$. We suggest that this
itself represents a simple marker for the relative importance of the Kitaev
terms in these systems. Somewhat surprisingly, both heat capacity and
susceptibility data on Na$_2$IrO$_3$ are consistent with a dominant {\it
antiferromagnetic} Kitaev exchange constant of about $300-400$ $K$.
",0,1,0,0,0,0
14456,Restoration of Images with Wavefront Aberrations,"  This contribution deals with image restoration in optical systems with
coherent illumination, which is an important topic in astronomy, coherent
microscopy and radar imaging. Such optical systems suffer from wavefront
distortions, which are caused by imperfect imaging components and conditions.
Known image restoration algorithms work well for incoherent imaging, they fail
in case of coherent images. In this paper a novel wavefront correction
algorithm is presented, which allows image restoration under coherent
conditions. In most coherent imaging systems, especially in astronomy, the
wavefront deformation is known. Using this information, the proposed algorithm
allows a high quality restoration even in case of severe wavefront distortions.
We present two versions of this algorithm, which are an evolution of the
Gerchberg-Saxton and the Hybrid-Input-Output algorithm. The algorithm is
verified on simulated and real microscopic images.
",1,1,0,0,0,0
1116,Bayesian Unification of Gradient and Bandit-based Learning for Accelerated Global Optimisation,"  Bandit based optimisation has a remarkable advantage over gradient based
approaches due to their global perspective, which eliminates the danger of
getting stuck at local optima. However, for continuous optimisation problems or
problems with a large number of actions, bandit based approaches can be
hindered by slow learning. Gradient based approaches, on the other hand,
navigate quickly in high-dimensional continuous spaces through local
optimisation, following the gradient in fine grained steps. Yet, apart from
being susceptible to local optima, these schemes are less suited for online
learning due to their reliance on extensive trial-and-error before the optimum
can be identified. In this paper, we propose a Bayesian approach that unifies
the above two paradigms in one single framework, with the aim of combining
their advantages. At the heart of our approach we find a stochastic linear
approximation of the function to be optimised, where both the gradient and
values of the function are explicitly captured. This allows us to learn from
both noisy function and gradient observations, and predict these properties
across the action space to support optimisation. We further propose an
accompanying bandit driven exploration scheme that uses Bayesian credible
bounds to trade off exploration against exploitation. Our empirical results
demonstrate that by unifying bandit and gradient based learning, one obtains
consistently improved performance across a wide spectrum of problem
environments. Furthermore, even when gradient feedback is unavailable, the
flexibility of our model, including gradient prediction, still allows us
outperform competing approaches, although with a smaller margin. Due to the
pervasiveness of bandit based optimisation, our scheme opens up for improved
performance both in meta-optimisation and in applications where gradient
related information is readily available.
",1,0,0,0,0,0
4062,Nucleosynthesis Predictions and High-Precision Deuterium Measurements,"  Two new high-precision measurements of the deuterium abundance from absorbers
along the line of sight to the quasar PKS1937--1009 were presented. The
absorbers have lower neutral hydrogen column densities (N(HI) $\approx$
18\,cm$^{-2}$) than for previous high-precision measurements, boding well for
further extensions of the sample due to the plenitude of low column density
absorbers. The total high-precision sample now consists of 12 measurements with
a weighted average deuterium abundance of D/H = $2.55\pm0.02\times10^{-5}$. The
sample does not favour a dipole similar to the one detected for the fine
structure constant. The increased precision also calls for improved
nucleosynthesis predictions. For that purpose we have updated the public
AlterBBN code including new reactions, updated nuclear reaction rates, and the
possibility of adding new physics such as dark matter. The standard Big Bang
Nucleosynthesis prediction of D/H = $2.456\pm0.057\times10^{-5}$ is consistent
with the observed value within 1.7 standard deviations.
",0,1,0,0,0,0
12585,A short note on Godbersen's Conjecture,"  In this short note we improve the best to date bound in Godbersen's
conjecture, and show some implications for unbalanced difference bodies.
",0,0,1,0,0,0
14813,Methods for finding leader--follower equilibria with multiple followers,"  The concept of leader--follower (or Stackelberg) equilibrium plays a central
role in a number of real--world applications of game theory. While the case
with a single follower has been thoroughly investigated, results with multiple
followers are only sporadic and the problem of designing and evaluating
computationally tractable equilibrium-finding algorithms is still largely open.
In this work, we focus on the fundamental case where multiple followers play a
Nash equilibrium once the leader has committed to a strategy---as we
illustrate, the corresponding equilibrium finding problem can be easily shown
to be $\mathcal{FNP}$--hard and not in Poly--$\mathcal{APX}$ unless
$\mathcal{P} = \mathcal{NP}$ and therefore it is one among the hardest problems
to solve and approximate. We propose nonconvex mathematical programming
formulations and global optimization methods to find both exact and approximate
equilibria, as well as a heuristic black box algorithm. All the methods and
formulations that we introduce are thoroughly evaluated computationally.
",1,0,0,0,0,0
15717,Adaptive Interference Removal for Un-coordinated Radar/Communication Co-existence,"  Most existing approaches to co-existing communication/radar systems assume
that the radar and communication systems are coordinated, i.e., they share
information, such as relative position, transmitted waveforms and channel
state. In this paper, we consider an un-coordinated scenario where a
communication receiver is to operate in the presence of a number of radars, of
which only a sub-set may be active, which poses the problem of estimating the
active waveforms and the relevant parameters thereof, so as to cancel them
prior to demodulation. Two algorithms are proposed for such a joint waveform
estimation/data demodulation problem, both exploiting sparsity of a proper
representation of the interference and of the vector containing the errors of
the data block, so as to implement an iterative joint interference removal/data
demodulation process. The former algorithm is based on classical on-grid
compressed sensing (CS), while the latter forces an atomic norm (AN)
constraint: in both cases the radar parameters and the communication
demodulation errors can be estimated by solving a convex problem. We also
propose a way to improve the efficiency of the AN-based algorithm. The
performance of these algorithms are demonstrated through extensive simulations,
taking into account a variety of conditions concerning both the interferers and
the respective channel states.
",1,0,0,1,0,0
20255,"E-polynomials of $PGL(2,\mathbb{C})$-character varieties of surface groups","  In this paper, we compute the E-polynomials of the
$PGL(2,\mathbb{C})$-character varieties associated to surfaces of genus $g$
with one puncture, for any holonomy around it, and compare it with its
Langlands dual case, $SL(2,\mathbb{C})$. The study is based on the
stratification of the space of representations and on the analysis of the
behaviour of the E-polynomial under fibrations.
",0,0,1,0,0,0
20381,Engineering Frequency-dependent Superfluidity in Bose-Fermi Mixtures,"  Unconventional superconductivity or superfluidity are among the most exciting
and fascinating quantum states in condensed matter physics. Usually these
states are characterized by non-trivial spatial symmetry of the pairing order
parameter, such as in $^{3}He$ and high-$T_{c}$ cuprates. Besides spatial
dependence the order parameter could have unconventional frequency dependence,
which is also allowed by Fermi-Dirac statistics. For instance, odd-frequency
pairing is an exciting paradigm when discussing exotic superfluidity or
superconductivity and is yet to be realized in the experiments. In this paper
we propose a symmetry-based method of controlling frequency dependence of the
pairing order parameter via manipulating the inversion symmetry of the system.
First, a toy model is introduced to illustrate that frequency dependence of the
order parameter can be adjusted by controlling the inversion symmetry of the
system. Second, taking advantage of the recent rapid developments of shaken
optical lattices in ultracold gases, we propose a Bose-Fermi mixture to realize
such frequency dependent superfluids. The key idea is introducing the
frequency-dependent attraction between Fermions mediated by Bogoliubov phonons
with asymmetric dispersion. Our proposal should pave an alternative way for
exploring frequency-dependent superconductors or superfluids with cold atoms.
",0,1,0,0,0,0
15789,Banach-Alaoglu theorem for Hilbert $H^*$-module,"  We provided an analogue Banach-Alaoglu theorem for Hilbert $H^*$-module. We
construct a $\Lambda$-weak$^*$ topology on a Hilbert $H^*$-module over a proper
$H^*$-algebra $\Lambda$, such that the unit ball is compact with respect to
$\Lambda$-weak$^*$ topology.
",0,0,1,0,0,0
9519,Time-triggering versus event-triggering control over communication channels,"  Time-triggered and event-triggered control strategies for stabilization of an
unstable plant over a rate-limited communication channel subject to unknown,
bounded delay are studied and compared. Event triggering carries implicit
information, revealing the state of the plant. However, the delay in the
communication channel causes information loss, as it makes the state
information out of date. There is a critical delay value, when the loss of
information due to the communication delay perfectly compensates the implicit
information carried by the triggering events. This occurs when the maximum
delay equals the inverse of the entropy rate of the plant. In this context,
extensions of our previous results for event triggering strategies are
presented for vector systems and are compared with the data-rate theorem for
time-triggered control, that is extended here to a setting with unknown delay.
",1,0,1,0,0,0
11562,Commutative positive varieties of languages,"  We study the commutative positive varieties of languages closed under various
operations: shuffle, renaming and product over one-letter alphabets.
",1,0,1,0,0,0
14450,On the relaxed mean-field stochastic control problem,"  This paper is concerned with optimal control problems for systems governed by
mean-field stochastic differential equation, in which the control enters both
the drift and the diffusion coefficient. We prove that the relaxed state
process, associated with measure valued controls, is governed by an orthogonal
martingale measure rather that a Brownian motion. In particular, we show by a
counter example that replacing the drift and diffusion coefficient by their
relaxed counterparts does not define a true relaxed control problem. We
establish the existence of an optimal relaxed control, which can be
approximated by a sequence of strict controls. Moreover under some convexity
conditions, we show that the optimal control is realized by a strict control.
",0,0,1,0,0,0
1194,Learning from MOM's principles: Le Cam's approach,"  We obtain estimation error rates for estimators obtained by aggregation of
regularized median-of-means tests, following a construction of Le Cam. The
results hold with exponentially large probability -- as in the gaussian
framework with independent noise- under only weak moments assumptions on data
and without assuming independence between noise and design. Any norm may be
used for regularization. When it has some sparsity inducing power we recover
sparse rates of convergence.
The procedure is robust since a large part of data may be corrupted, these
outliers have nothing to do with the oracle we want to reconstruct. Our general
risk bound is of order \begin{equation*} \max\left(\mbox{minimax rate in the
i.i.d. setup}, \frac{\text{number of outliers}}{\text{number of
observations}}\right) \enspace. \end{equation*}In particular, the number of
outliers may be as large as (number of data) $\times$(minimax rate) without
affecting this rate. The other data do not have to be identically distributed
but should only have equivalent $L^1$ and $L^2$ moments.
For example, the minimax rate $s \log(ed/s)/N$ of recovery of a $s$-sparse
vector in $\mathbb{R}^d$ is achieved with exponentially large probability by a
median-of-means version of the LASSO when the noise has $q_0$ moments for some
$q_0>2$, the entries of the design matrix should have $C_0\log(ed)$ moments and
the dataset can be corrupted up to $C_1 s \log(ed/s)$ outliers.
",0,0,1,1,0,0
9737,"Structural subnetwork evolution across the life-span: rich-club, feeder, seeder","  The impact of developmental and aging processes on brain connectivity and the
connectome has been widely studied. Network theoretical measures and certain
topological principles are computed from the entire brain, however there is a
need to separate and understand the underlying subnetworks which contribute
towards these observed holistic connectomic alterations. One organizational
principle is the rich-club - a core subnetwork of brain regions that are
strongly connected, forming a high-cost, high-capacity backbone that is
critical for effective communication in the network. Investigations primarily
focus on its alterations with disease and age. Here, we present a systematic
analysis of not only the rich-club, but also other subnetworks derived from
this backbone - namely feeder and seeder subnetworks. Our analysis is applied
to structural connectomes in a normal cohort from a large, publicly available
lifespan study. We demonstrate changes in rich-club membership with age
alongside a shift in importance from 'peripheral' seeder to feeder subnetworks.
Our results show a refinement within the rich-club structure (increase in
transitivity and betweenness centrality), as well as increased efficiency in
the feeder subnetwork and decreased measures of network integration and
segregation in the seeder subnetwork. These results demonstrate the different
developmental patterns when analyzing the connectome stratified according to
its rich-club and the potential of utilizing this subnetwork analysis to reveal
the evolution of brain architectural alterations across the life-span.
",0,0,0,0,1,0
10701,A general method to describe intersystem crossing dynamics in trajectory surface hopping,"  Intersystem crossing is a radiationless process that can take place in a
molecule irradiated by UV-Vis light, thereby playing an important role in many
environmental, biological and technological processes. This paper reviews
different methods to describe intersystem crossing dynamics, paying attention
to semiclassical trajectory theories, which are especially interesting because
they can be applied to large systems with many degrees of freedom. In
particular, a general trajectory surface hopping methodology recently developed
by the authors, which is able to include non-adiabatic and spin-orbit couplings
in excited-state dynamics simulations, is explained in detail. This method,
termed SHARC, can in principle include any arbitrary coupling, what makes it
generally applicable to photophysical and photochemical problems, also those
including explicit laser fields. A step-by-step derivation of the main
equations of motion employed in surface hopping based on the fewest-switches
method of Tully, adapted for the inclusion of spin-orbit interactions, is
provided. Special emphasis is put on describing the different possible choices
of the electronic bases in which spin-orbit can be included in surface hopping,
highlighting the advantages and inconsistencies of the different approaches.
",0,1,0,0,0,0
18739,Information-theoretic Limits for Community Detection in Network Models,"  We analyze the information-theoretic limits for the recovery of node labels
in several network models. This includes the Stochastic Block Model, the
Exponential Random Graph Model, the Latent Space Model, the Directed
Preferential Attachment Model, and the Directed Small-world Model. For the
Stochastic Block Model, the non-recoverability condition depends on the
probabilities of having edges inside a community, and between different
communities. For the Latent Space Model, the non-recoverability condition
depends on the dimension of the latent space, and how far and spread are the
communities in the latent space. For the Directed Preferential Attachment Model
and the Directed Small-world Model, the non-recoverability condition depends on
the ratio between homophily and neighborhood size. We also consider dynamic
versions of the Stochastic Block Model and the Latent Space Model.
",1,0,0,1,0,0
14656,LSTM Fully Convolutional Networks for Time Series Classification,"  Fully convolutional neural networks (FCN) have been shown to achieve
state-of-the-art performance on the task of classifying time series sequences.
We propose the augmentation of fully convolutional networks with long short
term memory recurrent neural network (LSTM RNN) sub-modules for time series
classification. Our proposed models significantly enhance the performance of
fully convolutional networks with a nominal increase in model size and require
minimal preprocessing of the dataset. The proposed Long Short Term Memory Fully
Convolutional Network (LSTM-FCN) achieves state-of-the-art performance compared
to others. We also explore the usage of attention mechanism to improve time
series classification with the Attention Long Short Term Memory Fully
Convolutional Network (ALSTM-FCN). Utilization of the attention mechanism
allows one to visualize the decision process of the LSTM cell. Furthermore, we
propose fine-tuning as a method to enhance the performance of trained models.
An overall analysis of the performance of our model is provided and compared to
other techniques.
",1,0,0,1,0,0
10219,"A Quantum-Proof Non-Malleable Extractor, With Application to Privacy Amplification against Active Quantum Adversaries","  In privacy amplification, two mutually trusted parties aim to amplify the
secrecy of an initial shared secret $X$ in order to establish a shared private
key $K$ by exchanging messages over an insecure communication channel. If the
channel is authenticated the task can be solved in a single round of
communication using a strong randomness extractor; choosing a quantum-proof
extractor allows one to establish security against quantum adversaries.
In the case that the channel is not authenticated, Dodis and Wichs (STOC'09)
showed that the problem can be solved in two rounds of communication using a
non-malleable extractor, a stronger pseudo-random construction than a strong
extractor.
We give the first construction of a non-malleable extractor that is secure
against quantum adversaries. The extractor is based on a construction by Li
(FOCS'12), and is able to extract from source of min-entropy rates larger than
$1/2$. Combining this construction with a quantum-proof variant of the
reduction of Dodis and Wichs, shown by Cohen and Vidick (unpublished), we
obtain the first privacy amplification protocol secure against active quantum
adversaries.
",1,0,0,0,0,0
143,Gaussian fluctuations of Jack-deformed random Young diagrams,"  We introduce a large class of random Young diagrams which can be regarded as
a natural one-parameter deformation of some classical Young diagram ensembles;
a deformation which is related to Jack polynomials and Jack characters. We show
that each such a random Young diagram converges asymptotically to some limit
shape and that the fluctuations around the limit are asymptotically Gaussian.
",0,0,1,0,0,0
3721,Medical applications of diamond magnetometry: commercial viability,"  The sensing of magnetic fields has important applications in medicine,
particularly to the sensing of signals in the heart and brain. The fields
associated with biomagnetism are exceptionally weak, being many orders of
magnitude smaller than the Earth's magnetic field. To measure them requires
that we use the most sensitive detection techniques, however, to be
commercially viable this must be done at an affordable cost. The current state
of the art uses costly SQUID magnetometers, although they will likely be
superseded by less costly, but otherwise limited, alkali vapour magnetometers.
Here, we discuss the application of diamond magnetometers to medical
applications. Diamond magnetometers are robust, solid state devices that work
in a broad range of environments, with the potential for sensitivity comparable
to the leading technologies.
",0,1,0,0,0,0
15942,Deriving Enhanced Geographical Representations via Similarity-based Spectral Analysis: Predicting Colorectal Cancer Survival Curves in Iowa,"  Neural networks are capable of learning rich, nonlinear feature
representations shown to be beneficial in many predictive tasks. In this work,
we use such models to explore different geographical feature representations in
the context of predicting colorectal cancer survival curves for patients in the
state of Iowa, spanning the years 1989 to 2013. Specifically, we compare model
performance using ""area between the curves"" (ABC) to assess (a) whether
survival curves can be reasonably predicted for colorectal cancer patients in
the state of Iowa, (b) whether geographical features improve predictive
performance, (c) whether a simple binary representation, or a richer, spectral
analysis-elicited representation perform better, and (d) whether spectral
analysis-based representations can be improved upon by leveraging
geographically-descriptive features. In exploring (d), we devise a
similarity-based spectral analysis procedure, which allows for the combination
of geographically relational and geographically descriptive features. Our
findings suggest that survival curves can be reasonably estimated on average,
with predictive performance deviating at the five-year survival mark among all
models. We also find that geographical features improve predictive performance,
and that better performance is obtained using richer, spectral
analysis-elicited features. Furthermore, we find that similarity-based spectral
analysis-elicited representations improve upon the original spectral analysis
results by approximately 40%.
",0,0,0,1,0,0
1546,Data Motif-based Proxy Benchmarks for Big Data and AI Workloads,"  For the architecture community, reasonable simulation time is a strong
requirement in addition to performance data accuracy. However, emerging big
data and AI workloads are too huge at binary size level and prohibitively
expensive to run on cycle-accurate simulators. The concept of data motif, which
is identified as a class of units of computation performed on initial or
intermediate data, is the first step towards building proxy benchmark to mimic
the real-world big data and AI workloads. However, there is no practical way to
construct a proxy benchmark based on the data motifs to help simulation-based
research. In this paper, we embark on a study to bridge the gap between data
motif and a practical proxy benchmark. We propose a data motif-based proxy
benchmark generating methodology by means of machine learning method, which
combine data motifs with different weights to mimic the big data and AI
workloads. Furthermore, we implement various data motifs using light-weight
stacks and apply the methodology to five real-world workloads to construct a
suite of proxy benchmarks, considering the data types, patterns, and
distributions. The evaluation results show that our proxy benchmarks shorten
the execution time by 100s times on real systems while maintaining the average
system and micro-architecture performance data accuracy above 90%, even
changing the input data sets or cluster configurations. Moreover, the generated
proxy benchmarks reflect consistent performance trends across different
architectures. To facilitate the community, we will release the proxy
benchmarks on the project homepage this http URL.
",1,0,0,0,0,0
18154,Caulking the Leakage Effect in MEEG Source Connectivity Analysis,"  Simplistic estimation of neural connectivity in MEEG sensor space is
impossible due to volume conduction. The only viable alternative is to carry
out connectivity estimation in source space. Among the neuroscience community
this is claimed to be impossible or misleading due to Leakage: linear mixing of
the reconstructed sources. To address this problematic we propose a novel
solution method that caulks the Leakage in MEEG source activity and
connectivity estimates: BC-VARETA. It is based on a joint estimation of source
activity and connectivity in the frequency domain representation of MEEG time
series. To achieve this, we go beyond current methods that assume a fixed
gaussian graphical model for source connectivity. In contrast we estimate this
graphical model in a Bayesian framework by placing priors on it, which allows
for highly optimized computations of the connectivity, via a new procedure
based on the local quadratic approximation under quite general prior models. A
further contribution of this paper is the rigorous definition of leakage via
the Spatial Dispersion Measure and Earth Movers Distance based on the geodesic
distances over the cortical manifold. Both measures are extended for the first
time to quantify Connectivity Leakage by defining them on the cartesian product
of cortical manifolds. Using these measures, we show that BC-VARETA outperforms
most state of the art inverse solvers by several orders of magnitude.
",0,0,0,0,1,0
15677,Topology in time-reversal symmetric crystals,"  The discovery of topological insulators has reformed modern materials
science, promising to be a platform for tabletop relativistic physics,
electronic transport without scattering, and stable quantum computation.
Topological invariants are used to label distinct types of topological
insulators. But it is not generally known how many or which invariants can
exist in any given crystalline material. Using a new and efficient counting
algorithm, we study the topological invariants that arise in time-reversal
symmetric crystals. This results in a unified picture that explains the
relations between all known topological invariants in these systems. It also
predicts new topological phases and one entirely new topological invariant. We
present explicitly the classification of all two-dimensional crystalline
fermionic materials, and give a straightforward procedure for finding the
analogous result in any three-dimensional structure. Our study represents a
single, intuitive physical picture applicable to all topological invariants in
real materials, with crystal symmetries.
",0,1,0,0,0,0
14753,Universal 3D Wearable Fingerprint Targets: Advancing Fingerprint Reader Evaluations,"  We present the design and manufacturing of high fidelity universal 3D
fingerprint targets, which can be imaged on a variety of fingerprint sensing
technologies, namely capacitive, contact-optical, and contactless-optical.
Universal 3D fingerprint targets enable, for the first time, not only a
repeatable and controlled evaluation of fingerprint readers, but also the
ability to conduct fingerprint reader interoperability studies. Fingerprint
reader interoperability refers to how robust fingerprint recognition systems
are to variations in the images acquired by different types of fingerprint
readers. To build universal 3D fingerprint targets, we adopt a molding and
casting framework consisting of (i) digital mapping of fingerprint images to a
negative mold, (ii) CAD modeling a scaffolding system to hold the negative
mold, (iii) fabricating the mold and scaffolding system with a high resolution
3D printer, (iv) producing or mixing a material with similar electrical,
optical, and mechanical properties to that of the human finger, and (v)
fabricating a 3D fingerprint target using controlled casting. Our experiments
conducted with PIV and Appendix F certified optical (contact and contactless)
and capacitive fingerprint readers demonstrate the usefulness of universal 3D
fingerprint targets for controlled and repeatable fingerprint reader
evaluations and also fingerprint reader interoperability studies.
",1,0,0,0,0,0
17643,DeepPicar: A Low-cost Deep Neural Network-based Autonomous Car,"  We present DeepPicar, a low-cost deep neural network based autonomous car
platform. DeepPicar is a small scale replication of a real self-driving car
called DAVE-2 by NVIDIA. DAVE-2 uses a deep convolutional neural network (CNN),
which takes images from a front-facing camera as input and produces car
steering angles as output. DeepPicar uses the same network architecture---9
layers, 27 million connections and 250K parameters---and can drive itself in
real-time using a web camera and a Raspberry Pi 3 quad-core platform. Using
DeepPicar, we analyze the Pi 3's computing capabilities to support end-to-end
deep learning based real-time control of autonomous vehicles. We also
systematically compare other contemporary embedded computing platforms using
the DeepPicar's CNN-based real-time control workload. We find that all tested
platforms, including the Pi 3, are capable of supporting the CNN-based
real-time control, from 20 Hz up to 100 Hz, depending on hardware platform.
However, we find that shared resource contention remains an important issue
that must be considered in applying CNN models on shared memory based embedded
computing platforms; we observe up to 11.6X execution time increase in the CNN
based control loop due to shared resource contention. To protect the CNN
workload, we also evaluate state-of-the-art cache partitioning and memory
bandwidth throttling techniques on the Pi 3. We find that cache partitioning is
ineffective, while memory bandwidth throttling is an effective solution.
",1,0,0,0,0,0
9672,"Circuit Treewidth, Sentential Decision, and Query Compilation","  The evaluation of a query over a probabilistic database boils down to
computing the probability of a suitable Boolean function, the lineage of the
query over the database. The method of query compilation approaches the task in
two stages: first, the query lineage is implemented (compiled) in a circuit
form where probability computation is tractable; and second, the desired
probability is computed over the compiled circuit. A basic theoretical quest in
query compilation is that of identifying pertinent classes of queries whose
lineages admit compact representations over increasingly succinct, tractable
circuit classes. Fostering previous work by Jha and Suciu (2012) and Petke and
Razgon (2013), we focus on queries whose lineages admit circuit implementations
with small treewidth, and investigate their compilability within tame classes
of decision diagrams. In perfect analogy with the characterization of bounded
circuit pathwidth by bounded OBDD width, we show that a class of Boolean
functions has bounded circuit treewidth if and only if it has bounded SDD
width. Sentential decision diagrams (SDDs) are central in knowledge
compilation, being essentially as tractable as OBDDs but exponentially more
succinct. By incorporating constant width SDDs and polynomial size SDDs, we
refine the panorama of query compilation for unions of conjunctive queries with
and without inequalities.
",1,0,0,0,0,0
13728,On Estimation of Conditional Modes Using Multiple Quantile Regressions,"  We propose an estimation method for the conditional mode when the
conditioning variable is high-dimensional. In the proposed method, we first
estimate the conditional density by solving quantile regressions multiple
times. We then estimate the conditional mode by finding the maximum of the
estimated conditional density. The proposed method has two advantages in that
it is computationally stable because it has no initial parameter dependencies,
and it is statistically efficient with a fast convergence rate. Synthetic and
real-world data experiments demonstrate the better performance of the proposed
method compared to other existing ones.
",0,0,1,1,0,0
6109,Online Estimation of Multiple Dynamic Graphs in Pattern Sequences,"  Many time-series data including text, movies, and biological signals can be
represented as sequences of correlated binary patterns. These patterns may be
described by weighted combinations of a few dominant structures that underpin
specific interactions among the binary elements. To extract the dominant
correlation structures and their contributions to generating data in a
time-dependent manner, we model the dynamics of binary patterns using the
state-space model of an Ising-type network that is composed of multiple
undirected graphs. We provide a sequential Bayes algorithm to estimate the
dynamics of weights on the graphs while gaining the graph structures online.
This model can uncover overlapping graphs underlying the data better than a
traditional orthogonal decomposition method, and outperforms an original
time-dependent full Ising model. We assess the performance of the method by
simulated data, and demonstrate that spontaneous activity of cultured
hippocampal neurons is represented by dynamics of multiple graphs.
",1,0,0,1,1,0
1300,Timing Solution and Single-pulse Properties for Eight Rotating Radio Transients,"  Rotating radio transients (RRATs), loosely defined as objects that are
discovered through only their single pulses, are sporadic pulsars that have a
wide range of emission properties. For many of them, we must measure their
periods and determine timing solutions relying on the timing of their
individual pulses, while some of the less sporadic RRATs can be timed by using
folding techniques as we do for other pulsars. Here, based on Parkes and Green
Bank Telescope (GBT) observations, we introduce our results on eight RRATs
including their timing-derived rotation parameters, positions, and dispersion
measures (DMs), along with a comparison of the spin-down properties of RRATs
and normal pulsars. Using data for 24 RRATs, we find that their period
derivatives are generally larger than those of normal pulsars, independent of
any intrinsic correlation with period, indicating that RRATs' highly sporadic
emission may be associated with intrinsically larger magnetic fields. We carry
out Lomb$-$Scargle tests to search for periodicities in RRATs' pulse detection
times with long timescales. Periodicities are detected for all targets, with
significant candidates of roughly 3.4 hr for PSR J1623$-$0841 and 0.7 hr for
PSR J1839$-$0141. We also analyze their single-pulse amplitude distributions,
finding that log-normal distributions provide the best fits, as is the case for
most pulsars. However, several RRATs exhibit power-law tails, as seen for
pulsars emitting giant pulses. This, along with consideration of the selection
effects against the detection of weak pulses, imply that RRAT pulses generally
represent the tail of a normal intensity distribution.
",0,1,0,0,0,0
15435,Attentive cross-modal paratope prediction,"  Antibodies are a critical part of the immune system, having the function of
directly neutralising or tagging undesirable objects (the antigens) for future
destruction. Being able to predict which amino acids belong to the paratope,
the region on the antibody which binds to the antigen, can facilitate antibody
design and contribute to the development of personalised medicine. The
suitability of deep neural networks has recently been confirmed for this task,
with Parapred outperforming all prior physical models. Our contribution is
twofold: first, we significantly outperform the computational efficiency of
Parapred by leveraging à trous convolutions and self-attention. Secondly, we
implement cross-modal attention by allowing the antibody residues to attend
over antigen residues. This leads to new state-of-the-art results on this task,
along with insightful interpretations.
",0,0,0,1,1,0
8289,Glitch Classification and Clustering for LIGO with Deep Transfer Learning,"  The detection of gravitational waves with LIGO and Virgo requires a detailed
understanding of the response of these instruments in the presence of
environmental and instrumental noise. Of particular interest is the study of
anomalous non-Gaussian noise transients known as glitches, since their high
occurrence rate in LIGO/Virgo data can obscure or even mimic true gravitational
wave signals. Therefore, successfully identifying and excising glitches is of
utmost importance to detect and characterize gravitational waves. In this
article, we present the first application of Deep Learning combined with
Transfer Learning for glitch classification, using real data from LIGO's first
discovery campaign labeled by Gravity Spy, showing that knowledge from
pre-trained models for real-world object recognition can be transferred for
classifying spectrograms of glitches. We demonstrate that this method enables
the optimal use of very deep convolutional neural networks for glitch
classification given small unbalanced training datasets, significantly reduces
the training time, and achieves state-of-the-art accuracy above 98.8%. Once
trained via transfer learning, we show that the networks can be truncated and
used as feature extractors for unsupervised clustering to automatically group
together new classes of glitches and anomalies. This novel capability is of
critical importance to identify and remove new types of glitches which will
occur as the LIGO/Virgo detectors gradually attain design sensitivity.
",1,1,0,1,0,0
19179,Strong coupling Bose polarons in a BEC,"  We use a non-perturbative renormalization group approach to develop a unified
picture of the Bose polaron problem, where a mobile impurity is strongly
interacting with a surrounding Bose-Einstein condensate (BEC). A detailed
theoretical analysis of the phase diagram is presented and the
polaron-to-molecule transition is discussed. For attractive polarons we argue
that a description in terms of an effective Fröhlich Hamiltonian with
renormalized parameters is possible. Its strong coupling regime is realized
close to a Feshbach resonance, where we predict a sharp increase of the
effective mass. Already for weaker interactions, before the polaron mass
diverges, we predict a transition to a regime where states exist below the
polaron energy and the attractive polaron is no longer the ground state. On the
repulsive side of the Feshbach resonance we recover the repulsive polaron,
which has a finite lifetime because it can decay into low-lying molecular
states. We show for the entire range of couplings that the polaron energy has
logarithmic corrections in comparison with predictions by the mean-field
approach. We demonstrate that they are a consequence of the polaronic mass
renormalization which is due to quantum fluctuations of correlated phonons in
the polaron cloud.
",0,1,0,0,0,0
18042,Characterizations of idempotent discrete uninorms,"  In this paper we provide an axiomatic characterization of the idempotent
discrete uninorms by means of three conditions only: conservativeness,
symmetry, and nondecreasing monotonicity. We also provide an alternative
characterization involving the bisymmetry property. Finally, we provide a
graphical characterization of these operations in terms of their contour plots,
and we mention a few open questions for further research.
",1,0,1,0,0,0
1935,Finite Sample Differentially Private Confidence Intervals,"  We study the problem of estimating finite sample confidence intervals of the
mean of a normal population under the constraint of differential privacy. We
consider both the known and unknown variance cases and construct differentially
private algorithms to estimate confidence intervals. Crucially, our algorithms
guarantee a finite sample coverage, as opposed to an asymptotic coverage.
Unlike most previous differentially private algorithms, we do not require the
domain of the samples to be bounded. We also prove lower bounds on the expected
size of any differentially private confidence set showing that our the
parameters are optimal up to polylogarithmic factors.
",1,0,1,1,0,0
19527,Robust Counterfactual Inferences using Feature Learning and their Applications,"  In a wide variety of applications, including personalization, we want to
measure the difference in outcome due to an intervention and thus have to deal
with counterfactual inference. The feedback from a customer in any of these
situations is only 'bandit feedback' - that is, a partial feedback based on
whether we chose to intervene or not. Typically randomized experiments are
carried out to understand whether an intervention is overall better than no
intervention. Here we present a feature learning algorithm to learn from a
randomized experiment where the intervention in consideration is most effective
and where it is least effective rather than only focusing on the overall
impact, thus adding a context to our learning mechanism and extract more
information. From the randomized experiment, we learn the feature
representations which divide the population into subpopulations where we
observe statistically significant difference in average customer feedback
between those who were subjected to the intervention and those who were not,
with a level of significance l, where l is a configurable parameter in our
model. We use this information to derive the value of the intervention in
consideration for each instance in the population. With experiments, we show
that using this additional learning, in future interventions, the context for
each instance could be leveraged to decide whether to intervene or not.
",0,0,0,1,0,0
10157,Accelerating equilibrium isotope effect calculations: I. Stochastic thermodynamic integration with respect to mass,"  Accurate path integral Monte Carlo or molecular dynamics calculations of
isotope effects have until recently been expensive because of the necessity to
reduce three types of errors present in such calculations: statistical errors
due to sampling, path integral discretization errors, and thermodynamic
integration errors. While the statistical errors can be reduced with virial
estimators and path integral discretization errors with high-order
factorization of the Boltzmann operator, here we propose a method for
accelerating isotope effect calculations by eliminating the integration error.
We show that the integration error can be removed entirely by changing particle
masses stochastically during the calculation and by using a piecewise linear
umbrella biasing potential. Moreover, we demonstrate numerically that this
approach does not increase the statistical error. The resulting acceleration of
isotope effect calculations is demonstrated on a model harmonic system and on
deuterated species of methane.
",0,1,0,0,0,0
2673,BARCHAN: Blob Alignment for Robust CHromatographic ANalysis,"  Comprehensive Two dimensional gas chromatography (GCxGC) plays a central role
into the elucidation of complex samples. The automation of the identification
of peak areas is of prime interest to obtain a fast and repeatable analysis of
chromatograms. To determine the concentration of compounds or pseudo-compounds,
templates of blobs are defined and superimposed on a reference chromatogram.
The templates then need to be modified when different chromatograms are
recorded. In this study, we present a chromatogram and template alignment
method based on peak registration called BARCHAN. Peaks are identified using a
robust mathematical morphology tool. The alignment is performed by a
probabilistic estimation of a rigid transformation along the first dimension,
and a non-rigid transformation in the second dimension, taking into account
noise, outliers and missing peaks in a fully automated way. Resulting aligned
chromatograms and masks are presented on two datasets. The proposed algorithm
proves to be fast and reliable. It significantly reduces the time to results
for GCxGC analysis.
",1,1,0,0,0,0
1411,A Distributed Online Pricing Strategy for Demand Response Programs,"  We study a demand response problem from utility (also referred to as
operator)'s perspective with realistic settings, in which the utility faces
uncertainty and limited communication. Specifically, the utility does not know
the cost function of consumers and cannot have multiple rounds of information
exchange with consumers. We formulate an optimization problem for the utility
to minimize its operational cost considering time-varying demand response
targets and responses of consumers. We develop a joint online learning and
pricing algorithm. In each time slot, the utility sends out a price signal to
all consumers and estimates the cost functions of consumers based on their
noisy responses. We measure the performance of our algorithm using regret
analysis and show that our online algorithm achieves logarithmic regret with
respect to the operating horizon. In addition, our algorithm employs linear
regression to estimate the aggregate response of consumers, making it easy to
implement in practice. Simulation experiments validate the theoretic results
and show that the performance gap between our algorithm and the offline
optimality decays quickly.
",1,0,1,0,0,0
8097,How to avoid the curse of dimensionality: scalability of particle filters with and without importance weights,"  Particle filters are a popular and flexible class of numerical algorithms to
solve a large class of nonlinear filtering problems. However, standard particle
filters with importance weights have been shown to require a sample size that
increases exponentially with the dimension D of the state space in order to
achieve a certain performance, which precludes their use in very
high-dimensional filtering problems. Here, we focus on the dynamic aspect of
this curse of dimensionality (COD) in continuous time filtering, which is
caused by the degeneracy of importance weights over time. We show that the
degeneracy occurs on a time-scale that decreases with increasing D. In order to
soften the effects of weight degeneracy, most particle filters use particle
resampling and improved proposal functions for the particle motion. We explain
why neither of the two can prevent the COD in general. In order to address this
fundamental problem, we investigate an existing filtering algorithm based on
optimal feedback control that sidesteps the use of importance weights. We use
numerical experiments to show that this Feedback Particle Filter (FPF) by Yang
et al. (2013) does not exhibit a COD.
",0,0,1,1,0,0
11939,Probabilistic Active Learning of Functions in Structural Causal Models,"  We consider the problem of learning the functions computing children from
parents in a Structural Causal Model once the underlying causal graph has been
identified. This is in some sense the second step after causal discovery.
Taking a probabilistic approach to estimating these functions, we derive a
natural myopic active learning scheme that identifies the intervention which is
optimally informative about all of the unknown functions jointly, given
previously observed data. We test the derived algorithms on simple examples, to
demonstrate that they produce a structured exploration policy that
significantly improves on unstructured base-lines.
",1,0,0,1,0,0
13841,On polynomially integrable convex bodies,"  An infinitely smooth convex body in $\mathbb R^n$ is called polynomially
integrable of degree $N$ if its parallel section functions are polynomials of
degree $N$. We prove that the only smooth convex bodies with this property in
odd dimensions are ellipsoids, if $N\ge n-1$. This is in contrast with the case
of even dimensions and the case of odd dimensions with $N<n-1$, where such
bodies do not exist, as it was recently shown by Agranovsky.
",0,0,1,0,0,0
16635,CT Image Reconstruction in a Low Dimensional Manifold,"  Regularization methods are commonly used in X-ray CT image reconstruction.
Different regularization methods reflect the characterization of different
prior knowledge of images. In a recent work, a new regularization method called
a low-dimensional manifold model (LDMM) is investigated to characterize the
low-dimensional patch manifold structure of natural images, where the manifold
dimensionality characterizes structural information of an image. In this paper,
we propose a CT image reconstruction method based on the prior knowledge of the
low-dimensional manifold of CT image. Using the clinical raw projection data
from GE clinic, we conduct comparisons for the CT image reconstruction among
the proposed method, the simultaneous algebraic reconstruction technique (SART)
with the total variation (TV) regularization, and the filtered back projection
(FBP) method. Results show that the proposed method can successfully recover
structural details of an imaging object, and achieve higher spatial and
contrast resolution of the reconstructed image than counterparts of FBP and
SART with TV.
",1,1,0,0,0,0
6805,A fast reconstruction algorithm for geometric inverse problems using topological sensitivity analysis and Dirichlet-Neumann cost functional approach,"  This paper is concerned with the detection of objects immersed in anisotropic
media from boundary measurements. We propose an accurate approach based on the
Kohn-Vogelius formulation and the topological sensitivity analysis method. The
inverse problem is formulated as a topology optimization one minimizing an
energy like functional. A topological asymptotic expansion is derived for the
anisotropic Laplace operator. The unknown object is reconstructed using a
level-set curve of the topological gradient. The efficiency and accuracy of the
proposed algorithm are illustrated by some numerical results. MOTS-CLÉS :
Problème inverse géométrique, Laplace anisotrope, formulation de
Kohn-Vogelius, analyse de sensibilité, optimisation topologique.
",0,0,1,0,0,0
14632,Local Density Approximation for Almost-Bosonic Anyons,"  We discuss the average-field approximation for a trapped gas of
non-interacting anyons in the quasi-bosonic regime. In the homogeneous case,
i.e., for a confinement to a bounded region, we prove that the energy in the
regime of large statistics parameter, i.e., for ""less-bosonic"" anyons, is
independent of boundary conditions and of the shape of the domain. When a
non-trivial trapping potential is present, we derive a local density
approximation in terms of a Thomas-Fermi-like model.
",0,1,1,0,0,0
8394,Obtaining Accurate Probabilistic Causal Inference by Post-Processing Calibration,"  Discovery of an accurate causal Bayesian network structure from observational
data can be useful in many areas of science. Often the discoveries are made
under uncertainty, which can be expressed as probabilities. To guide the use of
such discoveries, including directing further investigation, it is important
that those probabilities be well-calibrated. In this paper, we introduce a
novel framework to derive calibrated probabilities of causal relationships from
observational data. The framework consists of three components: (1) an
approximate method for generating initial probability estimates of the edge
types for each pair of variables, (2) the availability of a relatively small
number of the causal relationships in the network for which the truth status is
known, which we call a calibration training set, and (3) a calibration method
for using the approximate probability estimates and the calibration training
set to generate calibrated probabilities for the many remaining pairs of
variables. We also introduce a new calibration method based on a shallow neural
network. Our experiments on simulated data support that the proposed approach
improves the calibration of causal edge predictions. The results also support
that the approach often improves the precision and recall of predictions.
",1,0,0,1,0,0
3485,Cooperative Online Learning: Keeping your Neighbors Updated,"  We study an asynchronous online learning setting with a network of agents. At
each time step, some of the agents are activated, requested to make a
prediction, and pay the corresponding loss. The loss function is then revealed
to these agents and also to their neighbors in the network. When activations
are stochastic, we show that the regret achieved by $N$ agents running the
standard online Mirror Descent is $O(\sqrt{\alpha T})$, where $T$ is the
horizon and $\alpha \le N$ is the independence number of the network. This is
in contrast to the regret $\Omega(\sqrt{N T})$ which $N$ agents incur in the
same setting when feedback is not shared. We also show a matching lower bound
of order $\sqrt{\alpha T}$ that holds for any given network. When the pattern
of agent activations is arbitrary, the problem changes significantly: we prove
a $\Omega(T)$ lower bound on the regret that holds for any online algorithm
oblivious to the feedback source.
",1,0,0,1,0,0
14943,Stable Architectures for Deep Neural Networks,"  Deep neural networks have become invaluable tools for supervised machine
learning, e.g., classification of text or images. While often offering superior
results over traditional techniques and successfully expressing complicated
patterns in data, deep architectures are known to be challenging to design and
train such that they generalize well to new data. Important issues with deep
architectures are numerical instabilities in derivative-based learning
algorithms commonly called exploding or vanishing gradients. In this paper we
propose new forward propagation techniques inspired by systems of Ordinary
Differential Equations (ODE) that overcome this challenge and lead to
well-posed learning problems for arbitrarily deep networks.
The backbone of our approach is our interpretation of deep learning as a
parameter estimation problem of nonlinear dynamical systems. Given this
formulation, we analyze stability and well-posedness of deep learning and use
this new understanding to develop new network architectures. We relate the
exploding and vanishing gradient phenomenon to the stability of the discrete
ODE and present several strategies for stabilizing deep learning for very deep
networks. While our new architectures restrict the solution space, several
numerical experiments show their competitiveness with state-of-the-art
networks.
",1,0,1,0,0,0
6019,Convergence and submeasures in Boolean algebras,"  A Boolean algebra carries a strictly positive exhaustive submeasure if and
only if it has a sequential topology that is uniformly Frechet.
",0,0,1,0,0,0
10877,The SCUBA-2 Ambitious Sky Survey: a catalogue of beam-sized sources in the Galactic longitude range 120 to 140,"  The SCUBA-2 Ambitious Sky Survey (SASSy) is composed of shallow 850-$\umu$m
imaging using the Sub-millimetre Common-User Bolometer Array 2 (SCUBA-2) on the
James Clerk Maxwell Telescope. Here we describe the extraction of a catalogue
of beam-sized sources from a roughly $120\,{\rm deg}^2$ region of the Galactic
plane mapped uniformly (to an rms level of about 40\,mJy), covering longitude
120\degr\,$<$\,\textit{l}\,$<$\,140\degr\ and latitude
$\abs{\textit{b}}$\,$<$\,2.9\degr. We used a matched-filtering approach to
increase the signal-to-noise (S/N) ratio in these noisy maps and tested the
efficiency of our extraction procedure through estimates of the false discovery
rate, as well as by adding artificial sources to the real images. The primary
catalogue contains a total of 189 sources at 850\,$\umu$m, down to a S/N
threshold of approximately 4.6. Additionally, we list 136 sources detected down
to ${\rm S/N}=4.3$, but recognise that as we go lower in S/N, the reliability
of the catalogue rapidly diminishes. We perform follow-up observations of some
of our lower significance sources through small targeted SCUBA-2 images, and
list 265 sources detected in these maps down to ${\rm S/N}=5$. This illustrates
the real power of SASSy: inspecting the shallow maps for regions of 850-$\umu$m
emission and then using deeper targeted images to efficiently find fainter
sources. We also perform a comparison of the SASSy sources with the Planck
Catalogue of Compact Sources and the \textit{IRAS} Point Source Catalogue, to
determine which sources discovered in this field might be new, and hence
potentially cold regions at an early stage of star formation.
",0,1,0,0,0,0
14497,Deep Learning in Customer Churn Prediction: Unsupervised Feature Learning on Abstract Company Independent Feature Vectors,"  As companies increase their efforts in retaining customers, being able to
predict accurately ahead of time, whether a customer will churn in the
foreseeable future is an extremely powerful tool for any marketing team. The
paper describes in depth the application of Deep Learning in the problem of
churn prediction. Using abstract feature vectors, that can generated on any
subscription based company's user event logs, the paper proves that through the
use of the intrinsic property of Deep Neural Networks (learning secondary
features in an unsupervised manner), the complete pipeline can be applied to
any subscription based company with extremely good churn predictive
performance. Furthermore the research documented in the paper was performed for
Framed Data (a company that sells churn prediction as a service for other
companies) in conjunction with the Data Science Institute at Lancaster
University, UK. This paper is the intellectual property of Framed Data.
",1,0,0,1,0,0
17447,Managing the Public to Manage Data: Citizen Science and Astronomy,"  Citizen science projects recruit members of the public as volunteers to
process and produce datasets. These datasets must win the trust of the
scientific community. The task of securing credibility involves, in part,
applying standard scientific procedures to clean these datasets. However,
effective management of volunteer behavior also makes a significant
contribution to enhancing data quality. Through a case study of Galaxy Zoo, a
citizen science project set up to generate datasets based on volunteer
classifications of galaxy morphologies, this paper explores how those involved
in running the project manage volunteers. The paper focuses on how methods for
crediting volunteer contributions motivate volunteers to provide higher quality
contributions and to behave in a way that better corresponds to statistical
assumptions made when combining volunteer contributions into datasets. These
methods have made a significant contribution to the success of the project in
securing trust in these datasets, which have been well used by other
scientists. Implications for practice are then presented for citizen science
projects, providing a list of considerations to guide choices regarding how to
credit volunteer contributions to improve the quality and trustworthiness of
citizen science-produced datasets.
",1,1,0,0,0,0
2939,"Data-Efficient Multirobot, Multitask Transfer Learning for Trajectory Tracking","  Transfer learning has the potential to reduce the burden of data collection
and to decrease the unavoidable risks of the training phase. In this letter, we
introduce a multirobot, multitask transfer learning framework that allows a
system to complete a task by learning from a few demonstrations of another task
executed on another system. We focus on the trajectory tracking problem where
each trajectory represents a different task, since many robotic tasks can be
described as a trajectory tracking problem. The proposed multirobot transfer
learning framework is based on a combined $\mathcal{L}_1$ adaptive control and
an iterative learning control approach. The key idea is that the adaptive
controller forces dynamically different systems to behave as a specified
reference model. The proposed multitask transfer learning framework uses
theoretical control results (e.g., the concept of vector relative degree) to
learn a map from desired trajectories to the inputs that make the system track
these trajectories with high accuracy. This map is used to calculate the inputs
for a new, unseen trajectory. Experimental results using two different
quadrotor platforms and six different trajectories show that, on average, the
proposed framework reduces the first-iteration tracking error by 74% when
information from tracking a different single trajectory on a different
quadrotor is utilized.
",1,0,0,0,0,0
1782,A Verified Algorithm Enumerating Event Structures,"  An event structure is a mathematical abstraction modeling concepts as
causality, conflict and concurrency between events. While many other
mathematical structures, including groups, topological spaces, rings, abound
with algorithms and formulas to generate, enumerate and count particular sets
of their members, no algorithm or formulas are known to generate or count all
the possible event structures over a finite set of events. We present an
algorithm to generate such a family, along with a functional implementation
verified using Isabelle/HOL. As byproducts, we obtain a verified enumeration of
all possible preorders and partial orders. While the integer sequences counting
preorders and partial orders are already listed on OEIS (On-line Encyclopedia
of Integer Sequences), the one counting event structures is not. We therefore
used our algorithm to submit a formally verified addition, which has been
successfully reviewed and is now part of the OEIS.
",1,0,0,0,0,0
4493,Dzyaloshinskii Moriya interaction across antiferromagnet / ferromagnet interface,"  The antiferromagnet (AFM) / ferromagnet (FM) interfaces are of central
importance in recently developed pure electric or ultrafast control of FM
spins, where the underlying mechanisms remain unresolved. Here we report the
direct observation of Dzyaloshinskii Moriya interaction (DMI) across the AFM/FM
interface of IrMn/CoFeB thin films. The interfacial DMI is quantitatively
measured from the asymmetric spin wave dispersion in the FM layer using
Brillouin light scattering. The DMI strength is enhanced by a factor of 7 with
increasing IrMn layer thickness in the range of 1- 7.5 nm. Our findings provide
deeper insight into the coupling at AFM/FM interface and may stimulate new
device concepts utilizing chiral spin textures such as magnetic skyrmions in
AFM/FM heterostructures.
",0,1,0,0,0,0
8408,New ALMA constraints on the star-forming ISM at low metallicity: A 50 pc view of the blue compact dwarf galaxy SBS0335-052,"  Properties of the cold interstellar medium of low-metallicity galaxies are
not well-known due to the faintness and extremely small scale on which emission
is expected. We present deep ALMA band 6 (230GHz) observations of the nearby,
low-metallicity (12 + log(O/H) = 7.25) blue compact dwarf galaxy SBS0335-052 at
an unprecedented resolution of 0.2 arcsec (52 pc). The 12CO J=2-1 line is not
detected and we report a 3-sigma upper limit of LCO(2-1) = 3.6x10^4 K km/s
pc^2. Assuming that molecular gas is converted into stars with a given
depletion time, ranging from 0.02 to 2 Gyr, we find lower limits on the
CO-to-H2 conversion factor alpha_CO in the range 10^2-10^4 Msun pc^-2 (K
km/s)^-1. The continuum emission is detected and resolved over the two main
super star clusters. Re-analysis of the IR-radio spectral energy distribution
suggests that the mm-fluxes are not only free-free emission but are most likely
also associated with a cold dust component coincident with the position of the
brightest cluster. With standard dust properties, we estimate its mass to be as
large as 10^5 Msun. Both line and continuum results suggest the presence of a
large cold gas reservoir unseen in CO even with ALMA.
",0,1,0,0,0,0
1126,Nondestructive testing of grating imperfections using grating-based X-ray phase-contrast imaging,"  We reported the usage of grating-based X-ray phase-contrast imaging in
nondestructive testing of grating imperfections. It was found that
electroplating flaws could be easily detected by conventional absorption
signal, and in particular, we observed that the grating defects resulting from
uneven ultraviolet exposure could be clearly discriminated with phase-contrast
signal. The experimental results demonstrate that grating-based X-ray
phase-contrast imaging, with a conventional low-brilliance X-ray source, a
large field of view and a reasonable compact setup, which simultaneously yields
phase- and attenuation-contrast signal of the sample, can be ready-to-use in
fast nondestructive testing of various imperfections in gratings and other
similar photoetching products.
",0,1,0,0,0,0
17155,The weak rate of convergence for the Euler-Maruyama approximation of one-dimensional stochastic differential equations involving the local times of the unknown process,"  In this paper, we consider the weak convergence of the Euler-Maruyama
approximation for one dimensional stochastic differential equations involving
the local times of the unknown process. We use a transformation in order to
remove the local time from the stochastic differential equations and we provide
the approximation of Euler-maruyama for the stochastic differential equations
without local time. After that, we conclude the approximation of Euler-maruyama
for one dimensional stochastic differential equations involving the local times
of the unknown process , and we provide the rate of weak convergence for any
function G in a certain class.
",0,0,1,0,0,0
14705,The right tool for the right question --- beyond the encoding versus decoding dichotomy,"  There are two major questions that neuroimaging studies attempt to answer:
First, how are sensory stimuli represented in the brain (which we term the
stimulus-based setting)? And, second, how does the brain generate cognition
(termed the response-based setting)? There has been a lively debate in the
neuroimaging community whether encoding and decoding models can provide
insights into these questions. In this commentary, we construct two simple and
analytically tractable examples to demonstrate that while an encoding model
analysis helps with the former, neither model is appropriate to satisfactorily
answer the latter question. Consequently, we argue that if we want to
understand how the brain generates cognition, we need to move beyond the
encoding versus decoding dichotomy and instead discuss and develop tools that
are specifically tailored to our endeavour.
",0,0,0,1,0,0
127,Emittance preservation of an electron beam in a loaded quasi-linear plasma wakefield,"  We investigate beam loading and emittance preservation for a high-charge
electron beam being accelerated in quasi-linear plasma wakefields driven by a
short proton beam. The structure of the studied wakefields are similar to those
of a long, modulated proton beam, such as the AWAKE proton driver. We show that
by properly choosing the electron beam parameters and exploiting two well known
effects, beam loading of the wakefield and full blow out of plasma electrons by
the accelerated beam, the electron beam can gain large amounts of energy with a
narrow final energy spread (%-level) and without significant emittance growth.
",0,1,0,0,0,0
11801,A Conic Integer Programming Approach to Constrained Assortment Optimization under the Mixed Multinomial Logit Model,"  We consider the constrained assortment optimization problem under the mixed
multinomial logit model. Even moderately sized instances of this problem are
challenging to solve directly using standard mixed-integer linear optimization
formulations. This has motivated recent research exploring customized
optimization strategies and approximation techniques. In contrast, we develop a
novel conic quadratic mixed-integer formulation. This new formulation, together
with McCormick inequalities exploiting the capacity constraints, enables the
solution of large instances using commercial optimization software.
",0,0,1,0,0,0
10559,A note on the Diophantine equation $2^{n-1}(2^{n}-1)=x^3+y^3+z^3$,"  Motivated by the recent result of Farhi we show that for each $n\equiv \pm
1\pmod{6}$ the title Diophantine equation has at least two solutions in
integers. As a consequence, we get that each (even) perfect number is a sum of
three cubes of integers. Moreover, we present some computational results
concerning the considered equation and state some questions and conjectures.
",0,0,1,0,0,0
3806,Variational obstacle avoidance problem on Riemannian manifolds,"  We introduce variational obstacle avoidance problems on Riemannian manifolds
and derive necessary conditions for the existence of their normal extremals.
The problem consists of minimizing an energy functional depending on the
velocity and covariant acceleration, among a set of admissible curves, and also
depending on a navigation function used to avoid an obstacle on the workspace,
a Riemannian manifold.
We study two different scenarios, a general one on a Riemannian manifold and,
a sub-Riemannian problem. By introducing a left-invariant metric on a Lie
group, we also study the variational obstacle avoidance problem on a Lie group.
We apply the results to the obstacle avoidance problem of a planar rigid body
and an unicycle.
",1,0,1,0,0,0
5071,Degenerate and chiral states in the extended Heisenberg model in the kagome lattice,"  We present a study of the low temperature phases of the antiferromagnetic
extended classical Heisenberg model in the kagome lattice, up to third nearest
neighbors. First, we focus on the degenerate lines in the boundaries of the
well-known staggered chiral phases. These boundaries have either semi-extensive
or extensive degeneracy, and we discuss the partial selection of states by
thermal fluctuations. Then, we study the model under an external magnetic field
on these lines and in the staggered chiral phases. We pay particular attention
to the highly frustrated point, where the three exchange couplings are equal.
We show that this point can me mapped to a model with spin liquid behavior and
non-zero chirality. Finally, we explore the effect of Dzyaloshinskii-Moriya
(DM) interactions in two ways: an homogeneous and a staggered DM interaction.
In both cases, there is a rich low temperature phase diagram, with different
spontaneously broken symmetries and non trivial chiral phases.
",0,1,0,0,0,0
18522,Mean conservation for density estimation via diffusion using the finite element method,"  We propose boundary conditions for the diffusion equation that maintain the
initial mean and the total mass of a discrete data sample in the density
estimation process. A complete study of this framework with numerical
experiments using the finite element method is presented for the one
dimensional diffusion equation, some possible applications of this results are
presented as well. We also comment on a similar methodology for the
two-dimensional diffusion equation for future applications in two-dimensional
domains.
",0,0,1,0,0,0
12240,On the post-Keplerian corrections to the orbital periods of a two-body system and their application to the Galactic Center,"  Detailed numerical analyses of the orbital motion of a test particle around a
spinning primary are performed. They aim to investigate the possibility of
using the post-Keplerian (pK) corrections to the orbiter's periods (draconitic,
anomalistic and sidereal) as a further opportunity to perform new tests of
post-Newtonian (pN) gravity. As a specific scenario, the S-stars orbiting the
Massive Black Hole (MBH) supposedly lurking in Sgr A$^\ast$ at the center of
the Galaxy is adopted. We, first, study the effects of the pK Schwarzchild,
Lense-Thirring and quadrupole moment accelerations experienced by a target star
for various possible initial orbital configurations. It turns out that the
results of the numerical simulations are consistent with the analytical ones in
the small eccentricity approximation for which almost all the latter ones were
derived. For highly elliptical orbits, the size of all the three pK corrections
considered turn out to increase remarkably. The periods of the observed S2 and
S0-102 stars as functions of the MBH's spin axis orientation are considered as
well. The pK accelerations considered lead to corrections of the orbital
periods of the order of 1-100d (Schwarzschild), 0.1-10h (Lense-Thirring) and
1-10^3s (quadrupole) for a target star with a=300-800~AU and e ~ 0.8, which
could be possibly measurable by the future facilities.
",0,1,0,0,0,0
17601,Ordinary differential equations in algebras of generalized functions,"  A local existence and uniqueness theorem for ODEs in the special algebra of
generalized functions is established, as well as versions including parameters
and dependence on initial values in the generalized sense. Finally, a Frobenius
theorem is proved. In all these results, composition of generalized functions
is based on the notion of c-boundedness.
",0,0,1,0,0,0
2174,Finding Archetypal Spaces for Data Using Neural Networks,"  Archetypal analysis is a type of factor analysis where data is fit by a
convex polytope whose corners are ""archetypes"" of the data, with the data
represented as a convex combination of these archetypal points. While
archetypal analysis has been used on biological data, it has not achieved
widespread adoption because most data are not well fit by a convex polytope in
either the ambient space or after standard data transformations. We propose a
new approach to archetypal analysis. Instead of fitting a convex polytope
directly on data or after a specific data transformation, we train a neural
network (AAnet) to learn a transformation under which the data can best fit
into a polytope. We validate this approach on synthetic data where we add
nonlinearity. Here, AAnet is the only method that correctly identifies the
archetypes. We also demonstrate AAnet on two biological datasets. In a T cell
dataset measured with single cell RNA-sequencing, AAnet identifies several
archetypal states corresponding to naive, memory, and cytotoxic T cells. In a
dataset of gut microbiome profiles, AAnet recovers both previously described
microbiome states and identifies novel extrema in the data. Finally, we show
that AAnet has generative properties allowing us to uniformly sample from the
data geometry even when the input data is not uniformly distributed.
",1,0,0,1,0,0
11949,Differentially Private Query Learning: from Data Publishing to Model Publishing,"  With the development of Big Data and cloud data sharing, privacy preserving
data publishing becomes one of the most important topics in the past decade. As
one of the most influential privacy definitions, differential privacy provides
a rigorous and provable privacy guarantee for data publishing. Differentially
private interactive publishing achieves good performance in many applications;
however, the curator has to release a large number of queries in a batch or a
synthetic dataset in the Big Data era. To provide accurate non-interactive
publishing results in the constraint of differential privacy, two challenges
need to be tackled: one is how to decrease the correlation between large sets
of queries, while the other is how to predict on fresh queries. Neither is easy
to solve by the traditional differential privacy mechanism. This paper
transfers the data publishing problem to a machine learning problem, in which
queries are considered as training samples and a prediction model will be
released rather than query results or synthetic datasets. When the model is
published, it can be used to answer current submitted queries and predict
results for fresh queries from the public. Compared with the traditional
method, the proposed prediction model enhances the accuracy of query results
for non-interactive publishing. Experimental results show that the proposed
solution outperforms traditional differential privacy in terms of Mean Absolute
Value on a large group of queries. This also suggests the learning model can
successfully retain the utility of published queries while preserving privacy.
",1,0,0,0,0,0
4349,Unsupervised learning of object landmarks by factorized spatial embeddings,"  Learning automatically the structure of object categories remains an
important open problem in computer vision. In this paper, we propose a novel
unsupervised approach that can discover and learn landmarks in object
categories, thus characterizing their structure. Our approach is based on
factorizing image deformations, as induced by a viewpoint change or an object
deformation, by learning a deep neural network that detects landmarks
consistently with such visual effects. Furthermore, we show that the learned
landmarks establish meaningful correspondences between different object
instances in a category without having to impose this requirement explicitly.
We assess the method qualitatively on a variety of object types, natural and
man-made. We also show that our unsupervised landmarks are highly predictive of
manually-annotated landmarks in face benchmark datasets, and can be used to
regress these with a high degree of accuracy.
",1,0,0,1,0,0
14572,Joint Syntacto-Discourse Parsing and the Syntacto-Discourse Treebank,"  Discourse parsing has long been treated as a stand-alone problem independent
from constituency or dependency parsing. Most attempts at this problem are
pipelined rather than end-to-end, sophisticated, and not self-contained: they
assume gold-standard text segmentations (Elementary Discourse Units), and use
external parsers for syntactic features. In this paper we propose the first
end-to-end discourse parser that jointly parses in both syntax and discourse
levels, as well as the first syntacto-discourse treebank by integrating the
Penn Treebank with the RST Treebank. Built upon our recent span-based
constituency parser, this joint syntacto-discourse parser requires no
preprocessing whatsoever (such as segmentation or feature extraction), achieves
the state-of-the-art end-to-end discourse parsing accuracy.
",1,0,0,0,0,0
2301,Hermitian-Yang-Mills connections on collapsing elliptically fibered $K3$ surfaces,"  Let $X\rightarrow {\mathbb P}^1$ be an elliptically fibered $K3$ surface with
a section, admitting a sequence of Ricci-flat metrics collapsing the fibers.
Let $\mathcal E$ be a generic, holomoprhic $SU(n)$ bundle over $X$ such that
the restriction of $\mathcal E$ to each fiber is semi-stable. Given a sequence
$\Xi_i$ of Hermitian-Yang-Mills connections on $\mathcal E$ corresponding to
this degeneration, we prove that, if $E$ is a given fiber away from a finite
set, the restricted sequence $\Xi_i|_{E}$ converges to a flat connection
uniquely determined by the holomorphic structure on $\mathcal E$.
",0,0,1,0,0,0
15985,A tale of seven narrow spikes and a long trough: constraining the timing of the percolation of HII bubbles at the tail-end of reionization with ULAS J1120+0641,"  High-signal to noise observations of the Ly$\alpha$ forest transmissivity in
the z = 7.085 QSO ULAS J1120+0641 show seven narrow transmission spikes
followed by a long 240 cMpc/h trough. Here we use radiative transfer
simulations of cosmic reionization previously calibrated to match a wider range
of Ly$\alpha$ forest data to show that the occurrence of seven transmission
spikes in the narrow redshift range z = 5.85 - 6.1 is very sensitive to the
exact timing of reionization. Occurrence of the spikes requires the most under
dense regions of the IGM to be already fully ionised. The rapid onset of a long
trough at z = 6.12 requires a strong decrease of the photo-ionisation rate at
z$\sim$6.1 in this line-of-sight, consistent with the end of percolation at
this redshift. The narrow range of reionisation histories that we previously
found to be consistent with a wider range of Ly$\alpha$ forest data have a
reasonable probability of showing seven spikes and the mock absorption spectra
provide an excellent match to the spikes and the trough in the observed
spectrum of ULAS J1120+0641. Despite the large overall opacity of Ly$\alpha$ at
z > 5.8, larger samples of high signal-to-noise observations of rare
transmission spikes should therefore provide important further insights into
the exact timing of the percolation of HII bubbles at the tail-end of
reionization
",0,1,0,0,0,0
20461,Horcrux: A Password Manager for Paranoids,"  Vulnerabilities in password managers are unremitting because current designs
provide large attack surfaces, both at the client and server. We describe and
evaluate Horcrux, a password manager that is designed holistically to minimize
and decentralize trust, while retaining the usability of a traditional password
manager. The prototype Horcrux client, implemented as a Firefox add-on, is
split into two components, with code that has access to the user's master's
password and any key material isolated into a small auditable component,
separate from the complexity of managing the user interface. Instead of
exposing actual credentials to the DOM, a dummy username and password are
autofilled by the untrusted component. The trusted component intercepts and
modifies POST requests before they are encrypted and sent over the network. To
avoid trusting a centralized store, stored credentials are secret-shared over
multiple servers. To provide domain and username privacy, while maintaining
resilience to off-line attacks on a compromised password store, we incorporate
cuckoo hashing in a way that ensures an attacker cannot determine if a guessed
master password is correct. Our approach only works for websites that do not
manipulate entered credentials in the browser client, so we conducted a
large-scale experiment that found the technique appears to be compatible with
over 98% of tested login forms.
",1,0,0,0,0,0
9538,Towards Smart Proof Search for Isabelle,"  Despite the recent progress in automatic theorem provers, proof engineers are
still suffering from the lack of powerful proof automation. In this position
paper we first report our proof strategy language based on a meta-tool
approach. Then, we propose an AI-based approach to drastically improve proof
automation for Isabelle, while identifying three major challenges we plan to
address for this objective.
",1,0,0,0,0,0
16391,Fourier analysis of serial dependence measures,"  Classical spectral analysis is based on the discrete Fourier transform of the
auto-covariances. In this paper we investigate the asymptotic properties of new
frequency domain methods where the auto-covariances in the spectral density are
replaced by alternative dependence measures which can be estimated by
U-statistics. An interesting example is given by Kendall{'}s $\tau$ , for which
the limiting variance exhibits a surprising behavior.
",0,0,1,1,0,0
15610,Sparse Gaussian ICA,"  Independent component analysis (ICA) is a cornerstone of modern data
analysis. Its goal is to recover a latent random vector S with independent
components from samples of X=AS where A is an unknown mixing matrix.
Critically, all existing methods for ICA rely on and exploit strongly the
assumption that S is not Gaussian as otherwise A becomes unidentifiable. In
this paper, we show that in fact one can handle the case of Gaussian components
by imposing structure on the matrix A. Specifically, we assume that A is sparse
and generic in the sense that it is generated from a sparse Bernoulli-Gaussian
ensemble. Under this condition, we give an efficient algorithm to recover the
columns of A given only the covariance matrix of X as input even when S has
several Gaussian components.
",0,0,0,1,0,0
5240,The existence of positive least energy solutions for a class of Schrodinger-Poisson systems involving critical nonlocal term with general nonlinearity,"  The present study is concerned with the following Schrödinger-Poisson
system involving critical nonlocal term with general nonlinearity: $$ \left\{
\begin{array}{ll} -\Delta u+V(x)u- \phi |u|^3u= f(u), & x\in\mathbb{R}^3,
-\Delta \phi= |u|^5, & x\in\mathbb{R}^3,\\ \end{array} \right. $$ Under certain
assumptions on non-constant $V(x)$, the existence of a positive least energy
solution is obtained by using some new analytical skills and Pohožaev type
manifold. In particular, the Ambrosetti-Rabinowitz type condition or
monotonicity assumption on the nonlinearity is not necessary.
",0,0,1,0,0,0
7399,Faster algorithms for 1-mappability of a sequence,"  In the k-mappability problem, we are given a string x of length n and
integers m and k, and we are asked to count, for each length-m factor y of x,
the number of other factors of length m of x that are at Hamming distance at
most k from y. We focus here on the version of the problem where k = 1. The
fastest known algorithm for k = 1 requires time O(mn log n/ log log n) and
space O(n). We present two algorithms that require worst-case time O(mn) and
O(n log^2 n), respectively, and space O(n), thus greatly improving the state of
the art. Moreover, we present an algorithm that requires average-case time and
space O(n) for integer alphabets if m = {\Omega}(log n/ log {\sigma}), where
{\sigma} is the alphabet size.
",1,0,0,0,0,0
8431,Statistics of $K$-groups modulo $p$ for the ring of integers of a varying quadratic number field,"  For each odd prime $p$, we conjecture the distribution of the $p$-torsion
subgroup of $K_{2n}(\mathcal{O}_F)$ as $F$ ranges over real quadratic fields,
or over imaginary quadratic fields. We then prove that the average size of the
$3$-torsion subgroup of $K_{2n}(\mathcal{O}_F)$ is as predicted by this
conjecture.
",0,0,1,0,0,0
12350,Formal Methods for Adaptive Control of Dynamical Systems,"  We develop a method to control discrete-time systems with constant but
initially unknown parameters from linear temporal logic (LTL) specifications.
We introduce the notions of (non-deterministic) parametric and adaptive
transition systems and show how to use tools from formal methods to compute
adaptive control strategies for finite systems. For infinite systems, we first
compute abstractions in the form of parametric finite quotient transition
systems and then apply the techniques for finite systems. Unlike traditional
adaptive control methods, our approach is correct by design, does not require a
reference model, and can deal with a much wider range of systems and
specifications. Illustrative case studies are included.
",1,0,1,0,0,0
2574,On measures of edge-uncolorability of cubic graphs: A brief survey and some new results,"  There are many hard conjectures in graph theory, like Tutte's 5-flow
conjecture, and the 5-cycle double cover conjecture, which would be true in
general if they would be true for cubic graphs. Since most of them are
trivially true for 3-edge-colorable cubic graphs, cubic graphs which are not
3-edge-colorable, often called {\em snarks}, play a key role in this context.
Here, we survey parameters measuring how far apart a non 3-edge-colorable graph
is from being 3-edge-colorable. We study their interrelation and prove some new
results. Besides getting new insight into the structure of snarks, we show that
such measures give partial results with respect to these important conjectures.
The paper closes with a list of open problems and conjectures.
",0,0,1,0,0,0
10131,Data-adaptive smoothing for optimal-rate estimation of possibly non-regular parameters,"  We consider nonparametric inference of finite dimensional, potentially
non-pathwise differentiable target parameters. In a nonparametric model, some
examples of such parameters that are always non pathwise differentiable target
parameters include probability density functions at a point, or regression
functions at a point. In causal inference, under appropriate causal
assumptions, mean counterfactual outcomes can be pathwise differentiable or
not, depending on the degree at which the positivity assumption holds.
In this paper, given a potentially non-pathwise differentiable target
parameter, we introduce a family of approximating parameters, that are pathwise
differentiable. This family is indexed by a scalar. In kernel regression or
density estimation for instance, a natural choice for such a family is obtained
by kernel smoothing and is indexed by the smoothing level. For the
counterfactual mean outcome, a possible approximating family is obtained
through truncation of the propensity score, and the truncation level then plays
the role of the index.
We propose a method to data-adaptively select the index in the family, so as
to optimize mean squared error. We prove an asymptotic normality result, which
allows us to derive confidence intervals. Under some conditions, our estimator
achieves an optimal mean squared error convergence rate. Confidence intervals
are data-adaptive and have almost optimal width.
A simulation study demonstrates the practical performance of our estimators
for the inference of a causal dose-response curve at a given treatment dose.
",0,0,1,1,0,0
10645,"First principles investigations of electronic, magnetic and bonding peculiarities of uranium nitride-fluoride UNF","  Based on geometry optimization and magnetic structure investigations within
density functional theory, unique uranium nitride fluoride UNF, isoelectronic
with UO2, is shown to present peculiar differentiated physical properties. Such
specificities versus the oxide are related with the mixed anionic sublattices
and the layered-like tetragonal structure characterized by covalent like
[U2N2]2+motifs interlayered by ionic like [F2]2- ones and illustrated herein
with electron localization function graphs. Particularly the ionocovalent
chemical picture shows, based on overlap population analyses, stronger U-N
bonding versus N-F and d(U-N) < d(U-F) distances. Based on LDA+U calculations
the ground state magnetic structure is insulating antiferromagnet with 2 Bohr
Magnetons magnetization per magnetic subcell and ~2 eV band gap.
",0,1,0,0,0,0
3684,Consistency Guarantees for Permutation-Based Causal Inference Algorithms,"  Bayesian networks, or directed acyclic graph (DAG) models, are widely used to
represent complex causal systems. Since the basic task of learning a Bayesian
network from data is NP-hard, a standard approach is greedy search over the
space of DAGs or Markov equivalent DAGs. Since the space of DAGs on p nodes and
the associated space of Markov equivalence classes are both much larger than
the space of permutations, it is desirable to consider permutation-based
searches. We here provide the first consistency guarantees, both uniform and
high-dimensional, of a permutation-based greedy search. Geometrically, this
search corresponds to a simplex-type algorithm on a sub-polytope of the
permutohedron, the DAG associahedron. Every vertex in this polytope is
associated with a DAG, and hence with a collection of permutations that are
consistent with the DAG ordering. A walk is performed on the edges of the
polytope maximizing the sparsity of the associated DAGs. We show based on
simulations that this permutation search is competitive with standard
approaches.
",0,0,1,1,0,0
10165,Formal Synthesis of Control Strategies for Positive Monotone Systems,"  We design controllers from formal specifications for positive discrete-time
monotone systems that are subject to bounded disturbances. Such systems are
widely used to model the dynamics of transportation and biological networks.
The specifications are described using signal temporal logic (STL), which can
express a broad range of temporal properties. We formulate the problem as a
mixed-integer linear program (MILP) and show that under the assumptions made in
this paper, which are not restrictive for traffic applications, the existence
of open-loop control policies is sufficient and almost necessary to ensure the
satisfaction of STL formulas. We establish a relation between satisfaction of
STL formulas in infinite time and set-invariance theories and provide an
efficient method to compute robust control invariant sets in high dimensions.
We also develop a robust model predictive framework to plan controls optimally
while ensuring the satisfaction of the specification. Illustrative examples and
a traffic management case study are included.
",1,0,1,0,0,0
8790,Proof of a conjecture of Abdollahi-Akbari-Maimani concerning the non-commutative graph of finite groups,"  The non--commuting graph $\Gamma(G)$ of a non--abelian group $G$ is defined
as follows. The vertex set $V(\Gamma(G))$ of $\Gamma(G)$ is $G\setminus Z(G)$
where $Z(G)$ denotes the center of $G$ and two vertices $x$ and $y$ are
adjacent if and only if $xy\neq yx$. For non--abelian finite groups $G$ and $H$
it is conjectured that if $\Gamma(G) \cong \Gamma(H)$, then $|G|=|H|$. We prove
the conjecture.
",0,0,1,0,0,0
18003,Atomic-scale identification of novel planar defect phases in heteroepitaxial YBa$_2$Cu$_3$O$_{7-δ}$ thin films,"  We have discovered two novel types of planar defects that appear in
heteroepitaxial YBa$_2$Cu$_3$O$_{7-\delta}$ (YBCO123) thin films, grown by
pulsed-laser deposition (PLD) either with or without a
La$_{2/3}$Ca$_{1/3}$MnO$_3$ (LCMO) overlayer, using the combination of
high-angle annular dark-field scanning transmission electron microscopy
(HAADF-STEM) imaging and electron energy loss spectroscopy (EELS) mapping for
unambiguous identification. These planar lattice defects are based on the
intergrowth of either a BaO plane between two CuO chains or multiple Y-O layers
between two CuO$_2$ planes, resulting in non-stoichiometric layer sequences
that could directly impact the high-$T_c$ superconductivity.
",0,1,0,0,0,0
10212,On the Difference between Physics and Biology: Logical Branching and Biomolecules,"  Physical emergence - crystals, rocks, sandpiles, turbulent eddies, planets,
stars - is fundamentally different from biological emergence - amoeba, cells,
mice, humans - even though the latter is based in the former. This paper points
out that an essential difference is that as well as involving physical
causation, causation in biological systems has a logical nature at each level
of the hierarchy of emergence, from the biomolecular level up. The key link
between physics and life enabling this to happen is provided by biomolecules,
such as voltage gated ion channels, which enable branching logic to emerge from
the underlying physics and hence enable logically based cell processes to take
place in general, and in neurons in particular. These molecules can only have
come into being via the contextually dependent processes of natural selection,
which selects them for their biological function. A further major difference is
between life in general and intelligent life. We characterise intelligent
organisms as being engaged in deductive causation, which enables them to
transcend the physical limitations of their bodies through the power of
abstract thought, prediction, and planning. Ultimately this is enabled by the
biomolecules that underlie the propagation of action potentials in neuronal
axons in the brain.
",0,1,0,0,0,0
11743,Inverse Reinforcement Learning Under Noisy Observations,"  We consider the problem of performing inverse reinforcement learning when the
trajectory of the expert is not perfectly observed by the learner. Instead, a
noisy continuous-time observation of the trajectory is provided to the learner.
This problem exhibits wide-ranging applications and the specific application we
consider here is the scenario in which the learner seeks to penetrate a
perimeter patrolled by a robot. The learner's field of view is limited due to
which it cannot observe the patroller's complete trajectory. Instead, we allow
the learner to listen to the expert's movement sound, which it can also use to
estimate the expert's state and action using an observation model. We treat the
expert's state and action as hidden data and present an algorithm based on
expectation maximization and maximum entropy principle to solve the non-linear,
non-convex problem. Related work considers discrete-time observations and an
observation model that does not include actions. In contrast, our technique
takes expectations over both state and action of the expert, enabling learning
even in the presence of extreme noise and broader applications.
",1,0,0,0,0,0
15313,Robot gains Social Intelligence through Multimodal Deep Reinforcement Learning,"  For robots to coexist with humans in a social world like ours, it is crucial
that they possess human-like social interaction skills. Programming a robot to
possess such skills is a challenging task. In this paper, we propose a
Multimodal Deep Q-Network (MDQN) to enable a robot to learn human-like
interaction skills through a trial and error method. This paper aims to develop
a robot that gathers data during its interaction with a human and learns human
interaction behaviour from the high-dimensional sensory information using
end-to-end reinforcement learning. This paper demonstrates that the robot was
able to learn basic interaction skills successfully, after 14 days of
interacting with people.
",1,0,0,1,0,0
18086,MARGIN: Uncovering Deep Neural Networks using Graph Signal Analysis,"  Interpretability has emerged as a crucial aspect of machine learning, aimed
at providing insights into the working of complex neural networks. However,
existing solutions vary vastly based on the nature of the interpretability
task, with each use case requiring substantial time and effort. This paper
introduces MARGIN, a simple yet general approach to address a large set of
interpretability tasks ranging from identifying prototypes to explaining image
predictions. MARGIN exploits ideas rooted in graph signal analysis to determine
influential nodes in a graph, which are defined as those nodes that maximally
describe a function defined on the graph. By carefully defining task-specific
graphs and functions, we demonstrate that MARGIN outperforms existing
approaches in a number of disparate interpretability challenges.
",1,0,0,1,0,0
13799,Numerical analysis of a nonlinear free-energy diminishing Discrete Duality Finite Volume scheme for convection diffusion equations,"  We propose a nonlinear Discrete Duality Finite Volume scheme to approximate
the solutions of drift diffusion equations. The scheme is built to preserve at
the discrete level even on severely distorted meshes the energy / energy
dissipation relation. This relation is of paramount importance to capture the
long-time behavior of the problem in an accurate way. To enforce it, the linear
convection diffusion equation is rewritten in a nonlinear form before being
discretized. We establish the existence of positive solutions to the scheme.
Based on compactness arguments, the convergence of the approximate solution
towards a weak solution is established. Finally, we provide numerical evidences
of the good behavior of the scheme when the discretization parameters tend to 0
and when time goes to infinity.
",0,0,1,0,0,0
16618,Numerical Investigation of Unsteady Aerodynamic Effects on Thick Flatback Airfoils,"  The unsteady characteristics of the flow over thick flatback airfoils have
been investigated by means of CFD calculations. Sandia airfoils which have 35%
maximum thickness with three different trailing edge thicknesses were selected.
The calculations provided good results compared with available experimental
data with regard to the lift curve and the impact of trailing edge thickness.
Unsteady CFD simulations revealed that the Strouhal number is found to be
independent of the lift coefficient before stall and increases with the
trailing edge. The present work shows the dependency of the Strouhal number and
the wake development on the trailing edge thickness. A recommendation of the
Strouhal number definition is given for flatback airfoils by considering the
trailing edge separation at low angle of attack. The detailed unsteady
characteristics of thick flatback airfoils are discussed more in the present
paper.
",0,1,0,0,0,0
3909,Bloch line dynamics within moving domain walls in 3D ferromagnets,"  We study field-driven magnetic domain wall dynamics in garnet strips by
large-scale three-dimensional micromagnetic simulations. The domain wall
propagation velocity as a function of the applied field exhibits a low-field
linear part terminated by a sudden velocity drop at a threshold field
magnitude, related to the onset of excitations of internal degrees of freedom
of the domain wall magnetization. By considering a wide range of strip
thicknesses from 30 nm to 1.89 $\mu$m, we find a non-monotonic thickness
dependence of the threshold field for the onset of this instability, proceeding
via nucleation and propagation of Bloch lines within the domain wall. We
identify a critical strip thickness above which the velocity drop is due to
nucleation of horizontal Bloch lines, while for thinner strips and depending on
the boundary conditions employed, either generation of vertical Bloch lines, or
close-to-uniform precession of the domain wall internal magnetization takes
place. For strips of intermediate thicknesses, the vertical Bloch lines assume
a deformed structure due to demagnetizing fields at the strip surfaces,
breaking the symmetry between the top and bottom faces of the strip, and
resulting in circulating Bloch line dynamics along the perimeter of the domain
wall.
",0,1,0,0,0,0
2170,The Generalized Label Correcting Method for Optimal Kinodynamic Motion Planning,"  Nearly all autonomous robotic systems use some form of motion planning to
compute reference motions through their environment. An increasing use of
autonomous robots in a broad range of applications creates a need for
efficient, general purpose motion planning algorithms that are applicable in
any of these new application domains.
This thesis presents a resolution complete optimal kinodynamic motion
planning algorithm based on a direct forward search of the set of admissible
input signals to a dynamical model. The advantage of this generalized label
correcting method is that it does not require a local planning subroutine as in
the case of related methods.
Preliminary material focuses on new topological properties of the canonical
problem formulation that are used to show continuity of the performance
objective. These observations are used to derive a generalization of Bellman's
principle of optimality in the context of kinodynamic motion planning. A
generalized label correcting algorithm is then proposed which leverages these
results to prune candidate input signals from the search when their cost is
greater than related signals.
The second part of this thesis addresses admissible heuristics for
kinodynamic motion planning. An admissibility condition is derived that can be
used to verify the admissibility of candidate heuristics for a particular
problem. This condition also characterizes a convex set of admissible
heuristics.
A linear program is formulated to obtain a heuristic which is as close to the
optimal cost-to-go as possible while remaining admissible. This optimization is
justified by showing its solution coincides with the solution to the
Hamilton-Jacobi-Bellman equation. Lastly, a sum-of-squares relaxation of this
infinite-dimensional linear program is proposed for obtaining provably
admissible approximate solutions.
",1,0,0,0,0,0
16954,Learning to Imagine Manipulation Goals for Robot Task Planning,"  Prospection is an important part of how humans come up with new task plans,
but has not been explored in depth in robotics. Predicting multiple task-level
is a challenging problem that involves capturing both task semantics and
continuous variability over the state of the world. Ideally, we would combine
the ability of machine learning to leverage big data for learning the semantics
of a task, while using techniques from task planning to reliably generalize to
new environment. In this work, we propose a method for learning a model
encoding just such a representation for task planning. We learn a neural net
that encodes the $k$ most likely outcomes from high level actions from a given
world. Our approach creates comprehensible task plans that allow us to predict
changes to the environment many time steps into the future. We demonstrate this
approach via application to a stacking task in a cluttered environment, where
the robot must select between different colored blocks while avoiding
obstacles, in order to perform a task. We also show results on a simple
navigation task. Our algorithm generates realistic image and pose predictions
at multiple points in a given task.
",1,0,0,0,0,0
16209,Carbon stars in the X-Shooter Spectral Library: II. Comparison with models,"  In a previous paper, we assembled a collection of medium-resolution spectra
of 35 carbon stars, covering optical and near-infrared wavelengths from 400 to
2400 nm. The sample includes stars from the Milky Way and the Magellanic
Clouds, with a variety of $(J-K_s)$ colors and pulsation properties. In the
present paper, we compare these observations to a new set of high-resolution
synthetic spectra, based on hydrostatic model atmospheres. We find that the
broad-band colors and the molecular-band strengths measured by
spectrophotometric indices match those of the models when $(J-K_s)$ is bluer
than about 1.6, while the redder stars require either additional reddening or
dust emission or both. Using a grid of models to fit the full observed spectra,
we estimate the most likely atmospheric parameters $T_\mathrm{eff}$, $\log(g)$,
$[\mathrm{Fe/H}]$ and C/O. These parameters derived independently in the
optical and near-infrared are generally consistent when $(J-K_s)<1.6$. The
temperatures found based on either wavelength range are typically within
$\pm$100K of each other, and $\log(g)$ and $[\mathrm{Fe/H}]$ are consistent
with the values expected for this sample. The reddest stars ($(J-K_s)$ $>$ 1.6)
are divided into two families, characterized by the presence or absence of an
absorption feature at 1.53\,$\mu$m, generally associated with HCN and
C$_2$H$_2$. Stars from the first family begin to be more affected by
circumstellar extinction. The parameters found using optical or near-infrared
wavelengths are still compatible with each other, but the error bars become
larger. In stars showing the 1.53\,$\mu$m feature, which are all
large-amplitude variables, the effects of pulsation are strong and the spectra
are poorly matched with hydrostatic models. For these, atmospheric parameters
could not be derived reliably, and dynamical models are needed for proper
interpretation.
",0,1,0,0,0,0
6569,HourGlass: Predictable Time-based Cache Coherence Protocol for Dual-Critical Multi-Core Systems,"  We present a hardware mechanism called HourGlass to predictably share data in
a multi-core system where cores are explicitly designated as critical or
non-critical. HourGlass is a time-based cache coherence protocol for
dual-critical multi-core systems that ensures worst-case latency (WCL) bounds
for memory requests originating from critical cores. Although HourGlass does
not provide either WCL or bandwidth guarantees for memory requests from
non-critical cores, it promotes the use of timers to improve its bandwidth
utilization while still maintaining WCL bounds for critical cores. This
encourages a trade-off between the WCL bounds for critical cores, and the
improved memory bandwidth for non-critical cores via timer configurations. We
evaluate HourGlass using gem5, and with multithreaded benchmark suites
including SPLASH-2, and synthetic workloads. Our results show that the WCL for
critical cores with HourGlass is always within the analytical WCL bounds, and
provides a tighter WCL bound on critical cores compared to the state-of-the-art
real-time cache coherence protocol. Further, we show that HourGlass enables a
trade-off between provable WCL bounds for critical cores, and improved
bandwidth utilization for non-critical cores. The average-case performance of
HourGlass is comparable to the state-of-the-art real-time cache coherence
protocol, and suffers a slowdown of 1.43x and 1.46x compared to the
conventional MSI and MESI protocols.
",1,0,0,0,0,0
7679,Profit Driven Decision Trees for Churn Prediction,"  Customer retention campaigns increasingly rely on predictive models to detect
potential churners in a vast customer base. From the perspective of machine
learning, the task of predicting customer churn can be presented as a binary
classification problem. Using data on historic behavior, classification
algorithms are built with the purpose of accurately predicting the probability
of a customer defecting. The predictive churn models are then commonly selected
based on accuracy related performance measures such as the area under the ROC
curve (AUC). However, these models are often not well aligned with the core
business requirement of profit maximization, in the sense that, the models fail
to take into account not only misclassification costs, but also the benefits
originating from a correct classification. Therefore, the aim is to construct
churn prediction models that are profitable and preferably interpretable too.
The recently developed expected maximum profit measure for customer churn
(EMPC) has been proposed in order to select the most profitable churn model. We
present a new classifier that integrates the EMPC metric directly into the
model construction. Our technique, called ProfTree, uses an evolutionary
algorithm for learning profit driven decision trees. In a benchmark study with
real-life data sets from various telecommunication service providers, we show
that ProfTree achieves significant profit improvements compared to classic
accuracy driven tree-based methods.
",1,0,0,1,0,0
6514,Adaptation to Easy Data in Prediction with Limited Advice,"  We derive an online learning algorithm with improved regret guarantees for
`easy' loss sequences. We consider two types of `easiness': (a) stochastic loss
sequences and (b) adversarial loss sequences with small effective range of the
losses. While a number of algorithms have been proposed for exploiting small
effective range in the full information setting, Gerchinovitz and Lattimore
[2016] have shown the impossibility of regret scaling with the effective range
of the losses in the bandit setting. We show that just one additional
observation per round is sufficient to circumvent the impossibility result. The
proposed Second Order Difference Adjustments (SODA) algorithm requires no prior
knowledge of the effective range of the losses, $\varepsilon$, and achieves an
$O(\varepsilon \sqrt{KT \ln K}) + \tilde{O}(\varepsilon K \sqrt[4]{T})$
expected regret guarantee, where $T$ is the time horizon and $K$ is the number
of actions. The scaling with the effective loss range is achieved under
significantly weaker assumptions than those made by Cesa-Bianchi and Shamir
[2018] in an earlier attempt to circumvent the impossibility result. We also
provide a regret lower bound of $\Omega(\varepsilon\sqrt{T K})$, which almost
matches the upper bound. In addition, we show that in the stochastic setting
SODA achieves an $O\left(\sum_{a:\Delta_a>0}
\frac{K\varepsilon^2}{\Delta_a}\right)$ pseudo-regret bound that holds
simultaneously with the adversarial regret guarantee. In other words, SODA is
safe against an unrestricted oblivious adversary and provides improved regret
guarantees for at least two different types of `easiness' simultaneously.
",0,0,0,1,0,0
17283,Recent Advances in Neural Program Synthesis,"  In recent years, deep learning has made tremendous progress in a number of
fields that were previously out of reach for artificial intelligence. The
successes in these problems has led researchers to consider the possibilities
for intelligent systems to tackle a problem that humans have only recently
themselves considered: program synthesis. This challenge is unlike others such
as object recognition and speech translation, since its abstract nature and
demand for rigor make it difficult even for human minds to attempt. While it is
still far from being solved or even competitive with most existing methods,
neural program synthesis is a rapidly growing discipline which holds great
promise if completely realized. In this paper, we start with exploring the
problem statement and challenges of program synthesis. Then, we examine the
fascinating evolution of program induction models, along with how they have
succeeded, failed and been reimagined since. Finally, we conclude with a
contrastive look at program synthesis and future research recommendations for
the field.
",1,0,0,0,0,0
7175,Image-based immersed boundary model of the aortic root,"  Each year, approximately 300,000 heart valve repair or replacement procedures
are performed worldwide, including approximately 70,000 aortic valve
replacement surgeries in the United States alone. This paper describes progress
in constructing anatomically and physiologically realistic immersed boundary
(IB) models of the dynamics of the aortic root and ascending aorta. This work
builds on earlier IB models of fluid-structure interaction (FSI) in the aortic
root, which previously achieved realistic hemodynamics over multiple cardiac
cycles, but which also were limited to simplified aortic geometries and
idealized descriptions of the biomechanics of the aortic valve cusps. By
contrast, the model described herein uses an anatomical geometry reconstructed
from patient-specific computed tomography angiography (CTA) data, and employs a
description of the elasticity of the aortic valve leaflets based on a
fiber-reinforced constitutive model fit to experimental tensile test data.
Numerical tests show that the model is able to resolve the leaflet biomechanics
in diastole and early systole at practical grid spacings. The model is also
used to examine differences in the mechanics and fluid dynamics yielded by
fresh valve leaflets and glutaraldehyde-fixed leaflets similar to those used in
bioprosthetic heart valves. Although there are large differences in the leaflet
deformations during diastole, the differences in the open configurations of the
valve models are relatively small, and nearly identical hemodynamics are
obtained in all cases considered.
",1,1,0,0,0,0
955,The beamformer and correlator for the Large European Array for Pulsars,"  The Large European Array for Pulsars combines Europe's largest radio
telescopes to form a tied-array telescope that provides high signal-to-noise
observations of millisecond pulsars (MSPs) with the objective to increase the
sensitivity of detecting low-frequency gravitational waves. As part of this
endeavor we have developed a software correlator and beamformer which enables
the formation of a tied-array beam from the raw voltages from each of
telescopes. We explain the concepts and techniques involved in the process of
adding the raw voltages coherently. We further present the software processing
pipeline that is specifically designed to deal with data from widely spaced,
inhomogeneous radio telescopes and describe the steps involved in preparing,
correlating and creating the tied-array beam. This includes polarization
calibration, bandpass correction, frequency dependent phase correction,
interference mitigation and pulsar gating. A link is provided where the
software can be obtained.
",0,1,0,0,0,0
20015,Dynamic Input Structure and Network Assembly for Few-Shot Learning,"  The ability to learn from a small number of examples has been a difficult
problem in machine learning since its inception. While methods have succeeded
with large amounts of training data, research has been underway in how to
accomplish similar performance with fewer examples, known as one-shot or more
generally few-shot learning. This technique has been shown to have promising
performance, but in practice requires fixed-size inputs making it impractical
for production systems where class sizes can vary. This impedes training and
the final utility of few-shot learning systems. This paper describes an
approach to constructing and training a network that can handle arbitrary
example sizes dynamically as the system is used.
",1,0,0,1,0,0
20738,Self-regulation promotes cooperation in social networks,"  Cooperative behavior in real social dilemmas is often perceived as a
phenomenon emerging from norms and punishment. To overcome this paradigm, we
highlight the interplay between the influence of social networks on
individuals, and the activation of spontaneous self-regulating mechanisms,
which may lead them to behave cooperatively, while interacting with others and
taking conflicting decisions over time. By extending Evolutionary game theory
over networks, we prove that cooperation partially or fully emerges whether
self-regulating mechanisms are sufficiently stronger than social pressure.
Interestingly, even few cooperative individuals act as catalyzing agents for
the cooperation of others, thus activating a recruiting mechanism, eventually
driving the whole population to cooperate.
",1,0,0,0,0,0
7115,Comparison of the h-index for Different Fields of Research Using Bootstrap Methodology,"  An important disadvantage of the h-index is that typically it cannot take
into account the specific field of research of a researcher. Usually sample
point estimates of the average and median h-index values for the various fields
are reported that are highly variable and dependent of the specific samples and
it would be useful to provide confidence intervals of prediction accuracy. In
this paper we apply the non-parametric bootstrap technique for constructing
confidence intervals for the h-index for different fields of research. In this
way no specific assumptions about the distribution of the empirical hindex are
required as well as no large samples since that the methodology is based on
resampling from the initial sample. The results of the analysis showed
important differences between the various fields. The performance of the
bootstrap intervals for the mean and median h-index for most fields seems to be
rather satisfactory as revealed by the performed simulation.
",1,0,0,1,0,0
3268,An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution,"  Few ideas have enjoyed as large an impact on deep learning as convolution.
For any problem involving pixels or spatial representations, common intuition
holds that convolutional neural networks may be appropriate. In this paper we
show a striking counterexample to this intuition via the seemingly trivial
coordinate transform problem, which simply requires learning a mapping between
coordinates in (x,y) Cartesian space and one-hot pixel space. Although
convolutional networks would seem appropriate for this task, we show that they
fail spectacularly. We demonstrate and carefully analyze the failure first on a
toy problem, at which point a simple fix becomes obvious. We call this solution
CoordConv, which works by giving convolution access to its own input
coordinates through the use of extra coordinate channels. Without sacrificing
the computational and parametric efficiency of ordinary convolution, CoordConv
allows networks to learn either complete translation invariance or varying
degrees of translation dependence, as required by the end task. CoordConv
solves the coordinate transform problem with perfect generalization and 150
times faster with 10--100 times fewer parameters than convolution. This stark
contrast raises the question: to what extent has this inability of convolution
persisted insidiously inside other tasks, subtly hampering performance from
within? A complete answer to this question will require further investigation,
but we show preliminary evidence that swapping convolution for CoordConv can
improve models on a diverse set of tasks. Using CoordConv in a GAN produced
less mode collapse as the transform between high-level spatial latents and
pixels becomes easier to learn. A Faster R-CNN detection model trained on MNIST
showed 24% better IOU when using CoordConv, and in the RL domain agents playing
Atari games benefit significantly from the use of CoordConv layers.
",0,0,0,1,0,0
8795,Deep Stochastic Configuration Networks with Universal Approximation Property,"  This paper develops a randomized approach for incrementally building deep
neural networks, where a supervisory mechanism is proposed to constrain the
random assignment of the weights and biases, and all the hidden layers have
direct links to the output layer. A fundamental result on the universal
approximation property is established for such a class of randomized leaner
models, namely deep stochastic configuration networks (DeepSCNs). A learning
algorithm is presented to implement DeepSCNs with either specific architecture
or self-organization. The read-out weights attached with all direct links from
each hidden layer to the output layer are evaluated by the least squares
method. Given a set of training examples, DeepSCNs can speedily produce a
learning representation, that is, a collection of random basis functions with
the cascaded inputs together with the read-out weights. An empirical study on a
function approximation is carried out to demonstrate some properties of the
proposed deep learner model.
",1,0,0,0,0,0
14888,Deep Networks tag the location of bird vocalisations on audio spectrograms,"  This work focuses on reliable detection and segmentation of bird
vocalizations as recorded in the open field. Acoustic detection of avian sounds
can be used for the automatized monitoring of multiple bird taxa and querying
in long-term recordings for species of interest. These tasks are tackled in
this work, by suggesting two approaches: A) First, DenseNets are applied to
weekly labeled data to infer the attention map of the dataset (i.e. Salience
and CAM). We push further this idea by directing attention maps to the YOLO v2
Deepnet-based, detection framework to localize bird vocalizations. B) A deep
autoencoder, namely the U-net, maps the audio spectrogram of bird vocalizations
to its corresponding binary mask that encircles the spectral blobs of
vocalizations while suppressing other audio sources. We focus solely on
procedures requiring minimum human attendance, suitable to scan massive volumes
of data, in order to analyze them, evaluate insights and hypotheses and
identify patterns of bird activity. Hopefully, this approach will be valuable
to researchers, conservation practitioners, and decision makers that need to
design policies on biodiversity issues.
",1,0,0,0,0,0
12307,A Low-Complexity Approach to Distributed Cooperative Caching with Geographic Constraints,"  We consider caching in cellular networks in which each base station is
equipped with a cache that can store a limited number of files. The popularity
of the files is known and the goal is to place files in the caches such that
the probability that a user at an arbitrary location in the plane will find the
file that she requires in one of the covering caches is maximized.
We develop distributed asynchronous algorithms for deciding which contents to
store in which cache. Such cooperative algorithms require communication only
between caches with overlapping coverage areas and can operate in asynchronous
manner. The development of the algorithms is principally based on an
observation that the problem can be viewed as a potential game. Our basic
algorithm is derived from the best response dynamics. We demonstrate that the
complexity of each best response step is independent of the number of files,
linear in the cache capacity and linear in the maximum number of base stations
that cover a certain area. Then, we show that the overall algorithm complexity
for a discrete cache placement is polynomial in both network size and catalog
size. In practical examples, the algorithm converges in just a few iterations.
Also, in most cases of interest, the basic algorithm finds the best Nash
equilibrium corresponding to the global optimum. We provide two extensions of
our basic algorithm based on stochastic and deterministic simulated annealing
which find the global optimum.
Finally, we demonstrate the hit probability evolution on real and synthetic
networks numerically and show that our distributed caching algorithm performs
significantly better than storing the most popular content, probabilistic
content placement policy and Multi-LRU caching policies.
",1,0,0,0,0,0
12359,Putative spin liquid in the triangle-based iridate Ba$_3$IrTi$_2$O$_9$,"  We report on thermodynamic, magnetization, and muon spin relaxation
measurements of the strong spin-orbit coupled iridate Ba$_3$IrTi$_2$O$_9$,
which constitutes a new frustration motif made up a mixture of edge- and
corner-sharing triangles. In spite of strong antiferromagnetic exchange
interaction of the order of 100~K, we find no hint for long-range magnetic
order down to 23 mK. The magnetic specific heat data unveil the $T$-linear and
-squared dependences at low temperatures below 1~K. At the respective
temperatures, the zero-field muon spin relaxation features a persistent spin
dynamics, indicative of unconventional low-energy excitations. A comparison to
the $4d$ isostructural compound Ba$_3$RuTi$_2$O$_9$ suggests that a concerted
interplay of compass-like magnetic interactions and frustrated geometry
promotes a dynamically fluctuating state in a triangle-based iridate.
",0,1,0,0,0,0
11127,Learning Traffic as Images: A Deep Convolutional Neural Network for Large-Scale Transportation Network Speed Prediction,"  This paper proposes a convolutional neural network (CNN)-based method that
learns traffic as images and predicts large-scale, network-wide traffic speed
with a high accuracy. Spatiotemporal traffic dynamics are converted to images
describing the time and space relations of traffic flow via a two-dimensional
time-space matrix. A CNN is applied to the image following two consecutive
steps: abstract traffic feature extraction and network-wide traffic speed
prediction. The effectiveness of the proposed method is evaluated by taking two
real-world transportation networks, the second ring road and north-east
transportation network in Beijing, as examples, and comparing the method with
four prevailing algorithms, namely, ordinary least squares, k-nearest
neighbors, artificial neural network, and random forest, and three deep
learning architectures, namely, stacked autoencoder, recurrent neural network,
and long-short-term memory network. The results show that the proposed method
outperforms other algorithms by an average accuracy improvement of 42.91%
within an acceptable execution time. The CNN can train the model in a
reasonable time and, thus, is suitable for large-scale transportation networks.
",1,0,0,1,0,0
13856,"Dynamics of the nonlinear Klein-Gordon equation in the nonrelativistic limit, I","  The nonlinear Klein-Gordon (NLKG) equation on a manifold $M$ in the
nonrelativistic limit, namely as the speed of light $c$ tends to infinity, is
considered. In particular, a higher-order normalized approximation of NLKG
(which corresponds to the NLS at order $r=1$) is constructed, and when $M$ is a
smooth compact manifold or $\mathbb{R}^d$ it is proved that the solution of the
approximating equation approximates the solution of the NLKG locally uniformly
in time. When $M=\mathbb{R}^d$, $d \geq 3$, it is proved that solutions of the
linearized order $r$ normalized equation approximate solutions of linear
Klein-Gordon equation up to times of order $\mathcal{O}(c^{2(r-1)})$ for any
$r>1$.
",0,0,1,0,0,0
11685,Search for Common Minima in Joint Optimization of Multiple Cost Functions,"  We present a novel optimization method, named the Combined Optimization
Method (COM), for the joint optimization of two or more cost functions. Unlike
the conventional joint optimization schemes, which try to find minima in a
weighted sum of cost functions, the COM explores search space for common minima
shared by all the cost functions. Given a set of multiple cost functions that
have qualitatively different distributions of local minima with each other, the
proposed method finds the common minima with a high success rate without the
help of any metaheuristics. As a demonstration, we apply the COM to the crystal
structure prediction in materials science. By introducing the concept of data
assimilation, i.e., adopting the theoretical potential energy of the crystal
and the crystallinity, which characterizes the agreement with the theoretical
and experimental X-ray diffraction patterns, as cost functions, we show that
the correct crystal structures of Si diamond, low quartz, and low cristobalite
can be predicted with significantly higher success rates than the previous
methods.
",0,0,0,1,0,0
18422,On age of 6070 Rheinland and 54827 (2001 NQ8) asteroid pair,"  In this paper we present results of our studying of famous very young pair of
asteroids 6070 Rheinland and 54827 (2001 NQ8). We have done numeric integration
of orbits of pair with only planet perturbations and include Ceres and Vesta
effect. We have confirmed results of previous studying, obtained with different
integrators. And we confirm significant effect of Ceres and Vesta perturbation
on dynamic of this pair. We find that effect of other massive asteroids is
insignificant. According our results, more probable age of 6070 Rheinland and
54827 (2001 NQ8) pair is 16.2 kyrs. Our value of age is very close to most
recent age determination by Vokrouhlicky et al [12], obtained with different
method. After the compare our results, we can conclude, that non-gravitational
forces are small and large number of clones is not necessary in studying of
this pair. As an additional way of studying of close orbits dynamics, we
calculate relative velocity in pair during numeric integration. Normal
component of velocity show a very good convergence at epoch of closest
encounter in pair.
",0,1,0,0,0,0
5794,Learning a Deep Convolution Network with Turing Test Adversaries for Microscopy Image Super Resolution,"  Adversarially trained deep neural networks have significantly improved
performance of single image super resolution, by hallucinating photorealistic
local textures, thereby greatly reducing the perception difference between a
real high resolution image and its super resolved (SR) counterpart. However,
application to medical imaging requires preservation of diagnostically relevant
features while refraining from introducing any diagnostically confusing
artifacts. We propose using a deep convolutional super resolution network
(SRNet) trained for (i) minimising reconstruction loss between the real and SR
images, and (ii) maximally confusing learned relativistic visual Turing test
(rVTT) networks to discriminate between (a) pair of real and SR images (T1) and
(b) pair of patches in real and SR selected from region of interest (T2). The
adversarial loss of T1 and T2 while backpropagated through SRNet helps it learn
to reconstruct pathorealism in the regions of interest such as white blood
cells (WBC) in peripheral blood smears or epithelial cells in histopathology of
cancerous biopsy tissues, which are experimentally demonstrated here.
Experiments performed for measuring signal distortion loss using peak signal to
noise ratio (pSNR) and structural similarity (SSIM) with variation of SR scale
factors, impact of rVTT adversarial losses, and impact on reporting using SR on
a commercially available artificial intelligence (AI) digital pathology system
substantiate our claims.
",1,0,0,0,0,0
12157,Batched High-dimensional Bayesian Optimization via Structural Kernel Learning,"  Optimization of high-dimensional black-box functions is an extremely
challenging problem. While Bayesian optimization has emerged as a popular
approach for optimizing black-box functions, its applicability has been limited
to low-dimensional problems due to its computational and statistical challenges
arising from high-dimensional settings. In this paper, we propose to tackle
these challenges by (1) assuming a latent additive structure in the function
and inferring it properly for more efficient and effective BO, and (2)
performing multiple evaluations in parallel to reduce the number of iterations
required by the method. Our novel approach learns the latent structure with
Gibbs sampling and constructs batched queries using determinantal point
processes. Experimental validations on both synthetic and real-world functions
demonstrate that the proposed method outperforms the existing state-of-the-art
approaches.
",1,0,1,1,0,0
17636,Global Strong Solution of a 2D coupled Parabolic-Hyperbolic Magnetohydrodynamic System,"  The main objective of this paper is to study the global strong solution of
the parabolic-hyperbolic incompressible magnetohydrodynamic (MHD) model in two
dimensional space. Based on Agmon, Douglis and Nirenberg's estimates for the
stationary Stokes equation and the Solonnikov's theorem of
$L^p$-$L^q$-estimates for the evolution Stokes equation, it is shown that the
mixed-type MHD equations exist a global strong solution.
",0,0,1,0,0,0
679,"An efficient data structure for counting all linear extensions of a poset, calculating its jump number, and the likes","  Achieving the goals in the title (and others) relies on a cardinality-wise
scanning of the ideals of the poset. Specifically, the relevant numbers
attached to the k+1 element ideals are inferred from the corresponding numbers
of the k-element (order) ideals. Crucial in all of this is a compressed
representation (using wildcards) of the ideal lattice. The whole scheme invites
distributed computation.
",1,0,0,0,0,0
12581,Extended depth-range profilometry using the phase-difference and phase-sum of two close-sensitivity projected fringes,"  We propose a high signal-to-noise extended depth-range three-dimensional (3D)
profilometer projecting two linear-fringes with close phase-sensitivity. We use
temporal phase-shifting algorithms (PSAs) to phase-demodulate the two close
sensitivity phases. Then we calculate their phase-difference and their
phase-sum. If the sensitivity between the two phases is close enough, their
phase-difference is not-wrapped. The non-wrapped phase-difference as
extended-range profilometry is well known and has been widely used. However as
this paper shows, the closeness between the two demodulated phases makes their
difference quite noisy. On the other hand, as we show, their phase-sum has a
much higher phase-sensitivity and signal-to-noise ratio but it is highly
wrapped. Spatial unwrapping of the phase-sum is precluded for separate or
highly discontinuous objects. However it is possible to unwrap the phase-sum by
using the phase-difference as first approximation and our previously published
2-step temporal phase-unwrapping. Therefore the proposed profilometry technique
allows unwrapping the higher sensitivity phase-sum using the noisier
phase-difference as stepping stone. Due to the non-linear nature of the
extended 2-steps temporal-unwrapper, the harmonics and noise errors in the
phase-difference do not propagate towards the unwrapping phase-sum. To the best
of our knowledge this is the highest signal-to-noise ratio, extended
depth-range, 3D digital profilometry technique reported to this date.
",0,1,0,0,0,0
20337,Shape-constrained partial identification of a population mean under unknown probabilities of sample selection,"  A prevailing challenge in the biomedical and social sciences is to estimate a
population mean from a sample obtained with unknown selection probabilities.
Using a well-known ratio estimator, Aronow and Lee (2013) proposed a method for
partial identification of the mean by allowing the unknown selection
probabilities to vary arbitrarily between two fixed extreme values. In this
paper, we show how to leverage auxiliary shape constraints on the population
outcome distribution, such as symmetry or log-concavity, to obtain tighter
bounds on the population mean. We use this method to estimate the performance
of Aymara students---an ethnic minority in the north of Chile---in a national
educational standardized test. We implement this method in the new statistical
software package scbounds for R.
",0,0,1,1,0,0
12859,Photospheric Emission of Gamma-Ray Bursts,"  We review the physics of GRB production by relativistic jets that start
highly opaque near the central source and then expand to transparency. We
discuss dissipative and radiative processes in the jet and how radiative
transfer shapes the observed nonthermal spectrum released at the photosphere. A
comparison of recent detailed models with observations gives estimates for
important parameters of GRB jets, such as the Lorentz factor and magnetization.
We also discuss predictions for GRB polarization and neutrino emission.
",0,1,0,0,0,0
11758,Magnetic Field Dependence of Spin Glass Free Energy Barriers,"  We measure the field dependence of spin glass free energy barriers in a thin
amorphous Ge:Mn film through the time dependence of the magnetization. After
the correlation length $\xi(t, T)$ has reached the film thickness $\mathcal
{L}=155$~\AA~so that the dynamics are activated, we change the initial magnetic
field by $\delta H$. In agreement with the scaling behavior exhibited in a
companion Letter [Janus collaboration: M. Baity-Jesi {\it et al.}, Phys. Rev.
Lett. {\bf 118}, 157202 (2017)], we find the activation energy is increased
when $\delta H < 0$. The change is proportional to $(\delta H)^2$ with the
addition of a small $(\delta H)^4$ term. The magnitude of the change of the
spin glass free energy barriers is in near quantitative agreement with the
prediction of a barrier model.
",0,1,0,0,0,0
280,A Survey of Model Compression and Acceleration for Deep Neural Networks,"  Deep convolutional neural networks (CNNs) have recently achieved great
success in many visual recognition tasks. However, existing deep neural network
models are computationally expensive and memory intensive, hindering their
deployment in devices with low memory resources or in applications with strict
latency requirements. Therefore, a natural thought is to perform model
compression and acceleration in deep networks without significantly decreasing
the model performance. During the past few years, tremendous progress has been
made in this area. In this paper, we survey the recent advanced techniques for
compacting and accelerating CNNs model developed. These techniques are roughly
categorized into four schemes: parameter pruning and sharing, low-rank
factorization, transferred/compact convolutional filters, and knowledge
distillation. Methods of parameter pruning and sharing will be described at the
beginning, after that the other techniques will be introduced. For each scheme,
we provide insightful analysis regarding the performance, related applications,
advantages, and drawbacks etc. Then we will go through a few very recent
additional successful methods, for example, dynamic capacity networks and
stochastic depths networks. After that, we survey the evaluation matrix, the
main datasets used for evaluating the model performance and recent benchmarking
efforts. Finally, we conclude this paper, discuss remaining challenges and
possible directions on this topic.
",1,0,0,0,0,0
11803,Block Mean Approximation for Efficient Second Order Optimization,"  Advanced optimization algorithms such as Newton method and AdaGrad benefit
from second order derivative or second order statistics to achieve better
descent directions and faster convergence rates. At their heart, such
algorithms need to compute the inverse or inverse square root of a matrix whose
size is quadratic of the dimensionality of the search space. For high
dimensional search spaces, the matrix inversion or inversion of square root
becomes overwhelming which in turn demands for approximate methods. In this
work, we propose a new matrix approximation method which divides a matrix into
blocks and represents each block by one or two numbers. The method allows
efficient computation of matrix inverse and inverse square root. We apply our
method to AdaGrad in training deep neural networks. Experiments show
encouraging results compared to the diagonal approximation.
",0,0,0,1,0,0
8302,Distinct dynamical behavior in random and all-to-all neuronal networks,"  Neuronal network dynamics depends on network structure. It is often assumed
that neurons are connected at random when their actual connectivity structure
is unknown. Such models are then often approximated by replacing the random
network by an all-to-all network, where every neuron is connected to all other
neurons. This mean-field approximation is a common approach in statistical
physics. In this paper we show that such approximation can be invalid. We solve
analytically a neuronal network model with binary-state neurons in both random
and all-to-all networks. We find strikingly different phase diagrams
corresponding to each network structure. Neuronal network dynamics is not only
different within certain parameter ranges, but it also undergoes different
bifurcations. Our results therefore suggest cautiousness when using mean-field
models based on all-to-all network topologies to represent random networks.
",0,0,0,0,1,0
5827,A boundary integral equation method for mode elimination and vibration confinement in thin plates with clamped points,"  We consider the bi-Laplacian eigenvalue problem for the modes of vibration of
a thin elastic plate with a discrete set of clamped points. A high-order
boundary integral equation method is developed for efficient numerical
determination of these modes in the presence of multiple localized defects for
a wide range of two-dimensional geometries. The defects result in
eigenfunctions with a weak singularity that is resolved by decomposing the
solution as a superposition of Green's functions plus a smooth regular part.
This method is applied to a variety of regular and irregular domains and two
key phenomena are observed. First, careful placement of clamping points can
entirely eliminate particular eigenvalues and suggests a strategy for
manipulating the vibrational characteristics of rigid bodies so that
undesirable frequencies are removed. Second, clamping of the plate can result
in partitioning of the domain so that vibrational modes are largely confined to
certain spatial regions. This numerical method gives a precision tool for
tuning the vibrational characteristics of thin elastic plates.
",0,0,1,0,0,0
11648,Dispersion for the wave equation outside a ball and counterexamples,"  The purpose of this note is to prove dispersive estimates for the wave
equation outside a ball in R^d. If d = 3, we show that the linear flow
satisfies the dispersive estimates as in R^3. In higher dimensions d $\ge$ 4 we
show that losses in dispersion do appear and this happens at the Poisson spot.
",0,0,1,0,0,0
14837,Succinctness in subsystems of the spatial mu-calculus,"  In this paper we systematically explore questions of succinctness in modal
logics employed in spatial reasoning. We show that the closure operator,
despite being less expressive, is exponentially more succinct than the
limit-point operator, and that the $\mu$-calculus is exponentially more
succinct than the equally-expressive tangled limit operator. These results hold
for any class of spaces containing at least one crowded metric space or
containing all spaces based on ordinals below $\omega^\omega$, with the usual
limit operator. We also show that these results continue to hold even if we
enrich the less succinct language with the universal modality.
",0,0,1,0,0,0
12881,An age-structured continuum model for myxobacteria,"  Myxobacteria are social bacteria, that can glide in 2D and form
counter-propagating, interacting waves. Here we present a novel age-structured,
continuous macroscopic model for the movement of myxobacteria. The derivation
is based on microscopic interaction rules that can be formulated as a
particle-based model and set within the SOH (Self-Organized Hydrodynamics)
framework. The strength of this combined approach is that microscopic knowledge
or data can be incorporated easily into the particle model, whilst the
continuous model allows for easy numerical analysis of the different effects.
However we found that the derived macroscopic model lacks a diffusion term in
the density equations, which is necessary to control the number of waves,
indicating that a higher order approximation during the derivation is crucial.
Upon ad-hoc addition of the diffusion term, we found very good agreement
between the age-structured model and the biology. In particular we analyzed the
influence of a refractory (insensitivity) period following a reversal of
movement. Our analysis reveals that the refractory period is not necessary for
wave formation, but essential to wave synchronization, indicating separate
molecular mechanisms.
",0,1,1,0,0,0
5116,On the vanishing viscosity approximation of a nonlinear model for tumor growth,"  We investigate the dynamics of a nonlinear system modeling tumor growth with
drug application. The tumor is viewed as a mixture consisting of proliferating,
quiescent and dead cells as well as a nutrient in the presence of a drug. The
system is given by a multi-phase flow model: the densities of the different
cells are governed by a set of transport equations, the density of the nutrient
and the density of the drug are governed by rather general diffusion equations,
while the velocity of the tumor is given by Darcy's equation. The domain
occupied by the tumor in this setting is a growing continuum $\Omega$ with
boundary $\partial \Omega$ both of which evolve in time. Global-in-time weak
solutions are obtained using an approach based on the vanishing viscosity of
the Brinkman's regularization. Both the solutions and the domain are rather
general, no symmetry assumption is required and the result holds for large
initial data.
",0,0,1,0,0,0
5985,On the generation of drift flows in wall-bounded flows transiting to turbulence,"  Despite recent progress, laminar-turbulent coexistence in transitional planar
wall-bounded shear flows is still not well understood. Contrasting with the
processes by which chaotic flow inside turbulent patches is sustained at the
local (minimal flow unit) scale, the mechanisms controlling the obliqueness of
laminar-turbulent interfaces typically observed all along the coexistence range
are still mysterious. An extension of Waleffe's approach [Phys. Fluids 9 (1997)
883--900] is used to show that, already at the local scale, drift flows
breaking the problem's spanwise symmetry are generated just by slightly
detuning the modes involved in the self-sustainment process. This opens
perspectives for theorizing the formation of laminar-turbulent patterns.
",0,1,0,0,0,0
19069,Fractional Topological Elasticity and Fracton Order,"  We analyze the ""higher rank"" gauge theories, that capture some of the
phenomenology of the Fracton order. It is shown that these theories loose gauge
invariance when arbitrarily weak and smooth curvature is introduced. We propose
a resolution to this problem by introducing a theory invariant under
area-preserving diffeomorphisms, which reduce to the ""higher rank"" gauge
transformations upon linearization around a flat background. The proposed
theory is \emph{geometric} in nature and is interpreted as a theory of
\emph{fractional topological elasticity}. This theory exhibits the Fracton
phenomenology. We explore the conservation laws, topological excitations,
linear response, various kinematical constraints, and canonical structure of
the theory. Finally, we emphasize that the very structure of Riemann-Cartan
geometry, which we use to formulate the theory, encodes the Fracton
phenomenology, suggesting that the Fracton order itself is \emph{geometric} in
nature.
",0,1,0,0,0,0
11232,Bellman Gradient Iteration for Inverse Reinforcement Learning,"  This paper develops an inverse reinforcement learning algorithm aimed at
recovering a reward function from the observed actions of an agent. We
introduce a strategy to flexibly handle different types of actions with two
approximations of the Bellman Optimality Equation, and a Bellman Gradient
Iteration method to compute the gradient of the Q-value with respect to the
reward function. These methods allow us to build a differentiable relation
between the Q-value and the reward function and learn an approximately optimal
reward function with gradient methods. We test the proposed method in two
simulated environments by evaluating the accuracy of different approximations
and comparing the proposed method with existing solutions. The results show
that even with a linear reward function, the proposed method has a comparable
accuracy with the state-of-the-art method adopting a non-linear reward
function, and the proposed method is more flexible because it is defined on
observed actions instead of trajectories.
",1,0,0,0,0,0
12521,Active Learning for Accurate Estimation of Linear Models,"  We explore the sequential decision making problem where the goal is to
estimate uniformly well a number of linear models, given a shared budget of
random contexts independently sampled from a known distribution. The decision
maker must query one of the linear models for each incoming context, and
receives an observation corrupted by noise levels that are unknown, and depend
on the model instance. We present Trace-UCB, an adaptive allocation algorithm
that learns the noise levels while balancing contexts accordingly across the
different linear functions, and derive guarantees for simple regret in both
expectation and high-probability. Finally, we extend the algorithm and its
guarantees to high dimensional settings, where the number of linear models
times the dimension of the contextual space is higher than the total budget of
samples. Simulations with real data suggest that Trace-UCB is remarkably
robust, outperforming a number of baselines even when its assumptions are
violated.
",1,0,0,1,0,0
7983,Apparent and Intrinsic Evolution of Active Region Upflows,"  We analyze the evolution of Fe XII coronal plasma upflows from the edges of
ten active regions (ARs) as they cross the solar disk using the Hinode Extreme
Ultraviolet Imaging Spectrometer (EIS). Confirming the results of Demoulin et
al. (2013, Sol. Phys. 283, 341), we find that for each AR there is an observed
long term evolution of the upflows which is largely due to the solar rotation
progressively changing the viewpoint of dominantly stationary upflows. From
this projection effect, we estimate the unprojected upflow velocity and its
inclination to the local vertical. AR upflows typically fan away from the AR
core by 40 deg. to near vertical for the following polarity. The span of
inclination angles is more spread for the leading polarity with flows angled
from -29 deg. (inclined towards the AR center) to 28 deg. (directed away from
the AR). In addition to the limb-to-limb apparent evolution, we identify an
intrinsic evolution of the upflows due to coronal activity which is AR
dependent. Further, line widths are correlated with Doppler velocities only for
the few ARs having the largest velocities. We conclude that for the line widths
to be affected by the solar rotation, the spatial gradient of the upflow
velocities must be large enough such that the line broadening exceeds the
thermal line width of Fe XII. Finally, we find that upflows occurring in pairs
or multiple pairs is a common feature of ARs observed by Hinode/EIS, with up to
four pairs present in AR 11575. This is important for constraining the upflow
driving mechanism as it implies that the mechanism is not a local one occurring
over a single polarity. AR upflows originating from reconnection along
quasi-separatrix layers (QSLs) between over-pressure AR loops and neighboring
under-pressure loops is consistent with upflows occurring in pairs, unlike
other proposed mechanisms acting locally in one polarity.
",0,1,0,0,0,0
1292,Accelerated Consensus via Min-Sum Splitting,"  We apply the Min-Sum message-passing protocol to solve the consensus problem
in distributed optimization. We show that while the ordinary Min-Sum algorithm
does not converge, a modified version of it known as Splitting yields
convergence to the problem solution. We prove that a proper choice of the
tuning parameters allows Min-Sum Splitting to yield subdiffusive accelerated
convergence rates, matching the rates obtained by shift-register methods. The
acceleration scheme embodied by Min-Sum Splitting for the consensus problem
bears similarities with lifted Markov chains techniques and with multi-step
first order methods in convex optimization.
",0,0,1,0,0,0
19060,Deep Learning Approximation: Zero-Shot Neural Network Speedup,"  Neural networks offer high-accuracy solutions to a range of problems, but are
costly to run in production systems because of computational and memory
requirements during a forward pass. Given a trained network, we propose a
techique called Deep Learning Approximation to build a faster network in a tiny
fraction of the time required for training by only manipulating the network
structure and coefficients without requiring re-training or access to the
training data. Speedup is achieved by by applying a sequential series of
independent optimizations that reduce the floating-point operations (FLOPs)
required to perform a forward pass. First, lossless optimizations are applied,
followed by lossy approximations using singular value decomposition (SVD) and
low-rank matrix decomposition. The optimal approximation is chosen by weighing
the relative accuracy loss and FLOP reduction according to a single parameter
specified by the user. On PASCAL VOC 2007 with the YOLO network, we show an
end-to-end 2x speedup in a network forward pass with a 5% drop in mAP that can
be re-gained by finetuning.
",0,0,0,1,0,0
9234,Magnetic domains in thin ferromagnetic films with strong perpendicular anisotropy,"  We investigate the scaling of the ground state energy and optimal domain
patterns in thin ferromagnetic films with strong uniaxial anisotropy and the
easy axis perpendicular to the film plane. Starting from the full
three-dimensional micromagnetic model, we identify the critical scaling where
the transition from single domain to multidomain ground states such as bubble
or maze patterns occurs. Furthermore, we analyze the asymptotic behavior of the
energy in two regimes separated by a transition. In the single domain regime,
the energy $\Gamma$-converges towards a much simpler two-dimensional and local
model. In the second regime, we derive the scaling of the minimal energy and
deduce a scaling law for the typical domain size.
",0,1,1,0,0,0
378,On the nonparametric maximum likelihood estimator for Gaussian location mixture densities with application to Gaussian denoising,"  We study the Nonparametric Maximum Likelihood Estimator (NPMLE) for
estimating Gaussian location mixture densities in $d$-dimensions from
independent observations. Unlike usual likelihood-based methods for fitting
mixtures, NPMLEs are based on convex optimization. We prove finite sample
results on the Hellinger accuracy of every NPMLE. Our results imply, in
particular, that every NPMLE achieves near parametric risk (up to logarithmic
multiplicative factors) when the true density is a discrete Gaussian mixture
without any prior information on the number of mixture components. NPMLEs can
naturally be used to yield empirical Bayes estimates of the Oracle Bayes
estimator in the Gaussian denoising problem. We prove bounds for the accuracy
of the empirical Bayes estimate as an approximation to the Oracle Bayes
estimator. Here our results imply that the empirical Bayes estimator performs
at nearly the optimal level (up to logarithmic multiplicative factors) for
denoising in clustering situations without any prior knowledge of the number of
clusters.
",0,0,1,1,0,0
12130,Federated Tensor Factorization for Computational Phenotyping,"  Tensor factorization models offer an effective approach to convert massive
electronic health records into meaningful clinical concepts (phenotypes) for
data analysis. These models need a large amount of diverse samples to avoid
population bias. An open challenge is how to derive phenotypes jointly across
multiple hospitals, in which direct patient-level data sharing is not possible
(e.g., due to institutional policies). In this paper, we developed a novel
solution to enable federated tensor factorization for computational phenotyping
without sharing patient-level data. We developed secure data harmonization and
federated computation procedures based on alternating direction method of
multipliers (ADMM). Using this method, the multiple hospitals iteratively
update tensors and transfer secure summarized information to a central server,
and the server aggregates the information to generate phenotypes. We
demonstrated with real medical datasets that our method resembles the
centralized training model (based on combined datasets) in terms of accuracy
and phenotypes discovery while respecting privacy.
",1,0,0,1,0,0
3666,"Look Mum, no VM Exits! (Almost)","  Multi-core CPUs are a standard component in many modern embedded systems.
Their virtualisation extensions enable the isolation of services, and gain
popularity to implement mixed-criticality or otherwise split systems. We
present Jailhouse, a Linux-based, OS-agnostic partitioning hypervisor that uses
novel architectural approaches to combine Linux, a powerful general-purpose
system, with strictly isolated special-purpose components. Our design goals
favour simplicity over features, establish a minimal code base, and minimise
hypervisor activity.
Direct assignment of hardware to guests, together with a deferred
initialisation scheme, offloads any complex hardware handling and bootstrapping
issues from the hypervisor to the general purpose OS. The hypervisor
establishes isolated domains that directly access physical resources without
the need for emulation or paravirtualisation. This retains, with negligible
system overhead, Linux's feature-richness in uncritical parts, while frugal
safety and real-time critical workloads execute in isolated, safe domains.
",1,0,0,0,0,0
18498,The connectivity of graphs of graphs with self-loops and a given degree sequence,"  `Double edge swaps' transform one graph into another while preserving the
graph's degree sequence, and have thus been used in a number of popular Markov
chain Monte Carlo (MCMC) sampling techniques. However, while double edge-swaps
can transform, for any fixed degree sequence, any two graphs inside the classes
of simple graphs, multigraphs, and pseudographs, this is not true for graphs
which allow self-loops but not multiedges (loopy graphs). Indeed, we exactly
characterize the degree sequences where double edge swaps cannot reach every
valid loopy graph and develop an efficient algorithm to determine such degree
sequences. The same classification scheme to characterize degree sequences can
be used to prove that, for all degree sequences, loopy graphs are connected by
a combination of double and triple edge swaps. Thus, we contribute the first
MCMC sampler that uniformly samples loopy graphs with any given sequence.
",1,1,1,0,0,0
16939,Learning to Generate Music with BachProp,"  As deep learning advances, algorithms of music composition increase in
performance. However, most of the successful models are designed for specific
musical structures. Here, we present BachProp, an algorithmic composer that can
generate music scores in many styles given sufficient training data. To adapt
BachProp to a broad range of musical styles, we propose a novel representation
of music and train a deep network to predict the note transition probabilities
of a given music corpus. In this paper, new music scores generated by BachProp
are compared with the original corpora as well as with different network
architectures and other related models. We show that BachProp captures
important features of the original datasets better than other models and invite
the reader to a qualitative comparison on a large collection of generated
songs.
",1,0,0,0,0,0
15334,What drives transient behaviour in complex systems?,"  We study transient behaviour in the dynamics of complex systems described by
a set of non-linear ODE's. Destabilizing nature of transient trajectories is
discussed and its connection with the eigenvalue-based linearization procedure.
The complexity is realized as a random matrix drawn from a modified May-Wigner
model. Based on the initial response of the system, we identify a novel
stable-transient regime. We calculate exact abundances of typical and extreme
transient trajectories finding both Gaussian and Tracy-Widom distributions
known in extreme value statistics. We identify degrees of freedom driving
transient behaviour as connected to the eigenvectors and encoded in a
non-orthogonality matrix $T_0$. We accordingly extend the May-Wigner model to
contain a phase with typical transient trajectories present. An exact norm of
the trajectory is obtained in the vanishing $T_0$ limit where it describes a
normal matrix.
",0,1,0,0,0,0
15397,"Nonlinear Zeeman effect, line shapes and optical pumping in electromagnetically induced transparency","  We perform Zeeman spectroscopy on a Rydberg electromagnetically induced
transparency (EIT) system in a room-temperature Cs vapor cell, in magnetic
fields up to 50~Gauss and for several polarization configurations. The magnetic
interactions of the $\vert 6S_{1/2}, F_g=4 \rangle$ ground, $\vert 6P_{3/2},
F_e=5 \rangle$ intermediate, and $\vert 33S_{1/2} \rangle$ Rydberg states that
form the ladder-type EIT system are in the linear Zeeman, quadratic Zeeman, and
the deep hyperfine Paschen-Back regimes, respectively. Starting in magnetic
fields of about 5~Gauss, the spectra develop an asymmetry that becomes
paramount in fields $\gtrsim40$~Gauss. We use a quantum Monte Carlo
wave-function approach to quantitatively model the spectra. Simulated spectra
are in good agreement with experimental data. The asymmetry in the spectra is,
in part, due to level shifts caused by the quadratic Zeeman effect, but it also
reflects the complicated interplay between optical pumping and EIT in the
magnetic field. Relevance to measurement applications is discussed. %The
simulations are also used to study optical pumping in the magnetic field and to
investigate the interplay between optical pumping and EIT, which reduces photon
scattering and optical pumping.
",0,1,0,0,0,0
7731,Debugging Transactions and Tracking their Provenance with Reenactment,"  Debugging transactions and understanding their execution are of immense
importance for developing OLAP applications, to trace causes of errors in
production systems, and to audit the operations of a database. However,
debugging transactions is hard for several reasons: 1) after the execution of a
transaction, its input is no longer available for debugging, 2) internal states
of a transaction are typically not accessible, and 3) the execution of a
transaction may be affected by concurrently running transactions. We present a
debugger for transactions that enables non-invasive, post-mortem debugging of
transactions with provenance tracking and supports what-if scenarios (changes
to transaction code or data). Using reenactment, a declarative replay technique
we have developed, a transaction is replayed over the state of the DB seen by
its original execution including all its interactions with concurrently
executed transactions from the history. Importantly, our approach uses the
temporal database and audit logging capabilities available in many DBMS and
does not require any modifications to the underlying database system nor
transactional workload.
",1,0,0,0,0,0
13514,Separability by Piecewise Testable Languages is PTime-Complete,"  Piecewise testable languages form the first level of the Straubing-Thérien
hierarchy. The membership problem for this level is decidable and testing if
the language of a DFA is piecewise testable is NL-complete. The question has
not yet been addressed for NFAs. We fill in this gap by showing that it is
PSpace-complete. The main result is then the lower-bound complexity of
separability of regular languages by piecewise testable languages. Two regular
languages are separable by a piecewise testable language if the piecewise
testable language includes one of them and is disjoint from the other. For
languages represented by NFAs, separability by piecewise testable languages is
known to be decidable in PTime. We show that it is PTime-hard and that it
remains PTime-hard even for minimal DFAs.
",1,0,0,0,0,0
14884,Weil-Petersson geometry on the space of Bridgeland stability conditions,"  Inspired by mirror symmetry, we investigate some differential geometric
aspects of the space of Bridgeland stability conditions on a Calabi-Yau
triangulated category. The aim is to develop theory of Weil-Petersson geometry
on the stringy Kähler moduli space. A few basic examples are studied. In
particular, we identify our Weil-Petersson metric with the Bergman metric on a
Siegel modular variety in the case of the self-product of an elliptic curve.
",0,0,1,0,0,0
20310,Effects of Incomplete Ionization on Beta - Ga2O3 Power Devices: Unintentional Donor with Energy 110 meV,"  Understanding the origin of unintentional doping in Ga2O3 is key to
increasing breakdown voltages of Ga2O3 based power devices. Therefore,
transport and capacitance spectroscopy studies have been performed to better
understand the origin of unintentional doping in Ga2O3. Previously unobserved
unintentional donors in commercially available (-201) Ga2O3 substrates have
been electrically characterized via temperature dependent Hall effect
measurements up to 1000 K and found to have a donor energy of 110 meV. The
existence of the unintentional donor is confirmed by temperature dependent
admittance spectroscopy, with an activation energy of 131 meV determined via
that technique, in agreement with Hall effect measurements. With the
concentration of this donor determined to be in the mid to high 10^16 cm^-3
range, elimination of this donor from the drift layer of Ga2O3 power
electronics devices will be key to pushing the limits of device performance.
Indeed, analytical assessment of the specific on-resistance (Ronsp) and
breakdown voltage of Schottky diodes containing the 110 meV donor indicates
that incomplete ionization increases Ronsp and decreases breakdown voltage as
compared to Ga2O3 Schottky diodes containing only the shallow donor. The
reduced performance due to incomplete ionization occurs in addition to the
usual tradeoff between Ronsp and breakdown voltage. To achieve 10 kV operation
in Ga2O3 Schottky diode devices, analysis indicates that the concentration of
110 meV donors must be reduced below 5x10^14 cm^-3 to limit the increase in
Ronsp to one percent.
",0,1,0,0,0,0
11605,Zampa's systems theory: a comprehensive theory of measurement in dynamic systems,"  The article outlines in memoriam Prof. Pavel Zampa's concepts of system
theory which enable to devise a measurement in dynamic systems independently of
the particular system behaviour. From the point of view of Zampa's theory,
terms like system time, system attributes, system link, system element, input,
output, subsystems, and state variables are defined. In Conclusions, Zampa's
theory is discussed together with another mathematical approaches of
qualitative dynamics known since the 19th century. In Appendices, we present
applications of Zampa's technical approach to measurement of complex dynamical
(chemical and biological) systems at the Institute of Complex Systems,
University of South Bohemia in Ceske Budejovice.
",1,0,0,0,0,0
126,Superconductivity and Frozen Electronic States at the (111) LaAlO$_3$/SrTiO$_3$ Interface,"  In spite of Anderson's theorem, disorder is known to affect superconductivity
in conventional s-wave superconductors. In most superconductors, the degree of
disorder is fixed during sample preparation. Here we report measurements of the
superconducting properties of the two-dimensional gas that forms at the
interface between LaAlO$_3$ (LAO) and SrTiO$_3$ (STO) in the (111) crystal
orientation, a system that permits \emph{in situ} tuning of carrier density and
disorder by means of a back gate voltage $V_g$. Like the (001) oriented LAO/STO
interface, superconductivity at the (111) LAO/STO interface can be tuned by
$V_g$. In contrast to the (001) interface, superconductivity in these (111)
samples is anisotropic, being different along different interface crystal
directions, consistent with the strong anisotropy already observed other
transport properties at the (111) LAO/STO interface. In addition, we find that
the (111) interface samples ""remember"" the backgate voltage $V_F$ at which they
are cooled at temperatures near the superconducting transition temperature
$T_c$, even if $V_g$ is subsequently changed at lower temperatures. The low
energy scale and other characteristics of this memory effect ($<1$ K)
distinguish it from charge-trapping effects previously observed in (001)
interface samples.
",0,1,0,0,0,0
5235,Strict convexity of the Mabuchi functional for energy minimizers,"  There are two parts of this paper. First, we discovered an explicit formula
for the complex Hessian of the weighted log-Bergman kernel on a parallelogram
domain, and utilised this formula to give a new proof about the strict
convexity of the Mabuchi functional along a smooth geodesic. Second, when a
C^{1,1}-geodesic connects two non-degenerate energy minimizers, we also proved
this strict convexity, by showing that such a geodesic must be non-degenerate
and smooth.
",0,0,1,0,0,0
529,Failures of Gradient-Based Deep Learning,"  In recent years, Deep Learning has become the go-to solution for a broad
range of applications, often outperforming state-of-the-art. However, it is
important, for both theoreticians and practitioners, to gain a deeper
understanding of the difficulties and limitations associated with common
approaches and algorithms. We describe four types of simple problems, for which
the gradient-based algorithms commonly used in deep learning either fail or
suffer from significant difficulties. We illustrate the failures through
practical experiments, and provide theoretical insights explaining their
source, and how they might be remedied.
",1,0,0,1,0,0
10103,Constraints from Dust Mass and Mass Accretion Rate Measurements on Angular Momentum Transport in Protoplanetary Disks,"  We investigate the relation between disk mass and mass accretion rate to
constrain the mechanism of angular momentum transport in protoplanetary disks.
Dust mass and mass accretion rate in Chamaeleon I are correlated with a slope
close to linear, similar to the one recently identified in Lupus. We
investigate the effect of stellar mass and find that the intrinsic scatter
around the best-fit Mdust-Mstar and Macc-Mstar relations is uncorrelated. Disks
with a constant alpha viscosity can fit the observed relations between dust
mass, mass accretion rate, and stellar mass, but over-predict the strength of
the correlation between disk mass and mass accretion rate when using standard
initial conditions. We find two possible solutions. 1) The observed scatter in
Mdust and Macc is not primoridal, but arises from additional physical processes
or uncertainties in estimating the disk gas mass. Most likely grain growth and
radial drift affect the observable dust mass, while variability on large time
scales affects the mass accretion rates. 2) The observed scatter is primordial,
but disks have not evolved substantially at the age of Lupus and Chamaeleon I
due to a low viscosity or a large initial disk radius. More accurate estimates
of the disk mass and gas disk sizes in a large sample of protoplanetary disks,
either through direct observations of the gas or spatially resolved
multi-wavelength observations of the dust with ALMA, are needed to discriminate
between both scenarios or to constrain alternative angular momentum transport
mechanisms such as MHD disk winds.
",0,1,0,0,0,0
20170,Thicket Density,"  Thicket density is a new measure of the complexity of a set system, having
the same relationship to stable formulas that VC density has to NIP formulas.
It satisfies a Sauer-Shelah type dichotomy that has applications in both model
theory and the theory of algorithms
",0,0,1,0,0,0
11027,Shannon entropy: a study of confined hydrogenic-like atoms,"  The Shannon entropy in the atomic, molecular and chemical physics context is
presented by using as test cases the hydrogenic-like atoms $H_c$, ${He_c}^+$
and ${Li_c}^{2+}$ confined by an impenetrable spherical box. Novel expressions
for entropic uncertainty relation and Shannon entropies $S_r$ and $S_p$ are
proposed to ensure their physical dimensionless characteristic. The electronic
ground state energy and the quantities $S_r$, $S_p$ and $S_t$ are calculated
for the hydrogenic-like atoms to different confinement radii by using a
variational method. The global behavior of these quantities and different
conjectures are analyzed. The results are compared, when available, with those
previously published.
",0,1,0,0,0,0
10570,A sharpened Riesz-Sobolev inequality,"  The Riesz-Sobolev inequality provides an upper bound, in integral form, for
the convolution of indicator functions of subsets of Euclidean space. We
formulate and prove a sharper form of the inequality. This can be equivalently
phrased as a stability result, quantifying an inverse theorem of Burchard that
characterizes cases of equality.
",0,0,1,0,0,0
12415,Accelerations for Graph Isomorphism,"  In this paper, we present two main results. First, by only one conjecture
(Conjecture 2.9) for recognizing a vertex symmetric graph, which is the hardest
task for our problem, we construct an algorithm for finding an isomorphism
between two graphs in polynomial time $ O(n^{3}) $. Second, without that
conjecture, we prove the algorithm to be of quasi-polynomial time $
O(n^{1.5\log n}) $. The conjectures in this paper are correct for all graphs of
size no larger than $ 5 $ and all graphs we have encountered. At least the
conjecture for determining if a graph is vertex symmetric is quite true
intuitively. We are not able to prove them by hand, so we have planned to find
possible counterexamples by a computer. We also introduce new concepts like
collapse pattern and collapse tomography, which play important roles in our
algorithms.
",1,0,0,0,0,0
19529,Max-Pooling Loss Training of Long Short-Term Memory Networks for Small-Footprint Keyword Spotting,"  We propose a max-pooling based loss function for training Long Short-Term
Memory (LSTM) networks for small-footprint keyword spotting (KWS), with low
CPU, memory, and latency requirements. The max-pooling loss training can be
further guided by initializing with a cross-entropy loss trained network. A
posterior smoothing based evaluation approach is employed to measure keyword
spotting performance. Our experimental results show that LSTM models trained
using cross-entropy loss or max-pooling loss outperform a cross-entropy loss
trained baseline feed-forward Deep Neural Network (DNN). In addition,
max-pooling loss trained LSTM with randomly initialized network performs better
compared to cross-entropy loss trained LSTM. Finally, the max-pooling loss
trained LSTM initialized with a cross-entropy pre-trained network shows the
best performance, which yields $67.6\%$ relative reduction compared to baseline
feed-forward DNN in Area Under the Curve (AUC) measure.
",1,0,0,1,0,0
9811,HAWC Observations Strongly Favor Pulsar Interpretations of the Cosmic-Ray Positron Excess,"  Recent measurements of the Geminga and B0656+14 pulsars by the gamma-ray
telescope HAWC (along with earlier measurements by Milagro) indicate that these
objects generate significant fluxes of very high-energy electrons. In this
paper, we use the very high-energy gamma-ray intensity and spectrum of these
pulsars to calculate and constrain their expected contributions to the local
cosmic-ray positron spectrum. Among models that are capable of reproducing the
observed characteristics of the gamma-ray emission, we find that pulsars
invariably produce a flux of high-energy positrons that is similar in spectrum
and magnitude to the positron fraction measured by PAMELA and AMS-02. In light
of this result, we conclude that it is very likely that pulsars provide the
dominant contribution to the long perplexing cosmic-ray positron excess.
",0,1,0,0,0,0
3197,Counterfactual Learning for Machine Translation: Degeneracies and Solutions,"  Counterfactual learning is a natural scenario to improve web-based machine
translation services by offline learning from feedback logged during user
interactions. In order to avoid the risk of showing inferior translations to
users, in such scenarios mostly exploration-free deterministic logging policies
are in place. We analyze possible degeneracies of inverse and reweighted
propensity scoring estimators, in stochastic and deterministic settings, and
relate them to recently proposed techniques for counterfactual learning under
deterministic logging.
",1,0,0,1,0,0
18585,Generalized Uniformity Testing,"  In this work, we revisit the problem of uniformity testing of discrete
probability distributions. A fundamental problem in distribution testing,
testing uniformity over a known domain has been addressed over a significant
line of works, and is by now fully understood.
The complexity of deciding whether an unknown distribution is uniform over
its unknown (and arbitrary) support, however, is much less clear. Yet, this
task arises as soon as no prior knowledge on the domain is available, or
whenever the samples originate from an unknown and unstructured universe. In
this work, we introduce and study this generalized uniformity testing question,
and establish nearly tight upper and lower bound showing that -- quite
surprisingly -- its sample complexity significantly differs from the
known-domain case. Moreover, our algorithm is intrinsically adaptive, in
contrast to the overwhelming majority of known distribution testing algorithms.
",1,0,1,1,0,0
624,Perils of Zero-Interaction Security in the Internet of Things,"  The Internet of Things (IoT) demands authentication systems which can provide
both security and usability. Recent research utilizes the rich sensing
capabilities of smart devices to build security schemes operating without human
interaction, such as zero-interaction pairing (ZIP) and zero-interaction
authentication (ZIA). Prior work proposed a number of ZIP and ZIA schemes and
reported promising results. However, those schemes were often evaluated under
conditions which do not reflect realistic IoT scenarios. In addition, drawing
any comparison among the existing schemes is impossible due to the lack of a
common public dataset and unavailability of scheme implementations.
In this paper, we address these challenges by conducting the first
large-scale comparative study of ZIP and ZIA schemes, carried out under
realistic conditions. We collect and release the most comprehensive dataset in
the domain to date, containing over 4250 hours of audio recordings and 1
billion sensor readings from three different scenarios, and evaluate five
state-of-the-art schemes based on these data. Our study reveals that the
effectiveness of the existing proposals is highly dependent on the scenario
they are used in. In particular, we show that these schemes are subject to
error rates between 0.6% and 52.8%.
",1,0,0,0,0,0
17883,Unsupervised robotic sorting: Towards autonomous decision making robots,"  Autonomous sorting is a crucial task in industrial robotics which can be very
challenging depending on the expected amount of automation. Usually, to decide
where to sort an object, the system needs to solve either an instance retrieval
(known object) or a supervised classification (predefined set of classes)
problem. In this paper, we introduce a new decision making module, where the
robotic system chooses how to sort the objects in an unsupervised way. We call
this problem Unsupervised Robotic Sorting (URS) and propose an implementation
on an industrial robotic system, using deep CNN feature extraction and standard
clustering algorithms. We carry out extensive experiments on various standard
datasets to demonstrate the efficiency of the proposed image clustering
pipeline. To evaluate the robustness of our URS implementation, we also
introduce a complex real world dataset containing images of objects under
various background and lighting conditions. This dataset is used to fine tune
the design choices (CNN and clustering algorithm) for URS. Finally, we propose
a method combining our pipeline with ensemble clustering to use multiple images
of each object. This redundancy of information about the objects is shown to
increase the clustering results.
",1,0,0,0,0,0
14281,Convolutional Neural Networks for Page Segmentation of Historical Document Images,"  This paper presents a Convolutional Neural Network (CNN) based page
segmentation method for handwritten historical document images. We consider
page segmentation as a pixel labeling problem, i.e., each pixel is classified
as one of the predefined classes. Traditional methods in this area rely on
carefully hand-crafted features or large amounts of prior knowledge. In
contrast, we propose to learn features from raw image pixels using a CNN. While
many researchers focus on developing deep CNN architectures to solve different
problems, we train a simple CNN with only one convolution layer. We show that
the simple architecture achieves competitive results against other deep
architectures on different public datasets. Experiments also demonstrate the
effectiveness and superiority of the proposed method compared to previous
methods.
",1,0,0,1,0,0
3932,New indicators for assessing the quality of in silico produced biomolecules: the case study of the aptamer-Angiopoietin-2 complex,"  Computational procedures to foresee the 3D structure of aptamers are in
continuous progress. They constitute a crucial input to research, mainly when
the crystallographic counterpart of the structures in silico produced is not
present. At now, many codes are able to perform structure and binding
prediction, although their ability in scoring the results remains rather weak.
In this paper, we propose a novel procedure to complement the ranking outcomes
of free docking code, by applying it to a set of anti-angiopoietin aptamers,
whose performances are known. We rank the in silico produced configurations,
adopting a maximum likelihood estimate, based on their topological and
electrical properties. From the analysis, two principal kinds of conformers are
identified, whose ability to mimick the binding features of the natural
receptor is discussed. The procedure is easily generalizable to many biological
biomolecules, useful for increasing chances of success in designing
high-specificity biosensors (aptasensors).
",0,0,0,0,1,0
9188,Maximum a Posteriori Policy Optimisation,"  We introduce a new algorithm for reinforcement learning called Maximum
aposteriori Policy Optimisation (MPO) based on coordinate ascent on a relative
entropy objective. We show that several existing methods can directly be
related to our derivation. We develop two off-policy algorithms and demonstrate
that they are competitive with the state-of-the-art in deep reinforcement
learning. In particular, for continuous control, our method outperforms
existing methods with respect to sample efficiency, premature convergence and
robustness to hyperparameter settings while achieving similar or better final
performance.
",1,0,0,1,0,0
7323,From Pragmatic to Systematic Software Process Improvement: An Evaluated Approach,"  Software processes improvement (SPI) is a challenging task, as many different
stakeholders, project settings, and contexts and goals need to be considered.
SPI projects are often operated in a complex and volatile environment and,
thus, require a sound management that is resource-intensive requiring many
stakeholders to contribute to the process assessment, analysis, design,
realisation, and deployment. Although there exist many valuable SPI approaches,
none address the needs of both process engineers and project managers. This
article presents an Artefact-based Software Process Improvement & Management
approach (ArSPI) that closes this gap. ArSPI was developed and tested across
several SPI projects in large organisations in Germany and Eastern Europe. The
approach further encompasses a template for initiating, performing, and
managing SPI projects by defining a set of 5 key artefacts and 24 support
artefacts. We present ArSPI and discus results of its validation indicating
ArSPI to be a helpful instrument to set up and steer SPI projects.
",1,0,0,0,0,0
1349,Shape and Energy Consistent Pseudopotentials for Correlated Electron systems,"  A method is developed for generating pseudopotentials for use in
correlated-electron calculations. The paradigms of shape and energy consistency
are combined and defined in terms of correlated-electron wave-functions. The
resulting energy consistent correlated electron pseudopotentials (eCEPPs) are
constructed for H, Li--F, Sc--Fe, and Cu. Their accuracy is quantified by
comparing the relaxed molecular geometries and dissociation energies they
provide with all electron results, with all quantities evaluated using coupled
cluster singles doubles and triples calculations. Errors inherent in the
pseudopotentials are also compared with those arising from a number of
approximations commonly used with pseudopotentials. The eCEPPs provide a
significant improvement in optimised geometries and dissociation energies for
small molecules, with errors for the latter being an order-of-magnitude smaller
than for Hartree-Fock-based pseudopotentials available in the literature.
Gaussian basis sets are optimised for use with these pseudopotentials.
",0,1,0,0,0,0
3865,Automatic Trimap Generation for Image Matting,"  Image matting is a longstanding problem in computational photography.
Although, it has been studied for more than two decades, yet there is a
challenge of developing an automatic matting algorithm which does not require
any human efforts. Most of the state-of-the-art matting algorithms require
human intervention in the form of trimap or scribbles to generate the alpha
matte form the input image. In this paper, we present a simple and efficient
approach to automatically generate the trimap from the input image and make the
whole matting process free from human-in-the-loop. We use learning based
matting method to generate the matte from the automatically generated trimap.
Experimental results demonstrate that our method produces good quality trimap
which results into accurate matte estimation. We validate our results by
replacing the automatically generated trimap by manually created trimap while
using the same image matting algorithm.
",1,0,0,0,0,0
12717,"The World's First Real-Time Testbed for Massive MIMO: Design, Implementation, and Validation","  This paper sets up a framework for designing a massive multiple-input
multiple-output (MIMO) testbed by investigating hardware (HW) and system-level
requirements such as processing complexity, duplexing mode and frame structure.
Taking these into account, a generic system and processing partitioning is
proposed which allows flexible scaling and processing distribution onto a
multitude of physically separated devices. Based on the given HW constraints
such as maximum number of links and maximum throughput for peer-to-peer
interconnections combined with processing capabilities, the framework allows to
evaluate modular HW components. To verify our design approach, we present the
LuMaMi (Lund University Massive MIMO) testbed which constitutes the first
reconfigurable real-time HW platform for prototyping massive MIMO. Utilizing up
to 100 base station antennas and more than 50 Field Programmable Gate Arrays,
up to 12 user equipments are served on the same time/frequency resource using
an LTE-like Orthogonal Frequency Division Multiplexing time-division
duplex-based transmission scheme. Proof-of-concept tests with this system show
that massive MIMO can simultaneously serve a multitude of users in a static
indoor and static outdoor environment utilizing the same time/frequency
resource.
",1,0,1,0,0,0
15168,Max flow vitality in general and $st$-planar graphs,"  The \emph{vitality} of an arc/node of a graph with respect to the maximum
flow between two fixed nodes $s$ and $t$ is defined as the reduction of the
maximum flow caused by the removal of that arc/node. In this paper we address
the issue of determining the vitality of arcs and/or nodes for the maximum flow
problem. We show how to compute the vitality of all arcs in a general
undirected graph by solving only $2(n-1)$ max flow instances and, In
$st$-planar graphs (directed or undirected) we show how to compute the vitality
of all arcs and all nodes in $O(n)$ worst-case time. Moreover, after
determining the vitality of arcs and/or nodes, and given a planar embedding of
the graph, we can determine the vitality of a `contiguous' set of arcs/nodes in
time proportional to the size of the set.
",1,0,0,0,0,0
2202,Characterization of Thermal Neutron Beam Monitors,"  Neutron beam monitors with high efficiency, low gamma sensitivity, high time
and space resolution are required in neutron beam experiments to continuously
diagnose the delivered beam. In this work, commercially available neutron beam
monitors have been characterized using the R2D2 beamline at IFE (Norway) and
using a Be-based neutron source. For the gamma sensitivity measurements
different gamma sources have been used. The evaluation of the monitors
includes, the study of their efficiency, attenuation, scattering and
sensitivity to gamma. In this work we report the results of this
characterization.
",0,1,0,0,0,0
843,Dynamic Security Analysis of Power Systems by a Sampling-Based Algorithm,"  Dynamic security analysis is an important problem of power systems on
ensuring safe operation and stable power supply even when certain faults occur.
No matter such faults are caused by vulnerabilities of system components,
physical attacks, or cyber-attacks that are more related to cyber-security,
they eventually affect the physical stability of a power system. Examples of
the loss of physical stability include the Northeast blackout of 2003 in North
America and the 2015 system-wide blackout in Ukraine. The nonlinear hybrid
nature, that is, nonlinear continuous dynamics integrated with discrete
switching, and the high degree of freedom property of power system dynamics
make it challenging to conduct the dynamic security analysis. In this paper, we
use the hybrid automaton model to describe the dynamics of a power system and
mainly deal with the index-1 differential-algebraic equation models regarding
the continuous dynamics in different discrete states. The analysis problem is
formulated as a reachability problem of the associated hybrid model. A
sampling-based algorithm is then proposed by integrating modeling and
randomized simulation of the hybrid dynamics to search for a feasible execution
connecting an initial state of the post-fault system and a target set in the
desired operation mode. The proposed method enables the use of existing power
system simulators for the synthesis of discrete switching and control
strategies through randomized simulation. The effectiveness and performance of
the proposed approach are demonstrated with an application to the dynamic
security analysis of the New England 39-bus benchmark power system exhibiting
hybrid dynamics. In addition to evaluating the dynamic security, the proposed
method searches for a feasible strategy to ensure the dynamic security of the
system in face of disruptions.
",1,0,0,0,0,0
19710,Two weight bump conditions for matrix weights,"  In this paper we extend the theory of two weight, $A_p$ bump conditions to
the setting of matrix weights. We prove two matrix weight inequalities for
fractional maximal operators, fractional and singular integrals, sparse
operators and averaging operators. As applications we prove quantitative, one
weight estimates, in terms of the matrix $A_p$ constant, for singular
integrals, and prove a Poincaré inequality related to those that appear in
the study of degenerate elliptic PDEs.
",0,0,1,0,0,0
2780,Comparing Classical and Relativistic Kinematics in First-Order Logic,"  The aim of this paper is to present a new logic-based understanding of the
connection between classical kinematics and relativistic kinematics. We show
that the axioms of special relativity can be interpreted in the language of
classical kinematics. This means that there is a logical translation function
from the language of special relativity to the language of classical kinematics
which translates the axioms of special relativity into consequences of
classical kinematics. We will also show that if we distinguish a class of
observers (representing observers stationary with respect to the ""Ether"") in
special relativity and exclude the non-slower-than light observers from
classical kinematics by an extra axiom, then the two theories become
definitionally equivalent (i.e., they become equivalent theories in the sense
as the theory of lattices as algebraic structures is the same as the theory of
lattices as partially ordered sets). Furthermore, we show that classical
kinematics is definitionally equivalent to classical kinematics with only
slower-than-light inertial observers, and hence by transitivity of definitional
equivalence that special relativity theory extended with ""Ether"" is
definitionally equivalent to classical kinematics. So within an axiomatic
framework of mathematical logic, we explicitly show that the transition from
classical kinematics to relativistic kinematics is the knowledge acquisition
that there is no ""Ether"", accompanied by a redefinition of the concepts of time
and space.
",0,0,1,0,0,0
11417,Predicting Tactical Solutions to Operational Planning Problems under Imperfect Information,"  This paper offers a methodological contribution at the intersection of
machine learning and operations research. Namely, we propose a methodology to
quickly predict tactical solutions to a given operational problem. In this
context, the tactical solution is less detailed than the operational one but it
has to be computed in very short time and under imperfect information. The
problem is of importance in various applications where tactical and operational
planning problems are interrelated and information about the operational
problem is revealed over time. This is for instance the case in certain
capacity planning and demand management systems.
We formulate the problem as a two-stage optimal prediction stochastic program
whose solution we predict with a supervised machine learning algorithm. The
training data set consists of a large number of deterministic (second stage)
problems generated by controlled probabilistic sampling. The labels are
computed based on solutions to the deterministic problems (solved independently
and offline) employing appropriate aggregation and subselection methods to
address uncertainty. Results on our motivating application in load planning for
rail transportation show that deep learning algorithms produce highly accurate
predictions in very short computing time (milliseconds or less). The prediction
accuracy is comparable to solutions computed by sample average approximation of
the stochastic program.
",1,0,0,1,0,0
1382,Output Impedance Diffusion into Lossy Power Lines,"  Output impedances are inherent elements of power sources in the electrical
grids. In this paper, we give an answer to the following question: What is the
effect of output impedances on the inductivity of the power network? To address
this question, we propose a measure to evaluate the inductivity of a power
grid, and we compute this measure for various types of output impedances.
Following this computation, it turns out that network inductivity highly
depends on the algebraic connectivity of the network. By exploiting the derived
expressions of the proposed measure, one can tune the output impedances in
order to enforce a desired level of inductivity on the power system.
Furthermore, the results show that the more ""connected"" the network is, the
more the output impedances diffuse into the network. Finally, using Kron
reduction, we provide examples that demonstrate the utility and validity of the
method.
",1,0,0,0,0,0
18885,Anisotropic hydrodynamic turbulence in accretion disks,"  Recently, the vertical shear instability (VSI) has become an attractive
purely hydrodynamic candidate for the anomalous angular momentum transport
required for weakly ionized accretion disks. In direct three-dimensional
numerical simulations of VSI turbulence in disks, a meridional circulation
pattern was observed that is opposite to the usual viscous flow behavior. Here,
we investigate whether this feature can possibly be explained by an anisotropy
of the VSI turbulence. Using three-dimensional hydrodynamical simulations, we
calculate the turbulent Reynolds stresses relevant for angular momentum
transport for a representative section of a disk.
We find that the vertical stress is significantly stronger than the radial
stress. Using our results in viscous disk simulations with different viscosity
coefficients for the radial and vertical direction, we find good agreement with
the VSI turbulence for the stresses and meridional flow; this provides
additional evidence for the anisotropy. The results are important with respect
to the transport of small embedded particles in disks.
",0,1,0,0,0,0
9239,How does the accuracy of interatomic force constants affect the prediction of lattice thermal conductivity,"  Solving Peierls-Boltzmann transport equation with interatomic force constants
(IFCs) from first-principles calculations has been a widely used method for
predicting lattice thermal conductivity of three-dimensional materials. With
the increasing research interests in two-dimensional materials, this method is
directly applied to them but different works show quite different results. In
this work, classical potential was used to investigate the effect of the
accuracy of IFCs on the predicted thermal conductivity. Inaccuracies were
introduced to the third-order IFCs by generating errors in the input forces.
When the force error lies in the typical value from first-principles
calculations, the calculated thermal conductivity would be quite different from
the benchmark value. It is found that imposing translational invariance
conditions cannot always guarantee a better thermal conductivity result. It is
also shown that Grüneisen parameters cannot be used as a necessary and
sufficient criterion for the accuracy of third-order IFCs in the aspect of
predicting thermal conductivity.
",0,1,0,0,0,0
7737,A space-time finite element method for neural field equations with transmission delays,"  We present and analyze a new space-time finite element method for the
solution of neural field equations with transmission delays. The numerical
treatment of these systems is rare in the literature and currently has several
restrictions on the spatial domain and the functions involved, such as
connectivity and delay functions. The use of a space-time discretization, with
basis functions that are discontinuous in time and continuous in space
(dGcG-FEM), is a natural way to deal with space-dependent delays, which is
important for many neural field applications. In this article we provide a
detailed description of a space-time dGcG-FEM algorithm for neural delay
equations, including an a-priori error analysis. We demonstrate the application
of the dGcG-FEM algorithm on several neural field models, including problems
with an inhomogeneous kernel.
",0,0,1,0,0,0
12468,Multiplicities of Character Values of Binary Sidel'nikov-Lempel-Cohn-Eastman Sequences,"  Binary Sidel'nikov-Lempel-Cohn-Eastman sequences (or SLCE sequences) over F 2
have even period and almost perfect autocorrelation. However, the evaluation of
the linear complexity of these sequences is really difficult. In this paper, we
continue the study of [1]. We first express the multiple roots of character
polynomials of SLCE sequences into certain kinds of Jacobi sums. Then by making
use of Gauss sums and Jacobi sums in the ""semiprimitive"" case, we derive new
divisibility results for SLCE sequences.
",1,0,1,0,0,0
9746,On MASAs in $q$-deformed von Neumann algebras,"  We study certain $q$-deformed analogues of the maximal abelian subalgebras of
the group von Neumann algebras of free groups. The radial subalgebra is defined
for Hecke deformed von Neumann algebras of the Coxeter group
$(\mathbb{Z}/{2\mathbb{Z}})^{\star k}$ and shown to be a maximal abelian
subalgebra which is singular and with Pukánszky invariant $\{\infty\}$.
Further all non-equal generator masas in the $q$-deformed Gaussian von Neumann
algebras are shown to be mutually non-unitarily conjugate.
",0,0,1,0,0,0
1192,Repair Strategies for Storage on Mobile Clouds,"  We study the data reliability problem for a community of devices forming a
mobile cloud storage system. We consider the application of regenerating codes
for file maintenance within a geographically-limited area. Such codes require
lower bandwidth to regenerate lost data fragments compared to file replication
or reconstruction. We investigate threshold-based repair strategies where data
repair is initiated after a threshold number of data fragments have been lost
due to node mobility. We show that at a low departure-to-repair rate regime, a
lazy repair strategy in which repairs are initiated after several nodes have
left the system outperforms eager repair in which repairs are initiated after a
single departure. This optimality is reversed when nodes are highly mobile. We
further compare distributed and centralized repair strategies and derive the
optimal repair threshold for minimizing the average repair cost per unit of
time, as a function of underlying code parameters. In addition, we examine
cooperative repair strategies and show performance improvements compared to
non-cooperative codes. We investigate several models for the time needed for
node repair including a simple fixed time model that allows for the computation
of closed-form expressions and a more realistic model that takes into account
the number of repaired nodes. We derive the conditions under which the former
model approximates the latter. Finally, an extended model where additional
failures are allowed during the repair process is investigated. Overall, our
results establish the joint effect of code design and repair algorithms on the
maintenance cost of distributed storage systems.
",1,0,0,0,0,0
14419,Molecular simulations of entangled defect structures around nanoparticles in nematic liquid crystals,"  We investigate the defect structures forming around two nanoparticles in a
Gay-Berne nematic liquid crystal using molecular simulations. For small
separations, disclinations entangle both particles forming the figure of eight,
the figure of omega and the figure of theta. These defect structures are
similar in shape and occur with a comparable frequency to micron-sized
particles studied in experiments. The simulations reveal fast transitions from
one defect structure to another suggesting that particles of nanometre size
cannot be bound together effectively. We identify the 'three-ring' structure
observed in previous molecular simulations as a superposition of the different
entangled and non-entangled states over time and conclude that it is not itself
a stable defect structure.
",0,1,0,0,0,0
17425,Dynamics of homogeneous shear turbulence: A key role of the nonlinear transverse cascade in the bypass concept,"  To understand the self-sustenance of subcritical turbulence in spectrally
stable shear flows, we performed direct numerical simulations of homogeneous
shear turbulence for different aspect ratios of the flow domain and analyzed
the dynamical processes in Fourier space. There are no exponentially growing
modes in such flows and the turbulence is energetically supported only by the
linear growth of perturbation harmonics due to the shear flow non-normality.
This non-normality-induced, or nonmodal growth is anisotropic in spectral
space, which, in turn, leads to anisotropy of nonlinear processes in this
space. As a result, a transverse (angular) redistribution of harmonics in
Fourier space appears to be the main nonlinear process in these flows, rather
than direct or inverse cascades. We refer to this type of nonlinear
redistribution as the nonlinear transverse cascade. It is demonstrated that the
turbulence is sustained by a subtle interplay between the linear nonmodal
growth and the nonlinear transverse cascade that exemplifies a well-known
bypass scenario of subcritical turbulence. These two basic processes mainly
operate at large length scales, comparable to the domain size. Therefore, this
central, small wave number area of Fourier space is crucial in the
self-sustenance; we defined its size and labeled it as the vital area of
turbulence. Outside the vital area, the nonmodal growth and the transverse
cascade are of secondary importance. Although the cascades and the
self-sustaining process of turbulence are qualitatively the same at different
aspect ratios, the number of harmonics actively participating in this process
varies, but always remains quite large. This implies that the self-sustenance
of subcritical turbulence cannot be described by low-order models.
",0,1,0,0,0,0
3980,Convergence Results for Neural Networks via Electrodynamics,"  We study whether a depth two neural network can learn another depth two
network using gradient descent. Assuming a linear output node, we show that the
question of whether gradient descent converges to the target function is
equivalent to the following question in electrodynamics: Given $k$ fixed
protons in $\mathbb{R}^d,$ and $k$ electrons, each moving due to the attractive
force from the protons and repulsive force from the remaining electrons,
whether at equilibrium all the electrons will be matched up with the protons,
up to a permutation. Under the standard electrical force, this follows from the
classic Earnshaw's theorem. In our setting, the force is determined by the
activation function and the input distribution. Building on this equivalence,
we prove the existence of an activation function such that gradient descent
learns at least one of the hidden nodes in the target network. Iterating, we
show that gradient descent can be used to learn the entire network one node at
a time.
",1,1,0,0,0,0
1619,Composite Behavioral Modeling for Identity Theft Detection in Online Social Networks,"  In this work, we aim at building a bridge from poor behavioral data to an
effective, quick-response, and robust behavior model for online identity theft
detection. We concentrate on this issue in online social networks (OSNs) where
users usually have composite behavioral records, consisting of
multi-dimensional low-quality data, e.g., offline check-ins and online user
generated content (UGC). As an insightful result, we find that there is a
complementary effect among different dimensions of records for modeling users'
behavioral patterns. To deeply exploit such a complementary effect, we propose
a joint model to capture both online and offline features of a user's composite
behavior. We evaluate the proposed joint model by comparing with some typical
models on two real-world datasets: Foursquare and Yelp. In the widely-used
setting of theft simulation (simulating thefts via behavioral replacement), the
experimental results show that our model outperforms the existing ones, with
the AUC values $0.956$ in Foursquare and $0.947$ in Yelp, respectively.
Particularly, the recall (True Positive Rate) can reach up to $65.3\%$ in
Foursquare and $72.2\%$ in Yelp with the corresponding disturbance rate (False
Positive Rate) below $1\%$. It is worth mentioning that these performances can
be achieved by examining only one composite behavior (visiting a place and
posting a tip online simultaneously) per authentication, which guarantees the
low response latency of our method. This study would give the cybersecurity
community new insights into whether and how a real-time online identity
authentication can be improved via modeling users' composite behavioral
patterns.
",1,0,0,0,0,0
1049,Spin Distribution of Primordial Black Holes,"  We estimate the spin distribution of primordial black holes based on the
recent study of the critical phenomena in the gravitational collapse of a
rotating radiation fluid. We find that primordial black holes are mostly slowly
rotating.
",0,1,0,0,0,0
10709,A First Principle Study on Iron Substituted LiNi(BO3) to use as Cathode Material for Li-ion Batteries,"  In this work, the structural stability and the electronic properties of
LiNiBO 3 and LiFe x Ni (1-x) BO 3 are studied using first principle
calculations based on density functional theory. The calculated structural
parameters are in good agreement with the available theoretical data. The most
stable phases of the Fe substituted systems are predicted from the formation
energy hull generated using the cluster expansion method. The 66% of Fe
substitution at the Ni site gives the most stable structure among all the Fe
substituted systems. The bonding mechanisms of the considered systems are
discussed based on the density of states (DOS) and charge density plot. The
detailed analysis of the stability, electronic structure, and the bonding
mechanisms suggests that the systems can be a promising cathode material for Li
ion battery applications.
",0,1,0,0,0,0
1827,Lightweight Multilingual Software Analysis,"  Developer preferences, language capabilities and the persistence of older
languages contribute to the trend that large software codebases are often
multilingual, that is, written in more than one computer language. While
developers can leverage monolingual software development tools to build
software components, companies are faced with the problem of managing the
resultant large, multilingual codebases to address issues with security,
efficiency, and quality metrics. The key challenge is to address the opaque
nature of the language interoperability interface: one language calling
procedures in a second (which may call a third, or even back to the first),
resulting in a potentially tangled, inefficient and insecure codebase. An
architecture is proposed for lightweight static analysis of large multilingual
codebases: the MLSA architecture. Its modular and table-oriented structure
addresses the open-ended nature of multiple languages and language
interoperability APIs. We focus here as an application on the construction of
call-graphs that capture both inter-language and intra-language calls. The
algorithms for extracting multilingual call-graphs from codebases are
presented, and several examples of multilingual software engineering analysis
are discussed. The state of the implementation and testing of MLSA is
presented, and the implications for future work are discussed.
",1,0,0,0,0,0
18442,Forecasting the magnitude and onset of El Nino based on climate network,"  El Nino is probably the most influential climate phenomenon on interannual
time scales. It affects the global climate system and is associated with
natural disasters and serious consequences in many aspects of human life.
However, the forecasting of the onset and in particular the magnitude of El
Nino are still not accurate, at least more than half a year in advance. Here,
we introduce a new forecasting index based on network links representing the
similarity of low frequency temporal temperature anomaly variations between
different sites in the El Nino 3.4 region. We find that significant upward
trends and peaks in this index forecast with high accuracy both the onset and
magnitude of El Nino approximately 1 year ahead. The forecasting procedure we
developed improves in particular the prediction of the magnitude of El Nino and
is validated based on several, up to more than a century long, datasets.
",0,1,0,0,0,0
1134,Deep Multimodal Subspace Clustering Networks,"  We present convolutional neural network (CNN) based approaches for
unsupervised multimodal subspace clustering. The proposed framework consists of
three main stages - multimodal encoder, self-expressive layer, and multimodal
decoder. The encoder takes multimodal data as input and fuses them to a latent
space representation. The self-expressive layer is responsible for enforcing
the self-expressiveness property and acquiring an affinity matrix corresponding
to the data points. The decoder reconstructs the original input data. The
network uses the distance between the decoder's reconstruction and the original
input in its training. We investigate early, late and intermediate fusion
techniques and propose three different encoders corresponding to them for
spatial fusion. The self-expressive layers and multimodal decoders are
essentially the same for different spatial fusion-based approaches. In addition
to various spatial fusion-based methods, an affinity fusion-based network is
also proposed in which the self-expressive layer corresponding to different
modalities is enforced to be the same. Extensive experiments on three datasets
show that the proposed methods significantly outperform the state-of-the-art
multimodal subspace clustering methods.
",0,0,0,1,0,0
11805,Imbedding results in Musielak-Orlicz spaces with an application to anisotropic nonlinear Neumann problems,"  We prove a continuous embedding that allows us to obtain a boundary trace
imbedding result for anisotropic Musielak-Orlicz spaces, which we then apply to
obtain an existence result for Neumann problems with nonlinearities on the
boundary associated to some anisotropic nonlinear elliptic equations in
Musielak-Orlicz spaces constructed from Musielak-Orlicz functions on which and
on their conjugates we do not assume the $\Delta_2$-condition. The uniqueness
is also studied.
",0,0,1,0,0,0
14935,Network growth models: A behavioural basis for attachment proportional to fitness,"  Several growth models have been proposed in the literature for scale-free
complex networks, with a range of fitness-based attachment models gaining
prominence recently. However, the processes by which such fitness-based
attachment behaviour can arise are less well understood, making it difficult to
compare the relative merits of such models. This paper analyses an evolutionary
mechanism that would give rise to a fitness-based attachment process. In
particular, it is proven by analytical and numerical methods that in
homogeneous networks, the minimisation of maximum exposure to node unfitness
leads to attachment probabilities that are proportional to node fitness. This
result is then extended to heterogeneous networks, with supply chain networks
being used as an example.
",1,1,0,0,0,0
8518,Two-level Chebyshev filter based complementary subspace method: pushing the envelope of large-scale electronic structure calculations,"  We describe a novel iterative strategy for Kohn-Sham density functional
theory calculations aimed at large systems (> 1000 electrons), applicable to
metals and insulators alike. In lieu of explicit diagonalization of the
Kohn-Sham Hamiltonian on every self-consistent field (SCF) iteration, we employ
a two-level Chebyshev polynomial filter based complementary subspace strategy
to: 1) compute a set of vectors that span the occupied subspace of the
Hamiltonian; 2) reduce subspace diagonalization to just partially occupied
states; and 3) obtain those states in an efficient, scalable manner via an
inner Chebyshev-filter iteration. By reducing the necessary computation to just
partially occupied states, and obtaining these through an inner Chebyshev
iteration, our approach reduces the cost of large metallic calculations
significantly, while eliminating subspace diagonalization for insulating
systems altogether. We describe the implementation of the method within the
framework of the Discontinuous Galerkin (DG) electronic structure method and
show that this results in a computational scheme that can effectively tackle
bulk and nano systems containing tens of thousands of electrons, with chemical
accuracy, within a few minutes or less of wall clock time per SCF iteration on
large-scale computing platforms. We anticipate that our method will be
instrumental in pushing the envelope of large-scale ab initio molecular
dynamics. As a demonstration of this, we simulate a bulk silicon system
containing 8,000 atoms at finite temperature, and obtain an average SCF step
wall time of 51 seconds on 34,560 processors; thus allowing us to carry out 1.0
ps of ab initio molecular dynamics in approximately 28 hours (of wall time).
",0,1,0,0,0,0
9883,Master equation for She-Leveque scaling and its classification in terms of other Markov models of developed turbulence,"  We derive the Markov process equivalent to She-Leveque scaling in homogeneous
and isotropic turbulence. The Markov process is a jump process for velocity
increments $u(r)$ in scale $r$ in which the jumps occur randomly but with
deterministic width in $u$. From its master equation we establish a
prescription to simulate the She-Leveque process and compare it with Kolmogorov
scaling. To put the She-Leveque process into the context of other established
turbulence models on the Markov level, we derive a diffusion process for $u(r)$
from two properties of the Navier-Stokes equation. This diffusion process
already includes Kolmogorov scaling, extended self-similarity and a class of
random cascade models. The fluctuation theorem of this Markov process implies a
""second law"" that puts a loose bound on the multipliers of the random cascade
models. This bound explicitly allows for inverse cascades, which are necessary
to satisfy the fluctuation theorem. By adding a jump process to the diffusion
process, we go beyond Kolmogorov scaling and formulate the most general scaling
law for the class of Markov processes having both diffusion and jump parts.
This Markov scaling law includes She-Leveque scaling and a scaling law derived
by Yakhot.
",0,1,0,0,0,0
13805,Poverty Prediction with Public Landsat 7 Satellite Imagery and Machine Learning,"  Obtaining detailed and reliable data about local economic livelihoods in
developing countries is expensive, and data are consequently scarce. Previous
work has shown that it is possible to measure local-level economic livelihoods
using high-resolution satellite imagery. However, such imagery is relatively
expensive to acquire, often not updated frequently, and is mainly available for
recent years. We train CNN models on free and publicly available multispectral
daytime satellite images of the African continent from the Landsat 7 satellite,
which has collected imagery with global coverage for almost two decades. We
show that despite these images' lower resolution, we can achieve accuracies
that exceed previous benchmarks.
",1,0,0,1,0,0
1672,The Structure Transfer Machine Theory and Applications,"  Representation learning is a fundamental but challenging problem, especially
when the distribution of data is unknown. We propose a new representation
learning method, termed Structure Transfer Machine (STM), which enables feature
learning process to converge at the representation expectation in a
probabilistic way. We theoretically show that such an expected value of the
representation (mean) is achievable if the manifold structure can be
transferred from the data space to the feature space. The resulting structure
regularization term, named manifold loss, is incorporated into the loss
function of the typical deep learning pipeline. The STM architecture is
constructed to enforce the learned deep representation to satisfy the intrinsic
manifold structure from the data, which results in robust features that suit
various application scenarios, such as digit recognition, image classification
and object tracking. Compared to state-of-the-art CNN architectures, we achieve
the better results on several commonly used benchmarks\footnote{The source code
is available. this https URL }.
",0,0,0,1,0,0
10114,Integral representation of shallow neural network that attains the global minimum,"  We consider the supervised learning problem with shallow neural networks.
According to our unpublished experiments conducted several years prior to this
study, we had noticed an interesting similarity between the distribution of
hidden parameters after backprobagation (BP) training, and the ridgelet
spectrum of the same dataset. Therefore, we conjectured that the distribution
is expressed as a version of ridgelet transform, but it was not proven until
this study. One difficulty is that both the local minimizers and the ridgelet
transforms have an infinite number of varieties, and no relations are known
between them. By using the integral representation, we reformulate the BP
training as a strong-convex optimization problem and find a global minimizer.
Finally, by developing ridgelet analysis on a reproducing kernel Hilbert space
(RKHS), we write the minimizer explicitly and succeed to prove the conjecture.
The modified ridgelet transform has an explicit expression that can be computed
by numerical integration, which suggests that we can obtain the global
minimizer of BP, without BP.
",0,0,0,1,0,0
12818,Ambiguity set and learning via Bregman and Wasserstein,"  Construction of ambiguity set in robust optimization relies on the choice of
divergences between probability distributions. In distribution learning,
choosing appropriate probability distributions based on observed data is
critical for approximating the true distribution. To improve the performance of
machine learning models, there has recently been interest in designing
objective functions based on Lp-Wasserstein distance rather than the classical
Kullback-Leibler (KL) divergence. In this paper, we derive concentration and
asymptotic results using Bregman divergence. We propose a novel asymmetric
statistical divergence called Wasserstein-Bregman divergence as a
generalization of L2-Wasserstein distance. We discuss how these results can be
applied to the construction of ambiguity set in robust optimization.
",1,0,0,1,0,0
12212,Mean field repulsive Kuramoto models: Phase locking and spatial signs,"  The phenomenon of self-synchronization in populations of oscillatory units
appears naturally in neurosciences. However, in some situations, the formation
of a coherent state is damaging. In this article we study a repulsive
mean-field Kuramoto model that describes the time evolution of n points on the
unit circle, which are transformed into incoherent phase-locked states. It has
been recently shown that such systems can be reduced to a three-dimensional
system of ordinary differential equations, whose mathematical structure is
strongly related to hyperbolic geometry. The orbits of the Kuramoto dynamical
system are then described by a ow of Möbius transformations. We show this
underlying dynamic performs statistical inference by computing dynamically
M-estimates of scatter matrices. We also describe the limiting phase-locked
states for random initial conditions using Tyler's transformation matrix.
Moreover, we show the repulsive Kuramoto model performs dynamically not only
robust covariance matrix estimation, but also data processing: the initial
configuration of the n points is transformed by the dynamic into a limiting
phase-locked state that surprisingly equals the spatial signs from
nonparametric statistics. That makes the sign empirical covariance matrix to
equal 1 2 id2, the variance-covariance matrix of a random vector that is
uniformly distributed on the unit circle.
",0,0,0,0,1,0
17267,Spreading of localized attacks in spatial multiplex networks,"  Many real-world multilayer systems such as critical infrastructure are
interdependent and embedded in space with links of a characteristic length.
They are also vulnerable to localized attacks or failures, such as terrorist
attacks or natural catastrophes, which affect all nodes within a given radius.
Here we study the effects of localized attacks on spatial multiplex networks of
two layers. We find a metastable region where a localized attack larger than a
critical size induces a nucleation transition as a cascade of failures spreads
throughout the system, leading to its collapse. We develop a theory to predict
the critical attack size and find that it exhibits novel scaling behavior. We
further find that localized attacks in these multiplex systems can induce a
previously unobserved combination of random and spatial cascades. Our results
demonstrate important vulnerabilities in real-world interdependent networks and
show new theoretical features of spatial networks.
",1,1,0,0,0,0
17978,A Statistical Perspective on Inverse and Inverse Regression Problems,"  Inverse problems, where in broad sense the task is to learn from the noisy
response about some unknown function, usually represented as the argument of
some known functional form, has received wide attention in the general
scientific disciplines. How- ever, in mainstream statistics such inverse
problem paradigm does not seem to be as popular. In this article we provide a
brief overview of such problems from a statistical, particularly Bayesian,
perspective.
We also compare and contrast the above class of problems with the perhaps
more statistically familiar inverse regression problems, arguing that this
class of problems contains the traditional class of inverse problems. In course
of our review we point out that the statistical literature is very scarce with
respect to both the inverse paradigms, and substantial research work is still
necessary to develop the fields.
",0,0,1,1,0,0
7300,Associated Graded Rings and Connected Sums,"  In 2012, Ananthnarayan, Avramov and Moore gave a new construction of
Gorenstein rings from two Gorenstein local rings, called their connected sum.
In this article, we investigate conditions on the associated graded ring of a
Gorenstein Artin local ring Q, which force it to be a connected sum over its
residue field. In particular, we recover some results regarding short, and
stretched, Gorenstein Artin rings. Finally, using these decompositions, we
obtain results about the rationality of the Poincare series of Q.
",0,0,1,0,0,0
6487,"Crystal structure, site selectivity, and electronic structure of layered chalcogenide LaOBiPbS3","  We have investigated the crystal structure of LaOBiPbS3 using neutron
diffraction and synchrotron X-ray diffraction. From structural refinements, we
found that the two metal sites, occupied by Bi and Pb, were differently
surrounded by the sulfur atoms. Calculated bond valence sum suggested that one
metal site was nearly trivalent and the other was nearly divalent. Neutron
diffraction also revealed site selectivity of Bi and Pb in the LaOBiPbS3
structure. These results suggested that the crystal structure of LaOBiPbS3 can
be regarded as alternate stacks of the rock-salt-type Pb-rich sulfide layers
and the LaOBiS2-type Bi-rich layers. From band calculations for an ideal
(LaOBiS2)(PbS) system, we found that the S bands of the PbS layer were
hybridized with the Bi bands of the BiS plane at around the Fermi energy, which
resulted in the electronic characteristics different from that of LaOBiS2.
Stacking the rock-salt type sulfide (chalcogenide) layers and the BiS2-based
layered structure could be a new strategy to exploration of new BiS2-based
layered compounds, exotic two-dimensional electronic states, or novel
functionality.
",0,1,0,0,0,0
9102,On the Universality of Invariant Networks,"  Constraining linear layers in neural networks to respect symmetry
transformations from a group $G$ is a common design principle for invariant
networks that has found many applications in machine learning.
In this paper, we consider a fundamental question that has received little
attention to date: Can these networks approximate any (continuous) invariant
function?
We tackle the rather general case where $G\leq S_n$ (an arbitrary subgroup of
the symmetric group) that acts on $\mathbb{R}^n$ by permuting coordinates. This
setting includes several recent popular invariant networks. We present two main
results: First, $G$-invariant networks are universal if high-order tensors are
allowed. Second, there are groups $G$ for which higher-order tensors are
unavoidable for obtaining universality.
$G$-invariant networks consisting of only first-order tensors are of special
interest due to their practical value. We conclude the paper by proving a
necessary condition for the universality of $G$-invariant networks that
incorporate only first-order tensors. Lastly, we propose a conjecture stating
that this condition is also sufficient.
",1,0,0,1,0,0
16432,Trace Properties from Separation Logic Specifications,"  We propose a formal approach for relating abstract separation logic library
specifications with the trace properties they enforce on interactions between a
client and a library. Separation logic with abstract predicates enforces a
resource discipline that constrains when and how calls may be made between a
client and a library. Intuitively, this can enforce a protocol on the
interaction trace. This intuition is broadly used in the separation logic
community but has not previously been formalised. We provide just such a
formalisation. Our approach is based on using wrappers which instrument library
code to induce execution traces for the properties under examination. By
considering a separation logic extended with trace resources, we prove that
when a library satisfies its separation logic specification then its wrapped
version satisfies the same specification and, moreover, maintains the trace
properties as an invariant. Consequently, any client and library implementation
that are correct with respect to the separation logic specification will
satisfy the trace properties.
",1,0,0,0,0,0
18580,Personalization in Goal-Oriented Dialog,"  The main goal of modeling human conversation is to create agents which can
interact with people in both open-ended and goal-oriented scenarios. End-to-end
trained neural dialog systems are an important line of research for such
generalized dialog models as they do not resort to any situation-specific
handcrafting of rules. However, incorporating personalization into such systems
is a largely unexplored topic as there are no existing corpora to facilitate
such work. In this paper, we present a new dataset of goal-oriented dialogs
which are influenced by speaker profiles attached to them. We analyze the
shortcomings of an existing end-to-end dialog system based on Memory Networks
and propose modifications to the architecture which enable personalization. We
also investigate personalization in dialog as a multi-task learning problem,
and show that a single model which shares features among various profiles
outperforms separate models for each profile.
",1,0,0,0,0,0
17931,3D Morphology Prediction of Progressive Spinal Deformities from Probabilistic Modeling of Discriminant Manifolds,"  We introduce a novel approach for predicting the progression of adolescent
idiopathic scoliosis from 3D spine models reconstructed from biplanar X-ray
images. Recent progress in machine learning have allowed to improve
classification and prognosis rates, but lack a probabilistic framework to
measure uncertainty in the data. We propose a discriminative probabilistic
manifold embedding where locally linear mappings transform data points from
high-dimensional space to corresponding low-dimensional coordinates. A
discriminant adjacency matrix is constructed to maximize the separation between
progressive and non-progressive groups of patients diagnosed with scoliosis,
while minimizing the distance in latent variables belonging to the same class.
To predict the evolution of deformation, a baseline reconstruction is projected
onto the manifold, from which a spatiotemporal regression model is built from
parallel transport curves inferred from neighboring exemplars. Rate of
progression is modulated from the spine flexibility and curve magnitude of the
3D spine deformation. The method was tested on 745 reconstructions from 133
subjects using longitudinal 3D reconstructions of the spine, with results
demonstrating the discriminatory framework can identify between progressive and
non-progressive of scoliotic patients with a classification rate of 81% and
prediction differences of 2.1$^{o}$ in main curve angulation, outperforming
other manifold learning methods. Our method achieved a higher prediction
accuracy and improved the modeling of spatiotemporal morphological changes in
highly deformed spines compared to other learning methods.
",1,0,0,1,0,0
2897,On the Performance of a Canonical Labeling for Matching Correlated Erdős-Rényi Graphs,"  Graph matching in two correlated random graphs refers to the task of
identifying the correspondence between vertex sets of the graphs. Recent
results have characterized the exact information-theoretic threshold for graph
matching in correlated Erdős-Rényi graphs. However, very little is known
about the existence of efficient algorithms to achieve graph matching without
seeds. In this work we identify a region in which a straightforward $O(n^2\log
n)$-time canonical labeling algorithm, initially introduced in the context of
graph isomorphism, succeeds in matching correlated Erdős-Rényi graphs.
The algorithm has two steps. In the first step, all vertices are labeled by
their degrees and a trivial minimum distance matching (i.e., simply sorting
vertices according to their degrees) matches a fixed number of highest degree
vertices in the two graphs. Having identified this subset of vertices, the
remaining vertices are matched using a matching algorithm for bipartite graphs.
",0,0,0,1,0,0
20302,Type-II Dirac Photons,"  The Dirac equation for relativistic electron waves is the parent model for
Weyl and Majorana fermions as well as topological insulators. Simulation of
Dirac physics in three-dimensional photonic crystals, though fundamentally
important for topological phenomena at optical frequencies, encounters the
challenge of synthesis of both Kramers double degeneracy and parity inversion.
Here we show how type-II Dirac points---exotic Dirac relativistic waves yet to
be discovered---are robustly realized through the nonsymmorphic screw symmetry.
The emergent type-II Dirac points carry nontrivial topology and are the mother
states of type-II Weyl points. The proposed all-dielectric architecture enables
robust cavity states at photonic-crystal---air interfaces and anomalous
refraction, with very low energy dissipation.
",0,1,0,0,0,0
10868,Modeling Grasp Motor Imagery through Deep Conditional Generative Models,"  Grasping is a complex process involving knowledge of the object, the
surroundings, and of oneself. While humans are able to integrate and process
all of the sensory information required for performing this task, equipping
machines with this capability is an extremely challenging endeavor. In this
paper, we investigate how deep learning techniques can allow us to translate
high-level concepts such as motor imagery to the problem of robotic grasp
synthesis. We explore a paradigm based on generative models for learning
integrated object-action representations, and demonstrate its capacity for
capturing and generating multimodal, multi-finger grasp configurations on a
simulated grasping dataset.
",1,0,0,1,0,0
2922,Dynamic Clearing and Contagion in Financial Networks,"  In this paper we will consider a generalized extension of the Eisenberg-Noe
model of financial contagion to allow for time dynamics in both discrete and
continuous time. Derivation and interpretation of the financial implications
will be provided. Emphasis will be placed on the continuous-time framework and
its formulation as a differential equation driven by the operating cash flows.
Mathematical results on existence and uniqueness of firm wealths under the
discrete and continuous-time models will be provided. Finally, the financial
implications of time dynamics will be considered. The focus will be on how the
dynamic clearing solutions differ from those of the static Eisenberg-Noe model.
",0,0,0,0,0,1
1111,Continuous-wave virtual-state lasing from cold ytterbium atoms,"  While conventional lasers are based on gain media with three or four real
levels, unconventional lasers including virtual levels and two-photon processes
offer new opportunities. We study lasing that involves a two-photon process
through a virtual lower level, which we realize in a cloud of cold ytterbium
atoms that are magneto-optically trapped inside a cavity. We pump the atoms on
the narrow $^1$S$_0$ $\to$ $^3$P$_1$ line and generate laser emission on the
same transition. Lasing is verified by a threshold behavior of output power
vs.\ pump power and atom number, a flat $g^{(2)}$ correlation function above
threshold, and the polarization properties of the output. In the proposed
lasing mechanism the MOT beams create the virtual lower level of the lasing
transition. The laser process runs continuously, needs no further repumping,
and might be adapted to other atoms or transitions such as the ultra narrow
$^1$S$_0$ $\to$ $^3$P$_0$ clock transition in ytterbium.
",0,1,0,0,0,0
5085,"Attentive Convolutional Neural Network based Speech Emotion Recognition: A Study on the Impact of Input Features, Signal Length, and Acted Speech","  Speech emotion recognition is an important and challenging task in the realm
of human-computer interaction. Prior work proposed a variety of models and
feature sets for training a system. In this work, we conduct extensive
experiments using an attentive convolutional neural network with multi-view
learning objective function. We compare system performance using different
lengths of the input signal, different types of acoustic features and different
types of emotion speech (improvised/scripted). Our experimental results on the
Interactive Emotional Motion Capture (IEMOCAP) database reveal that the
recognition performance strongly depends on the type of speech data independent
of the choice of input features. Furthermore, we achieved state-of-the-art
results on the improvised speech data of IEMOCAP.
",1,0,0,0,0,0
10250,Self-supporting Topology Optimization for Additive Manufacturing,"  The paper presents a topology optimization approach that designs an optimal
structure, called a self-supporting structure, which is ready to be fabricated
via additive manufacturing without the usage of additional support structures.
Such supports in general have to be created during the fabricating process so
that the primary object can be manufactured layer by layer without collapse,
which is very time-consuming and waste of material.
The proposed approach resolves this problem by formulating the
self-supporting requirements as a novel explicit quadratic continuous
constraint in the topology optimization problem, or specifically, requiring the
number of unsupported elements (in terms of the sum of squares of their
densities) to be zero. Benefiting form such novel formulations, computing
sensitivity of the self-supporting constraint with respect to the design
density is straightforward, which otherwise would require lots of research
efforts in general topology optimization studies. The derived sensitivity for
each element is only linearly dependent on its sole density, which, different
from previous layer-based sensitivities, consequently allows for a parallel
implementation and possible higher convergence rate. In addition, a discrete
convolution operator is also designed to detect the unsupported elements as
involved in each step of optimization iteration, and improves the detection
process 100 times as compared with simply enumerating these elements. The
approach works for cases of general overhang angle, or general domain, and
produces an optimized structures, and their associated optimal compliance, very
close to that of the reference structure obtained without considering the
self-supporting constraint, as demonstrated by extensive 2D and 3D benchmark
examples.
",1,0,0,0,0,0
5183,Parent Oriented Teacher Selection Causes Language Diversity,"  An evolutionary model for emergence of diversity in language is developed. We
investigated the effects of two real life observations, namely, people prefer
people that they communicate with well, and people interact with people that
are physically close to each other. Clearly these groups are relatively small
compared to the entire population. We restrict selection of the teachers from
such small groups, called imitation sets, around parents. Then the child learns
language from a teacher selected within the imitation set of her parent. As a
result, there are subcommunities with their own languages developed. Within
subcommunity comprehension is found to be high. The number of languages is
related to the relative size of imitation set by a power law.
",1,0,0,0,0,0
5742,Early warning signal for interior crises in excitable systems,"  The ability to reliably predict critical transitions in dynamical systems is
a long-standing goal of diverse scientific communities. Previous work focused
on early warning signals related to local bifurcations (critical slowing down)
and non-bifurcation type transitions. We extend this toolbox and report on a
characteristic scaling behavior (critical attractor growth) which is indicative
of an impending global bifurcation, an interior crisis in excitable systems. We
demonstrate our early warning signal in a conceptual climate model as well as
in a model of coupled neurons known to exhibit extreme events. We observed
critical attractor growth prior to interior crises of chaotic as well as
strange-nonchaotic attractors. These observations promise to extend the classes
of transitions that can be predicted via early warning signals.
",0,1,0,0,0,0
12290,Metrologically useful states of spin-1 Bose condensates with macroscopic magnetization,"  We study theoretically the usefulness of spin-1 Bose condensates with
macroscopic magnetization in a homogeneous magnetic field for quantum
metrology. We demonstrate Heisenberg scaling of the quantum Fisher information
for states in thermal equilibrium. The scaling applies to both
antiferromagnetic and ferromagnetic interactions. The effect preserves as long
as fluctuations of magnetization are sufficiently small. Scaling of the quantum
Fisher information with the total particle number is derived within the
mean-field approach in the zero temperature limit and exactly in the high
magnetic field limit for any temperature. The precision gain is intuitively
explained owing to subtle features of the quasi-distribution function in phase
space.
",0,1,0,0,0,0
12096,Techniques for proving Asynchronous Convergence results for Markov Chain Monte Carlo methods,"  Markov Chain Monte Carlo (MCMC) methods such as Gibbs sampling are finding
widespread use in applied statistics and machine learning. These often lead to
difficult computational problems, which are increasingly being solved on
parallel and distributed systems such as compute clusters. Recent work has
proposed running iterative algorithms such as gradient descent and MCMC in
parallel asynchronously for increased performance, with good empirical results
in certain problems. Unfortunately, for MCMC this parallelization technique
requires new convergence theory, as it has been explicitly demonstrated to lead
to divergence on some examples. Recent theory on Asynchronous Gibbs sampling
describes why these algorithms can fail, and provides a way to alter them to
make them converge. In this article, we describe how to apply this theory in a
generic setting, to understand the asynchronous behavior of any MCMC algorithm,
including those implemented using parameter servers, and those not based on
Gibbs sampling.
",1,0,0,1,0,0
17163,Sign reversal of magnetoresistance and p to n transition in Ni doped ZnO thin film,"  We report the magnetoresistance and nonlinear Hall effect studies over a wide
temperature range in pulsed laser deposited Ni0.07Zn0.93O thin film. Negative
and positive contributions to magnetoresistance at high and low temperatures
have been successfully modeled by the localized magnetic moment and two band
conduction process involving heavy and light hole subbands, respectively.
Nonlinearity in the Hall resistance also agrees well with the two channel
conduction model. A negative Hall voltage has been found for T $\gte 50 K$,
implying a dominant conduction mainly by electrons whereas positive Hall
voltage for T less than 50 K shows hole dominated conduction in this material.
Crossover in the sign of magnetoresistance from negative to positive reveals
the spin polarization of the charge carriers and hence the applicability of Ni
doped ZnO thin film for spintronic applications.
",0,1,0,0,0,0
7689,Negative refraction in Weyl semimetals,"  We theoretically propose that Weyl semimetals may exhibit negative refraction
at some frequencies close to the plasmon frequency, allowing transverse
magnetic (TM) electromagnetic waves with frequencies smaller than the plasmon
frequency to propagate in the Weyl semimetals. The idea is justified by the
calculation of reflection spectra, in which \textit{negative} refractive index
at such frequencies gives physically correct spectra. In this case, a TM
electromagnetic wave incident to the surface of the Weyl semimetal will be bent
with a negative angle of refraction. We argue that the negative refractive
index at the specified frequencies of the electromagnetic wave is required to
conserve the energy of the wave, in which the incident energy should propagate
away from the point of incidence.
",0,1,0,0,0,0
17045,Superradiant Mott Transition,"  The combination of strong correlation and emergent lattice can be achieved
when quantum gases are confined in a superradiant Fabry-Perot cavity. In
addition to the discoveries of exotic phases, such as density wave ordered Mott
insulator and superfluid, a surprising kink structure is found in the slope of
the cavity strength as a function of the pumping strength. In this Letter, we
show that the appearance of such a kink is a manifestation of a liquid-gas like
transition between two superfluids with different densities. The slopes in the
immediate neighborhood of the kink become divergent at the liquid-gas critical
points and display a critical scaling law with a critical exponent 1 in the
quantum critical region. Our predictions could be tested in current
experimental set-up.
",0,1,0,0,0,0
1288,Scalable Realistic Recommendation Datasets through Fractal Expansions,"  Recommender System research suffers currently from a disconnect between the
size of academic data sets and the scale of industrial production systems. In
order to bridge that gap we propose to generate more massive user/item
interaction data sets by expanding pre-existing public data sets. User/item
incidence matrices record interactions between users and items on a given
platform as a large sparse matrix whose rows correspond to users and whose
columns correspond to items. Our technique expands such matrices to larger
numbers of rows (users), columns (items) and non zero values (interactions)
while preserving key higher order statistical properties. We adapt the
Kronecker Graph Theory to user/item incidence matrices and show that the
corresponding fractal expansions preserve the fat-tailed distributions of user
engagements, item popularity and singular value spectra of user/item
interaction matrices. Preserving such properties is key to building large
realistic synthetic data sets which in turn can be employed reliably to
benchmark Recommender Systems and the systems employed to train them. We
provide algorithms to produce such expansions and apply them to the MovieLens
20 million data set comprising 20 million ratings of 27K movies by 138K users.
The resulting expanded data set has 10 billion ratings, 2 million items and
864K users in its smaller version and can be scaled up or down. A larger
version features 655 billion ratings, 7 million items and 17 million users.
",1,0,0,1,0,0
9070,Cosmology and Convention,"  I argue that some important elements of the current cosmological model are
""conventionalist"" in the sense defined by Karl Popper. These elements include
dark matter and dark energy; both are auxiliary hypotheses that were invoked in
response to observations that falsified the standard model as it existed at the
time. The use of conventionalist stratagems in response to unexpected
observations implies that the field of cosmology is in a state of ""degenerating
problemshift"" in the language of Imre Lakatos. I show that the ""concordance""
argument, often put forward by cosmologists in support of the current paradigm,
is weaker than the convergence arguments that were made in the past in support
of the atomic theory of matter or the quantization of energy.
",0,1,0,0,0,0
13106,Thermal Molecular Focusing: Tunable Cross Effect of Phoresis and Advection,"  The control of solute fluxes through either microscopic phoresis or
hydrodynamic advection is a fundamental way to transport molecules, which are
ubiquitously present in nature and technology. We study the transport of large
solute such as DNA driven by a time-dependent thermal field in a polymer
solution. Heat propagation of a single heat spot moving back and forth gives
rise to the molecular focusing of DNA with frequency-tunable control. We
developed a theoretical model, where heat conduction, viscoelastic expansion of
walls, and the viscosity gradient of a smaller solute are coupled, and that can
explain the underlying hydrodynamic focusing and its interplay with phoretic
transports. This cross effect may allow one to design a unique miniaturized
pump in microfluidics.
",0,1,0,0,0,0
4031,Optical and structural study of the pressure-induced phase transition of CdWO$_4$,"  The optical absorption of CdWO$_4$ is reported at high pressures up to 23
GPa. The onset of a phase transition was detected at 19.5 GPa, in good
agreement with a previous Raman spectroscopy study. The crystal structure of
the high-pressure phase of CdWO$_4$ was solved at 22 GPa employing
single-crystal synchrotron x-ray diffraction. The symmetry changes from space
group $P$2/$c$ in the low-pressure wolframite phase to $P2_1/c$ in the
high-pressure post-wolframite phase accompanied by a doubling of the unit-cell
volume. The octahedral oxygen coordination of the tungsten and cadmium ions is
increased to [7]-fold and [6+1]-fold, respectively, at the phase transition.
The compressibility of the low-pressure phase of CdWO$_4$ has been reevaluated
with powder x-ray diffraction up to 15 GPa finding a bulk modulus of $B_0$ =
123 GPa. The direct band gap of the low-pressure phase increases with
compression up to 16.9 GPa at 12 meV/GPa. At this point an indirect band gap
crosses the direct band gap and decreases at -2 meV/GPa up to 19.5 GPa where
the phase transition starts. At the phase transition the band gap collapses by
0.7 eV and another direct band gap decreases at -50 meV/GPa up to the maximum
measured pressure. The structural stability of the post-wolframite structure is
confirmed by \textit{ab initio} calculations finding the post-wolframite-type
phase to be more stable than the wolframite at 18 GPa. Lattice dynamic
calculations based on space group $P2_1/c$ explain well the Raman-active modes
previously measured in the high-pressure post-wolframite phase. The
pressure-induced band gap crossing in the wolframite phase as well as the
pressure dependence of the direct band gap in the high-pressure phase are
further discussed with respect to the calculations.
",0,1,0,0,0,0
7552,Downgrade Attack on TrustZone,"  Security-critical tasks require proper isolation from untrusted software.
Chip manufacturers design and include trusted execution environments (TEEs) in
their processors to secure these tasks. The integrity and security of the
software in the trusted environment depend on the verification process of the
system.
We find a form of attack that can be performed on the current implementations
of the widely deployed ARM TrustZone technology. The attack exploits the fact
that the trustlet (TA) or TrustZone OS loading verification procedure may use
the same verification key and may lack proper rollback prevention across
versions. If an exploit works on an out-of-date version, but the vulnerability
is patched on the latest version, an attacker can still use the same exploit to
compromise the latest system by downgrading the software to an older and
exploitable version.
We did experiments on popular devices on the market including those from
Google, Samsung and Huawei, and found that all of them have the risk of being
attacked. Also, we show a real-world example to exploit Qualcomm's QSEE.
In addition, in order to find out which device images share the same
verification key, pattern matching schemes for different vendors are analyzed
and summarized.
",1,0,0,0,0,0
20831,Understanding Career Progression in Baseball Through Machine Learning,"  Professional baseball players are increasingly guaranteed expensive long-term
contracts, with over 70 deals signed in excess of \$90 million, mostly in the
last decade. These are substantial sums compared to a typical franchise
valuation of \$1-2 billion. Hence, the players to whom a team chooses to give
such a contract can have an enormous impact on both competitiveness and profit.
Despite this, most published approaches examining career progression in
baseball are fairly simplistic. We applied four machine learning algorithms to
the problem and soundly improved upon existing approaches, particularly for
batting data.
",1,0,0,1,0,0
7857,Minimax Regret Bounds for Reinforcement Learning,"  We consider the problem of provably optimal exploration in reinforcement
learning for finite horizon MDPs. We show that an optimistic modification to
value iteration achieves a regret bound of $\tilde{O}( \sqrt{HSAT} +
H^2S^2A+H\sqrt{T})$ where $H$ is the time horizon, $S$ the number of states,
$A$ the number of actions and $T$ the number of time-steps. This result
improves over the best previous known bound $\tilde{O}(HS \sqrt{AT})$ achieved
by the UCRL2 algorithm of Jaksch et al., 2010. The key significance of our new
results is that when $T\geq H^3S^3A$ and $SA\geq H$, it leads to a regret of
$\tilde{O}(\sqrt{HSAT})$ that matches the established lower bound of
$\Omega(\sqrt{HSAT})$ up to a logarithmic factor. Our analysis contains two key
insights. We use careful application of concentration inequalities to the
optimal value function as a whole, rather than to the transitions probabilities
(to improve scaling in $S$), and we define Bernstein-based ""exploration
bonuses"" that use the empirical variance of the estimated values at the next
states (to improve scaling in $H$).
",1,0,0,1,0,0
20223,The Combinatorics of Weighted Vector Compositions,"  A vector composition of a vector $\mathbf{\ell}$ is a matrix $\mathbf{A}$
whose rows sum to $\mathbf{\ell}$. We define a weighted vector composition as a
vector composition in which the column values of $\mathbf{A}$ may appear in
different colors. We study vector compositions from different viewpoints: (1)
We show how they are related to sums of random vectors and (2) how they allow
to derive formulas for partial derivatives of composite functions. (3) We study
congruence properties of the number of weighted vector compositions, for fixed
and arbitrary number of parts, many of which are analogous to those of ordinary
binomial coefficients and related quantities. Via the Central Limit Theorem and
their multivariate generating functions, (4) we also investigate the asymptotic
behavior of several special cases of numbers of weighted vector compositions.
Finally, (5) we conjecture an extension of a primality criterion due to Mann
and Shanks in the context of weighted vector compositions.
",1,0,1,0,0,0
17543,Making Deep Q-learning methods robust to time discretization,"  Despite remarkable successes, Deep Reinforcement Learning (DRL) is not robust
to hyperparameterization, implementation details, or small environment changes
(Henderson et al. 2017, Zhang et al. 2018). Overcoming such sensitivity is key
to making DRL applicable to real world problems. In this paper, we identify
sensitivity to time discretization in near continuous-time environments as a
critical factor; this covers, e.g., changing the number of frames per second,
or the action frequency of the controller. Empirically, we find that
Q-learning-based approaches such as Deep Q- learning (Mnih et al., 2015) and
Deep Deterministic Policy Gradient (Lillicrap et al., 2015) collapse with small
time steps. Formally, we prove that Q-learning does not exist in continuous
time. We detail a principled way to build an off-policy RL algorithm that
yields similar performances over a wide range of time discretizations, and
confirm this robustness empirically.
",1,0,0,1,0,0
12434,The role of cosmology in modern physics,"  Subject of this article is the relationship between modern cosmology and
fundamental physics, in particular general relativity as a theory of gravity on
one side, together with its unique application in cosmology, and the formation
of structures and their statistics on the other. It summarises arguments for
the formulation for a metric theory of gravity and the uniqueness of the
construction of general relativity. It discusses symmetry arguments in the
construction of Friedmann-Lemaître cosmologies as well as assumptions in
relation to the presence of dark matter, when adopting general relativity as
the gravitational theory. A large section is dedicated to $\Lambda$CDM as the
standard model for structure formation and the arguments that led to its
construction, and to the of role statistics and to the problem of scientific
inference in cosmology as an empirical science. The article concludes with an
outlook on current and future developments in cosmology.
",0,1,0,0,0,0
18625,Multilevel nested simulation for efficient risk estimation,"  We investigate the problem of computing a nested expectation of the form
$\mathbb{P}[\mathbb{E}[X|Y]
\!\geq\!0]\!=\!\mathbb{E}[\textrm{H}(\mathbb{E}[X|Y])]$ where $\textrm{H}$ is
the Heaviside function. This nested expectation appears, for example, when
estimating the probability of a large loss from a financial portfolio. We
present a method that combines the idea of using Multilevel Monte Carlo (MLMC)
for nested expectations with the idea of adaptively selecting the number of
samples in the approximation of the inner expectation, as proposed by (Broadie
et al., 2011). We propose and analyse an algorithm that adaptively selects the
number of inner samples on each MLMC level and prove that the resulting MLMC
method with adaptive sampling has an $\mathcal{O}\left(
\varepsilon^{-2}|\log\varepsilon|^2 \right)$ complexity to achieve a root
mean-squared error $\varepsilon$. The theoretical analysis is verified by
numerical experiments on a simple model problem. We also present a stochastic
root-finding algorithm that, combined with our adaptive methods, can be used to
compute other risk measures such as Value-at-Risk (VaR) and Conditional
Value-at-Risk (CVaR), with the latter being achieved with
$\mathcal{O}\left(\varepsilon^{-2}\right)$ complexity.
",0,0,0,0,0,1
5581,Revisiting Elementary Denotational Semantics,"  Operational semantics have been enormously successful, in large part due to
its flexibility and simplicity, but they are not compositional. Denotational
semantics, on the other hand, are compositional but the lattice-theoretic
models are complex and difficult to scale to large languages. However, there
are elementary models of the $\lambda$-calculus that are much less complex: by
Coppo, Dezani-Ciancaglini, and Salle (1979), Engeler (1981), and Plotkin
(1993).
This paper takes first steps toward answering the question: can elementary
models be good for the day-to-day work of language specification,
mechanization, and compiler correctness? The elementary models in the
literature are simple, but they are not as intuitive as they could be. To
remedy this, we create a new model that represents functions literally as
finite graphs. Regarding mechanization, we give the first machine-checked proof
of soundness and completeness of an elementary model with respect to an
operational semantics. Regarding compiler correctness, we define a polyvariant
inliner for the call-by-value $\lambda$-calculus and prove that its output is
contextually equivalent to its input. Toward scaling elementary models to
larger languages, we formulate our semantics in a monadic style, give a
semantics for System F with general recursion, and mechanize the proof of type
soundness.
",1,0,0,0,0,0
5587,Annealed Generative Adversarial Networks,"  We introduce a novel framework for adversarial training where the target
distribution is annealed between the uniform distribution and the data
distribution. We posited a conjecture that learning under continuous annealing
in the nonparametric regime is stable irrespective of the divergence measures
in the objective function and proposed an algorithm, dubbed {\ss}-GAN, in
corollary. In this framework, the fact that the initial support of the
generative network is the whole ambient space combined with annealing are key
to balancing the minimax game. In our experiments on synthetic data, MNIST, and
CelebA, {\ss}-GAN with a fixed annealing schedule was stable and did not suffer
from mode collapse.
",1,0,0,1,0,0
11043,Dictionary-based Monitoring of Premature Ventricular Contractions: An Ultra-Low-Cost Point-of-Care Service,"  While cardiovascular diseases (CVDs) are prevalent across economic strata,
the economically disadvantaged population is disproportionately affected due to
the high cost of traditional CVD management. Accordingly, developing an
ultra-low-cost alternative, affordable even to groups at the bottom of the
economic pyramid, has emerged as a societal imperative. Against this backdrop,
we propose an inexpensive yet accurate home-based electrocardiogram(ECG)
monitoring service. Specifically, we seek to provide point-of-care monitoring
of premature ventricular contractions (PVCs), high frequency of which could
indicate the onset of potentially fatal arrhythmia. Note that a traditional
telecardiology system acquires the ECG, transmits it to a professional
diagnostic centre without processing, and nearly achieves the diagnostic
accuracy of a bedside setup, albeit at high bandwidth cost. In this context, we
aim at reducing cost without significantly sacrificing reliability. To this
end, we develop a dictionary-based algorithm that detects with high sensitivity
the anomalous beats only which are then transmitted. We further compress those
transmitted beats using class-specific dictionaries subject to suitable
reconstruction/diagnostic fidelity. Such a scheme would not only reduce the
overall bandwidth requirement, but also localising anomalous beats, thereby
reducing physicians' burden. Finally, using Monte Carlo cross validation on
MIT/BIH arrhythmia database, we evaluate the performance of the proposed
system. In particular, with a sensitivity target of at most one undetected PVC
in one hundred beats, and a percentage root mean squared difference less than
9% (a clinically acceptable level of fidelity), we achieved about 99.15%
reduction in bandwidth cost, equivalent to 118-fold savings over traditional
telecardiology.
",1,0,0,0,0,0
2442,Kepler sheds new and unprecedented light on the variability of a blue supergiant: gravity waves in the O9.5Iab star HD 188209,"  Stellar evolution models are most uncertain for evolved massive stars.
Asteroseismology based on high-precision uninterrupted space photometry has
become a new way to test the outcome of stellar evolution theory and was
recently applied to a multitude of stars, but not yet to massive evolved
supergiants.Our aim is to detect, analyse and interpret the photospheric and
wind variability of the O9.5Iab star HD 188209 from Kepler space photometry and
long-term high-resolution spectroscopy. We used Kepler scattered-light
photometry obtained by the nominal mission during 1460d to deduce the
photometric variability of this O-type supergiant. In addition, we assembled
and analysed high-resolution high signal-to-noise spectroscopy taken with four
spectrographs during some 1800d to interpret the temporal spectroscopic
variability of the star. The variability of this blue supergiant derived from
the scattered-light space photometry is in full in agreement with the one found
in the ground-based spectroscopy. We find significant low-frequency variability
that is consistently detected in all spectral lines of HD 188209. The
photospheric variability propagates into the wind, where it has similar
frequencies but slightly higher amplitudes. The morphology of the frequency
spectra derived from the long-term photometry and spectroscopy points towards a
spectrum of travelling waves with frequency values in the range expected for an
evolved O-type star. Convectively-driven internal gravity waves excited in the
stellar interior offer the most plausible explanation of the detected
variability.
",0,1,0,0,0,0
13981,Election Bias: Comparing Polls and Twitter in the 2016 U.S. Election,"  While the polls have been the most trusted source for election predictions
for decades, in the recent presidential election they were called inaccurate
and biased. How inaccurate were the polls in this election and can social media
beat the polls as an accurate election predictor? Polls from several news
outlets and sentiment analysis on Twitter data were used, in conjunction with
the results of the election, to answer this question and outline further
research on the best method for predicting the outcome of future elections.
",1,0,0,0,0,0
19109,Weighted Surface Algebras,"  A finite-dimensional algebra $A$ over an algebraically closed field $K$ is
called periodic if it is periodic under the action of the syzygy operator in
the category of $A-A-$ bimodules. The periodic algebras are self-injective and
occur naturally in the study of tame blocks of group algebras, actions of
finite groups on spheres, hypersurface singularities of finite Cohen-Macaulay
type, and Jacobian algebras of quivers with potentials. Recently, the tame
periodic algebras of polynomial growth have been classified and it is natural
to attempt to classify all tame periodic algebras. We introduce the weighted
surface algebras of triangulated surfaces with arbitrarily oriented triangles
and describe their basic properties. In particular, we prove that all these
algebras, except the singular tetrahedral algebras, are symmetric tame periodic
algebras of period $4$. Moreover, we describe the socle deformations of the
weighted surface algebras and prove that all these algebras are symmetric tame
periodic algebras of period $4$. The main results of the paper form an
important step towards a classification of all periodic symmetric tame algebras
of non-polynomial growth, and lead to a complete description of all algebras of
generalized quaternion type. Further, the orbit closures of the weighted
surface algebras (and their socle deformations) in the affine varieties of
associative $K$-algebra structures contain wide classes of tame symmetric
algebras related to algebras of dihedral and semidihedral types, which occur in
the study of blocks of group algebras with dihedral and semidihedral defect
groups.
",0,0,1,0,0,0
12546,Coupled Compound Poisson Factorization,"  We present a general framework, the coupled compound Poisson factorization
(CCPF), to capture the missing-data mechanism in extremely sparse data sets by
coupling a hierarchical Poisson factorization with an arbitrary data-generating
model. We derive a stochastic variational inference algorithm for the resulting
model and, as examples of our framework, implement three different
data-generating models---a mixture model, linear regression, and factor
analysis---to robustly model non-random missing data in the context of
clustering, prediction, and matrix factorization. In all three cases, we test
our framework against models that ignore the missing-data mechanism on large
scale studies with non-random missing data, and we show that explicitly
modeling the missing-data mechanism substantially improves the quality of the
results, as measured using data log likelihood on a held-out test set.
",1,0,0,1,0,0
12167,Critical Vertices and Edges in $H$-free Graphs,"  A vertex or edge in a graph is critical if its deletion reduces the chromatic
number of the graph by 1. We consider the problems of deciding whether a graph
has a critical vertex or edge, respectively. We give a complexity dichotomy for
both problems restricted to $H$-free graphs, that is, graphs with no induced
subgraph isomorphic to $H$. Moreover, we show that an edge is critical if and
only if its contraction reduces the chromatic number by 1. Hence, we also
obtain a complexity dichotomy for the problem of deciding if a graph has an
edge whose contraction reduces the chromatic number by 1.
",1,0,0,0,0,0
14360,Cavity-enhanced photoionization of an ultracold rubidium beam for application in focused ion beams,"  A two-step photoionization strategy of an ultracold rubidium beam for
application in a focused ion beam instrument is analyzed and implemented. In
this strategy the atomic beam is partly selected with an aperture after which
the transmitted atoms are ionized in the overlap of a tightly cylindrically
focused excitation laser beam and an ionization laser beam whose power is
enhanced in a build-up cavity. The advantage of this strategy, as compared to
without the use of a build-up cavity, is that higher ionization degrees can be
reached at higher currents. Optical Bloch equations including the
photoionization process are used to calculate what ionization degree and
ionization position distribution can be reached. Furthermore, the ionization
strategy is tested on an ultracold beam of $^{85}$Rb atoms. The beam current is
measured as a function of the excitation and ionization laser beam intensity
and the selection aperture size. Although details are different, the global
trends of the measurements agree well with the calculation. With a selection
aperture diameter of 52 $\mu$m, a current of $\left(170\pm4\right)$ pA is
measured, which according to calculations is 63% of the current equivalent of
the transmitted atomic flux. Taking into account the ionization degree the ion
beam peak reduced brightness is estimated at $1\times10^7$ A/(m$^2\,$sr$\,$eV).
",0,1,0,0,0,0
3205,On the area of constrained polygonal linkages,"  We study configuration spaces of linkages whose underlying graph are polygons
with diagonal constrains, or more general, partial two-trees. We show that
(with an appropriate definition) the oriented area is a Bott-Morse function on
the configuration space. Its critical points are described and Bott-Morse
indices are computed. This paper is a generalization of analogous results for
polygonal linkages (obtained earlier by G. Khimshiashvili, G. Panina, and A.
Zhukova).
",0,0,1,0,0,0
6814,Astrophotonics: molding the flow of light in astronomical instruments,"  Since its emergence two decades ago, astrophotonics has found broad
application in scientific instruments at many institutions worldwide. The case
for astrophotonics becomes more compelling as telescopes push for AO-assisted,
diffraction-limited performance, a mode of observing that is central to the
next-generation of extremely large telescopes (ELTs). Even AO systems are
beginning to incorporate advanced photonic principles as the community pushes
for higher performance and more complex guide-star configurations. Photonic
instruments like Gravity on the Very Large Telescope achieve milliarcsec
resolution at 2000 nm which would be very difficult to achieve with
conventional optics. While space photonics is not reviewed here, we foresee
that remote sensing platforms will become a major beneficiary of astrophotonic
components in the years ahead. The field has given back with the development of
new technologies (e.g. photonic lantern, large area multi-core fibres) already
finding widespread use in other fields; Google Scholar lists more than 400
research papers making reference to this technology. This short review covers
representative key developments since the 2009 Focus issue on Astrophotonics.
",0,1,0,0,0,0
16969,Asymptotic behaviour of the fifth Painlevé transcendents in the space of initial values,"  We study the asymptotic behaviour of the solutions of the fifth Painlevé
equation as the independent variable approaches zero and infinity in the space
of initial values. We show that the limit set of each solution is compact and
connected and, moreover, that any solution with the essential singularity at
zero has an infinite number of poles and zeroes, and any solution with the
essential singularity at infinity has infinite number of poles and takes value
$1$ infinitely many times.
",0,1,1,0,0,0
9221,Spectral Sparsification of Simplicial Complexes for Clustering and Label Propagation,"  As a generalization of the use of graphs to describe pairwise interactions,
simplicial complexes can be used to model higher-order interactions between
three or more objects in complex systems. There has been a recent surge in
activity for the development of data analysis methods applicable to simplicial
complexes, including techniques based on computational topology, higher-order
random processes, generalized Cheeger inequalities, isoperimetric inequalities,
and spectral methods. In particular, spectral learning methods (e.g. label
propagation and clustering) that directly operate on simplicial complexes
represent a new direction for analyzing such complex datasets.
To apply spectral learning methods to massive datasets modeled as simplicial
complexes, we develop a method for sparsifying simplicial complexes that
preserves the spectrum of the associated Laplacian matrices. We show that the
theory of Spielman and Srivastava for the sparsification of graphs extends to
simplicial complexes via the up Laplacian. In particular, we introduce a
generalized effective resistance for simplices, provide an algorithm for
sparsifying simplicial complexes at a fixed dimension, and give a specific
version of the generalized Cheeger inequality for weighted simplicial
complexes. Finally, we introduce higher-order generalizations of spectral
clustering and label propagation for simplicial complexes and demonstrate via
experiments the utility of the proposed spectral sparsification method for
these applications.
",1,0,0,0,0,0
18290,Food for Thought: Analyzing Public Opinion on the Supplemental Nutrition Assistance Program,"  This project explores public opinion on the Supplemental Nutrition Assistance
Program (SNAP) in news and social media outlets, and tracks elected
representatives' voting records on issues relating to SNAP and food insecurity.
We used machine learning, sentiment analysis, and text mining to analyze
national and state level coverage of SNAP in order to gauge perceptions of the
program over time across these outlets. Results indicate that the majority of
news coverage has negative sentiment, more partisan news outlets have more
extreme sentiment, and that clustering of negative reporting on SNAP occurs in
the Midwest. Our final results and tools will be displayed in an on-line
application that the ACFB Advocacy team can use to inform their communication
to relevant stakeholders.
",1,0,0,0,0,0
311,A System of Three Super Earths Transiting the Late K-Dwarf GJ 9827 at Thirty Parsecs,"  We report the discovery of three small transiting planets orbiting GJ 9827, a
bright (K = 7.2) nearby late K-type dwarf star. GJ 9827 hosts a $1.62\pm0.11$
$R_{\rm \oplus}$ super Earth on a 1.2 day period, a $1.269^{+0.087}_{-0.089}$
$R_{\rm \oplus}$ super Earth on a 3.6 day period, and a $2.07\pm0.14$ $R_{\rm
\oplus}$ super Earth on a 6.2 day period. The radii of the planets transiting
GJ 9827 span the transition between predominantly rocky and gaseous planets,
and GJ 9827 b and c fall in or close to the known gap in the radius
distribution of small planets between these populations. At a distance of 30
parsecs, GJ 9827 is the closest exoplanet host discovered by K2 to date, making
these planets well-suited for atmospheric studies with the upcoming James Webb
Space Telescope. The GJ 9827 system provides a valuable opportunity to
characterize interior structure and atmospheric properties of coeval planets
spanning the rocky to gaseous transition.
",0,1,0,0,0,0
18464,Introduction to Nonnegative Matrix Factorization,"  In this paper, we introduce and provide a short overview of nonnegative
matrix factorization (NMF). Several aspects of NMF are discussed, namely, the
application in hyperspectral imaging, geometry and uniqueness of NMF solutions,
complexity, algorithms, and its link with extended formulations of polyhedra.
In order to put NMF into perspective, the more general problem class of
constrained low-rank matrix approximation problems is first briefly introduced.
",1,0,1,1,0,0
17258,One Model To Learn Them All,"  Deep learning yields great results across many fields, from speech
recognition, image classification, to translation. But for each problem,
getting a deep model to work well involves research into the architecture and a
long period of tuning. We present a single model that yields good results on a
number of problems spanning multiple domains. In particular, this single model
is trained concurrently on ImageNet, multiple translation tasks, image
captioning (COCO dataset), a speech recognition corpus, and an English parsing
task. Our model architecture incorporates building blocks from multiple
domains. It contains convolutional layers, an attention mechanism, and
sparsely-gated layers. Each of these computational blocks is crucial for a
subset of the tasks we train on. Interestingly, even if a block is not crucial
for a task, we observe that adding it never hurts performance and in most cases
improves it on all tasks. We also show that tasks with less data benefit
largely from joint training with other tasks, while performance on large tasks
degrades only slightly if at all.
",1,0,0,1,0,0
14636,Lipschitz Properties for Deep Convolutional Networks,"  In this paper we discuss the stability properties of convolutional neural
networks. Convolutional neural networks are widely used in machine learning. In
classification they are mainly used as feature extractors. Ideally, we expect
similar features when the inputs are from the same class. That is, we hope to
see a small change in the feature vector with respect to a deformation on the
input signal. This can be established mathematically, and the key step is to
derive the Lipschitz properties. Further, we establish that the stability
results can be extended for more general networks. We give a formula for
computing the Lipschitz bound, and compare it with other methods to show it is
closer to the optimal value.
",1,0,1,0,0,0
18795,Autonomous Electric Race Car Design,"  Autonomous driving and electric vehicles are nowadays very active research
and development areas. In this paper we present the conversion of a standard
Kyburz eRod into an autonomous vehicle that can be operated in challenging
environments such as Swiss mountain passes. The overall hardware and software
architectures are described in detail with a special emphasis on the sensor
requirements for autonomous vehicles operating in partially structured
environments. Furthermore, the design process itself and the finalized system
architecture are presented. The work shows state of the art results in
localization and controls for self-driving high-performance electric vehicles.
Test results of the overall system are presented, which show the importance of
generalizable state estimation algorithms to handle a plethora of conditions.
",1,0,0,0,0,0
2882,Faithfulness of Probability Distributions and Graphs,"  A main question in graphical models and causal inference is whether, given a
probability distribution $P$ (which is usually an underlying distribution of
data), there is a graph (or graphs) to which $P$ is faithful. The main goal of
this paper is to provide a theoretical answer to this problem. We work with
general independence models, which contain probabilistic independence models as
a special case. We exploit a generalization of ordering, called preordering, of
the nodes of (mixed) graphs. This allows us to provide sufficient conditions
for a given independence model to be Markov to a graph with the minimum
possible number of edges, and more importantly, necessary and sufficient
conditions for a given probability distribution to be faithful to a graph. We
present our results for the general case of mixed graphs, but specialize the
definitions and results to the better-known subclasses of undirected
(concentration) and bidirected (covariance) graphs as well as directed acyclic
graphs.
",0,0,1,1,0,0
7491,The Velocity of the Decoding Wave for Spatially Coupled Codes on BMS Channels,"  We consider the dynamics of belief propagation decoding of spatially coupled
Low-Density Parity-Check codes. It has been conjectured that after a short
transient phase, the profile of ""error probabilities"" along the spatial
direction of a spatially coupled code develops a uniquely-shaped wave-like
solution that propagates with constant velocity v. Under this assumption, and
for transmission over general Binary Memoryless Symmetric channels, we derive a
formula for v. We also propose approximations that are simpler to compute and
support our findings using numerical data.
",1,0,1,0,0,0
8810,Bayesian Belief Updating of Spatiotemporal Seizure Dynamics,"  Epileptic seizure activity shows complicated dynamics in both space and time.
To understand the evolution and propagation of seizures spatially extended sets
of data need to be analysed. We have previously described an efficient
filtering scheme using variational Laplace that can be used in the Dynamic
Causal Modelling (DCM) framework [Friston, 2003] to estimate the temporal
dynamics of seizures recorded using either invasive or non-invasive electrical
recordings (EEG/ECoG). Spatiotemporal dynamics are modelled using a partial
differential equation -- in contrast to the ordinary differential equation used
in our previous work on temporal estimation of seizure dynamics [Cooray, 2016].
We provide the requisite theoretical background for the method and test the
ensuing scheme on simulated seizure activity data and empirical invasive ECoG
data. The method provides a framework to assimilate the spatial and temporal
dynamics of seizure activity, an aspect of great physiological and clinical
importance.
",1,0,0,1,0,0
20533,Nonlinear Dynamics of a Viscous Bubbly Fluid,"  A physical model of a three-dimensional flow of a viscous bubbly fluid in an
intermediate regime between bubble formation and breakage is presented. The
model is based on mechanics and thermodynamics of a single bubble coupled to
the dynamics of a viscous fluid as a whole, and takes into account multiple
physical effects, including gravity, viscosity, and surface tension.
Dimensionless versions of the resulting nonlinear model are obtained, and
values of dimensionless parameters are estimated for typical magma flows in
horizontal subaerial lava fields and vertical volcanic conduits.
Exact solutions of the resulting system of nonlinear equations corresponding
to equilibrium flows and traveling waves are analyzed in the one-dimensional
setting. Generalized Su-Gardner-type perturbation analysis is employed to study
approximate solutions of the model in the long-wave ansatz. Simplified
nonlinear partial differential equations (PDE) satisfied by the leading terms
of the perturbation solutions are systematically derived. It is shown that for
specific classes of perturbations, approximate solutions of the bubbly fluid
model arise from solutions of the classical diffusion, Burgers,
variable-coefficient Burgers, and Korteweg-de Vries equations.
",0,1,0,0,0,0
9171,A Stochastic Large-scale Machine Learning Algorithm for Distributed Features and Observations,"  As the size of modern data sets exceeds the disk and memory capacities of a
single computer, machine learning practitioners have resorted to parallel and
distributed computing. Given that optimization is one of the pillars of machine
learning and predictive modeling, distributed optimization methods have
recently garnered ample attention, in particular when either observations or
features are distributed, but not both. We propose a general stochastic
algorithm where observations, features, and gradient components can be sampled
in a double distributed setting, i.e., with both features and observations
distributed. Very technical analyses establish convergence properties of the
algorithm under different conditions on the learning rate (diminishing to zero
or constant). Computational experiments in Spark demonstrate a superior
performance of our algorithm versus a benchmark in early iterations of the
algorithm, which is due to the stochastic components of the algorithm.
",0,0,0,1,0,0
6363,Short Presburger arithmetic is hard,"  We study the computational complexity of short sentences in Presburger
arithmetic (Short-PA). Here by ""short"" we mean sentences with a bounded number
of variables, quantifiers, inequalities and Boolean operations; the input
consists only of the integer coefficients involved in the linear inequalities.
We prove that satisfiability of Short-PA sentences with $m+2$ alternating
quantifiers is $\Sigma_{P}^m$-complete or $\Pi_{P}^m$-complete, when the first
quantifier is $\exists$ or $\forall$, respectively. Counting versions and
restricted systems are also analyzed. Further application are given to hardness
of two natural problems in Integer Optimizations.
",1,0,1,0,0,0
10469,Fundamental bounds on MIMO antennas,"  Antenna current optimization is often used to analyze the optimal performance
of antennas. Antenna performance can be quantified in e.g., minimum Q-factor
and efficiency. The performance of MIMO antennas is more involved and, in
general, a single parameter is not sufficient to quantify it. Here, the
capacity of an idealized channel is used as the main performance quantity. An
optimization problem in the current distribution for optimal capacity, measured
in spectral efficiency, given a fixed Q-factor and efficiency is formulated as
a semi-definite optimization problem. A model order reduction based on
characteristic and energy modes is employed to improve the computational
efficiency. The performance bound is illustrated by solving the optimization
problem numerically for rectangular plates and spherical shells.
",0,1,1,0,0,0
18958,Safe Robotic Grasping: Minimum Impact-Force Grasp Selection,"  This paper addresses the problem of selecting from a choice of possible
grasps, so that impact forces will be minimised if a collision occurs while the
robot is moving the grasped object along a post-grasp trajectory. Such
considerations are important for safety in human-robot interaction, where even
a certified ""human-safe"" (e.g. compliant) arm may become hazardous once it
grasps and begins moving an object, which may have significant mass, sharp
edges or other dangers. Additionally, minimising collision forces is critical
to preserving the longevity of robots which operate in uncertain and hazardous
environments, e.g. robots deployed for nuclear decommissioning, where removing
a damaged robot from a contaminated zone for repairs may be extremely difficult
and costly. Also, unwanted collisions between a robot and critical
infrastructure (e.g. pipework) in such high-consequence environments can be
disastrous. In this paper, we investigate how the safety of the post-grasp
motion can be considered during the pre-grasp approach phase, so that the
selected grasp is optimal in terms applying minimum impact forces if a
collision occurs during a desired post-grasp manipulation. We build on the
methods of augmented robot-object dynamics models and ""effective mass"" and
propose a method for combining these concepts with modern grasp and trajectory
planners, to enable the robot to achieve a grasp which maximises the safety of
the post-grasp trajectory, by minimising potential collision forces. We
demonstrate the effectiveness of our approach through several experiments with
both simulated and real robots.
",1,0,0,0,0,0
4416,Harmonic analysis and distribution-free inference for spherical distributions,"  Fourier analysis and representation of circular distributions in terms of
their Fourier coefficients, is quite commonly discussed and used for model-free
inference such as testing uniformity and symmetry etc. in dealing with
2-dimensional directions. However a similar discussion for spherical
distributions, which are used to model 3-dimensional directional data, has not
been fully developed in the literature in terms of their harmonics. This paper,
in what we believe is the first such attempt, looks at the probability
distributions on a unit sphere, through the perspective of spherical harmonics,
analogous to the Fourier analysis for distributions on a unit circle. Harmonic
representations of many currently used spherical models are presented and
discussed. A very general family of spherical distributions is then introduced,
special cases of which yield many known spherical models. Through the prism of
harmonic analysis, one can look at the mean direction, dispersion, and various
forms of symmetry for these models in a generic setting. Aspects of
distribution free inference such as estimation and large-sample tests for these
symmetries, are provided. The paper concludes with a real-data example
analyzing the longitudinal sunspot activity.
",0,0,0,1,0,0
6833,Tracing Networks of Knowledge in the Digital Age,"  The emergence of new digital technologies has allowed the study of human
behaviour at a scale and at level of granularity that were unthinkable just a
decade ago. In particular, by analysing the digital traces left by people
interacting in the online and offline worlds, we are able to trace the
spreading of knowledge and ideas at both local and global scales.
In this article we will discuss how these digital traces can be used to map
knowledge across the world, outlining both the limitations and the challenges
in performing this type of analysis. We will focus on data collected from
social media platforms, large-scale digital repositories and mobile data.
Finally, we will provide an overview of the tools that are available to
scholars and practitioners for understanding these processes using these
emerging forms of data.
",1,1,0,0,0,0
1286,Surjective H-Colouring over Reflexive Digraphs,"  The Surjective H-Colouring problem is to test if a given graph allows a
vertex-surjective homomorphism to a fixed graph H. The complexity of this
problem has been well studied for undirected (partially) reflexive graphs. We
introduce endo-triviality, the property of a structure that all of its
endomorphisms that do not have range of size 1 are automorphisms, as a means to
obtain complexity-theoretic classifications of Surjective H-Colouring in the
case of reflexive digraphs.
Chen [2014] proved, in the setting of constraint satisfaction problems, that
Surjective H-Colouring is NP-complete if H has the property that all of its
polymorphisms are essentially unary. We give the first concrete application of
his result by showing that every endo-trivial reflexive digraph H has this
property. We then use the concept of endo-triviality to prove, as our main
result, a dichotomy for Surjective H-Colouring when H is a reflexive
tournament: if H is transitive, then Surjective H-Colouring is in NL, otherwise
it is NP-complete.
By combining this result with some known and new results we obtain a
complexity classification for Surjective H-Colouring when H is a partially
reflexive digraph of size at most 3.
",1,0,0,0,0,0
12733,On the maximum principle for a time-fractional diffusion equation,"  In this paper, we discuss the maximum principle for a time-fractional
diffusion equation $$ \partial_t^\alpha u(x,t) = \sum_{i,j=1}^n
\partial_i(a_{ij}(x)\partial_j u(x,t)) + c(x)u(x,t) + F(x,t),\ t>0,\ x \in
\Omega \subset {\mathbb R}^n$$ with the Caputo time-derivative of the order
$\alpha \in (0,1)$ in the case of the homogeneous Dirichlet boundary condition.
Compared to the already published results, our findings have two important
special features. First, we derive a maximum principle for a suitably defined
weak solution in the fractional Sobolev spaces, not for the strong solution.
Second, for the non-negative source functions $F = F(x,t)$ we prove the
non-negativity of the weak solution to the problem under consideration without
any restrictions on the sign of the coefficient $c=c(x)$ by the derivative of
order zero in the spatial differential operator. Moreover, we prove the
monotonicity of the solution with respect to the coefficient $c=c(x)$.
",0,0,1,0,0,0
18085,A Multiscale-Analysis of Stochastic Bistable Reaction-Diffusion Equations,"  A multiscale analysis of 1D stochastic bistable reaction-diffusion equations
with additive noise is carried out w.r.t. travelling waves within the
variational approach to stochastic partial differential equations. It is shown
with explicit error estimates on appropriate function spaces that up to lower
order w.r.t. the noise amplitude, the solution can be decomposed into the
orthogonal sum of a travelling wave moving with random speed and into Gaussian
fluctuations. A stochastic differential equation describing the speed of the
travelling wave and a linear stochastic partial differential equation
describing the fluctuations are derived in terms of the coefficients. Our
results extend corresponding results obtained for stochastic neural field
equations to the present class of stochastic dynamics.
",0,0,1,0,0,0
7043,What is a hierarchically hyperbolic space?,"  The first part of this survey is a heuristic, non-technical discussion of
what an HHS is, and the aim is to provide a good mental picture both to those
actively doing research on HHSs and to those who only seek a basic
understanding out of pure curiosity. It can be read independently of the second
part, which is a detailed technical discussion of the axioms and the main tools
to deal with HHSs.
",0,0,1,0,0,0
10964,Weak saturation and weak amalgamation property,"  The two model-theoretic concepts of weak saturation and weak amalgamation
property are studied in the context of accessible categories. We relate these
two concepts providing sufficient conditions for existence and uniqueness of
weakly saturated objects of an accessible category K. We discuss the
implications of this fact in classical model theory.
",0,0,1,0,0,0
5542,Boosting the power factor with resonant states: a model study,"  A particularly promising pathway to enhance the efficiency of thermoelectric
materials lies in the use of resonant states, as suggested by experimentalists
and theorists alike. In this paper, we go over the mechanisms used in the
literature to explain how resonant levels affect the thermoelectric properties,
and we suggest that the effects of hybridization are crucial yet
ill-understood. In order to get a good grasp of the physical picture and to
draw guidelines for thermoelectric enhancement, we use a tight-binding model
containing a conduction band hybridized with a flat band. We find that the
conductivity is suppressed in a wide energy range near the resonance, but that
the Seebeck coefficient can be boosted for strong enough hybridization, thus
allowing for a significant increase of the power factor. The Seebeck
coefficient can also display a sign change as the Fermi level crosses the
resonance. Our results suggest that in order to boost the power factor, the
hybridization strength must not be too low, the resonant level must not be too
close to the conduction (or valence) band edge, and the Fermi level must be
located around, but not inside, the resonant peak.
",0,1,0,0,0,0
19471,"eXpose: A Character-Level Convolutional Neural Network with Embeddings For Detecting Malicious URLs, File Paths and Registry Keys","  For years security machine learning research has promised to obviate the need
for signature based detection by automatically learning to detect indicators of
attack. Unfortunately, this vision hasn't come to fruition: in fact, developing
and maintaining today's security machine learning systems can require
engineering resources that are comparable to that of signature-based detection
systems, due in part to the need to develop and continuously tune the
""features"" these machine learning systems look at as attacks evolve. Deep
learning, a subfield of machine learning, promises to change this by operating
on raw input signals and automating the process of feature design and
extraction. In this paper we propose the eXpose neural network, which uses a
deep learning approach we have developed to take generic, raw short character
strings as input (a common case for security inputs, which include artifacts
like potentially malicious URLs, file paths, named pipes, named mutexes, and
registry keys), and learns to simultaneously extract features and classify
using character-level embeddings and convolutional neural network. In addition
to completely automating the feature design and extraction process, eXpose
outperforms manual feature extraction based baselines on all of the intrusion
detection problems we tested it on, yielding a 5%-10% detection rate gain at
0.1% false positive rate compared to these baselines.
",1,0,0,0,0,0
435,Hausdorff dimensions in $p$-adic analytic groups,"  Let $G$ be a finitely generated pro-$p$ group, equipped with the $p$-power
series. The associated metric and Hausdorff dimension function give rise to the
Hausdorff spectrum, which consists of the Hausdorff dimensions of closed
subgroups of $G$. In the case where $G$ is $p$-adic analytic, the Hausdorff
dimension function is well understood; in particular, the Hausdorff spectrum
consists of finitely many rational numbers closely linked to the analytic
dimensions of subgroups of $G$.
Conversely, it is a long-standing open question whether the finiteness of the
Hausdorff spectrum implies that $G$ is $p$-adic analytic. We prove that the
answer is yes, in a strong sense, under the extra condition that $G$ is
soluble.
Furthermore, we explore the problem and related questions also for other
filtration series, such as the lower $p$-series, the Frattini series, the
modular dimension subgroup series and quite general filtration series. For
instance, we prove, for odd primes $p$, that every countably based pro-$p$
group $G$ with an open subgroup mapping onto 2 copies of the $p$-adic integers
admits a filtration series such that the corresponding Hausdorff spectrum
contains an infinite real interval.
",0,0,1,0,0,0
7667,Barcode Embeddings for Metric Graphs,"  Stable topological invariants are a cornerstone of persistence theory and
applied topology, but their discriminative properties are often
poorly-understood. In this paper we study a rich homology-based invariant first
defined by Dey, Shi, and Wang, which we think of as embedding a metric graph in
the barcode space. We prove that this invariant is locally injective on the
space of metric graphs and globally injective on a GH-dense subset. Moreover,
we define a new topology on the space of metric graphs, which we call the
fibered topology, for which the barcode transform is injective on a generic
(open and dense) subset.
",0,0,1,0,0,0
6658,Solvability of the operator Riccati equation in the Feshbach case,"  We consider a bounded block operator matrix of the form $$
L=\left(\begin{array}{cc} A & B \\ C & D \end{array} \right), $$ where the
main-diagonal entries $A$ and $D$ are self-adjoint operators on Hilbert spaces
$H_{_A}$ and $H_{_D}$, respectively; the coupling $B$ maps $H_{_D}$ to $H_{_A}$
and $C$ is an operator from $H_{_A}$ to $H_{_D}$. It is assumed that the
spectrum $\sigma_{_D}$ of $D$ is absolutely continuous and uniform, being
presented by a single band $[\alpha,\beta]\subset\mathbb{R}$, $\alpha<\beta$,
and the spectrum $\sigma_{_A}$ of $A$ is embedded into $\sigma_{_D}$, that is,
$\sigma_{_A}\subset(\alpha,\beta)$. We formulate conditions under which there
are bounded solutions to the operator Riccati equations associated with the
complexly deformed block operator matrix $L$; in such a case the deformed
operator matrix $L$ admits a block diagonalization. The same conditions also
ensure the Markus-Matsaev-type factorization of the Schur complement
$M_{_A}(z)=A-z-B(D-z)^{-1}C$ analytically continued onto the unphysical
sheet(s) of the complex $z$ plane adjacent to the band $[\alpha,\beta]$. We
prove that the operator roots of the continued Schur complement $M_{_A}$ are
explicitly expressed through the respective solutions to the deformed Riccati
equations.
",0,0,1,0,0,0
20209,Detection of the Stellar Intracluster Medium in Perseus (Abell 426),"  Hubble Space Telescope photometry from the ACS/WFC and WFPC2 cameras is used
to detect and measure globular clusters (GCs) in the central region of the rich
Perseus cluster of galaxies. A detectable population of Intragalactic GCs is
found extending out to at least 500 kpc from the cluster center. These objects
display luminosity and color (metallicity) distributions that are entirely
normal for GC populations. Extrapolating from the limited spatial coverage of
the HST fields, we estimate very roughly that the entire Perseus cluster should
contain ~50000 or more IGCs, but a targetted wide-field survey will be needed
for a more definitive answer. Separate brief results are presented for the rich
GC systems in NGC 1272 and NGC 1275, the two largest Perseus ellipticals. For
NGC 1272 we find a specific frequency S_N = 8, while for the central giant NGC
1275, S_N ~ 12. In both these giant galaxies, the GC colors are well matched by
bimodal distributions, with the majority in the blue (metal-poor) component.
This preliminary study suggests that Perseus is a prime target for a more
comprehensive deep imaging survey of Intragalactic GCs.
",0,1,0,0,0,0
18523,Asymptotic properties of maximum likelihood estimator for the growth rate of a stable CIR process based on continuous time observations,"  We consider a stable Cox--Ingersoll--Ross process driven by a standard Wiener
process and a spectrally positive strictly stable Lévy process, and we study
asymptotic properties of the maximum likelihood estimator (MLE) for its growth
rate based on continuous time observations. We distinguish three cases:
subcritical, critical and supercritical. In all cases we prove strong
consistency of the MLE in question, in the subcritical case asymptotic
normality, and in the supercritical case asymptotic mixed normality are shown
as well. In the critical case the description of the asymptotic behavior of the
MLE in question remains open.
",0,0,1,1,0,0
4131,Truncation in Hahn Fields is Undecidable and Wild,"  We show that in any nontrivial Hahn field with truncation as a primitive
operation we can interpret the monadic second-order logic of the additive
monoid of natural numbers and are thus undecidable. We also specify a definable
binary relation on such a structure that has $\SOP$ and $\TP$.
",0,0,1,0,0,0
11568,Smooth equivalence of deformations of domains in complex euclidean spaces,"  We prove that two smooth families of 2-connected domains in $\cc$ are
smoothly equivalent if they are equivalent under a possibly discontinuous
family of biholomorphisms. We construct, for $m \geq 3$, two smooth families of
smoothly bounded $m$-connected domains in $\cc$, and for $n\geq2$, two families
of strictly pseudoconvex domains in $\cc^n$, that are equivalent under
discontinuous families of biholomorphisms but not under any continuous family
of biholomorphisms. Finally, we give sufficient conditions for the smooth
equivalence of two smooth families of domains.
",0,0,1,0,0,0
5078,Tunable coupling-induced resonance splitting in self-coupled Silicon ring cavity with robust spectral characteristics,"  We propose and demonstrate a self-coupled microring resonator for resonance
splitting by mutual mode coupling of cavity mode and counter-propagating mode
in Silicon-on-Insulator platform The resonator is constructed with a
self-coupling region that can excite counter-propagating mode. We
experimentally study the effect of self-coupling on the resonance splitting,
resonance extinction, and quality-factor evolution and stability. Based on the
coupling, we achieve 72% of FSR splitting for a cavity with FSR 2.1 nm with <
5% variation in the cavity quality factor. The self-coupled resonance splitting
shows highly robust spectral characteristic that can be exploited for sensing
and optical signal processing.
",0,1,0,0,0,0
3608,"Supercharacters and the discrete Fourier, cosine, and sine transforms","  Using supercharacter theory, we identify the matrices that are diagonalized
by the discrete cosine and discrete sine transforms, respectively. Our method
affords a combinatorial interpretation for the matrix entries.
",0,0,1,0,0,0
6929,Universality for eigenvalue algorithms on sample covariance matrices,"  We prove a universal limit theorem for the halting time, or iteration count,
of the power/inverse power methods and the QR eigenvalue algorithm.
Specifically, we analyze the required number of iterations to compute extreme
eigenvalues of random, positive-definite sample covariance matrices to within a
prescribed tolerance. The universality theorem provides a complexity estimate
for the algorithms which, in this random setting, holds with high probability.
The method of proof relies on recent results on the statistics of the
eigenvalues and eigenvectors of random sample covariance matrices (i.e.,
delocalization, rigidity and edge universality).
",0,0,1,0,0,0
3833,Efficient exploration with Double Uncertain Value Networks,"  This paper studies directed exploration for reinforcement learning agents by
tracking uncertainty about the value of each available action. We identify two
sources of uncertainty that are relevant for exploration. The first originates
from limited data (parametric uncertainty), while the second originates from
the distribution of the returns (return uncertainty). We identify methods to
learn these distributions with deep neural networks, where we estimate
parametric uncertainty with Bayesian drop-out, while return uncertainty is
propagated through the Bellman equation as a Gaussian distribution. Then, we
identify that both can be jointly estimated in one network, which we call the
Double Uncertain Value Network. The policy is directly derived from the learned
distributions based on Thompson sampling. Experimental results show that both
types of uncertainty may vastly improve learning in domains with a strong
exploration challenge.
",1,0,0,1,0,0
12055,Superpixel-based Semantic Segmentation Trained by Statistical Process Control,"  Semantic segmentation, like other fields of computer vision, has seen a
remarkable performance advance by the use of deep convolution neural networks.
However, considering that neighboring pixels are heavily dependent on each
other, both learning and testing of these methods have a lot of redundant
operations. To resolve this problem, the proposed network is trained and tested
with only 0.37% of total pixels by superpixel-based sampling and largely
reduced the complexity of upsampling calculation. The hypercolumn feature maps
are constructed by pyramid module in combination with the convolution layers of
the base network. Since the proposed method uses a very small number of sampled
pixels, the end-to-end learning of the entire network is difficult with a
common learning rate for all the layers. In order to resolve this problem, the
learning rate after sampling is controlled by statistical process control (SPC)
of gradients in each layer. The proposed method performs better than or equal
to the conventional methods that use much more samples on Pascal Context,
SUN-RGBD dataset.
",1,0,0,0,0,0
10293,A Deep Causal Inference Approach to Measuring the Effects of Forming Group Loans in Online Non-profit Microfinance Platform,"  Kiva is an online non-profit crowdsouring microfinance platform that raises
funds for the poor in the third world. The borrowers on Kiva are small business
owners and individuals in urgent need of money. To raise funds as fast as
possible, they have the option to form groups and post loan requests in the
name of their groups. While it is generally believed that group loans pose less
risk for investors than individual loans do, we study whether this is the case
in a philanthropic online marketplace. In particular, we measure the effect of
group loans on funding time while controlling for the loan sizes and other
factors. Because loan descriptions (in the form of texts) play an important
role in lenders' decision process on Kiva, we make use of this information
through deep learning in natural language processing. In this aspect, this is
the first paper that uses one of the most advanced deep learning techniques to
deal with unstructured data in a way that can take advantage of its superior
prediction power to answer causal questions. We find that on average, forming
group loans speeds up the funding time by about 3.3 days.
",0,0,0,1,0,0
17728,A path integral approach to Bayesian inference in Markov processes,"  We formulate Bayesian updates in Markov processes by means of path integral
techniques and derive the imaginary-time Schrödinger equation with
likelihood to direct the inference incorporated as a potential for the
posterior probability distribution
",0,0,1,1,0,0
5384,Infinite horizon asymptotic average optimality for large-scale parallel server networks,"  We study infinite-horizon asymptotic average optimality for parallel server
network with multiple classes of jobs and multiple server pools in the
Halfin-Whitt regime. Three control formulations are considered: 1) minimizing
the queueing and idleness cost, 2) minimizing the queueing cost under a
constraints on idleness at each server pool, and 3) fairly allocating the idle
servers among different server pools. For the third problem, we consider a
class of bounded-queue, bounded-state (BQBS) stable networks, in which any
moment of the state is bounded by that of the queue only (for both the limiting
diffusion and diffusion-scaled state processes). We show that the optimal
values for the diffusion-scaled state processes converge to the corresponding
values of the ergodic control problems for the limiting diffusion. We present a
family of state-dependent Markov balanced saturation policies (BSPs) that
stabilize the controlled diffusion-scaled state processes. It is shown that
under these policies, the diffusion-scaled state process is exponentially
ergodic, provided that at least one class of jobs has a positive abandonment
rate. We also establish useful moment bounds, and study the ergodic properties
of the diffusion-scaled state processes, which play a crucial role in proving
the asymptotic optimality.
",1,0,1,0,0,0
2851,Biologically inspired protection of deep networks from adversarial attacks,"  Inspired by biophysical principles underlying nonlinear dendritic computation
in neural circuits, we develop a scheme to train deep neural networks to make
them robust to adversarial attacks. Our scheme generates highly nonlinear,
saturated neural networks that achieve state of the art performance on gradient
based adversarial examples on MNIST, despite never being exposed to
adversarially chosen examples during training. Moreover, these networks exhibit
unprecedented robustness to targeted, iterative schemes for generating
adversarial examples, including second-order methods. We further identify
principles governing how these networks achieve their robustness, drawing on
methods from information geometry. We find these networks progressively create
highly flat and compressed internal representations that are sensitive to very
few input dimensions, while still solving the task. Moreover, they employ
highly kurtotic weight distributions, also found in the brain, and we
demonstrate how such kurtosis can protect even linear classifiers from
adversarial attack.
",1,0,0,1,0,0
20059,Inverse problem for multi-species mean field models in the low temperature phase,"  In this paper we solve the inverse problem for a class of mean field models
(Curie-Weiss model and its multi-species version) when multiple thermodynamic
states are present, as in the low temperature phase where the phase space is
clustered. The inverse problem consists in reconstructing the model parameters
starting from configuration data generated according to the distribution of the
model. We show that the application of the inversion procedure without taking
into account the presence of many states produces very poor inference results.
This problem is overcomed using the clustering algorithm. When the system has
two symmetric states of positive and negative magnetization, the parameter
reconstruction can be also obtained with smaller computational effort simply by
flipping the sign of the magnetizations from positive to negative (or
viceversa). The parameter reconstruction fails when the system is critical: in
this case we give the correct inversion formulas for the Curie-Weiss model and
we show that they can be used to measuring how much the system is close to
criticality.
",0,1,0,0,0,0
8897,A Data-Driven Framework for Assessing Cold Load Pick-up Demand in Service Restoration,"  Cold load pick-up (CLPU) has been a critical concern to utilities.
Researchers and industry practitioners have underlined the impact of CLPU on
distribution system design and service restoration. The recent large-scale
deployment of smart meters has provided the industry with a huge amount of data
that is highly granular, both temporally and spatially. In this paper, a
data-driven framework is proposed for assessing CLPU demand of residential
customers using smart meter data. The proposed framework consists of two
interconnected layers: 1) At the feeder level, a nonlinear auto-regression
model is applied to estimate the diversified demand during the system
restoration and calculate the CLPU demand ratio. 2) At the customer level,
Gaussian Mixture Models (GMM) and probabilistic reasoning are used to quantify
the CLPU demand increase. The proposed methodology has been verified using real
smart meter data and outage cases.
",1,0,0,0,0,0
9375,The Galaxy-Halo Connection Over The Last 13.3 Gyrs,"  We present new determinations of the stellar-to-halo mass relation (SHMR) at
$z=0-10$ that match the evolution of the galaxy stellar mass function, the
SFR$-M_*$ relation,and the cosmic star formation rate. We utilize a compilation
of 40 observational studies from the literature and correct them for potential
biases. Using our robust determinations of halo mass assembly and the SHMR, we
infer star formation histories, merger rates, and structural properties for
average galaxies, combining star-forming and quenched galaxies. Our main
findings: (1) The halo mass $M_{50}$ above which 50\% of galaxies are quenched
coincides with sSFR/sMAR$\sim1$, where sMAR is the specific halo mass accretion
rate. (2) $M_{50}$ increases with redshift, presumably due to cold streams
being more efficient at high redshift while virial shocks and AGN feedback
become more relevant at lower redshifts. (3) The ratio sSFR/sMAR has a peak
value, which occurs around $M_{\rm vir}\sim2\times10^{11}M_{\odot}$. (4) The
stellar mass density within 1 kpc, $\Sigma_1$, is a good indicator of the
galactic global sSFR. (5) Galaxies are statistically quenched after they reach
a maximum in $\Sigma_1$, consistent with theoretical expectations of the gas
compaction model; this maximum depends on redshift. (6) In-situ star formation
is responsible for most galactic stellar mass growth, especially for lower-mass
galaxies. (7) Galaxies grow inside out. The marked change in the slope of the
size--mass relation when galaxies became quenched, from $d\log R_{\rm
eff}/d\log M_*\sim0.35$ to $\sim2.5$, could be the result of dry minor mergers.
",0,1,0,0,0,0
4727,Optimal paths on the road network as directed polymers,"  We analyze the statistics of the shortest and fastest paths on the road
network between randomly sampled end points. To a good approximation, these
optimal paths are found to be directed in that their lengths (at large scales)
are linearly proportional to the absolute distance between them. This motivates
comparisons to universal features of directed polymers in random media. There
are similarities in scalings of fluctuations in length/time and transverse
wanderings, but also important distinctions in the scaling exponents, likely
due to long-range correlations in geographic and man-made features. At short
scales the optimal paths are not directed due to circuitous excursions governed
by a fat-tailed (power-law) probability distribution.
",0,1,0,0,0,0
7968,Duality of deconfined quantum critical point in two dimensional Dirac semimetals,"  In this paper we discuss the N$\acute{e}$el and Kekul$\acute{e}$ valence bond
solids quantum criticality in graphene Dirac semimetal. Considering the quartic
four-fermion interaction $g(\bar{\psi}_i\Gamma_{ij}\psi_j)^2$ that contains
spin,valley, and sublattice degrees of freedom in the continuum field theory,
we find the microscopic symmetry is spontaneously broken when the coupling $g$
is greater than a critical value $g_c$. The symmetry breaking gaps out the
fermion and leads to semimetal-insulator transition. All possible quartic
fermion-bilinear interactions give rise to the uniform critical coupling, which
exhibits the multicritical point for various orders and the Landau-forbidden
quantum critical point. We also investigate the typical critical point between
N$\acute{e}$el and Kekul$\acute{e}$ valence bond solid transition when the
symmetry is broken. The quantum criticality is captured by the
Wess-Zumino-Witten term and there exist a mutual-duality for
N$\acute{e}$el-Kekul$\acute{e}$ VBS order. We show the emergent spinon in the
N$\acute{e}$el-Kekul$\acute{e}$ VBS transition , from which we conclude the
phase transition is a deconfined quantum critical point. Additionally, the
connection between the index theorem and zero energy mode bounded by the
topological defect in the Kekul$\acute{e}$ VBS phase is studied to reveal the
N$\acute{e}$el-Kekul$\acute{e}$ VBS duality.
",0,1,0,0,0,0
14537,Local Linear Constraint based Optimization Model for Dual Spectral CT,"  Dual spectral computed tomography (DSCT) can achieve energy- and
material-selective images, and has a superior distinguishability of some
materials than conventional single spectral computed tomography (SSCT).
However, the decomposition process is illposed, which is sensitive with noise,
thus the quality of decomposed images are usually degraded, especially the
signal-to-noise ratio (SNR) is much lower than single spectra based directly
reconstructions. In this work, we first establish a local linear relationship
between dual spectra based decomposed results and single spectra based directly
reconstructed images. Then, based on this constraint, we propose an
optimization model for DSCT and develop a guided image filtering based
iterative solution method. Both simulated and real experiments are provided to
validate the effectiveness of the proposed approach.
",0,0,1,0,0,0
19481,On 2d-4d motivic wall-crossing formulas,"  In this paper we propose definitions and examples of categorical enhancements
of the data involved in the $2d$-$4d$ wall-crossing formulas which generalize
both Cecotti-Vafa and Kontsevich-Soibelman motivic wall-crossing formulas.
",0,0,1,0,0,0
8145,Driver Drowsiness Estimation from EEG Signals Using Online Weighted Adaptation Regularization for Regression (OwARR),"  One big challenge that hinders the transition of brain-computer interfaces
(BCIs) from laboratory settings to real-life applications is the availability
of high-performance and robust learning algorithms that can effectively handle
individual differences, i.e., algorithms that can be applied to a new subject
with zero or very little subject-specific calibration data. Transfer learning
and domain adaptation have been extensively used for this purpose. However,
most previous works focused on classification problems. This paper considers an
important regression problem in BCI, namely, online driver drowsiness
estimation from EEG signals. By integrating fuzzy sets with domain adaptation,
we propose a novel online weighted adaptation regularization for regression
(OwARR) algorithm to reduce the amount of subject-specific calibration data,
and also a source domain selection (SDS) approach to save about half of the
computational cost of OwARR. Using a simulated driving dataset with 15
subjects, we show that OwARR and OwARR-SDS can achieve significantly smaller
estimation errors than several other approaches. We also provide comprehensive
analyses on the robustness of OwARR and OwARR-SDS.
",1,0,0,0,0,0
20415,An active-learning algorithm that combines sparse polynomial chaos expansions and bootstrap for structural reliability analysis,"  Polynomial chaos expansions (PCE) have seen widespread use in the context of
uncertainty quantification. However, their application to structural
reliability problems has been hindered by the limited performance of PCE in the
tails of the model response and due to the lack of local metamodel error
estimates. We propose a new method to provide local metamodel error estimates
based on bootstrap resampling and sparse PCE. An initial experimental design is
iteratively updated based on the current estimation of the limit-state surface
in an active learning algorithm. The greedy algorithm uses the bootstrap-based
local error estimates for the polynomial chaos predictor to identify the best
candidate set of points to enrich the experimental design. We demonstrate the
effectiveness of this approach on a well-known analytical benchmark
representing a series system, on a truss structure and on a complex realistic
frame structure problem.
",0,0,0,1,0,0
5803,An iterative aggregation and disaggregation approach to the calculation of steady-state distributions of continuous processes,"  A mapping of the process on a continuous configuration space to the symbolic
representation of the motion on a discrete state space will be combined with an
iterative aggregation and disaggregation (IAD) procedure to obtain steady state
distributions of the process. The IAD speeds up the convergence to the unit
eigenvector, which is the steady state distribution, by forming smaller
aggregated matrices whose unit eigenvector solutions are used to refine
approximations of the steady state vector until convergence is reached. This
method works very efficiently and can be used together with distributed or
parallel computing methods to obtain high resolution images of the steady state
distribution of complex atomistic or energy landscape type problems. The method
is illustrated in two numerical examples. In the first example the transition
matrix is assumed to be known. The second example represents an overdamped
Brownian motion process subject to a dichotomously changing external potential.
",0,1,0,0,0,0
1983,Learning Heuristic Search via Imitation,"  Robotic motion planning problems are typically solved by constructing a
search tree of valid maneuvers from a start to a goal configuration. Limited
onboard computation and real-time planning constraints impose a limit on how
large this search tree can grow. Heuristics play a crucial role in such
situations by guiding the search towards potentially good directions and
consequently minimizing search effort. Moreover, it must infer such directions
in an efficient manner using only the information uncovered by the search up
until that time. However, state of the art methods do not address the problem
of computing a heuristic that explicitly minimizes search effort. In this
paper, we do so by training a heuristic policy that maps the partial
information from the search to decide which node of the search tree to expand.
Unfortunately, naively training such policies leads to slow convergence and
poor local minima. We present SaIL, an efficient algorithm that trains
heuristic policies by imitating ""clairvoyant oracles"" - oracles that have full
information about the world and demonstrate decisions that minimize search
effort. We leverage the fact that such oracles can be efficiently computed
using dynamic programming and derive performance guarantees for the learnt
heuristic. We validate the approach on a spectrum of environments which show
that SaIL consistently outperforms state of the art algorithms. Our approach
paves the way forward for learning heuristics that demonstrate an anytime
nature - finding feasible solutions quickly and incrementally refining it over
time.
",1,0,0,0,0,0
14126,Stochastic Functional Gradient Path Planning in Occupancy Maps,"  Planning safe paths is a major building block in robot autonomy. It has been
an active field of research for several decades, with a plethora of planning
methods. Planners can be generally categorised as either trajectory optimisers
or sampling-based planners. The latter is the predominant planning paradigm for
occupancy maps. Trajectory optimisation entails major algorithmic changes to
tackle contextual information gaps caused by incomplete sensor coverage of the
map. However, the benefits are substantial, as trajectory optimisers can reason
on the trade-off between path safety and efficiency.
In this work, we improve our previous work on stochastic functional gradient
planners. We introduce a novel expressive path representation based on kernel
approximation, that allows cost effective model updates based on stochastic
samples. The main drawback of the previous stochastic functional gradient
planner was the cubic cost, stemming from its non-parametric path
representation. Our novel approximate kernel based model, on the other hand,
has a fixed linear cost that depends solely on the number of features used to
represent the path. We show that the stochasticity of the samples is crucial
for the planner and present comparisons to other state-of-the-art planning
methods in both simulation and with real occupancy data. The experiments
demonstrate the advantages of the stochastic approximate kernel method for path
planning in occupancy maps.
",1,0,0,0,0,0
18833,Borg's Periodicity Theorems for first order self-adjoint systems with complex potentials,"  A self-adjoint first order system with Hermitian $\pi$-periodic potential
$Q(z)$, integrable on compact sets, is considered. It is shown that all zeros
of $\Delta + 2e^{-i\int_0^\pi \Im q dt}$ are double zeros if and only if this
self-adjoint system is unitarily equivalent to one in which $Q(z)$ is
$\frac{\pi}{2}$-periodic. Furthermore, the zeros of $\Delta - 2e^{-i\int_0^\pi
\Im q dt}$ are all double zeros if and only if the associated self-adjoint
system is unitarily equivalent to one in which $Q(z) = \sigma_2 Q(z) \sigma_2$.
Here $\Delta$ denotes the discriminant of the system and $\sigma_0$, $\sigma_2$
are Pauli matrices. Finally, it is shown that all instability intervals vanish
if and only if $Q = r\sigma_0 + q\sigma_2$, for some real valued $\pi$-periodic
functions $r$ and $q$ integrable on compact sets.
",0,0,1,0,0,0
3453,Extracting urban impervious surface from GF-1 imagery using one-class classifiers,"  Impervious surface area is a direct consequence of the urbanization, which
also plays an important role in urban planning and environmental management.
With the rapidly technical development of remote sensing, monitoring urban
impervious surface via high spatial resolution (HSR) images has attracted
unprecedented attention recently. Traditional multi-classes models are
inefficient for impervious surface extraction because it requires labeling all
needed and unneeded classes that occur in the image exhaustively. Therefore, we
need to find a reliable one-class model to classify one specific land cover
type without labeling other classes. In this study, we investigate several
one-class classifiers, such as Presence and Background Learning (PBL), Positive
Unlabeled Learning (PUL), OCSVM, BSVM and MAXENT, to extract urban impervious
surface area using high spatial resolution imagery of GF-1, China's new
generation of high spatial remote sensing satellite, and evaluate the
classification accuracy based on artificial interpretation results. Compared to
traditional multi-classes classifiers (ANN and SVM), the experimental results
indicate that PBL and PUL provide higher classification accuracy, which is
similar to the accuracy provided by ANN model. Meanwhile, PBL and PUL
outperforms OCSVM, BSVM, MAXENT and SVM models. Hence, the one-class
classifiers only need a small set of specific samples to train models without
losing predictive accuracy, which is supposed to gain more attention on urban
impervious surface extraction or other one specific land cover type.
",1,0,0,0,0,0
13011,Real-space analysis of scanning tunneling microscopy topography datasets using sparse modeling approach,"  A sparse modeling approach is proposed for analyzing scanning tunneling
microscopy topography data, which contains numerous peaks corresponding to
surface atoms. The method, based on the relevance vector machine with
$\mathrm{L}_1$ regularization and $k$-means clustering, enables separation of
the peaks and atomic center positioning with accuracy beyond the resolution of
the measurement grid. The validity and efficiency of the proposed method are
demonstrated using synthetic data in comparison to the conventional
least-square method. An application of the proposed method to experimental data
of a metallic oxide thin film clearly indicates the existence of defects and
corresponding local lattice deformations.
",0,1,0,0,0,0
7281,Towards Audio to Scene Image Synthesis using Generative Adversarial Network,"  Humans can imagine a scene from a sound. We want machines to do so by using
conditional generative adversarial networks (GANs). By applying the techniques
including spectral norm, projection discriminator and auxiliary classifier,
compared with naive conditional GAN, the model can generate images with better
quality in terms of both subjective and objective evaluations. Almost
three-fourth of people agree that our model have the ability to generate images
related to sounds. By inputting different volumes of the same sound, our model
output different scales of changes based on the volumes, showing that our model
truly knows the relationship between sounds and images to some extent.
",1,0,0,0,0,0
11205,Distributed Nesterov gradient methods over arbitrary graphs,"  In this letter, we introduce a distributed Nesterov method, termed as
$\mathcal{ABN}$, that does not require doubly-stochastic weight matrices.
Instead, the implementation is based on a simultaneous application of both row-
and column-stochastic weights that makes this method applicable to arbitrary
(strongly-connected) graphs. Since constructing column-stochastic weights needs
additional information (the number of outgoing neighbors at each agent), not
available in certain communication protocols, we derive a variation, termed as
FROZEN, that only requires row-stochastic weights but at the expense of
additional iterations for eigenvector learning. We numerically study these
algorithms for various objective functions and network parameters and show that
the proposed distributed Nesterov methods achieve acceleration compared to the
current state-of-the-art methods for distributed optimization.
",1,0,0,1,0,0
17506,Spatial heterogeneities shape collective behavior of signaling amoeboid cells,"  We present novel experimental results on pattern formation of signaling
Dictyostelium discoideum amoeba in the presence of a periodic array of
millimeter-sized pillars. We observe concentric cAMP waves that initiate almost
synchronously at the pillars and propagate outwards. These waves have higher
frequency than the other firing centers and dominate the system dynamics. The
cells respond chemotactically to these circular waves and stream towards the
pillars, forming periodic Voronoi domains that reflect the periodicity of the
underlying lattice. We performed comprehensive numerical simulations of a
reaction-diffusion model to study the characteristics of the boundary
conditions given by the obstacles. Our simulations show that, the obstacles can
act as the wave source depending on the imposed boundary condition.
Interestingly, a critical minimum accumulation of cAMP around the obstacles is
needed for the pillars to act as the wave source. This critical value is lower
at smaller production rates of the intracellular cAMP which can be controlled
in our experiments using caffeine. Experiments and simulations also show that
in the presence of caffeine the number of firing centers is reduced which is
crucial in our system for circular waves emitted from the pillars to
successfully take over the dynamics. These results are crucial to understand
the signaling mechanism of Dictyostelium cells that experience spatial
heterogeneities in its natural habitat.
",0,0,0,0,1,0
15270,Towards Sparse Hierarchical Graph Classifiers,"  Recent advances in representation learning on graphs, mainly leveraging graph
convolutional networks, have brought a substantial improvement on many
graph-based benchmark tasks. While novel approaches to learning node embeddings
are highly suitable for node classification and link prediction, their
application to graph classification (predicting a single label for the entire
graph) remains mostly rudimentary, typically using a single global pooling step
to aggregate node features or a hand-designed, fixed heuristic for hierarchical
coarsening of the graph structure. An important step towards ameliorating this
is differentiable graph coarsening---the ability to reduce the size of the
graph in an adaptive, data-dependent manner within a graph neural network
pipeline, analogous to image downsampling within CNNs. However, the previous
prominent approach to pooling has quadratic memory requirements during training
and is therefore not scalable to large graphs. Here we combine several recent
advances in graph neural network design to demonstrate that competitive
hierarchical graph classification results are possible without sacrificing
sparsity. Our results are verified on several established graph classification
benchmarks, and highlight an important direction for future research in
graph-based neural networks.
",1,0,0,0,0,0
15905,Programming from Metaphorisms,"  This paper presents a study of the metaphorism pattern of relational
specification, showing how it can be refined into recursive programs.
Metaphorisms express input-output relationships which preserve relevant
information while at the same time some intended optimization takes place. Text
processing, sorting, representation changers, etc., are examples of
metaphorisms. The kind of metaphorism refinement studied in this paper is a
strategy known as change of virtual data structure. By framing metaphorisms in
the class of (inductive) regular relations, sufficient conditions are given for
such implementations to be calculated using relation algebra. The strategy is
illustrated with examples including the derivation of the quicksort and
mergesort algorithms, showing what they have in common and what makes them
different from the very start of development.
",1,0,0,0,0,0
5476,Deep adversarial neural decoding,"  Here, we present a novel approach to solve the problem of reconstructing
perceived stimuli from brain responses by combining probabilistic inference
with deep learning. Our approach first inverts the linear transformation from
latent features to brain responses with maximum a posteriori estimation and
then inverts the nonlinear transformation from perceived stimuli to latent
features with adversarial training of convolutional neural networks. We test
our approach with a functional magnetic resonance imaging experiment and show
that it can generate state-of-the-art reconstructions of perceived faces from
brain activations.
",1,0,0,1,0,0
11783,Divergence and Sufficiency for Convex Optimization,"  Logarithmic score and information divergence appear in information theory,
statistics, statistical mechanics, and portfolio theory. We demonstrate that
all these topics involve some kind of optimization that leads directly to
regret functions and such regret functions are often given by a Bregman
divergence. If the regret function also fulfills a sufficiency condition it
must be proportional to information divergence. We will demonstrate that
sufficiency is equivalent to the apparently weaker notion of locality and it is
also equivalent to the apparently stronger notion of monotonicity. These
sufficiency conditions have quite different relevance in the different areas of
application, and often they are not fulfilled. Therefore sufficiency conditions
can be used to explain when results from one area can be transferred directly
to another and when one will experience differences.
",1,1,1,0,0,0
14988,On the contribution of thermal excitation to the total 630.0 nm emissions in the northern cusp ionosphere,"  Direct impact excitation by precipitating electrons is believed to be the
main source of 630.0 nm emissions in the cusp ionosphere. However, this paper
investigates a different source, 630.0 emissions caused by thermally excited
atomic oxygen O$(^{1}$D) when high electron temperature prevail in the cusp. On
22 January 2012 and 14 January 2013, the European Incoherent Scatter Scientific
Association (EISCAT) radar on Svalbard measured electron temperature
enhancements exceeding 3000 K near magnetic noon in the cusp ionosphere over
Svalbard. The electron temperature enhancements corresponded to electron
density enhancements exceeding $10^{11}$m$^{-3}$ accompanied by intense 630.0
nm emissions in a field of view common to both the EISCAT Svalbard radar and a
meridian scanning photometer. This offered an excellent opportunity to
investigate the role of thermally excited O$(^{1}$D) 630.0 nm emissions in the
cusp ionosphere. The thermal component was derived from the EISCAT Radar
measurements and compared with optical data. For both events the calculated
thermal component had a correlation coefficient greater than 0.8 to the total
observed 630.0 nm intensity which contains both thermal and particle impact
components. Despite fairly constant solar wind, the calculated thermal
component intensity fluctuated possibly due to dayside transients in the
aurora.
",0,1,0,0,0,0
10454,Constructing and Understanding New and Old Scales on Slide Rules,"  We discuss the practical problems arising when constructing any (new or old)
scales on slide rules, i.e. realizing the theory in the practice. This might
help anyone in planning and realizing (mainly the magnitude and labeling of)
new scales on slide rules in the future. In Sections 1-7 we deal with technical
problems, Section 8 is devoted to the relationship among different scales. In
the last Section we provide an interesting fact as a surprise to those readers
who wish to skip this long article.
",0,0,1,0,0,0
14856,Pressure impact on the stability and distortion of the crystal structure of CeScO3,"  The effects of high pressure on the crystal structure of orthorhombic (Pnma)
perovskite type cerium scandate have been studied in situ under high pressure
by means of synchrotron x-ray powder diffraction, using a diamond anvil cell.
We have found that the perovskite type crystal structure remains stable up to
40 GPa, the highest pressure reached in the experiments. The evolution of
unit-cell parameters with pressure has indicated an anisotropic compression.
The room-temperature pressure-volume equation of state is obtained from the
experiments. From the evolution of microscopic structural parameters like bond
distances and coordination polyhedra of cerium and scandium, the macroscopic
behavior of CeScO3 under compression has been explained and reasoned for its
large pressure stability. The reported results are discussed in comparison with
high-pressure results from other perovskites.
",0,1,0,0,0,0
20002,Consumption smoothing in the working-class households of interwar Japan,"  I analyze Osaka factory worker households in the early 1920s, whether
idiosyncratic income shocks were shared efficiently, and which consumption
categories were robust to shocks. While the null hypothesis of full
risk-sharing of total expenditures was rejected, factory workers maintained
their households, in that they paid for essential expenditures (rent,
utilities, and commutation) during economic hardship. Additionally, children's
education expenditures were possibly robust to idiosyncratic income shocks. The
results suggest that temporary income is statistically significantly increased
if disposable income drops due to idiosyncratic shocks. Historical documents
suggest microfinancial lending and saving institutions helped mitigate
risk-based vulnerabilities.
",0,0,0,0,0,1
3825,Direct mapping of the temperature and velocity gradients in discs. Imaging the vertical CO snow line around IM Lupi,"  Accurate measurements of the physical structure of protoplanetary discs are
critical inputs for planet formation models. These constraints are
traditionally established via complex modelling of continuum and line
observations. Instead, we present an empirical framework to locate the CO
isotopologue emitting surfaces from high spectral and spatial resolution ALMA
observations. We apply this framework to the disc surrounding IM Lupi, where we
report the first direct, i.e. model independent, measurements of the radial and
vertical gradients of temperature and velocity in a protoplanetary disc. The
measured disc structure is consistent with an irradiated self-similar disc
structure, where the temperature increases and the velocity decreases towards
the disc surface. We also directly map the vertical CO snow line, which is
located at about one gas scale height at radii between 150 and 300 au, with a
CO freeze-out temperature of $21\pm2$ K. In the outer disc ($> 300$ au), where
the gas surface density transitions from a power law to an exponential taper,
the velocity rotation field becomes significantly sub-Keplerian, in agreement
with the expected steeper pressure gradient. The sub-Keplerian velocities
should result in a very efficient inward migration of large dust grains,
explaining the lack of millimetre continuum emission outside of 300 au. The
sub-Keplerian motions may also be the signature of the base of an externally
irradiated photo-evaporative wind. In the same outer region, the measured CO
temperature above the snow line decreases to $\approx$ 15 K because of the
reduced gas density, which can result in a lower CO freeze-out temperature,
photo-desorption, or deviations from local thermodynamic equilibrium.
",0,1,0,0,0,0
15233,A Novel Receiver Design with Joint Coherent and Non-Coherent Processing,"  In this paper, we propose a novel splitting receiver, which involves joint
processing of coherently and non-coherently received signals. Using a passive
RF power splitter, the received signal at each receiver antenna is split into
two streams which are then processed by a conventional coherent detection (CD)
circuit and a power-detection (PD) circuit, respectively. The streams of the
signals from all the receiver antennas are then jointly used for information
detection. We show that the splitting receiver creates a three-dimensional
received signal space, due to the joint coherent and non-coherent processing.
We analyze the achievable rate of a splitting receiver, which shows that the
splitting receiver provides a rate gain of $3/2$ compared to either the
conventional (CD-based) coherent receiver or the PD-based non-coherent receiver
in the high SNR regime. We also analyze the symbol error rate (SER) for
practical modulation schemes, which shows that the splitting receiver achieves
asymptotic SER reduction by a factor of at least $\sqrt{M}-1$ for $M$-QAM
compared to either the conventional (CD-based) coherent receiver or the
PD-based non-coherent receiver.
",1,0,0,0,0,0
4053,Chunk-Based Bi-Scale Decoder for Neural Machine Translation,"  In typical neural machine translation~(NMT), the decoder generates a sentence
word by word, packing all linguistic granularities in the same time-scale of
RNN. In this paper, we propose a new type of decoder for NMT, which splits the
decode state into two parts and updates them in two different time-scales.
Specifically, we first predict a chunk time-scale state for phrasal modeling,
on top of which multiple word time-scale states are generated. In this way, the
target sentence is translated hierarchically from chunks to words, with
information in different granularities being leveraged. Experiments show that
our proposed model significantly improves the translation performance over the
state-of-the-art NMT model.
",1,0,0,0,0,0
16247,Robustness Analysis of Systems' Safety through a New Notion of Input-to-State Safety,"  In this paper, we propose a new robustness notion that is applicable for
certifying systems' safety with respect to external disturbance signals. The
proposed input-to-state safety (ISSf) notion allows us to certify systems'
safety in the presence of the disturbances which is analogous to the notion of
input-to-state stability (ISS) for analyzing systems' stability.
",1,0,1,0,0,0
14090,A generalisation of Kani-Rosen decomposition theorem for Jacobian varieties,"  In this short paper we generalise a theorem due to Kani and Rosen on
decomposition of Jacobian varieties of Riemann surfaces with group action. This
generalisation extends the set of Jacobians for which it is possible to obtain
an isogeny decomposition where all the factors are Jacobians.
",0,0,1,0,0,0
7805,Conditional Optimal Stopping: A Time-Inconsistent Optimization,"  Inspired by recent work of P.-L. Lions on conditional optimal control, we
introduce a problem of optimal stopping under bounded rationality: the
objective is the expected payoff at the time of stopping, conditioned on
another event. For instance, an agent may care only about states where she is
still alive at the time of stopping, or a company may condition on not being
bankrupt. We observe that conditional optimization is time-inconsistent due to
the dynamic change of the conditioning probability and develop an equilibrium
approach in the spirit of R. H. Strotz' work for sophisticated agents in
discrete time. Equilibria are found to be essentially unique in the case of a
finite time horizon whereas an infinite horizon gives rise to non-uniqueness
and other interesting phenomena. We also introduce a theory which generalizes
the classical Snell envelope approach for optimal stopping by considering a
pair of processes with Snell-type properties.
",0,0,0,0,0,1
12194,Structured Parallel Programming for Monte Carlo Tree Search,"  In this paper, we present a new algorithm for parallel Monte Carlo tree
search (MCTS). It is based on the pipeline pattern and allows flexible
management of the control flow of the operations in parallel MCTS. The pipeline
pattern provides for the first structured parallel programming approach to
MCTS. Moreover, we propose a new lock-free tree data structure for parallel
MCTS which removes synchronization overhead. The Pipeline Pattern for Parallel
MCTS algorithm (called 3PMCTS), scales very well to higher numbers of cores
when compared to the existing methods.
",1,0,0,0,0,0
9843,Two-species boson mixture on a ring: A group theoretic approach to the quantum dynamics of low-energy excitations,"  We investigate the weak excitations of a system made up of two condensates
trapped in a Bose-Hubbard ring and coupled by an interspecies repulsive
interaction. Our approach, based on the Bogoliubov approximation scheme, shows
that one can reduce the problem Hamiltonian to the sum of sub-Hamiltonians
$\hat{H}_k$, each one associated to momentum modes $\pm k$. Each $\hat{H}_k$ is
then recognized to be an element of a dynamical algebra. This uncommon and
remarkable property allows us to present a straightforward diagonalization
scheme, to find constants of motion, to highlight the significant microscopic
processes, and to compute their time evolution. The proposed solution scheme is
applied to a simple but still very interesting closed circuit, the trimer. The
dynamics of low-energy excitations, corresponding to weakly-populated vortices,
is investigated considering different choices of the initial conditions, and
the angular-momentum transfer between the two condensates is evidenced.
Finally, the condition for which the spectral collapse and dynamical
instability are observed is derived analytically.
",0,1,0,0,0,0
8204,On the optimal investment-consumption and life insurance selection problem with an external stochastic factor,"  In this paper, we study a stochastic optimal control problem with stochastic
volatility. We prove the sufficient and necessary maximum principle for the
proposed problem. Then we apply the results to solve an investment, consumption
and life insurance problem with stochastic volatility, that is, we consider a
wage earner investing in one risk-free asset and one risky asset described by a
jump-diffusion process and has to decide concerning consumption and life
insurance purchase. We assume that the life insurance for the wage earner is
bought from a market composed of $M>1$ life insurance companies offering
pairwise distinct life insurance contracts. The goal is to maximize the
expected utilities derived from the consumption, the legacy in the case of a
premature death and the investor's terminal wealth.
",0,0,0,0,0,1
9470,An RKHS model for variable selection in functional regression,"  A mathematical model for variable selection in functional regression models
with scalar response is proposed. By ""variable selection"" we mean a procedure
to replace the whole trajectories of the functional explanatory variables with
their values at a finite number of carefully selected instants (or ""impact
points""). The basic idea of our approach is to use the Reproducing Kernel
Hilbert Space (RKHS) associated with the underlying process, instead of the
more usual L2[0,1] space, in the definition of the linear model. This turns out
to be especially suitable for variable selection purposes, since the
finite-dimensional linear model based on the selected ""impact points"" can be
seen as a particular case of the RKHS-based linear functional model. In this
framework, we address the consistent estimation of the optimal design of impact
points and we check, via simulations and real data examples, the performance of
the proposed method.
",0,0,0,1,0,0
13154,Adiabatic approach for natural gas pipeline computations,"  We consider slowly evolving, i.e. ADIABATIC, operational regime within a
transmission level (continental scale) natural gas pipeline system. This allows
us to introduce a set of nodal equations of reduced complexity describing gas
transients in injection/consumption UNBALANCED (so-called line-pack) cases. We
discuss, in details, construction of the UNBALANCED ADIABATIC (UA)
approximation on the basic example of a single pipe. The UA approximation is
expected to play a significant ""model reduction"" role in solving control,
optimization and planning problems relevant for flawless functioning of modern
natural gas networks.
",1,1,0,0,0,0
11885,Improving drug sensitivity predictions in precision medicine through active expert knowledge elicitation,"  Predicting the efficacy of a drug for a given individual, using
high-dimensional genomic measurements, is at the core of precision medicine.
However, identifying features on which to base the predictions remains a
challenge, especially when the sample size is small. Incorporating expert
knowledge offers a promising alternative to improve a prediction model, but
collecting such knowledge is laborious to the expert if the number of candidate
features is very large. We introduce a probabilistic model that can incorporate
expert feedback about the impact of genomic measurements on the sensitivity of
a cancer cell for a given drug. We also present two methods to intelligently
collect this feedback from the expert, using experimental design and
multi-armed bandit models. In a multiple myeloma blood cancer data set (n=51),
expert knowledge decreased the prediction error by 8%. Furthermore, the
intelligent approaches can be used to reduce the workload of feedback
collection to less than 30% on average compared to a naive approach.
",1,0,0,1,0,0
8462,"Crowd Science: Measurements, Models, and Methods","  The increasing practice of engaging crowds, where organizations use IT to
connect with dispersed individuals for explicit resource creation purposes, has
precipitated the need to measure the precise processes and benefits of these
activities over myriad different implementations. In this work, we seek to
address these salient and non-trivial considerations by laying a foundation of
theory, measures, and research methods that allow us to test crowd-engagement
efficacy across organizations, industries, technologies, and geographies. To do
so, we anchor ourselves in the Theory of Crowd Capital, a generalizable
framework for studying IT-mediated crowd-engagement phenomena, and put forth an
empirical apparatus of testable measures and generalizable methods to begin to
unify the field of crowd science.
",1,0,0,0,0,0
20517,Mixed Cages,"  We introduce the notion of a $[z, r; g]$-mixed cage. A $[z, r; g]$-mixed cage
is a mixed graph $G$, $z$-regular by arcs, $r$-regular by edges, with girth $g$
and minimum order. In this paper we prove the existence of $[z, r ;g]$-mixed
cages and exhibit families of mixed cages for some specific values. We also
give lower and upper bounds for some choices of $z, r$ and $g$. In particular
we present the first results on $[z,r;g]$- mixed cages for $z=1$ and any $r\geq
1$ and $g\geq 3$, and for any $z\geq 1$, $r=1$ and $g=4$.
",0,0,1,0,0,0
6923,Derivation of the cutoff length from the quantum quadratic enhancement of a mass in vacuum energy constant Lambda,"  Ultraviolet self-interaction energies in field theory sometimes contain
meaningful physical quantities. The self-energies in such as classical
electrodynamics are usually subtracted from the rest mass. For the consistent
treatment of energies as sources of curvature in the Einstein field equations,
this study includes these subtracted self-energies into vacuum energy expressed
by the constant Lambda (used in such as Lambda-CDM). In this study, the
self-energies in electrodynamics and macroscopic classical Einstein field
equations are examined, using the formalisms with the ultraviolet cutoff
scheme. One of the cutoff formalisms is the field theory in terms of the
step-function-type basis functions, developed by the present authors. The other
is a continuum theory of a fundamental particle with the same cutoff length.
Based on the effectiveness of the continuum theory with the cutoff length shown
in the examination, the dominant self-energy is the quadratic term of the Higgs
field at a quantum level (classical self-energies are reduced to logarithmic
forms by quantum corrections). The cutoff length is then determined to
reproduce today's tiny value of Lambda for vacuum energy. Additionally, a field
with nonperiodic vanishing boundary conditions is treated, showing that the
field has no zero-point energy.
",0,1,0,0,0,0
5710,Correlating Cellular Features with Gene Expression using CCA,"  To understand the biology of cancer, joint analysis of multiple data
modalities, including imaging and genomics, is crucial. The involved nature of
gene-microenvironment interactions necessitates the use of algorithms which
treat both data types equally. We propose the use of canonical correlation
analysis (CCA) and a sparse variant as a preliminary discovery tool for
identifying connections across modalities, specifically between gene expression
and features describing cell and nucleus shape, texture, and stain intensity in
histopathological images. Applied to 615 breast cancer samples from The Cancer
Genome Atlas, CCA revealed significant correlation of several image features
with expression of PAM50 genes, known to be linked to outcome, while Sparse CCA
revealed associations with enrichment of pathways implicated in cancer without
leveraging prior biological understanding. These findings affirm the utility of
CCA for joint phenotype-genotype analysis of cancer.
",0,0,0,1,1,0
5963,Quantum torus algebras and B(C) type Toda systems,"  In this paper, we construct a new even constrained B(C) type Toda hierarchy
and derive its B(C) type Block type additional symmetry. Also we generalize the
B(C) type Toda hierarchy to the $N$-component B(C) type Toda hierarchy which is
proved to have symmetries of a coupled $\bigotimes^NQT_+ $ algebra ( $N$-folds
direct product of the positive half of the quantum torus algebra $QT$).
",0,1,1,0,0,0
18373,Finite-Range Coulomb Gas Models of Banded Random Matrices and Quantum Kicked Rotors,"  Dyson demonstrated an equivalence between infinite-range Coulomb gas models
and classical random matrix ensembles for study of eigenvalue statistics. We
introduce finite-range Coulomb gas (FRCG) models via a Brownian matrix process,
and study them analytically and by Monte-Carlo simulations. These models yield
new universality classes, and provide a theoretical framework for study of
banded random matrices (BRM) and quantum kicked rotors (QKR). We demonstrate
that, for a BRM of bandwidth b and a QKR of chaos parameter {\alpha}, the
appropriate FRCG model has the effective range d = (b^2)/N = ({\alpha}^2)/N,
for large N matrix dimensionality. As d increases, there is a transition from
Poisson to classical random matrix statistics.
",0,1,0,0,0,0
1248,Reliable Clustering of Bernoulli Mixture Models,"  A Bernoulli Mixture Model (BMM) is a finite mixture of random binary vectors
with independent Bernoulli dimensions. The problem of clustering BMM data
arises in a variety of real-world applications, ranging from population
genetics to activity analysis in social networks. In this paper, we have
analyzed the information-theoretic PAC-learnability of BMMs, when the number of
clusters is unknown. In particular, we stipulate certain conditions on both
sample complexity and the dimension of the model in order to guarantee the
Probably Approximately Correct (PAC)-clusterability of a given dataset. To the
best of our knowledge, these findings are the first non-asymptotic (PAC) bounds
on the sample complexity of learning BMMs.
",1,0,0,1,0,0
9351,Field-free nucleation of antivortices and giant vortices in non-superconducting materials,"  Giant vortices with higher phase-winding than $2\pi$ are usually
energetically unfavorable, but geometric symmetry constraints on a
superconductor in a magnetic field are known to stabilize such objects. Here,
we show via microscopic calculations that giant vortices can appear in
intrinsically non-superconducting materials, even without any applied magnetic
field. The enabling mechanism is the proximity effect to a host superconductor
where a current flows, and we also demonstrate that antivortices can appear in
this setup. Our results open the possibility to study electrically controllable
topological defects in unusual environments, which do not have to be exposed to
magnetic fields or intrinsically superconducting, but instead display other
types of order.
",0,1,0,0,0,0
19666,Tuples of polynomials over finite fields with pairwise coprimality conditions,"  Let $q$ be a prime power. We estimate the number of tuples of degree bounded
monic polynomials $(Q_1,\ldots,Q_v) \in (\mathbb{F}_q[z])^v$ that satisfy given
pairwise coprimality conditions. We show how this generalises from monic
polynomials in finite fields to Dedekind domains with finite norms.
",0,0,1,0,0,0
6241,Controlling thermal emission of phonon by magnetic metasurfaces,"  Our experiment shows that the thermal emission of phonon can be controlled by
magnetic resonance (MR) mode in a metasurface (MTS). Through changing the
structural parameter of metasurface, the MR wavelength can be tuned to the
phonon resonance wavelength. This introduces a strong coupling between phonon
and MR, which results in an anticrossing phonon-plasmons mode. In the process,
we can manipulate the polarization and angular radiation of thermal emission of
phonon. Such metasurface provides a new kind of thermal emission structures for
various thermal management applications.
",0,1,0,0,0,0
12900,Theoretical calculations for precision polarimetry based on Mott scattering,"  Electron polarimeters based on Mott scattering are extensively used in
different fields in physics such as atomic, nuclear or particle physics. This
is because spin-dependent measurements gives additional information on the
physical processes under study. The main quantity that needs to be understood
in very much detail, both experimentally and theoretically, is the
spin-polarization function, so called analyzing power or Sherman function. A
detailed theoretical analysis on all the contributions to the effective
interaction potential that are relevant at the typical electron beam energies
and angles commonly used in the calibration of the experimental apparatus is
presented. The main contribution leading the theoretical error on the Sherman
function is found to correspond to radiative corrections that have been
qualitatively estimated to be below the 0.5% for the considered kinematical
conditions: unpolarized electron beams of few MeV elastically scattered from a
gold and silver targets at backward angles.
",0,1,0,0,0,0
20484,Quantitative Results on Diophantine Equations in Many Variables,"  We consider a system of polynomials $f_1,\ldots, f_R\in
\mathbb{Z}[x_1,\ldots, x_n]$ of the same degree with non-singular local zeros
and in many variables. Generalising the work of Birch (1962) we find
quantitative asymptotics (in terms of the maximum of the absolute value of the
coefficients of these polynomials) for the number of integer zeros of this
system within a growing box. Using a quantitative version of the
Nullstellensatz, we obtain a quantitative strong approximation result, i.e. an
upper bound on the smallest integer zero provided the system of polynomials is
non-singular.
",0,0,1,0,0,0
14697,Full replica symmetry breaking in p-spin-glass-like systems,"  It is shown that continuously changing the effective number of interacting
particles in p-spin-glass-like model allows to describe the transition from the
full replica symmetry breaking glass solution to stable first replica symmetry
breaking glass solution in the case of non-reflective symmetry diagonal
operators used instead of Ising spins. As an example, axial quadrupole moments
in place of Ising spins are considered and the boundary value $p_{c_{1}}\cong
2.5$ is found.
",0,1,0,0,0,0
7831,A Compressed Sensing Approach for Distribution Matching,"  In this work, we formulate the fixed-length distribution matching as a
Bayesian inference problem. Our proposed solution is inspired from the
compressed sensing paradigm and the sparse superposition (SS) codes. First, we
introduce sparsity in the binary source via position modulation (PM). We then
present a simple and exact matcher based on Gaussian signal quantization. At
the receiver, the dematcher exploits the sparsity in the source and performs
low-complexity dematching based on generalized approximate message-passing
(GAMP). We show that GAMP dematcher and spatial coupling lead to asymptotically
optimal performance, in the sense that the rate tends to the entropy of the
target distribution with vanishing reconstruction error in a proper limit.
Furthermore, we assess the performance of the dematcher on practical
Hadamard-based operators. A remarkable feature of our proposed solution is the
possibility to: i) perform matching at the symbol level (nonbinary); ii)
perform joint channel coding and matching.
",0,0,0,1,0,0
11617,Asymptotic Normality of Extensible Grid Sampling,"  Recently, He and Owen (2016) proposed the use of Hilbert's space filling
curve (HSFC) in numerical integration as a way of reducing the dimension from
$d>1$ to $d=1$. This paper studies the asymptotic normality of the HSFC-based
estimate when using scrambled van der Corput sequence as input. We show that
the estimate has an asymptotic normal distribution for functions in
$C^1([0,1]^d)$, excluding the trivial case of constant functions. The
asymptotic normality also holds for discontinuous functions under mild
conditions. It was previously known only that scrambled $(0,m,d)$-net
quadratures enjoy the asymptotic normality for smooth enough functions, whose
mixed partial gradients satisfy a Hölder condition. As a by-product, we find
lower bounds for the variance of the HSFC-based estimate. Particularly, for
nontrivial functions in $C^1([0,1]^d)$, the low bound is of order $n^{-1-2/d}$,
which matches the rate of the upper bound established in He and Owen (2016).
",0,0,1,1,0,0
5844,Evaluation of matrix factorisation approaches for muscle synergy extraction,"  The muscle synergy concept provides a widely-accepted paradigm to break down
the complexity of motor control. In order to identify the synergies, different
matrix factorisation techniques have been used in a repertoire of fields such
as prosthesis control and biomechanical and clinical studies. However, the
relevance of these matrix factorisation techniques is still open for discussion
since there is no ground truth for the underlying synergies. Here, we evaluate
factorisation techniques and investigate the factors that affect the quality of
estimated synergies. We compared commonly used matrix factorisation methods:
Principal component analysis (PCA), Independent component analysis (ICA),
Non-negative matrix factorization (NMF) and second-order blind identification
(SOBI). Publicly available real data were used to assess the synergies
extracted by each factorisation method in the classification of wrist
movements. Synthetic datasets were utilised to explore the effect of muscle
synergy sparsity, level of noise and number of channels on the extracted
synergies. Results suggest that the sparse synergy model and a higher number of
channels would result in better-estimated synergies. Without dimensionality
reduction, SOBI showed better results than other factorisation methods. This
suggests that SOBI would be an alternative when a limited number of electrodes
is available but its performance was still poor in that case. Otherwise, NMF
had the best performance when the number of channels was higher than the number
of synergies. Therefore, NMF would be the best method for muscle synergy
extraction.
",0,0,0,0,1,0
13077,High-field transport properties of a P-doped BaFe2As2 film on technical substrate,"  High temperature (high-Tc) superconductors like cuprates have superior
critical current properties in magnetic fields over other superconductors.
However, superconducting wires for high-field-magnet applications are still
dominated by low-Tc Nb3Sn due probably to cost and processing issues. The
recent discovery of a second class of high-Tc materials, Fe-based
superconductors, may provide another option for high-field-magnet wires. In
particular, AEFe2As2 (AE: Alkali earth elements, AE-122) is one of the best
candidates for high-field-magnet applications because of its high upper
critical field, Hc2, moderate Hc2 anisotropy, and intermediate Tc. Here we
report on in-field transport properties of P-doped BaFe2As2 (Ba-122) thin films
grown on technical substrates (i.e., biaxially textured oxides templates on
metal tapes) by pulsed laser deposition. The P-doped Ba-122 coated conductor
sample exceeds a transport Jc of 10^5 A/cm^2 at 15 T for both major
crystallographic directions of the applied magnetic field, which is favourable
for practical applications. Our P-doped Ba-122 coated conductors show a
superior in-field Jc over MgB2 and NbTi, and a comparable level to Nb3Sn above
20 T. By analysing the E-J curves for determining Jc, a non-Ohmic linear
differential signature is observed at low field due to flux flow along the
grain boundaries. However, grain boundaries work as flux pinning centres as
demonstrated by the pinning force analysis.
",0,1,0,0,0,0
6982,Non-parametric estimation of Jensen-Shannon Divergence in Generative Adversarial Network training,"  Generative Adversarial Networks (GANs) have become a widely popular framework
for generative modelling of high-dimensional datasets. However their training
is well-known to be difficult. This work presents a rigorous statistical
analysis of GANs providing straight-forward explanations for common training
pathologies such as vanishing gradients. Furthermore, it proposes a new
training objective, Kernel GANs, and demonstrates its practical effectiveness
on large-scale real-world data sets. A key element in the analysis is the
distinction between training with respect to the (unknown) data distribution,
and its empirical counterpart. To overcome issues in GAN training, we pursue
the idea of smoothing the Jensen-Shannon Divergence (JSD) by incorporating
noise in the input distributions of the discriminator. As we show, this
effectively leads to an empirical version of the JSD in which the true and the
generator densities are replaced by kernel density estimates, which leads to
Kernel GANs.
",0,0,0,1,0,0
9854,A new astrophysical solution to the Too Big To Fail problem - Insights from the MoRIA simulations,"  We test whether advanced galaxy models and analysis techniques of simulations
can alleviate the Too Big To Fail problem (TBTF) for late-type galaxies, which
states that isolated dwarf galaxy kinematics imply that dwarfs live in
lower-mass halos than is expected in a {\Lambda}CDM universe. Furthermore, we
want to explain this apparent tension between theory and observations. To do
this, we use the MoRIA suite of dwarf galaxy simulations to investigate whether
observational effects are involved in TBTF for late-type field dwarf galaxies.
To this end, we create synthetic radio data cubes of the simulated MoRIA
galaxies and analyse their HI kinematics as if they were real, observed
galaxies. We find that for low-mass galaxies, the circular velocity profile
inferred from the HI kinematics often underestimates the true circular velocity
profile, as derived directly from the enclosed mass. Fitting the HI kinematics
of MoRIA dwarfs with a theoretical halo profile results in a systematic
underestimate of the mass of their host halos. We attribute this effect to the
fact that the interstellar medium of a low-mass late-type dwarf is continuously
stirred by supernova explosions into a vertically puffed-up, turbulent state to
the extent that the rotation velocity of the gas is simply no longer a good
tracer of the underlying gravitational force field. If this holds true for real
dwarf galaxies as well, it implies that they inhabit more massive dark matter
halos than would be inferred from their kinematics, solving TBTF for late-type
field dwarf galaxies.
",0,1,0,0,0,0
3098,Nonlinear probability. A theory with incompatible stochastic variables,"  In 1991 J.F. Aarnes introduced the concept of quasi-measures in a compact
topological space $\Omega$ and established the connection between quasi-states
on $C (\Omega)$ and quasi-measures in $\Omega$. This work solved the linearity
problem of quasi-states on $C^*$-algebras formulated by R.V. Kadison in 1965.
The answer is that a quasi-state need not be linear, so a quasi-state need not
be a state. We introduce nonlinear measures in a space $\Omega$ which is a
generalization of a measurable space. In this more general setting we are still
able to define integration and establish a representation theorem for the
corresponding functionals. A probabilistic language is choosen since we feel
that the subject should be of some interest to probabilists. In particular we
point out that the theory allows for incompatible stochastic variables. The
need for incompatible variables is well known in quantum mechanics, but the
need seems natural also in other contexts as we try to explain by a questionary
example.
Keywords and phrases: Epistemic probability, Integration with respect to mea-
sures and other set functions, Banach algebras of continuous functions, Set
func- tions and measures on topological spaces, States, Logical foundations of
quantum mechanics.
",0,0,1,1,0,0
3544,Semi-supervised model-based clustering with controlled clusters leakage,"  In this paper, we focus on finding clusters in partially categorized data
sets. We propose a semi-supervised version of Gaussian mixture model, called
C3L, which retrieves natural subgroups of given categories. In contrast to
other semi-supervised models, C3L is parametrized by user-defined leakage
level, which controls maximal inconsistency between initial categorization and
resulting clustering. Our method can be implemented as a module in practical
expert systems to detect clusters, which combine expert knowledge with true
distribution of data. Moreover, it can be used for improving the results of
less flexible clustering techniques, such as projection pursuit clustering. The
paper presents extensive theoretical analysis of the model and fast algorithm
for its efficient optimization. Experimental results show that C3L finds high
quality clustering model, which can be applied in discovering meaningful groups
in partially classified data.
",1,0,0,1,0,0
10858,A novel approach to the Lindelöf hypothesis,"  Lindel{ö}f's hypothesis, one of the most important open problems in the
history of mathematics, states that for large $t$, Riemann's zeta function
$\zeta(\frac{1}{2}+it)$ is of order $O(t^{\varepsilon})$ for any
$\varepsilon>0$. It is well known that for large $t$, the leading order
asymptotics of the Riemann zeta function can be expressed in terms of a
transcendental exponential sum. The usual approach to the Lindelöf hypothesis
involves the use of ingenious techniques for the estimation of this sum.
However, since such estimates can not yield an asymptotic formula for the above
sum, it appears that this approach cannot lead to the proof of the Lindelöf
hypothesis. Here, a completely different approach is introduced: the Riemann
zeta function is embedded in a classical problem in the theory of complex
analysis known as a Riemann-Hilbert problem, and then, the large
$t$-asymptotics of the associated integral equation is formally computed. This
yields two different results. First, the formal proof that a certain Riemann
zeta-type double exponential sum satisfies the asymptotic estimate of the
Lindelöf hypothesis. Second, it is formally shown that the sum of
$|\zeta(1/2+it)|^2$ and of a certain sum which depends on $\epsilon$, satisfies
for large $t$ the estimate of the Lindelöf hypothesis. Hence, since the above
identity is valid for all $\epsilon$, this asymptotic identity suggests the
validity of Lindelöf's hypothesis. The completion of the rigorous derivation
of the above results will be presented in a companion paper.
",0,0,1,0,0,0
15470,Tied Hidden Factors in Neural Networks for End-to-End Speaker Recognition,"  In this paper we propose a method to model speaker and session variability
and able to generate likelihood ratios using neural networks in an end-to-end
phrase dependent speaker verification system. As in Joint Factor Analysis, the
model uses tied hidden variables to model speaker and session variability and a
MAP adaptation of some of the parameters of the model. In the training
procedure our method jointly estimates the network parameters and the values of
the speaker and channel hidden variables. This is done in a two-step
backpropagation algorithm, first the network weights and factor loading
matrices are updated and then the hidden variables, whose gradients are
calculated by aggregating the corresponding speaker or session frames, since
these hidden variables are tied. The last layer of the network is defined as a
linear regression probabilistic model whose inputs are the previous layer
outputs. This choice has the advantage that it produces likelihoods and
additionally it can be adapted during the enrolment using MAP without the need
of a gradient optimization. The decisions are made based on the ratio of the
output likelihoods of two neural network models, speaker adapted and universal
background model. The method was evaluated on the RSR2015 database.
",1,0,0,0,0,0
20127,Lagrangian solutions to the Vlasov-Poisson system with a point charge,"  We consider the Cauchy problem for the repulsive Vlasov-Poisson system in the
three dimensional space, where the initial datum is the sum of a diffuse
density, assumed to be bounded and integrable, and a point charge. Under some
decay assumptions for the diffuse density close to the point charge, under
bounds on the total energy, and assuming that the initial total diffuse charge
is strictly less than one, we prove existence of global Lagrangian solutions.
Our result extends the Eulerian theory of [16], proving that solutions are
transported by the flow trajectories. The proof is based on the ODE theory
developed in [8] in the setting of vector fields with anisotropic regularity,
where some components of the gradient of the vector field is a singular
integral of a measure.
",0,0,1,0,0,0
18066,Existence and convexity of local solutions to degenerate hessian equations,"  In this work, we prove the existence of local convex solution to the
degenerate Hessian equation
",0,0,1,0,0,0
12854,On the ground state of spiking network activity in mammalian cortex,"  Electrophysiological recordings of spiking activity are limited to a small
number of neurons. This spatial subsampling has hindered characterizing even
most basic properties of collective spiking in cortical networks. In
particular, two contradictory hypotheses prevailed for over a decade: the first
proposed an asynchronous irregular state, the second a critical state. While
distinguishing them is straightforward in models, we show that in experiments
classical approaches fail to correctly infer network dynamics because of
subsampling. Deploying a novel, subsampling-invariant estimator, we find that
in vivo dynamics do not comply with either hypothesis, but instead occupy a
narrow ""reverberating"" state consistently across multiple mammalian species and
cortical areas. A generic model tuned to this reverberating state predicts
single neuron, pairwise, and population properties. With these predictions we
first validate the model and then deduce network properties that are
challenging to obtain experimentally, like the network timescale and strength
of cortical input.
",0,0,0,1,1,0
15695,An invitation to 2D TQFT and quantization of Hitchin spectral curves,"  This article consists of two parts. In Part 1, we present a formulation of
two-dimensional topological quantum field theories in terms of a functor from a
category of Ribbon graphs to the endofuntor category of a monoidal category.
The key point is that the category of ribbon graphs produces all Frobenius
objects. Necessary backgrounds from Frobenius algebras, topological quantum
field theories, and cohomological field theories are reviewed. A result on
Frobenius algebra twisted topological recursion is included at the end of Part
1.
In Part 2, we explain a geometric theory of quantum curves. The focus is
placed on the process of quantization as a passage from families of Hitchin
spectral curves to families of opers. To make the presentation simpler, we
unfold the story using SL_2(\mathbb{C})-opers and rank 2 Higgs bundles defined
on a compact Riemann surface $C$ of genus greater than $1$. In this case,
quantum curves, opers, and projective structures in $C$ all become the same
notion. Background materials on projective coordinate systems, Higgs bundles,
opers, and non-Abelian Hodge correspondence are explained.
",0,0,1,0,0,0
8663,"Transversal fluctuations of the ASEP, stochastic six vertex model, and Hall-Littlewood Gibbsian line ensembles","  We consider the ASEP and the stochastic six vertex model started with step
initial data. After a long time, $T$, it is known that the one-point height
function fluctuations for these systems are of order $T^{1/3}$. We prove the
KPZ prediction of $T^{2/3}$ scaling in space. Namely, we prove tightness (and
Brownian absolute continuity of all subsequential limits) as $T$ goes to
infinity of the height function with spatial coordinate scaled by $T^{2/3}$ and
fluctuations scaled by $T^{1/3}$. The starting point for proving these results
is a connection discovered recently by Borodin-Bufetov-Wheeler between the
stochastic six vertex height function and the Hall-Littlewood process (a
certain measure on plane partitions). Interpreting this process as a line
ensemble with a Gibbsian resampling invariance, we show that the one-point
tightness of the top curve can be propagated to the tightness of the entire
curve.
",0,0,1,0,0,0
2537,Essentially Finite Vector Bundles on Normal Pseudo-proper Algebraic Stacks,"  Let $X$ be a normal, connected and projective variety over an algebraically
closed field $k$. It is known that a vector bundle $V$ on $X$ is essentially
finite if and only if it is trivialized by a proper surjective morphism $f:Y\to
X$. In this paper we introduce a different approach to this problem which
allows to extend the results to normal, connected and strongly pseudo-proper
algebraic stack of finite type over an arbitrary field $k$.
",0,0,1,0,0,0
19692,Facilitating information system development with Panoramic view on data,"  The increasing amount of information and the absence of an effective tool for
assisting users with minimal technical knowledge lead us to use associative
thinking paradigm for implementation of a software solution - Panorama. In this
study, we present object recognition process, based on context + focus
information visualization techniques, as a foundation for realization of
Panorama. We show that user can easily define data vocabulary of selected
domain that is furthermore used as the application framework. The purpose of
Panorama approach is to facilitate software development of certain problem
domains by shortening the Software Development Life Cycle with minimizing the
impact of implementation, review and maintenance phase. Our approach is focused
on using and updating data vocabulary by users without extensive programming
skills. Panorama therefore facilitates traversing through data by following
associations where user does not need to be familiar with the query language,
the data structure and does not need to know the problem domain fully. Our
approach has been verified by detailed comparison to existing approaches and in
an experiment by implementing selected use cases. The results confirmed that
Panorama fits problem domains with emphasis on data oriented rather than ones
with process oriented aspects. In such cases the development of selected
problem domains is shortened up to 25%, where emphasis is mainly on analysis,
logical design and testing, while omitting physical design and programming,
which is performed automatically by Panorama tool.
",1,0,0,0,0,0
19041,Experience Recommendation for Long Term Safe Learning-based Model Predictive Control in Changing Operating Conditions,"  Learning has propelled the cutting edge of performance in robotic control to
new heights, allowing robots to operate with high performance in conditions
that were previously unimaginable. The majority of the work, however, assumes
that the unknown parts are static or slowly changing. This limits them to
static or slowly changing environments. However, in the real world, a robot may
experience various unknown conditions. This paper presents a method to extend
an existing single mode GP-based safe learning controller to learn an
increasing number of non-linear models for the robot dynamics. We show that
this approach enables a robot to re-use past experience from a large number of
previously visited operating conditions, and to safely adapt when a new and
distinct operating condition is encountered. This allows the robot to achieve
safety and high performance in an large number of operating conditions that do
not have to be specified ahead of time. Our approach runs independently from
the controller, imposing no additional computation time on the control loop
regardless of the number of previous operating conditions considered. We
demonstrate the effectiveness of our approach in experiment on a 900\,kg ground
robot with both physical and artificial changes to its dynamics. All of our
experiments are conducted using vision for localization.
",1,0,0,0,0,0
11148,Tunneling estimates and approximate controllability for hypoelliptic equations,"  This article is concerned with quantitative unique continuation estimates for
equations involving a ""sum of squares"" operator $\mathcal{L}$ on a compact
manifold $\mathcal{M}$ assuming: $(i)$ the Chow-Rashevski-Hörmander condition
ensuring the hypoellipticity of $\mathcal{L}$, and $(ii)$ the analyticity of
$\mathcal{M}$ and the coefficients of $\mathcal{L}$.
The first result is the tunneling estimate $\|\varphi\|_{L^2(\omega)} \geq
Ce^{- \lambda^{\frac{k}{2}}}$ for normalized eigenfunctions $\varphi$ of
$\mathcal{L}$ from a nonempty open set $\omega\subset \mathcal{M}$, where $k$
is the hypoellipticity index of $\mathcal{L}$ and $\lambda$ the eigenvalue.
The main result is a stability estimate for solutions to the hypoelliptic
wave equation $(\partial_t^2+\mathcal{L})u=0$: for $T>2 \sup_{x \in
\mathcal{M}}(dist(x,\omega))$ (here, $dist$ is the sub-Riemannian distance),
the observation of the solution on $(0,T)\times \omega$ determines the data.
The constant involved in the estimate is $Ce^{c\Lambda^k}$ where $\Lambda$ is
the typical frequency of the data.
We then prove the approximate controllability of the hypoelliptic heat
equation $(\partial_t+\mathcal{L})v=1_\omega f$ in any time, with appropriate
(exponential) cost, depending on $k$. In case $k=2$ (Grushin, Heisenberg...),
we further show approximate controllability to trajectories with polynomial
cost in large time.
We also explain how the analyticity assumption can be relaxed, and a boundary
$\partial \mathcal{M}$ can be added in some situations.
Most results turn out to be optimal on a family of Grushin-type operators.
The main proof relies on the general strategy developed by the authors in
arXiv:1506.04254.
",0,0,1,0,0,0
20244,A Memristor-Based Optimization Framework for AI Applications,"  Memristors have recently received significant attention as ubiquitous
device-level components for building a novel generation of computing systems.
These devices have many promising features, such as non-volatility, low power
consumption, high density, and excellent scalability. The ability to control
and modify biasing voltages at the two terminals of memristors make them
promising candidates to perform matrix-vector multiplications and solve systems
of linear equations. In this article, we discuss how networks of memristors
arranged in crossbar arrays can be used for efficiently solving optimization
and machine learning problems. We introduce a new memristor-based optimization
framework that combines the computational merit of memristor crossbars with the
advantages of an operator splitting method, alternating direction method of
multipliers (ADMM). Here, ADMM helps in splitting a complex optimization
problem into subproblems that involve the solution of systems of linear
equations. The capability of this framework is shown by applying it to linear
programming, quadratic programming, and sparse optimization. In addition to
ADMM, implementation of a customized power iteration (PI) method for
eigenvalue/eigenvector computation using memristor crossbars is discussed. The
memristor-based PI method can further be applied to principal component
analysis (PCA). The use of memristor crossbars yields a significant speed-up in
computation, and thus, we believe, has the potential to advance optimization
and machine learning research in artificial intelligence (AI).
",1,0,0,1,0,0
1215,Influence of Heat Treatment on the Corrosion Behavior of Purified Magnesium and AZ31 Alloy,"  Magnesium and its alloys are ideal for biodegradable implants due to their
biocompatibility and their low-stress shielding. However, they can corrode too
rapidly in the biological environment. The objective of this research was to
develop heat treatments to slow the corrosion of high purified magnesium and
AZ31 alloy in simulated body fluid at 37°C. Heat treatments were performed
at different temperatures and times. Hydrogen evolution, weight loss, PDP, and
EIS methods were used to measure the corrosion rates. Results show that heat
treating can increase the corrosion resistance of HP-Mg by 2x and AZ31 by 10x.
",0,1,0,0,0,0
9983,Lossy Image Compression with Compressive Autoencoders,"  We propose a new approach to the problem of optimizing autoencoders for lossy
image compression. New media formats, changing hardware technology, as well as
diverse requirements and content types create a need for compression algorithms
which are more flexible than existing codecs. Autoencoders have the potential
to address this need, but are difficult to optimize directly due to the
inherent non-differentiabilty of the compression loss. We here show that
minimal changes to the loss are sufficient to train deep autoencoders
competitive with JPEG 2000 and outperforming recently proposed approaches based
on RNNs. Our network is furthermore computationally efficient thanks to a
sub-pixel architecture, which makes it suitable for high-resolution images.
This is in contrast to previous work on autoencoders for compression using
coarser approximations, shallower architectures, computationally expensive
methods, or focusing on small images.
",1,0,0,1,0,0
2612,The Complexity of Graph-Based Reductions for Reachability in Markov Decision Processes,"  We study the never-worse relation (NWR) for Markov decision processes with an
infinite-horizon reachability objective. A state q is never worse than a state
p if the maximal probability of reaching the target set of states from p is at
most the same value from q, regard- less of the probabilities labelling the
transitions. Extremal-probability states, end components, and essential states
are all special cases of the equivalence relation induced by the NWR. Using the
NWR, states in the same equivalence class can be collapsed. Then, actions
leading to sub- optimal states can be removed. We show the natural decision
problem associated to computing the NWR is coNP-complete. Finally, we ex- tend
a previously known incomplete polynomial-time iterative algorithm to
under-approximate the NWR.
",1,0,0,0,0,0
5393,End-to-End Multi-View Networks for Text Classification,"  We propose a multi-view network for text classification. Our method
automatically creates various views of its input text, each taking the form of
soft attention weights that distribute the classifier's focus among a set of
base features. For a bag-of-words representation, each view focuses on a
different subset of the text's words. Aggregating many such views results in a
more discriminative and robust representation. Through a novel architecture
that both stacks and concatenates views, we produce a network that emphasizes
both depth and width, allowing training to converge quickly. Using our
multi-view architecture, we establish new state-of-the-art accuracies on two
benchmark tasks.
",1,0,0,0,0,0
8031,Combinatorial cost: a coarse setting,"  The main inspiration for this paper is a paper by Elek where he introduces
combinatorial cost for graph sequences. We show that having cost equal to 1 and
hyperfiniteness are coarse invariants. We also show `cost-1' for box spaces
behaves multiplicatively when taking subgroups. We show that graph sequences
coming from Farber sequences of a group have property A if and only if the
group is amenable. The same is true for hyperfiniteness. This generalises a
theorem by Elek. Furthermore we optimise this result when Farber sequences are
replaced by sofic approximations. In doing so we introduce a new concept:
property almost-A.
",0,0,1,0,0,0
18376,On Long Memory Origins and Forecast Horizons,"  Most long memory forecasting studies assume that the memory is generated by
the fractional difference operator. We argue that the most cited theoretical
arguments for the presence of long memory do not imply the fractional
difference operator, and assess the performance of the autoregressive
fractionally integrated moving average $(ARFIMA)$ model when forecasting series
with long memory generated by nonfractional processes. We find that high-order
autoregressive $(AR)$ models produce similar or superior forecast performance
than $ARFIMA$ models at short horizons. Nonetheless, as the forecast horizon
increases, the $ARFIMA$ models tend to dominate in forecast performance. Hence,
$ARFIMA$ models are well suited for forecasts of long memory processes
regardless of the long memory generating mechanism, particularly for medium and
long forecast horizons. Additionally, we analyse the forecasting performance of
the heterogeneous autoregressive ($HAR$) model which imposes restrictions on
high-order $AR$ models. We find that the structure imposed by the $HAR$ model
produces better long horizon forecasts than $AR$ models of the same order, at
the price of inferior short horizon forecasts in some cases. Our results have
implications for, among others, Climate Econometrics and Financial Econometrics
models dealing with long memory series at different forecast horizons. We show
in an example that while a short memory autoregressive moving average $(ARMA)$
model gives the best performance when forecasting the Realized Variance of the
S\&P 500 up to a month ahead, the $ARFIMA$ model gives the best performance for
longer forecast horizons.
",0,0,1,0,0,0
4898,"On annihilators of bounded $(\frak g, \frak k)$-modules","  Let $\frak g$ be a semisimple Lie algebra and $\frak k\subset\frak g$ be a
reductive subalgebra. We say that a $\frak g$-module $M$ is a bounded $(\frak
g, \frak k)$-module if $M$ is a direct sum of simple finite-dimensional $\frak
k$-modules and the multiplicities of all simple $\frak k$-modules in that
direct sum are universally bounded.
The goal of this article is to show that the ""boundedness"" property for a
simple $(\frak g, \frak k)$-module $M$ is equivalent to a property of the
associated variety of the annihilator of $M$ (this is the closure of a
nilpotent coadjoint orbit inside $\frak g^*$) under the assumption that the
main field is algebraically closed and of characteristic 0. In particular this
implies that if $M_1, M_2$ are simple $(\frak g, \frak k)$-modules such that
$M_1$ is bounded and the associated varieties of the annihilators of $M_1$ and
$M_2$ coincide then $M_2$ is also bounded. This statement is a geometric
analogue of a purely algebraic fact due to I. Penkov and V. Serganova and it
was posed as a conjecture in my Ph.D. thesis.
",0,0,1,0,0,0
10188,Generalized End-to-End Loss for Speaker Verification,"  In this paper, we propose a new loss function called generalized end-to-end
(GE2E) loss, which makes the training of speaker verification models more
efficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike
TE2E, the GE2E loss function updates the network in a way that emphasizes
examples that are difficult to verify at each step of the training process.
Additionally, the GE2E loss does not require an initial stage of example
selection. With these properties, our model with the new loss function
decreases speaker verification EER by more than 10%, while reducing the
training time by 60% at the same time. We also introduce the MultiReader
technique, which allows us to do domain adaptation - training a more accurate
model that supports multiple keywords (i.e. ""OK Google"" and ""Hey Google"") as
well as multiple dialects.
",1,0,0,1,0,0
20730,Unidirectional control of optically induced spin waves,"  Unidirectional control of optically induced spin waves in a rare-earth iron
garnet crystal is demonstrated. We observed the interference of two spin-wave
packets with different initial phases generated by circularly polarized light
pulses. This interference results in unidirectional propagation if the
spin-wave sources are spaced apart at 1/4 of the wavelength of the spin waves
and the initial phase difference is set to pi/2. The propagating direction of
the spin wave is switched by the polarization helicity of the light pulses.
Moreover, in a numerical simulation, applying more than two spin-wave sources
with a suitable polarization and spot shape, arbitrary manipulation of the spin
wave by the phased array method was replicated.
",0,1,0,0,0,0
4423,Representation Theorems for Solvable Sesquilinear Forms,"  New results are added to the paper [4] about q-closed and solvable
sesquilinear forms. The structure of the Banach space
$\mathcal{D}[||\cdot||_\Omega]$ defined on the domain $\mathcal{D}$ of a
q-closed sesquilinear form $\Omega$ is unique up to isomorphism, and the
adjoint of a sesquilinear form has the same property of q-closure or of
solvability. The operator associated to a solvable sesquilinear form is the
greatest which represents the form and it is self-adjoint if, and only if, the
form is symmetric. We give more criteria of solvability for q-closed
sesquilinear forms. Some of these criteria are related to the numerical range,
and we analyse in particular the forms which are solvable with respect to inner
products. The theory of solvable sesquilinear forms generalises those of many
known sesquilinear forms in literature.
",0,0,1,0,0,0
18587,Balancing Efficiency and Coverage in Human-Robot Dialogue Collection,"  We describe a multi-phased Wizard-of-Oz approach to collecting human-robot
dialogue in a collaborative search and navigation task. The data is being used
to train an initial automated robot dialogue system to support collaborative
exploration tasks. In the first phase, a wizard freely typed robot utterances
to human participants. For the second phase, this data was used to design a GUI
that includes buttons for the most common communications, and templates for
communications with varying parameters. Comparison of the data gathered in
these phases show that the GUI enabled a faster pace of dialogue while still
maintaining high coverage of suitable responses, enabling more efficient
targeted data collection, and improvements in natural language understanding
using GUI-collected data. As a promising first step towards interactive
learning, this work shows that our approach enables the collection of useful
training data for navigation-based HRI tasks.
",1,0,0,0,0,0
6992,The problem of boundary conditions for the shallow water equations (Russian),"  The problem of choice of boundary conditions are discussed for the case of
numerical integration of the shallow water equations on a substantially
irregular relief. In modeling of unsteady surface water flows has a dynamic
boundary partitioning liquid and dry bottom. The situation is complicated by
the emergence of sub- and supercritical flow regimes for the problems of
seasonal floodplain flooding, flash floods, tsunami landfalls. Analysis of the
use of various methods of setting conditions for the physical quantities of
liquid when the settlement of the boundary shows the advantages of using the
waterfall type conditions in the presence of strong inhomogeneities landforms.
When there is a waterfall on the border of the computational domain and
heterogeneity of the relief in the vicinity of the boundary portion may occur,
which is formed by the region of critical flow with the formation of a
hydraulic jump, which greatly weakens the effect of the waterfall on the flow
pattern upstream.
",0,1,0,0,0,0
11503,Constraints on neutrino masses from Lyman-alpha forest power spectrum with BOSS and XQ-100,"  We present constraints on masses of active and sterile neutrinos. We use the
one-dimensional Ly$\alpha$-forest power spectrum from the Baryon Oscillation
Spectroscopic Survey (BOSS) of the Sloan Digital Sky Survey (SDSS-III) and from
the VLT/XSHOOTER legacy survey (XQ-100). In this paper, we present our own
measurement of the power spectrum with the publicly released XQ-100 quasar
spectra.
Fitting Ly$\alpha$ data alone leads to cosmological parameters in excellent
agreement with the values derived independently from Planck 2015 Cosmic
Microwave Background (CMB) data. Combining BOSS and XQ-100 Ly$\alpha$ power
spectra, we constrain the sum of neutrino masses to $\sum m_\nu < 0.8$ eV (95\%
C.L). With the addition of CMB data, this bound is tightened to $\sum m_\nu <
0.14$ eV (95\% C.L.).
With their sensitivity to small scales, Ly$\alpha$ data are ideal to
constrain $\Lambda$WDM models. Using XQ-100 alone, we issue lower bounds on
pure dark matter particles: $m_X \gtrsim 2.08 \: \rm{keV}$ (95\% C.L.) for
early decoupled thermal relics, and $m_s \gtrsim 10.2 \: \rm{keV}$ (95\% C.L.)
for non-resonantly produced right-handed neutrinos. Combining the 1D Ly$\alpha$
forest power spectrum measured by BOSS and XQ-100, we improve the two bounds to
$m_X \gtrsim 4.17 \: \rm{keV}$ and $m_s \gtrsim 25.0 \: \rm{keV}$ (95\% C.L.).
The $3~\sigma$ bound shows a more significant improvement, increasing from $m_X
\gtrsim 2.74 \: \rm{keV}$ for BOSS alone to $m_X \gtrsim 3.10 \: \rm{keV}$ for
the combined BOSS+XQ-100 data set.
Finally, we include in our analysis the first two redshift bins ($z=4.2$ and
$z=4.6$) of the power spectrum measured with the high-resolution HIRES/MIKE
spectrographs. The addition of HIRES/MIKE power spectrum allows us to further
improve the two limits to $m_X \gtrsim 4.65 \: \rm{keV}$ and $m_s \gtrsim 28.8
\: \rm{keV}$ (95\% C.L.).
",0,1,0,0,0,0
5186,A Unified Analysis of Extra-gradient and Optimistic Gradient Methods for Saddle Point Problems: Proximal Point Approach,"  We consider solving convex-concave saddle point problems. We focus on two
variants of gradient decent-ascent algorithms, Extra-gradient (EG) and
Optimistic Gradient (OGDA) methods, and show that they admit a unified analysis
as approximations of the classical proximal point method for solving
saddle-point problems. This viewpoint enables us to generalize EG (in terms of
extrapolation steps) and OGDA (in terms of parameters) and obtain new
convergence rate results for these algorithms for the bilinear case as well as
the strongly convex-concave case.
",1,0,0,1,0,0
1328,Extracting 3D Vascular Structures from Microscopy Images using Convolutional Recurrent Networks,"  Vasculature is known to be of key biological significance, especially in the
study of cancer. As such, considerable effort has been focused on the automated
measurement and analysis of vasculature in medical and pre-clinical images. In
tumors in particular, the vascular networks may be extremely irregular and the
appearance of the individual vessels may not conform to classical descriptions
of vascular appearance. Typically, vessels are extracted by either a
segmentation and thinning pipeline, or by direct tracking. Neither of these
methods are well suited to microscopy images of tumor vasculature. In order to
address this we propose a method to directly extract a medial representation of
the vessels using Convolutional Neural Networks. We then show that these
two-dimensional centerlines can be meaningfully extended into 3D in anisotropic
and complex microscopy images using the recently popularized Convolutional Long
Short-Term Memory units (ConvLSTM). We demonstrate the effectiveness of this
hybrid convolutional-recurrent architecture over both 2D and 3D convolutional
comparators.
",1,0,0,0,0,0
3997,Prediction of helium vapor quality in steady state Two-phase operation for SST-1 Toroidal field magnets,"  Steady State Superconducting Tokamak (SST-1) at the Institute for Plasma
Research (IPR) is an operational device and is the first superconducting
Tokamak in India. Superconducting Magnets System (SCMS) in SST-1 comprises of
sixteen Toroidal field (TF) magnets and nine Poloidal Field (PF) magnets
manufactured using NbTi/Cu based cable-in-conduit-conductor (CICC) concept.
SST-1, superconducting TF magnets are operated in a Cryo-stable manner being
cooled with two-phase (TP) flow helium. The typical operating pressure of the
TP helium is 1.6 bar (a) at corresponding saturation temperature. The SCMS has
a typical cool-down time of about 14 days from 300 K down to 4.5 K using Helium
plant of equivalent cooling capacity of 1350 W at 4.5 K. Using the onset of
experimental data from the HRL, we estimated the vapor quality for the input
heat load on to the TF magnets system. In this paper, we report the
characteristics of two-phase flow for given thermo-hydraulic conditions during
long steady state operation of the SST-1 TF magnets. Finally, the
experimentally obtained results have been compared with the well-known
correlations of two-phase flow.
",0,1,0,0,0,0
4288,Deep Learning from Shallow Dives: Sonar Image Generation and Training for Underwater Object Detection,"  Among underwater perceptual sensors, imaging sonar has been highlighted for
its perceptual robustness underwater. The major challenge of imaging sonar,
however, arises from the difficulty in defining visual features despite limited
resolution and high noise levels. Recent developments in deep learning provide
a powerful solution for computer-vision researches using optical images.
Unfortunately, deep learning-based approaches are not well established for
imaging sonars, mainly due to the scant data in the training phase. Unlike the
abundant publically available terrestrial images, obtaining underwater images
is often costly, and securing enough underwater images for training is not
straightforward. To tackle this issue, this paper presents a solution to this
field's lack of data by introducing a novel end-to-end image-synthesizing
method in the training image preparation phase. The proposed method present
image synthesizing scheme to the images captured by an underwater simulator.
Our synthetic images are based on the sonar imaging models and noisy
characteristics to represent the real data obtained from the sea. We validate
the proposed scheme by training using a simulator and by testing the simulated
images with real underwater sonar images obtained from a water tank and the
sea.
",1,0,0,0,0,0
9798,Asymptotic Goodness-of-Fit Tests for Point Processes Based on Scaled Empirical K-Functions,"  We study sequences of scaled edge-corrected empirical (generalized)
K-functions (modifying Ripley's K-function) each of them constructed from a
single observation of a $d$-dimensional fourth-order stationary point process
in a sampling window W_n which grows together with some scaling rate
unboundedly as n --> infty. Under some natural assumptions it is shown that the
normalized difference between scaled empirical and scaled theoretical
K-function converges weakly to a mean zero Gaussian process with simple
covariance function. This result suggests discrepancy measures between
empirical and theoretical K-function with known limit distribution which allow
to perform goodness-of-fit tests for checking a hypothesized point process
based only on its intensity and (generalized) K-function. Similar test
statistics are derived for testing the hypothesis that two independent point
processes in W_n have the same distribution without explicit knowledge of their
intensities and K-functions.
",0,0,1,1,0,0
20812,A second main theorem for holomorphic curve intersecting hypersurfaces,"  In this paper, we establish a second main theorem for holomorphic curve
intersecting hypersurfaces in general position in projective space with level
of truncation. As an application, we reduce the number hypersurfaces in
uniqueness problem for holomorphic curve of authors before.
",0,0,1,0,0,0
2346,Adversarial Examples that Fool Detectors,"  An adversarial example is an example that has been adjusted to produce a
wrong label when presented to a system at test time. To date, adversarial
example constructions have been demonstrated for classifiers, but not for
detectors. If adversarial examples that could fool a detector exist, they could
be used to (for example) maliciously create security hazards on roads populated
with smart vehicles. In this paper, we demonstrate a construction that
successfully fools two standard detectors, Faster RCNN and YOLO. The existence
of such examples is surprising, as attacking a classifier is very different
from attacking a detector, and that the structure of detectors - which must
search for their own bounding box, and which cannot estimate that box very
accurately - makes it quite likely that adversarial patterns are strongly
disrupted. We show that our construction produces adversarial examples that
generalize well across sequences digitally, even though large perturbations are
needed. We also show that our construction yields physical objects that are
adversarial.
",1,0,0,0,0,0
8148,Topological $\mathbb{Z}_2$ Resonating-Valence-Bond Spin Liquid on the Square Lattice,"  A one-parameter family of long-range resonating valence bond (RVB) state on
the square lattice was previously proposed to describe a critical spin liquid
(SL) phase of the spin-$1/2$ frustrated Heisenberg model. We provide evidence
that this RVB state in fact also realises a topological (long-range entangled)
$\mathbb{Z}_2$ SL, limited by two transitions to critical SL phases. The
topological phase is naturally connected to the $\mathbb{Z}_2$ gauge symmetry
of the local tensor. This work shows that, on one hand, spin-$1/2$ topological
SL with $C_{4v}$ point group symmetry and $SU(2)$ spin rotation symmetry exists
on the square lattice and, on the other hand, criticality and nonbipartiteness
are compatible. We also point out that, strong similarities between our phase
diagram and the ones of classical interacting dimer models suggest both can be
described by similar Kosterlitz-Thouless transitions. This scenario is further
supported by the analysis of the one-dimensional boundary state.
",0,1,0,0,0,0
13792,The Application of SNiPER to the JUNO Simulation,"  JUNO is a multipurpose neutrino experiment which is designed to determine
neutrino mass hierarchy and precisely measure oscillation parameters. As one of
the important systems, the JUNO offline software is being developed using the
SNiPER software. In this proceeding, we focus on the requirements of JUNO
simulation and present the working solution based on the SNiPER.
The JUNO simulation framework is in charge of managing event data, detector
geometries and materials, physics processes, simulation truth information etc.
It glues physics generator, detector simulation and electronics simulation
modules together to achieve a full simulation chain. In the implementation of
the framework, many attractive characteristics of the SNiPER have been used,
such as dynamic loading, flexible flow control, multiple event management and
Python binding. Furthermore, additional efforts have been made to make both
detector and electronics simulation flexible enough to accommodate and optimize
different detector designs.
For the Geant4-based detector simulation, each sub-detector component is
implemented as a SNiPER tool which is a dynamically loadable and configurable
plugin. So it is possible to select the detector configuration at runtime. The
framework provides the event loop to drive the detector simulation and
interacts with the Geant4 which is implemented as a passive service. All levels
of user actions are wrapped into different customizable tools, so that user
functions can be easily extended by just adding new tools. The electronics
simulation has been implemented by following an event driven scheme. The SNiPER
task component is used to simulate data processing steps in the electronics
modules. The electronics and trigger are synchronized by triggered events
containing possible physics signals.
",0,1,0,0,0,0
706,Outage analysis in two-way communication with RF energy harvesting relay and co-channel interference,"  The study of relays with the scope of energy-harvesting (EH) looks
interesting as a means of enabling sustainable, wireless communication without
the need to recharge or replace the battery driving the relays. However,
reliability of such communication systems becomes an important design challenge
when such relays scavenge energy from the information bearing RF signals
received from the source, using the technique of simultaneous wireless
information and power transfer (SWIPT). To this aim, this work studies
bidirectional communication in a decode-and-forward (DF) relay assisted
cooperative wireless network in presence of co-channel interference (CCI). In
order to quantify the reliability of the bidirectional communication systems, a
closed form expression for the outage probability of the system is derived for
both power splitting (PS) and time switching (TS) mode of operation of the
relay. Simulation results are used to validate the accuracy of our analytical
results and illustrate the dependence of the outage probability on various
system parameters, like PS factor, TS factor, and distance of the relay from
both the users. Results of performance comparison between PS relaying (PSR) and
TS relaying (TSR) schemes are also presented. Besides, simulation results are
also used to illustrate the spectral-efficiency and the energy-efficiency of
the proposed system. The results show that, both in terms of spectral
efficiency and the energy-efficiency, the two-way communication system in
presence of moderate CCI power, performs better than the similar system without
CCI. Additionally, it is also found that PSR is superior to TSR protocol in
terms of peak energy-efficiency.
",1,0,0,0,0,0
13364,Implementing universal nonadiabatic holonomic quantum gates with transmons,"  Geometric phases are well known to be noise-resilient in quantum
evolutions/operations. Holonomic quantum gates provide us with a robust way
towards universal quantum computation, as these quantum gates are actually
induced by nonabelian geometric phases. Here we propose and elaborate how to
efficiently implement universal nonadiabatic holonomic quantum gates on simpler
superconducting circuits, with a single transmon serving as a qubit. In our
proposal, an arbitrary single-qubit holonomic gate can be realized in a
single-loop scenario, by varying the amplitudes and phase difference of two
microwave fields resonantly coupled to a transmon, while nontrivial two-qubit
holonomic gates may be generated with a transmission-line resonator being
simultaneously coupled to the two target transmons in an effective resonant
way. Moreover, our scenario may readily be scaled up to a two-dimensional
lattice configuration, which is able to support large scalable quantum
computation, paving the way for practically implementing universal nonadiabatic
holonomic quantum computation with superconducting circuits.
",0,1,0,0,0,0
8532,A geometric second-order-rectifiable stratification for closed subsets of Euclidean space,"  Defining the $m$-th stratum of a closed subset of an $n$ dimensional
Euclidean space to consist of those points, where it can be touched by a ball
from at least $n-m$ linearly independent directions, we establish that the
$m$-th stratum is second-order rectifiable of dimension $m$ and a Borel set.
This was known for convex sets, but is new even for sets of positive reach. The
result is based on a new criterion for second-order rectifiability.
",0,0,1,0,0,0
12068,Quantum Teleportation and Super-dense Coding in Operator Algebras,"  Let $\mathcal{B}_d$ be the unital $C^*$-algebra generated by the elements
$u_{jk}, \, 0 \le i, j \le d-1$, satisfying the relations that $[u_{j,k}]$ is a
unitary operator, and let $C^*(\mathbb{F}_{d^2})$ be the full group
$C^*$-algebra of free group of $d^2$ generators. Based on the idea of
teleportation and super-dense coding in quantum information theory, we exhibit
the two $*$-isomorphisms $M_d(C^*(\mathbb{F}_{d^2}))\cong \mathcal{B}_d\rtimes
\mathbb{Z}_d\rtimes \mathbb{Z}_d$ and $M_d(\mathcal{B}_d)\cong
C^*(\mathbb{F}_{d^2})\rtimes \mathbb{Z}_d\rtimes \mathbb{Z}_d$, for certain
actions of $\mathbb{Z}_d$. As an application, we show that for any $d,m\ge 2$
with $(d,m)\neq (2,2)$, the matrix-valued generalization of the (tensor
product) quantum correlation set of $d$ inputs and $m$ outputs is not closed.
",0,0,1,0,0,0
15374,"Neural Networks for Beginners. A fast implementation in Matlab, Torch, TensorFlow","  This report provides an introduction to some Machine Learning tools within
the most common development environments. It mainly focuses on practical
problems, skipping any theoretical introduction. It is oriented to both
students trying to approach Machine Learning and experts looking for new
frameworks.
",1,0,0,1,0,0
1332,BOLD5000: A public fMRI dataset of 5000 images,"  Vision science, particularly machine vision, has been revolutionized by
introducing large-scale image datasets and statistical learning approaches.
Yet, human neuroimaging studies of visual perception still rely on small
numbers of images (around 100) due to time-constrained experimental procedures.
To apply statistical learning approaches that integrate neuroscience, the
number of images used in neuroimaging must be significantly increased. We
present BOLD5000, a human functional MRI (fMRI) study that includes almost
5,000 distinct images depicting real-world scenes. Beyond dramatically
increasing image dataset size relative to prior fMRI studies, BOLD5000 also
accounts for image diversity, overlapping with standard computer vision
datasets by incorporating images from the Scene UNderstanding (SUN), Common
Objects in Context (COCO), and ImageNet datasets. The scale and diversity of
these image datasets, combined with a slow event-related fMRI design, enable
fine-grained exploration into the neural representation of a wide range of
visual features, categories, and semantics. Concurrently, BOLD5000 brings us
closer to realizing Marr's dream of a singular vision science - the intertwined
study of biological and computer vision.
",0,0,0,0,1,0
9243,Does warm debris dust stem from asteroid belts?,"  Many debris discs reveal a two-component structure, with a cold outer and a
warm inner component. While the former are likely massive analogues of the
Kuiper belt, the origin of the latter is still a matter of debate. In this work
we investigate whether the warm dust may be a signature of asteroid belt
analogues. In the scenario tested here the current two-belt architecture stems
from an originally extended protoplanetary disc, in which planets have opened a
gap separating it into the outer and inner discs which, after the gas
dispersal, experience a steady-state collisional decay. This idea is explored
with an analytic collisional evolution model for a sample of 225 debris discs
from a Spitzer/IRS catalogue that are likely to possess a two-component
structure. We find that the vast majority of systems (220 out of 225, or 98%)
are compatible with this scenario. For their progenitors, original
protoplanetary discs, we find an average surface density slope of
$-0.93\pm0.06$ and an average initial mass of
$\left(3.3^{+0.4}_{-0.3}\right)\times 10^{-3}$ solar masses, both of which are
in agreement with the values inferred from submillimetre surveys. However, dust
production by short-period comets and - more rarely - inward transport from the
outer belts may be viable, and not mutually excluding, alternatives to the
asteroid belt scenario. The remaining five discs (2% of the sample: HIP 11486,
HIP 23497, HIP 57971, HIP 85790, HIP 89770) harbour inner components that
appear inconsistent with dust production in an ""asteroid belt."" Warm dust in
these systems must either be replenished from cometary sources or represent an
aftermath of a recent rare event, such as a major collision or planetary system
instability.
",0,1,0,0,0,0
20960,Universal features of price formation in financial markets: perspectives from Deep Learning,"  Using a large-scale Deep Learning approach applied to a high-frequency
database containing billions of electronic market quotes and transactions for
US equities, we uncover nonparametric evidence for the existence of a universal
and stationary price formation mechanism relating the dynamics of supply and
demand for a stock, as revealed through the order book, to subsequent
variations in its market price. We assess the model by testing its
out-of-sample predictions for the direction of price moves given the history of
price and order flow, across a wide range of stocks and time periods. The
universal price formation model is shown to exhibit a remarkably stable
out-of-sample prediction accuracy across time, for a wide range of stocks from
different sectors. Interestingly, these results also hold for stocks which are
not part of the training sample, showing that the relations captured by the
model are universal and not asset-specific.
The universal model --- trained on data from all stocks --- outperforms, in
terms of out-of-sample prediction accuracy, asset-specific linear and nonlinear
models trained on time series of any given stock, showing that the universal
nature of price formation weighs in favour of pooling together financial data
from various stocks, rather than designing asset- or sector-specific models as
commonly done. Standard data normalizations based on volatility, price level or
average spread, or partitioning the training data into sectors or categories
such as large/small tick stocks, do not improve training results. On the other
hand, inclusion of price and order flow history over many past observations is
shown to improve forecasting performance, showing evidence of path-dependence
in price dynamics.
",0,0,0,1,0,1
17028,Integral curvatures of Finsler manifolds and applications,"  In this paper, we study the integral curvatures of Finsler manifolds. Some
Bishop-Gromov relative volume comparisons and several Myers type theorems are
obtained. We also establish a Gromov type precompactness theorem and a
Yamaguchi type finiteness theorem. Furthermore, the isoperimetric and Sobolev
constants of a closed Finsler manifold are estimated by integral curvature
bounds.
",0,0,1,0,0,0
9225,Two-dimensional off-lattice Boltzmann model for van der Waals fluids with variable temperature,"  We develop a two-dimensional Lattice Boltzmann model for liquid-vapour
systems with variable temperature. Our model is based on a single particle
distribution function expanded with respect to the full-range Hermite
polynomials. In order to ensure the recovery of the hydrodynamic equations for
thermal flows, we use a fourth order expansion together with a set of momentum
vectors with 25 elements whose Cartesian projections are the roots of the
Hermite polynomial of order Q = 5. Since these vectors are off-lattice, a
fifth-order projection scheme is used to evolve the corresponding set of
distribution functions. A fourth order scheme employing a 49 point stencil is
used to compute the gradient operators in the force term that ensures the
liquid-vapour phase separation and diffuse reflection boundary conditions are
used on the walls. We demonstrate at least fourth order convergence with
respect to the lattice spacing in the contexts of shear and longitudinal wave
propagation through the van der Waals fluid. For the planar interface, fourth
order convergence can be seen at small enough lattice spacings, while the
effect of the spurious velocity on the temperature profile is found to be
smaller than 1.0%, even when T w ' 0.7 T c . We further validate our scheme by
considering the Laplace pressure test. Galilean invariance is shown to be
preserved up to second order with respect to the background velocity. We
further investigate the liquid-vapour phase separation between two parallel
walls kept at a constant temperature T w smaller than the critical temperature
T c and discuss the main features of this process.
",0,1,0,0,0,0
7230,Speeding-up Object Detection Training for Robotics with FALKON,"  Latest deep learning methods for object detection provide remarkable
performance, but have limits when used in robotic applications. One of the most
relevant issues is the long training time, which is due to the large size and
imbalance of the associated training sets, characterized by few positive and a
large number of negative examples (i.e. background). Proposed approaches are
based on end-to-end learning by back-propagation [22] or kernel methods trained
with Hard Negatives Mining on top of deep features [8]. These solutions are
effective, but prohibitively slow for on-line applications. In this paper we
propose a novel pipeline for object detection that overcomes this problem and
provides comparable performance, with a 60x training speedup. Our pipeline
combines (i) the Region Proposal Network and the deep feature extractor from
[22] to efficiently select candidate RoIs and encode them into powerful
representations, with (ii) the FALKON [23] algorithm, a novel kernel-based
method that allows fast training on large scale problems (millions of points).
We address the size and imbalance of training data by exploiting the stochastic
subsampling intrinsic into the method and a novel, fast, bootstrapping
approach. We assess the effectiveness of the approach on a standard Computer
Vision dataset (PASCAL VOC 2007 [5]) and demonstrate its applicability to a
real robotic scenario with the iCubWorld Transformations [18] dataset.
",1,0,0,0,0,0
13610,Reduced-Order Modeling through Machine Learning Approaches for Brittle Fracture Applications,"  In this paper, five different approaches for reduced-order modeling of
brittle fracture in geomaterials, specifically concrete, are presented and
compared. Four of the five methods rely on machine learning (ML) algorithms to
approximate important aspects of the brittle fracture problem. In addition to
the ML algorithms, each method incorporates different physics-based assumptions
in order to reduce the computational complexity while maintaining the physics
as much as possible. This work specifically focuses on using the ML approaches
to model a 2D concrete sample under low strain rate pure tensile loading
conditions with 20 preexisting cracks present. A high-fidelity finite
element-discrete element model is used to both produce a training dataset of
150 simulations and an additional 35 simulations for validation. Results from
the ML approaches are directly compared against the results from the
high-fidelity model. Strengths and weaknesses of each approach are discussed
and the most important conclusion is that a combination of physics-informed and
data-driven features are necessary for emulating the physics of crack
propagation, interaction and coalescence. All of the models presented here have
runtimes that are orders of magnitude faster than the original high-fidelity
model and pave the path for developing accurate reduced order models that could
be used to inform larger length-scale models with important sub-scale physics
that often cannot be accounted for due to computational cost.
",0,0,0,1,0,0
6969,A Computational Study of the Role of Tonal Tension in Expressive Piano Performance,"  Expressive variations of tempo and dynamics are an important aspect of music
performances, involving a variety of underlying factors. Previous work has
showed a relation between such expressive variations (in particular expressive
tempo) and perceptual characteristics derived from the musical score, such as
musical expectations, and perceived tension. In this work we use a
computational approach to study the role of three measures of tonal tension
proposed by Herremans and Chew (2016) in the prediction of expressive
performances of classical piano music. These features capture tonal
relationships of the music represented in Chew's spiral array model, a three
dimensional representation of pitch classes, chords and keys constructed in
such a way that spatial proximity represents close tonal relationships. We use
non-linear sequential models (recurrent neural networks) to assess the
contribution of these features to the prediction of expressive dynamics and
expressive tempo using a dataset of Mozart piano sonatas performed by a
professional concert pianist. Experiments of models trained with and without
tonal tension features show that tonal tension helps predict change of tempo
and dynamics more than absolute tempo and dynamics values. Furthermore, the
improvement is stronger for dynamics than for tempo.
",1,0,0,0,0,0
19581,Statistical analysis of the ambiguities in the asteroid period determinations,"  Among asteroids there exist ambiguities in their rotation period
determinations. They are due to incomplete coverage of the rotation, noise
and/or aliases resulting from gaps between separate lightcurves. To help to
remove such uncertainties, basic characteristic of the lightcurves resulting
from constraints imposed by the asteroid shapes and geometries of observations
should be identified. We simulated light variations of asteroids which shapes
were modelled as Gaussian random spheres, with random orientations of spin
vectors and phase angles changed every $5^\circ$ from $0^\circ$ to $65^\circ$.
This produced 1.4 mln lightcurves. For each simulated lightcurve Fourier
analysis has been made and the harmonic of the highest amplitude was recorded.
From the statistical point of view, all lightcurves observed at phase angles
$\alpha < 30^\circ$, with peak-to-peak amplitudes $A>0.2$ mag are bimodal.
Second most frequently dominating harmonic is the first one, with the 3rd
harmonic following right after. For 1% of lightcurves with amplitudes $A < 0.1$
mag and phase angles $\alpha < 40^\circ$ 4th harmonic dominates.
",0,1,0,0,0,0
9845,A Supervised STDP-based Training Algorithm for Living Neural Networks,"  Neural networks have shown great potential in many applications like speech
recognition, drug discovery, image classification, and object detection. Neural
network models are inspired by biological neural networks, but they are
optimized to perform machine learning tasks on digital computers. The proposed
work explores the possibilities of using living neural networks in vitro as
basic computational elements for machine learning applications. A new
supervised STDP-based learning algorithm is proposed in this work, which
considers neuron engineering constrains. A 74.7% accuracy is achieved on the
MNIST benchmark for handwritten digit recognition.
",1,0,0,1,0,0
11990,"Methodology for Multi-stage, Operations- and Uncertainty-Aware Placement and Sizing of FACTS Devices in a Large Power Transmission System","  We develop new optimization methodology for planning installation of Flexible
Alternating Current Transmission System (FACTS) devices of the parallel and
shunt types into large power transmission systems, which allows to delay or
avoid installations of generally much more expensive power lines. Methodology
takes as an input projected economic development, expressed through a paced
growth of the system loads, as well as uncertainties, expressed through
multiple scenarios of the growth. We price new devices according to their
capacities. Installation cost contributes to the optimization objective in
combination with the cost of operations integrated over time and averaged over
the scenarios. The multi-stage (-time-frame) optimization aims to achieve a
gradual distribution of new resources in space and time. Constraints on the
investment budget, or equivalently constraint on building capacity, is
introduced at each time frame. Our approach adjusts operationally not only
newly installed FACTS devices but also other already existing flexible degrees
of freedom. This complex optimization problem is stated using the most general
AC Power Flows. Non-linear, non-convex, multiple-scenario and multi-time-frame
optimization is resolved via efficient heuristics, consisting of a sequence of
alternating Linear Programmings or Quadratic Programmings (depending on the
generation cost) and AC-PF solution steps designed to maintain operational
feasibility for all scenarios. Computational scalability and application of the
newly developed approach is illustrated on the example of the 2736-nodes large
Polish system. One most important advantage of the framework is that the
optimal capacity of FACTS is build up gradually at each time frame in a limited
number of locations, thus allowing to prepare the system better for possible
congestion due to future economic and other uncertainties.
",1,1,1,0,0,0
11467,Inference in Deep Networks in High Dimensions,"  Deep generative networks provide a powerful tool for modeling complex data in
a wide range of applications. In inverse problems that use these networks as
generative priors on data, one must often perform inference of the inputs of
the networks from the outputs. Inference is also required for sampling during
stochastic training on these generative models. This paper considers inference
in a deep stochastic neural network where the parameters (e.g., weights, biases
and activation functions) are known and the problem is to estimate the values
of the input and hidden units from the output. While several approximate
algorithms have been proposed for this task, there are few analytic tools that
can provide rigorous guarantees in the reconstruction error. This work presents
a novel and computationally tractable output-to-input inference method called
Multi-Layer Vector Approximate Message Passing (ML-VAMP). The proposed
algorithm, derived from expectation propagation, extends earlier AMP methods
that are known to achieve the replica predictions for optimality in simple
linear inverse problems. Our main contribution shows that the mean-squared
error (MSE) of ML-VAMP can be exactly predicted in a certain large system limit
(LSL) where the numbers of layers is fixed and weight matrices are random and
orthogonally-invariant with dimensions that grow to infinity. ML-VAMP is thus a
principled method for output-to-input inference in deep networks with a
rigorous and precise performance achievability result in high dimensions.
",1,0,0,1,0,0
12049,"Private Information, Credit Risk and Graph Structure in P2P Lending Networks","  This research investigated the potential for improving Peer-to-Peer (P2P)
credit scoring by using ""private information"" about communications and travels
of borrowers. We found that P2P borrowers' ego networks exhibit scale-free
behavior driven by underlying preferential attachment mechanisms that connect
borrowers in a fashion that can be used to predict loan profitability. The
projection of these private networks onto networks of mobile phone
communication and geographical locations from mobile phone GPS potentially give
loan providers access to private information through graph and location metrics
which we used to predict loan profitability. Graph topology was found to be an
important predictor of loan profitability, explaining over 5.5% of variability.
Networks of borrower location information explain an additional 19% of the
profitability. Machine learning algorithms were applied to the data set
previously analyzed to develop the predictive model and resulted in a 4%
reduction in mean squared error.
",1,0,0,0,0,1
10324,Automated Algorithm Selection on Continuous Black-Box Problems By Combining Exploratory Landscape Analysis and Machine Learning,"  In this paper, we build upon previous work on designing informative and
efficient Exploratory Landscape Analysis features for characterizing problems'
landscapes and show their effectiveness in automatically constructing algorithm
selection models in continuous black-box optimization problems. Focussing on
algorithm performance results of the COCO platform of several years, we
construct a representative set of high-performing complementary solvers and
present an algorithm selection model that - compared to the portfolio's single
best solver - on average requires less than half of the resources for solving a
given problem. Therefore, there is a huge gain in efficiency compared to
classical ensemble methods combined with an increased insight into problem
characteristics and algorithm properties by using informative features. Acting
on the assumption that the function set of the Black-Box Optimization Benchmark
is representative enough for practical applications the model allows for
selecting the best suited optimization algorithm within the considered set for
unseen problems prior to the optimization itself based on a small sample of
function evaluations. Note that such a sample can even be reused for the
initial population of an evolutionary (optimization) algorithm so that even the
feature costs become negligible.
",1,0,0,1,0,0
9842,An Amateur Drone Surveillance System Based on Cognitive Internet of Things,"  Drones, also known as mini-unmanned aerial vehicles, have attracted
increasing attention due to their boundless applications in communications,
photography, agriculture, surveillance and numerous public services. However,
the deployment of amateur drones poses various safety, security and privacy
threats. To cope with these challenges, amateur drone surveillance becomes a
very important but largely unexplored topic. In this article, we firstly
present a brief survey to show the state-of-the-art studies on amateur drone
surveillance. Then, we propose a vision, named Dragnet, by tailoring the recent
emerging cognitive internet of things framework for amateur drone surveillance.
Next, we discuss the key enabling techniques for Dragnet in details,
accompanied with the technical challenges and open issues. Furthermore, we
provide an exemplary case study on the detection and classification of
authorized and unauthorized amateur drones, where, for example, an important
event is being held and only authorized drones are allowed to fly over.
",1,0,0,0,0,0
15946,Statistical inference for high dimensional regression via Constrained Lasso,"  In this paper, we propose a new method for estimation and constructing
confidence intervals for low-dimensional components in a high-dimensional
model. The proposed estimator, called Constrained Lasso (CLasso) estimator, is
obtained by simultaneously solving two estimating equations---one imposing a
zero-bias constraint for the low-dimensional parameter and the other forming an
$\ell_1$-penalized procedure for the high-dimensional nuisance parameter. By
carefully choosing the zero-bias constraint, the resulting estimator of the low
dimensional parameter is shown to admit an asymptotically normal limit
attaining the Cramér-Rao lower bound in a semiparametric sense. We propose
a tuning-free iterative algorithm for implementing the CLasso. We show that
when the algorithm is initialized at the Lasso estimator, the de-sparsified
estimator proposed in van de Geer et al. [\emph{Ann. Statist.} {\bf 42} (2014)
1166--1202] is asymptotically equivalent to the first iterate of the algorithm.
We analyse the asymptotic properties of the CLasso estimator and show the
globally linear convergence of the algorithm. We also demonstrate encouraging
empirical performance of the CLasso through numerical studies.
",0,0,1,1,0,0
7977,Overcoming data scarcity with transfer learning,"  Despite increasing focus on data publication and discovery in materials
science and related fields, the global view of materials data is highly sparse.
This sparsity encourages training models on the union of multiple datasets, but
simple unions can prove problematic as (ostensibly) equivalent properties may
be measured or computed differently depending on the data source. These hidden
contextual differences introduce irreducible errors into analyses,
fundamentally limiting their accuracy. Transfer learning, where information
from one dataset is used to inform a model on another, can be an effective tool
for bridging sparse data while preserving the contextual differences in the
underlying measurements. Here, we describe and compare three techniques for
transfer learning: multi-task, difference, and explicit latent variable
architectures. We show that difference architectures are most accurate in the
multi-fidelity case of mixed DFT and experimental band gaps, while multi-task
most improves classification performance of color with band gaps. For
activation energies of steps in NO reduction, the explicit latent variable
method is not only the most accurate, but also enjoys cancellation of errors in
functions that depend on multiple tasks. These results motivate the publication
of high quality materials datasets that encode transferable information,
independent of industrial or academic interest in the particular labels, and
encourage further development and application of transfer learning methods to
materials informatics problems.
",1,0,0,1,0,0
8390,A statistical physics approach to learning curves for the Inverse Ising problem,"  Using methods of statistical physics, we analyse the error of learning
couplings in large Ising models from independent data (the inverse Ising
problem). We concentrate on learning based on local cost functions, such as the
pseudo-likelihood method for which the couplings are inferred independently for
each spin. Assuming that the data are generated from a true Ising model, we
compute the reconstruction error of the couplings using a combination of the
replica method with the cavity approach for densely connected systems. We show
that an explicit estimator based on a quadratic cost function achieves minimal
reconstruction error, but requires the length of the true coupling vector as
prior knowledge. A simple mean field estimator of the couplings which does not
need such knowledge is asymptotically optimal, i.e. when the number of
observations is much large than the number of spins. Comparison of the theory
with numerical simulations shows excellent agreement for data generated from
two models with random couplings in the high temperature region: a model with
independent couplings (Sherrington-Kirkpatrick model), and a model where the
matrix of couplings has a Wishart distribution.
",0,1,0,1,0,0
4525,Simulated Tornado Optimization,"  We propose a swarm-based optimization algorithm inspired by air currents of a
tornado. Two main air currents - spiral and updraft - are mimicked. Spiral
motion is designed for exploration of new search areas and updraft movements is
deployed for exploitation of a promising candidate solution. Assignment of just
one search direction to each particle at each iteration, leads to low
computational complexity of the proposed algorithm respect to the conventional
algorithms. Regardless of the step size parameters, the only parameter of the
proposed algorithm, called tornado diameter, can be efficiently adjusted by
randomization. Numerical results over six different benchmark cost functions
indicate comparable and, in some cases, better performance of the proposed
algorithm respect to some other metaheuristics.
",1,0,1,0,0,0
7377,Linear Programming Formulations of Deterministic Infinite Horizon Optimal Control Problems in Discrete Time,"  This paper is devoted to a study of infinite horizon optimal control problems
with time discounting and time averaging criteria in discrete time. We
establish that these problems are related to certain infinite-dimensional
linear programming (IDLP) problems. We also establish asymptotic relationships
between the optimal values of problems with time discounting and long-run
average criteria.
",0,0,1,0,0,0
8697,Application of Coulomb energy density functional for atomic nuclei: Case studies of local density approximation and generalized gradient approximation,"  We test the Coulomb exchange and correlation energy density functionals of
electron systems for atomic nuclei in the local density approximation (LDA) and
the generalized gradient approximation (GGA). For the exchange Coulomb
energies, it is found that the deviation between the LDA and GGA ranges from
around $ 11 \, \% $ in $ {}^{4} \mathrm{He} $ to around $ 2.2 \, \% $ in $
{}^{208} \mathrm{Pb} $, by taking the Perdew-Burke-Ernzerhof (PBE) functional
as an example of the GGA\@. For the correlation Coulomb energies, it is shown
that those functionals of electron systems are not suitable for atomic nuclei.
",0,1,0,0,0,0
7582,Integrating sentiment and social structure to determine preference alignments: The Irish Marriage Referendum,"  We examine the relationship between social structure and sentiment through
the analysis of a large collection of tweets about the Irish Marriage
Referendum of 2015. We obtain the sentiment of every tweet with the hashtags
#marref and #marriageref that was posted in the days leading to the referendum,
and construct networks to aggregate sentiment and use it to study the
interactions among users. Our results show that the sentiment of mention tweets
posted by users is correlated with the sentiment of received mentions, and
there are significantly more connections between users with similar sentiment
scores than among users with opposite scores in the mention and follower
networks. We combine the community structure of the two networks with the
activity level of the users and sentiment scores to find groups of users who
support voting `yes' or `no' in the referendum. There were numerous
conversations between users on opposing sides of the debate in the absence of
follower connections, which suggests that there were efforts by some users to
establish dialogue and debate across ideological divisions. Our analysis shows
that social structure can be integrated successfully with sentiment to analyse
and understand the disposition of social media users. These results have
potential applications in the integration of data and meta-data to study
opinion dynamics, public opinion modelling, and polling.
",1,1,0,0,0,0
11560,Estimation of the multifractional function and the stability index of linear multifractional stable processes,"  In this paper we are interested in multifractional stable processes where the
self-similarity index $H$ is a function of time, in other words $H$ becomes
time changing, and the stability index $\alpha$ is a constant. Using $\beta$-
negative power variations ($-1/2<\beta<0$), we propose estimators for the value
of the multifractional function $H$ at a fixed time $t_0$ and for $\alpha$ for
two cases: multifractional Brownian motion ($\alpha=2$) and linear
multifractional stable motion ($0<\alpha<2$). We get the consistency of our
estimates for the underlying processes with the rate of convergence.
",0,0,1,1,0,0
16906,Property Testing in High Dimensional Ising models,"  This paper explores the information-theoretic limitations of graph property
testing in zero-field Ising models. Instead of learning the entire graph
structure, sometimes testing a basic graph property such as connectivity, cycle
presence or maximum clique size is a more relevant and attainable objective.
Since property testing is more fundamental than graph recovery, any necessary
conditions for property testing imply corresponding conditions for graph
recovery, while custom property tests can be statistically and/or
computationally more efficient than graph recovery based algorithms.
Understanding the statistical complexity of property testing requires the
distinction of ferromagnetic (i.e., positive interactions only) and general
Ising models. Using combinatorial constructs such as graph packing and strong
monotonicity, we characterize how target properties affect the corresponding
minimax upper and lower bounds within the realm of ferromagnets. On the other
hand, by studying the detection of an antiferromagnetic (i.e., negative
interactions only) Curie-Weiss model buried in Rademacher noise, we show that
property testing is strictly more challenging over general Ising models. In
terms of methodological development, we propose two types of correlation based
tests: computationally efficient screening for ferromagnets, and score type
tests for general models, including a fast cycle presence test. Our correlation
screening tests match the information-theoretic bounds for property testing in
ferromagnets.
",0,0,1,1,0,0
10386,Schrödinger's Man,"  What if someone built a ""box"" that applies quantum superposition not just to
quantum bits in the microscopic but also to macroscopic everyday ""objects"",
such as Schrödinger's cat or a human being? If that were possible, and if the
different ""copies"" of a man could exploit quantum interference to synchronize
and collapse into their preferred state, then one (or they?) could in a sense
choose their future, win the lottery, break codes and other security devices,
and become king of the world, or actually of the many-worlds. We set up the
plot-line of a new episode of Black Mirror to reflect on what might await us if
one were able to build such a technology.
",1,0,0,0,0,0
19943,The evolution of magnetic fields in hot stars,"  Over the last decade, tremendous strides have been achieved in our
understanding of magnetism in main sequence hot stars. In particular, the
statistical occurrence of their surface magnetism has been established (~10%)
and the field origin is now understood to be fossil. However, fundamental
questions remain: how do these fossil fields evolve during the post-main
sequence phases, and how do they influence the evolution of hot stars from the
main sequence to their ultimate demise? Filling the void of known magnetic
evolved hot (OBA) stars, studying the evolution of their fossil magnetic fields
along stellar evolution, and understanding the impact of these fields on the
angular momentum, rotation, mass loss, and evolution of the star itself, is
crucial to answering these questions, with far reaching consequences, in
particular for the properties of the precursors of supernovae explosions and
stellar remnants. In the framework of the BRITE spectropolarimetric survey and
LIFE project, we have discovered the first few magnetic hot supergiants. Their
longitudinal surface magnetic field is very weak but their configuration
resembles those of main sequence hot stars. We present these first
observational results and propose to interpret them at first order in the
context of magnetic flux conservation as the radius of the star expands with
evolution. We then also consider the possible impact of stellar structure
changes along evolution.
",0,1,0,0,0,0
13737,Finding Network Motifs in Large Graphs using Compression as a Measure of Relevance,"  We introduce a new method for finding network motifs: interesting or
informative subgraph patterns in a network. Current methods for finding motifs
rely on the frequency of the motif: specifically, subgraphs are motifs when
their frequency in the data is high compared to the expected frequency under a
null model. To compute this expectation, the search for motifs is normally
repeated on as many as 1000 random graphs sampled from the null model; a
prohibitively expensive step. We use ideas from the Minimum Description Length
(MDL) literature to define a new measure of motif relevance, and a new
algorithm for detecting motifs. Our method allows motif analysis to scale to
networks with billions of links, while still resulting in informative motifs.
",1,0,0,0,0,0
15708,On a Surprising Oversight by John S. Bell in the Proof of his Famous Theorem,"  Bell inequalities are usually derived by assuming locality and realism, and
therefore experimental violations of Bell inequalities are usually taken to
imply violations of either locality or realism, or both. But, after reviewing
an oversight by Bell, here we derive the Bell-CHSH inequality by assuming only
that Bob can measure along the directions b and b' simultaneously while Alice
measures along either a or a', and likewise Alice can measure along the
directions a and a' simultaneously while Bob measures along either b or b',
without assuming locality. The observed violations of the Bell-CHSH inequality
therefore simply verify the manifest impossibility of measuring along the
directions b and b' (or along the directions a and a') simultaneously, in any
realizable EPR-Bohm type experiment.
",0,1,0,0,0,0
3649,Wandering domains for diffeomorphisms of the k-torus: a remark on a theorem by Norton and Sullivan,"  We show that there is no C^{k+1} diffeomorphism of the k-torus which is
semiconjugate to a minimal translation and has a wandering domain all of whose
iterates are Euclidean balls.
",0,0,1,0,0,0
19646,That's Enough: Asynchrony with Standard Choreography Primitives,"  Choreographies are widely used for the specification of concurrent and
distributed software architectures. Since asynchronous communications are
ubiquitous in real-world systems, previous works have proposed different
approaches for the formal modelling of asynchrony in choreographies. Such
approaches typically rely on ad-hoc syntactic terms or semantics for capturing
the concept of messages in transit, yielding different formalisms that have to
be studied separately.
In this work, we take a different approach, and show that such extensions are
not needed to reason about asynchronous communications in choreographies.
Rather, we demonstrate how a standard choreography calculus already has all the
needed expressive power to encode messages in transit (and thus asynchronous
communications) through the primitives of process spawning and name mobility.
The practical consequence of our results is that we can reason about real-world
systems within a choreography formalism that is simpler than those hitherto
proposed.
",1,0,0,0,0,0
5309,S-OHEM: Stratified Online Hard Example Mining for Object Detection,"  One of the major challenges in object detection is to propose detectors with
highly accurate localization of objects. The online sampling of high-loss
region proposals (hard examples) uses the multitask loss with equal weight
settings across all loss types (e.g, classification and localization, rigid and
non-rigid categories) and ignores the influence of different loss distributions
throughout the training process, which we find essential to the training
efficacy. In this paper, we present the Stratified Online Hard Example Mining
(S-OHEM) algorithm for training higher efficiency and accuracy detectors.
S-OHEM exploits OHEM with stratified sampling, a widely-adopted sampling
technique, to choose the training examples according to this influence during
hard example mining, and thus enhance the performance of object detectors. We
show through systematic experiments that S-OHEM yields an average precision
(AP) improvement of 0.5% on rigid categories of PASCAL VOC 2007 for both the
IoU threshold of 0.6 and 0.7. For KITTI 2012, both results of the same metric
are 1.6%. Regarding the mean average precision (mAP), a relative increase of
0.3% and 0.5% (1% and 0.5%) is observed for VOC07 (KITTI12) using the same set
of IoU threshold. Also, S-OHEM is easy to integrate with existing region-based
detectors and is capable of acting with post-recognition level regressors.
",1,0,0,0,0,0
13082,Incorporating Feedback into Tree-based Anomaly Detection,"  Anomaly detectors are often used to produce a ranked list of statistical
anomalies, which are examined by human analysts in order to extract the actual
anomalies of interest. Unfortunately, in realworld applications, this process
can be exceedingly difficult for the analyst since a large fraction of
high-ranking anomalies are false positives and not interesting from the
application perspective. In this paper, we aim to make the analyst's job easier
by allowing for analyst feedback during the investigation process. Ideally, the
feedback influences the ranking of the anomaly detector in a way that reduces
the number of false positives that must be examined before discovering the
anomalies of interest. In particular, we introduce a novel technique for
incorporating simple binary feedback into tree-based anomaly detectors. We
focus on the Isolation Forest algorithm as a representative tree-based anomaly
detector, and show that we can significantly improve its performance by
incorporating feedback, when compared with the baseline algorithm that does not
incorporate feedback. Our technique is simple and scales well as the size of
the data increases, which makes it suitable for interactive discovery of
anomalies in large datasets.
",1,0,0,1,0,0
18826,One look at the rating of scientific publications and corresponding toy-model,"  A toy-model of publications and citations processes is proposed. The model
shows that the role of randomness in the processes is essential and cannot be
ignored. Some other aspects of scientific publications rating are discussed.
",1,0,0,1,0,0
13857,Surface plasmons in superintense laser-solid interactions,"  We review studies of superintense laser interaction with solid targets where
the generation of propagating surface plasmons (or surface waves) plays a key
role. These studies include the onset of plasma instabilities at the irradiated
surface, the enhancement of secondary emissions (protons, electrons, and
photons as high harmonics in the XUV range) in femtosecond interactions with
grating targets, and the generation of unipolar current pulses with picosecond
duration. The experimental results give evidence of the existence of surface
plasmons in the nonlinear regime of relativistic electron dynamics. These
findings open up a route to the improvement of ultrashort laser-driven sources
of energetic radiation and, more in general, to the extension of plasmonics in
a high field regime.
",0,1,0,0,0,0
16775,Stellar Abundances for Galactic Archaeology Database IV - Compilation of Stars in Dwarf Galaxies,"  We have constructed the database of stars in the local group using the
extended version of the SAGA (Stellar Abundances for Galactic Archaeology)
database that contains stars in 24 dwarf spheroidal galaxies and ultra faint
dwarfs. The new version of the database includes more than 4500 stars in the
Milky Way, by removing the previous metallicity criterion of [Fe/H] <= -2.5,
and more than 6000 stars in the local group galaxies. We examined a validity of
using a combined data set for elemental abundances. We also checked a
consistency between the derived distances to individual stars and those to
galaxies in the literature values. Using the updated database, the
characteristics of stars in dwarf galaxies are discussed. Our statistical
analyses of alpha-element abundances show that the change of the slope of the
[alpha/Fe] relative to [Fe/H] (so-called ""knee"") occurs at [Fe/H] = -1.0+-0.1
for the Milky Way. The knee positions for selected galaxies are derived by
applying the same method. Star formation history of individual galaxies are
explored using the slope of the cumulative metallicity distribution function.
Radial gradients along the four directions are inspected in six galaxies where
we find no direction dependence of metallicity gradients along the major and
minor axes. The compilation of all the available data shows a lack of CEMP-s
population in dwarf galaxies, while there may be some CEMP-no stars at [Fe/H]
<~ -3 even in the very small sample. The inspection of the relationship between
Eu and Ba abundances confirms an anomalously Ba-rich population in Fornax,
which indicates a pre-enrichment of interstellar gas with r-process elements.
We do not find any evidence of anti-correlations in O-Na and Mg-Al abundances,
which characterises the abundance trends in the Galactic globular clusters.
",0,1,0,0,0,0
4237,The Genus-One Global Mirror Theorem for the Quintic Threefold,"  We prove the genus-one restriction of the all-genus
Landau-Ginzburg/Calabi-Yau conjecture of Chiodo and Ruan, stated in terms of
the geometric quantization of an explicit symplectomorphism determined by
genus-zero invariants. This provides the first evidence supporting the
higher-genus Landau-Ginzburg/Calabi-Yau correspondence for the quintic
threefold, and exhibits the first instance of the ""genus zero controls higher
genus"" principle, in the sense of Givental's quantization formalism, for
non-semisimple cohomological field theories.
",0,0,1,0,0,0
5703,Quantizing Euclidean motions via double-coset decomposition,"  Concepts from mathematical crystallography and group theory are used here to
quantize the group of rigid-body motions, resulting in a ""motion alphabet"" with
which to express robot motion primitives. From these primitives it is possible
to develop a dictionary of physical actions. Equipped with an alphabet of the
sort developed here, intelligent actions of robots in the world can be
approximated with finite sequences of characters, thereby forming the
foundation of a language in which to articulate robot motion. In particular, we
use the discrete handedness-preserving symmetries of macromolecular crystals
(known in mathematical crystallography as Sohncke space groups) to form a
coarse discretization of the space $\rm{SE}(3)$ of rigid-body motions. This
discretization is made finer by subdividing using the concept of double-coset
decomposition. More specifically, a very efficient, equivolumetric quantization
of spatial motion can be defined using the group-theoretic concept of a
double-coset decomposition of the form $\Gamma \backslash \rm{SE}(3) / \Delta$,
where $\Gamma$ is a Sohncke space group and $\Delta$ is a finite group of
rotational symmetries such as those of the icosahedron. The resulting discrete
alphabet is based on a very uniform sampling of $\rm{SE}(3)$ and is a tool for
describing the continuous trajectories of robots and humans. The general
""signals to symbols"" problem in artificial intelligence is cast in this
framework for robots moving continuously in the world, and we present a
coarse-to-fine search scheme here to efficiently solve this decoding problem in
practice.
",1,0,0,0,0,0
918,WOMBAT: A Scalable and High Performance Astrophysical MHD Code,"  We present a new code for astrophysical magneto-hydrodynamics specifically
designed and optimized for high performance and scaling on modern and future
supercomputers. We describe a novel hybrid OpenMP/MPI programming model that
emerged from a collaboration between Cray, Inc. and the University of
Minnesota. This design utilizes MPI-RMA optimized for thread scaling, which
allows the code to run extremely efficiently at very high thread counts ideal
for the latest generation of the multi-core and many-core architectures. Such
performance characteristics are needed in the era of ""exascale"" computing. We
describe and demonstrate our high-performance design in detail with the intent
that it may be used as a model for other, future astrophysical codes intended
for applications demanding exceptional performance.
",0,1,0,0,0,0
14014,On the complexity of non-orientable Seifert fibre spaces,"  In this paper we deal with Seifert fibre spaces, which are compact
3-manifolds admitting a foliation by circles. We give a combinatorial
description for these manifolds in all the possible cases: orientable,
non-orientable, closed, with boundary. Moreover, we compute a potentially sharp
upper bound for their complexity in terms of the invariants of the
combinatorial description, extending to the non-orientable case results by
Fominykh and Wiest for the orientable case with boundary and by Martelli and
Petronio for the closed orientable case.
",0,0,1,0,0,0
20636,Privacy-Aware Guessing Efficiency,"  We investigate the problem of guessing a discrete random variable $Y$ under a
privacy constraint dictated by another correlated discrete random variable $X$,
where both guessing efficiency and privacy are assessed in terms of the
probability of correct guessing. We define $h(P_{XY}, \epsilon)$ as the maximum
probability of correctly guessing $Y$ given an auxiliary random variable $Z$,
where the maximization is taken over all $P_{Z|Y}$ ensuring that the
probability of correctly guessing $X$ given $Z$ does not exceed $\epsilon$. We
show that the map $\epsilon\mapsto h(P_{XY}, \epsilon)$ is strictly increasing,
concave, and piecewise linear, which allows us to derive a closed form
expression for $h(P_{XY}, \epsilon)$ when $X$ and $Y$ are connected via a
binary-input binary-output channel. For $(X^n, Y^n)$ being pairs of independent
and identically distributed binary random vectors, we similarly define
$\underline{h}_n(P_{X^nY^n}, \epsilon)$ under the assumption that $Z^n$ is also
a binary vector. Then we obtain a closed form expression for
$\underline{h}_n(P_{X^nY^n}, \epsilon)$ for sufficiently large, but nontrivial
values of $\epsilon$.
",1,0,1,1,0,0
6457,Error Forward-Propagation: Reusing Feedforward Connections to Propagate Errors in Deep Learning,"  We introduce Error Forward-Propagation, a biologically plausible mechanism to
propagate error feedback forward through the network. Architectural constraints
on connectivity are virtually eliminated for error feedback in the brain;
systematic backward connectivity is not used or needed to deliver error
feedback. Feedback as a means of assigning credit to neurons earlier in the
forward pathway for their contribution to the final output is thought to be
used in learning in the brain. How the brain solves the credit assignment
problem is unclear. In machine learning, error backpropagation is a highly
successful mechanism for credit assignment in deep multilayered networks.
Backpropagation requires symmetric reciprocal connectivity for every neuron.
From a biological perspective, there is no evidence of such an architectural
constraint, which makes backpropagation implausible for learning in the brain.
This architectural constraint is reduced with the use of random feedback
weights. Models using random feedback weights require backward connectivity
patterns for every neuron, but avoid symmetric weights and reciprocal
connections. In this paper, we practically remove this architectural
constraint, requiring only a backward loop connection for effective error
feedback. We propose reusing the forward connections to deliver the error
feedback by feeding the outputs into the input receiving layer. This mechanism,
Error Forward-Propagation, is a plausible basis for how error feedback occurs
deep in the brain independent of and yet in support of the functionality
underlying intricate network architectures. We show experimentally that
recurrent neural networks with two and three hidden layers can be trained using
Error Forward-Propagation on the MNIST and Fashion MNIST datasets, achieving
$1.90\%$ and $11\%$ generalization errors respectively.
",0,0,0,0,1,0
9713,The infrared to X-ray correlation spectra of unobscured type 1 active galactic nuclei,"  We use new X-ray data obtained with the Nuclear Spectroscopic Telescope Array
(NuSTAR), near-infrared (NIR) fluxes, and mid-infrared (MIR) spectra of a
sample of 24 unobscured type 1 active galactic nuclei (AGN) to study the
correlation between various hard X-ray bands between 3 and 80 keV and the
infrared (IR) emission. The IR to X-ray correlation spectrum (IRXCS) shows a
maximum at ~15-20 micron, coincident with the peak of the AGN contribution to
the MIR spectra of the majority of the sample. There is also a NIR correlation
peak at ~2 micron, which we associate with the NIR bump observed in some type 1
AGN at ~1-5 micron and is likely produced by nuclear hot dust emission. The
IRXCS shows practically the same behaviour in all the X-ray bands considered,
indicating a common origin for all of them. We finally evaluated correlations
between the X-ray luminosities and various MIR emission lines. All the lines
show a good correlation with the hard X-rays (rho>0.7), but we do not find the
expected correlation between their ionization potentials and the strength of
the IRXCS.
",0,1,0,0,0,0
2561,Mott metal-insulator transition in the Doped Hubbard-Holstein model,"  Motivated by the current interest in the understanding of the Mott insulators
away from half filling, observed in many perovskite oxides, we study the Mott
metal-insulator transition (MIT) in the doped Hubbard-Holstein model using the
Hatree-Fock mean field theory. The Hubbard-Holstein model is the simplest model
containing both the Coulomb and the electron-lattice interactions, which are
important ingredients in the physics of the perovskite oxides. In contrast to
the half-filled Hubbard model, which always results in a single phase (either
metallic or insulating), our results show that away from half-filling, a mixed
phase of metallic and insulating regions occur. As the dopant concentration is
increased, the metallic part progressively grows in volume, until it exceeds
the percolation threshold, leading to percolative conduction. This happens
above a critical dopant concentration $\delta_c$, which, depending on the
strength of the electron-lattice interaction, can be a significant fraction of
unity. This means that the material could be insulating even for a substantial
amount of doping, in contrast to the expectation that doped holes would destroy
the insulating behavior of the half-filled Hubbard model. Our theory provides a
framework for the understanding of the density-driven metal-insulator
transition observed in many complex oxides.
",0,1,0,0,0,0
19398,The OSIRIS-REx Visible and InfraRed Spectrometer (OVIRS): Spectral Maps of the Asteroid Bennu,"  The OSIRIS-REx Visible and Infrared Spectrometer (OVIRS) is a point
spectrometer covering the spectral range of 0.4 to 4.3 microns (25,000-2300
cm-1). Its primary purpose is to map the surface composition of the asteroid
Bennu, the target asteroid of the OSIRIS-REx asteroid sample return mission.
The information it returns will help guide the selection of the sample site. It
will also provide global context for the sample and high spatial resolution
spectra that can be related to spatially unresolved terrestrial observations of
asteroids. It is a compact, low-mass (17.8 kg), power efficient (8.8 W
average), and robust instrument with the sensitivity needed to detect a 5%
spectral absorption feature on a very dark surface (3% reflectance) in the
inner solar system (0.89-1.35 AU). It, in combination with the other
instruments on the OSIRIS-REx Mission, will provide an unprecedented view of an
asteroid's surface.
",0,1,0,0,0,0
59,Detecting Adversarial Samples Using Density Ratio Estimates,"  Machine learning models, especially based on deep architectures are used in
everyday applications ranging from self driving cars to medical diagnostics. It
has been shown that such models are dangerously susceptible to adversarial
samples, indistinguishable from real samples to human eye, adversarial samples
lead to incorrect classifications with high confidence. Impact of adversarial
samples is far-reaching and their efficient detection remains an open problem.
We propose to use direct density ratio estimation as an efficient model
agnostic measure to detect adversarial samples. Our proposed method works
equally well with single and multi-channel samples, and with different
adversarial sample generation methods. We also propose a method to use density
ratio estimates for generating adversarial samples with an added constraint of
preserving density ratio.
",1,0,0,1,0,0
7991,Nonequilibrium quantum dynamics of partial symmetry breaking for ultracold bosons in an optical lattice ring trap,"  A vortex in a Bose-Einstein condensate on a ring undergoes quantum dynamics
in response to a quantum quench in terms of partial symmetry breaking from a
uniform lattice to a biperiodic one. Neither the current, a macroscopic
measure, nor fidelity, a microscopic measure, exhibit critical behavior.
Instead, the symmetry memory succeeds in identifying the point at which the
system begins to forget its initial symmetry state. We further identify a
symmetry gap in the low lying excited states which trends with the symmetry
memory.
",0,1,0,0,0,0
751,Analytic heating rate of neutron star merger ejecta derived from Fermi's theory of beta decay,"  Macronovae (kilonovae) that arise in binary neutron star mergers are powered
by radioactive beta decay of hundreds of $r$-process nuclides. We derive, using
Fermi's theory of beta decay, an analytic estimate of the nuclear heating rate.
We show that the heating rate evolves as a power law ranging between $t^{-6/5}$
to $t^{-4/3}$. The overall magnitude of the heating rate is determined by the
mean values of nuclear quantities, e.g., the nuclear matrix elements of beta
decay. These values are specified by using nuclear experimental data. We
discuss the role of higher order beta transitions and the robustness of the
power law. The robust and simple form of the heating rate suggests that
observations of the late-time bolometric light curve $\propto t^{-\frac{4}{3}}$
would be a direct evidence of a $r$-process driven macronova. Such observations
could also enable us to estimate the total amount of $r$-process nuclei
produced in the merger.
",0,1,0,0,0,0
14161,Qualitative Measurements of Policy Discrepancy for Return-based Deep Q-Network,"  The deep Q-network (DQN) and return-based reinforcement learning are two
promising algorithms proposed in recent years. DQN brings advances to complex
sequential decision problems, while return-based algorithms have advantages in
making use of sample trajectories. In this paper, we propose a general
framework to combine DQN and most of the return-based reinforcement learning
algorithms, named R-DQN. We show the performance of traditional DQN can be
improved effectively by introducing return-based reinforcement learning. In
order to further improve the R-DQN, we design a strategy with two measurements
which can qualitatively measure the policy discrepancy. Moreover, we give the
two measurements' bounds in the proposed R-DQN framework. We show that
algorithms with our strategy can accurately express the trace coefficient and
achieve a better approximation to return. The experiments, conducted on several
representative tasks from the OpenAI Gym library, validate the effectiveness of
the proposed measurements. The results also show that the algorithms with our
strategy outperform the state-of-the-art methods.
",0,0,0,1,0,0
3400,Geometrical optimization approach to isomerization: Models and limitations,"  We study laser-driven isomerization reactions through an excited electronic
state using the recently developed Geometrical Optimization procedure [J. Phys.
Chem. Lett. 6, 1724 (2015)]. The goal is to analyze whether an initial wave
packet in the ground state, with optimized amplitudes and phases, can be used
to enhance the yield of the reaction at faster rates, exploring how the
geometrical restrictions induced by the symmetry of the system impose
limitations in the optimization procedure. As an example we model the
isomerization in an oriented 2,2'-dimethyl biphenyl molecule with a simple
quartic potential. Using long (picosecond) pulses we find that the
isomerization can be achieved driven by a single pulse. The phase of the
initial superposition state does not affect the yield. However, using short
(femtosecond) pulses, one always needs a pair of pulses to force the reaction.
High yields can only be obtained by optimizing both the initial state, and the
wave packet prepared in the excited state, implying the well known pump-dump
mechanism.
",0,1,0,0,0,0
7371,Credal Networks under Epistemic Irrelevance,"  A credal network under epistemic irrelevance is a generalised type of
Bayesian network that relaxes its two main building blocks. On the one hand,
the local probabilities are allowed to be partially specified. On the other
hand, the assessments of independence do not have to hold exactly.
Conceptually, these two features turn credal networks under epistemic
irrelevance into a powerful alternative to Bayesian networks, offering a more
flexible approach to graph-based multivariate uncertainty modelling. However,
in practice, they have long been perceived as very hard to work with, both
theoretically and computationally.
The aim of this paper is to demonstrate that this perception is no longer
justified. We provide a general introduction to credal networks under epistemic
irrelevance, give an overview of the state of the art, and present several new
theoretical results. Most importantly, we explain how these results can be
combined to allow for the design of recursive inference methods. We provide
numerous concrete examples of how this can be achieved, and use these to
demonstrate that computing with credal networks under epistemic irrelevance is
most definitely feasible, and in some cases even highly efficient. We also
discuss several philosophical aspects, including the lack of symmetry, how to
deal with probability zero, the interpretation of lower expectations, the
axiomatic status of graphoid properties, and the difference between updating
and conditioning.
",1,0,1,0,0,0
4652,Microwave SQUID Multiplexer demonstration for Cosmic Microwave Background Imagers,"  Key performance characteristics are demonstrated for the microwave SQUID
multiplexer ($\mu$MUX) coupled to transition edge sensor (TES) bolometers that
have been optimized for cosmic microwave background (CMB) observations. In a
64-channel demonstration, we show that the $\mu$MUX produces a white, input
referred current noise level of 29~pA$/\sqrt{\mathrm{Hz}}$ at -77~dB microwave
probe tone power, which is well below expected fundamental detector and photon
noise sources for a ground-based CMB-optimized bolometer. Operated with
negligible photon loading, we measure 98~pA$/\sqrt{\mathrm{Hz}}$ in the
TES-coupled channels biased at 65% of the sensor normal resistance. This noise
level is consistent with that predicted from bolometer thermal fluctuation
(i.e., phonon) noise. Furthermore, the power spectral density exhibits a white
spectrum at low frequencies ($\sim$~100~mHz), which enables CMB mapping on
large angular scales that constrain the physics of inflation. Additionally, we
report cross-talk measurements that indicate a level below 0.3%, which is less
than the level of cross-talk from multiplexed readout systems in deployed CMB
imagers. These measurements demonstrate the $\mu$MUX as a viable readout
technique for future CMB imaging instruments.
",0,1,0,0,0,0
13888,Guided Machine Learning for power grid segmentation,"  The segmentation of large scale power grids into zones is crucial for control
room operators when managing the grid complexity near real time. In this paper
we propose a new method in two steps which is able to automatically do this
segmentation, while taking into account the real time context, in order to help
them handle shifting dynamics. Our method relies on a ""guided"" machine learning
approach. As a first step, we define and compute a task specific ""Influence
Graph"" in a guided manner. We indeed simulate on a grid state chosen
interventions, representative of our task of interest (managing active power
flows in our case). For visualization and interpretation, we then build a
higher representation of the grid relevant to this task by applying the graph
community detection algorithm \textit{Infomap} on this Influence Graph. To
illustrate our method and demonstrate its practical interest, we apply it on
commonly used systems, the IEEE-14 and IEEE-118. We show promising and original
interpretable results, especially on the previously well studied RTS-96 system
for grid segmentation. We eventually share initial investigation and results on
a large-scale system, the French power grid, whose segmentation had a
surprising resemblance with RTE's historical partitioning.
",0,0,0,1,0,0
15259,Resolving Local Electrochemistry at the Nanoscale via Electrochemical Strain Microscopy: Modeling and Experiments,"  Electrochemistry is the underlying mechanism in a variety of energy
conversion and storage systems, and it is well known that the composition,
structure, and properties of electrochemical materials near active interfaces
often deviates substantially and inhomogeneously from the bulk properties. A
universal challenge facing the development of electrochemical systems is our
lack of understanding of physical and chemical rates at local length scales,
and the recently developed electrochemical strain microscopy (ESM) provides a
promising method to probe crucial local information regarding the underlying
electrochemical mechanisms. Here we develop a computational model that couples
mechanics and electrochemistry relevant for ESM experiments, with the goal to
enable quantitative analysis of electrochemical processes underneath a charged
scanning probe. We show that the model captures the essence of a number of
different ESM experiments, making it possible to de-convolute local ionic
concentration and diffusivity via combined ESM mapping, spectroscopy, and
relaxation studies. Through the combination of ESM experiments and
computations, it is thus possible to obtain deep insight into the local
electrochemistry at the nanoscale.
",0,1,0,0,0,0
10256,Exact Recovery with Symmetries for the Doubly-Stochastic Relaxation,"  Graph matching or quadratic assignment, is the problem of labeling the
vertices of two graphs so that they are as similar as possible. A common method
for approximately solving the NP-hard graph matching problem is relaxing it to
a convex optimization problem over the set of doubly stochastic (DS) matrices.
Recent analysis has shown that for almost all pairs of isomorphic and
asymmetric graphs, the DS relaxation succeeds in correctly retrieving the
isomorphism between the graphs. Our goal in this paper is to analyze the case
of symmetric isomorphic graphs. This goal is motivated by shape matching
applications where the graphs of interest usually have reflective symmetry.
For symmetric problems the graph matching problem has multiple isomorphisms
and so convex relaxations admit all convex combinations of these isomorphisms
as viable solutions. If the convex relaxation does not admit any additional
superfluous solution we say that it is convex exact. In this case there are
tractable algorithms to retrieve an isomorphism from the convex relaxation.
We show that convex exactness depends strongly on the symmetry group of the
graphs; For a fixed symmetry group $G$, either the DS relaxation will be convex
exact for almost all pairs of isomorphic graphs with symmetry group $G$, or the
DS relaxation will fail for all such pairs. We show that for reflective groups
with at least one full orbit convex exactness holds almost everywhere, and
provide some simple examples of non-reflective symmetry groups for which convex
exactness always fails.
When convex exactness holds, the isomorphisms of the graphs are the extreme
points of the convex solution set. We suggest an efficient algorithm for
retrieving an isomorphism in this case. We also show that the ""convex to
concave"" projection method will also retrieve an isomorphism in this case.
",0,0,1,0,0,0
19169,Natasha: Faster Non-Convex Stochastic Optimization Via Strongly Non-Convex Parameter,"  Given a nonconvex function that is an average of $n$ smooth functions, we
design stochastic first-order methods to find its approximate stationary
points. The convergence of our new methods depends on the smallest (negative)
eigenvalue $-\sigma$ of the Hessian, a parameter that describes how nonconvex
the function is.
Our methods outperform known results for a range of parameter $\sigma$, and
can be used to find approximate local minima. Our result implies an interesting
dichotomy: there exists a threshold $\sigma_0$ so that the currently fastest
methods for $\sigma>\sigma_0$ and for $\sigma<\sigma_0$ have different
behaviors: the former scales with $n^{2/3}$ and the latter scales with
$n^{3/4}$.
",1,0,1,1,0,0
9678,The Complexity of Factors of Multivariate Polynomials,"  The existence of string functions, which are not polynomial time computable,
but whose graph is checkable in polynomial time, is a basic assumption in
cryptography. We prove that in the framework of algebraic complexity, there are
no such families of polynomial functions of polynomially bounded degree over
fields of characteristic zero. The proof relies on a polynomial upper bound on
the approximative complexity of a factor g of a polynomial f in terms of the
(approximative) complexity of f and the degree of the factor g. This extends a
result by Kaltofen (STOC 1986). The concept of approximative complexity allows
to cope with the case that a factor has an exponential multiplicity, by using a
perturbation argument. Our result extends to randomized (two-sided error)
decision complexity.
",1,0,0,0,0,0
15716,Intrinsic Gaussian processes on complex constrained domains,"  We propose a class of intrinsic Gaussian processes (in-GPs) for
interpolation, regression and classification on manifolds with a primary focus
on complex constrained domains or irregular shaped spaces arising as subsets or
submanifolds of R, R2, R3 and beyond. For example, in-GPs can accommodate
spatial domains arising as complex subsets of Euclidean space. in-GPs respect
the potentially complex boundary or interior conditions as well as the
intrinsic geometry of the spaces. The key novelty of the proposed approach is
to utilise the relationship between heat kernels and the transition density of
Brownian motion on manifolds for constructing and approximating valid and
computationally feasible covariance kernels. This enables in-GPs to be
practically applied in great generality, while existing approaches for
smoothing on constrained domains are limited to simple special cases. The broad
utilities of the in-GP approach is illustrated through simulation studies and
data examples.
",0,0,0,1,0,0
7967,Regularized arrangements of cellular complexes,"  In this paper we propose a novel algorithm to combine two or more cellular
complexes, providing a minimal fragmentation of the cells of the resulting
complex. We introduce here the idea of arrangement generated by a collection of
cellular complexes, producing a cellular decomposition of the embedding space.
The algorithm that executes this computation is called \emph{Merge} of
complexes. The arrangements of line segments in 2D and polygons in 3D are
special cases, as well as the combination of closed triangulated surfaces or
meshed models. This algorithm has several important applications, including
Boolean and other set operations over large geometric models, the extraction of
solid models of biomedical structures at the cellular scale, the detailed
geometric modeling of buildings, the combination of 3D meshes, and the repair
of graphical models. The algorithm is efficiently implemented using the Linear
Algebraic Representation (LAR) of argument complexes, i.e., on sparse
representation of binary characteristic matrices of $d$-cell bases, well-suited
for implementation in last generation accelerators and GPGPU applications.
",1,0,0,0,0,0
3320,Flipping growth orientation of nanographitic structures by plasma enhanced chemical vapor deposition,"  Nanographitic structures (NGSs) with multitude of morphological features are
grown on SiO2/Si substrates by electron cyclotron resonance - plasma enhanced
chemical vapor deposition (ECR-PECVD). CH4 is used as source gas with Ar and H2
as dilutants. Field emission scanning electron microscopy, high resolution
transmission electron microscopy (HRTEM) and Raman spectroscopy are used to
study the structural and morphological features of the grown films. Herein, we
demonstrate, how the morphology can be tuned from planar to vertical structure
using single control parameter namely, dilution of CH4 with Ar and/or H2. Our
results show that the competitive growth and etching processes dictate the
morphology of the NGSs. While Ar-rich composition favors vertically oriented
graphene nanosheets, H2-rich composition aids growth of planar films. Raman
analysis reveals dilution of CH4 with either Ar or H2 or in combination helps
to improve the structural quality of the films. Line shape analysis of Raman 2D
band shows nearly symmetric Lorentzian profile which confirms the turbostratic
nature of the grown NGSs. Further, this aspect is elucidated by HRTEM studies
by observing elliptical diffraction pattern. Based on these experiments, a
comprehensive understanding is obtained on the growth and structural properties
of NGSs grown over a wide range of feedstock compositions.
",0,1,0,0,0,0
1287,A modal typing system for self-referential programs and specifications,"  This paper proposes a modal typing system that enables us to handle
self-referential formulae, including ones with negative self-references, which
on one hand, would introduce a logical contradiction, namely Russell's paradox,
in the conventional setting, while on the other hand, are necessary to capture
a certain class of programs such as fixed-point combinators and objects with
so-called binary methods in object-oriented programming. The proposed system
provides a basis for axiomatic semantics of such a wider range of programs and
a new framework for natural construction of recursive programs in the
proofs-as-programs paradigm.
",1,0,0,0,0,0
14631,Faster Algorithms for Weighted Recursive State Machines,"  Pushdown systems (PDSs) and recursive state machines (RSMs), which are
linearly equivalent, are standard models for interprocedural analysis. Yet RSMs
are more convenient as they (a) explicitly model function calls and returns,
and (b) specify many natural parameters for algorithmic analysis, e.g., the
number of entries and exits. We consider a general framework where RSM
transitions are labeled from a semiring and path properties are algebraic with
semiring operations, which can model, e.g., interprocedural reachability and
dataflow analysis problems.
Our main contributions are new algorithms for several fundamental problems.
As compared to a direct translation of RSMs to PDSs and the best-known existing
bounds of PDSs, our analysis algorithm improves the complexity for
finite-height semirings (that subsumes reachability and standard dataflow
properties). We further consider the problem of extracting distance values from
the representation structures computed by our algorithm, and give efficient
algorithms that distinguish the complexity of a one-time preprocessing from the
complexity of each individual query. Another advantage of our algorithm is that
our improvements carry over to the concurrent setting, where we improve the
best-known complexity for the context-bounded analysis of concurrent RSMs.
Finally, we provide a prototype implementation that gives a significant
speed-up on several benchmarks from the SLAM/SDV project.
",1,0,0,0,0,0
3925,ADINE: An Adaptive Momentum Method for Stochastic Gradient Descent,"  Two major momentum-based techniques that have achieved tremendous success in
optimization are Polyak's heavy ball method and Nesterov's accelerated
gradient. A crucial step in all momentum-based methods is the choice of the
momentum parameter $m$ which is always suggested to be set to less than $1$.
Although the choice of $m < 1$ is justified only under very strong theoretical
assumptions, it works well in practice even when the assumptions do not
necessarily hold. In this paper, we propose a new momentum based method
$\textit{ADINE}$, which relaxes the constraint of $m < 1$ and allows the
learning algorithm to use adaptive higher momentum. We motivate our hypothesis
on $m$ by experimentally verifying that a higher momentum ($\ge 1$) can help
escape saddles much faster. Using this motivation, we propose our method
$\textit{ADINE}$ that helps weigh the previous updates more (by setting the
momentum parameter $> 1$), evaluate our proposed algorithm on deep neural
networks and show that $\textit{ADINE}$ helps the learning algorithm to
converge much faster without compromising on the generalization error.
",1,0,0,1,0,0
20623,Detection Estimation and Grid matching of Multiple Targets with Single Snapshot Measurements,"  In this work, we explore the problems of detecting the number of narrow-band,
far-field targets and estimating their corresponding directions from single
snapshot measurements. The principles of sparse signal recovery (SSR) are used
for the single snapshot detection and estimation of multiple targets. In the
SSR framework, the DoA estimation problem is grid based and can be posed as the
lasso optimization problem. However, the SSR framework for DoA estimation gives
rise to the grid mismatch problem, when the unknown targets (sources) are not
matched with the estimation grid chosen for the construction of the array
steering matrix at the receiver. The block sparse recovery framework is known
to mitigate the grid mismatch problem by jointly estimating the targets and
their corresponding offsets from the estimation grid using the group lasso
estimator. The corresponding detection problem reduces to estimating the
optimal regularization parameter ($\tau$) of the lasso (in case of perfect
grid-matching) or group-lasso estimation problem for achieving the required
probability of correct detection ($P_c$). We propose asymptotic and finite
sample test statistics for detecting the number of sources with the required
$P_c$ at moderate to high signal to noise ratios. Once the number of sources
are detected, or equivalently the optimal $\hat{\tau}$ is estimated, the
corresponding estimation and grid matching of the DoAs can be performed by
solving the lasso or group-lasso problem at $\hat{\tau}$
",0,0,0,1,0,0
15488,Foreign English Accent Adjustment by Learning Phonetic Patterns,"  State-of-the-art automatic speech recognition (ASR) systems struggle with the
lack of data for rare accents. For sufficiently large datasets, neural engines
tend to outshine statistical models in most natural language processing
problems. However, a speech accent remains a challenge for both approaches.
Phonologists manually create general rules describing a speaker's accent, but
their results remain underutilized. In this paper, we propose a model that
automatically retrieves phonological generalizations from a small dataset. This
method leverages the difference in pronunciation between a particular dialect
and General American English (GAE) and creates new accented samples of words.
The proposed model is able to learn all generalizations that previously were
manually obtained by phonologists. We use this statistical method to generate a
million phonological variations of words from the CMU Pronouncing Dictionary
and train a sequence-to-sequence RNN to recognize accented words with 59%
accuracy.
",1,0,0,1,0,0
20046,Non-linear motor control by local learning in spiking neural networks,"  Learning weights in a spiking neural network with hidden neurons, using
local, stable and online rules, to control non-linear body dynamics is an open
problem. Here, we employ a supervised scheme, Feedback-based Online Local
Learning Of Weights (FOLLOW), to train a network of heterogeneous spiking
neurons with hidden layers, to control a two-link arm so as to reproduce a
desired state trajectory. The network first learns an inverse model of the
non-linear dynamics, i.e. from state trajectory as input to the network, it
learns to infer the continuous-time command that produced the trajectory.
Connection weights are adjusted via a local plasticity rule that involves
pre-synaptic firing and post-synaptic feedback of the error in the inferred
command. We choose a network architecture, termed differential feedforward,
that gives the lowest test error from different feedforward and recurrent
architectures. The learned inverse model is then used to generate a
continuous-time motor command to control the arm, given a desired trajectory.
",1,0,0,1,0,0
1234,Instantons and Fluctuations in a Lagrangian Model of Turbulence,"  We perform a detailed analytical study of the Recent Fluid Deformation (RFD)
model for the onset of Lagrangian intermittency, within the context of the
Martin-Siggia-Rose-Janssen-de Dominicis (MSRJD) path integral formalism. The
model is based, as a key point, upon local closures for the pressure Hessian
and the viscous dissipation terms in the stochastic dynamical equations for the
velocity gradient tensor. We carry out a power counting hierarchical
classification of the several perturbative contributions associated to
fluctuations around the instanton-evaluated MSRJD action, along the lines of
the cumulant expansion. The most relevant Feynman diagrams are then integrated
out into the renormalized effective action, for the computation of velocity
gradient probability distribution functions (vgPDFs). While the subleading
perturbative corrections do not affect the global shape of the vgPDFs in an
appreciable qualitative way, it turns out that they have a significant role in
the accurate description of their non-Gaussian cores.
",0,1,0,0,0,0
6949,A Multiobjective Approach to Multimicrogrid System Design,"  The main goal of this paper is to design a market operator (MO) and a
distribution network operator (DNO) for a network of microgrids in
consideration of multiple objectives. This is a high-level design and only
those microgrids with nondispatchable renewable energy sources are considered.
For a power grid in the network, the net value derived from providing power to
the network must be maximized. For a microgrid, it is desirable to maximize the
net gain derived from consuming the received power. Finally, for an independent
system operator, stored energy levels at microgrids must be maintained as close
as possible to storage capacity to secure network emergency operation. To
achieve these objectives, a multiobjective approach is proposed. The price
signal generated by the MO and power distributed by the DNO are assigned based
on a Pareto optimal solution of a multiobjective optimization problem. By using
the proposed approach, a fair scheme that does not advantage one particular
objective can be attained. Simulations are provided to validate the proposed
methodology.
",1,0,0,0,0,0
13638,YouTube-8M Video Understanding Challenge Approach and Applications,"  This paper introduces the YouTube-8M Video Understanding Challenge hosted as
a Kaggle competition and also describes my approach to experimenting with
various models. For each of my experiments, I provide the score result as well
as possible improvements to be made. Towards the end of the paper, I discuss
the various ensemble learning techniques that I applied on the dataset which
significantly boosted my overall competition score. At last, I discuss the
exciting future of video understanding research and also the many applications
that such research could significantly improve.
",0,0,0,1,0,0
9719,Modelling and characterization of a pneumatically actuated peristaltic micropump,"  There is an emerging class of microfluidic bioreactors which possess
long-term, closed circuit perfusion under sterile conditions with in vivo-like
flow parameters. Integrated into microfluidics, peristaltic-like pneumatically
actuated displacement micropumps are able to meet these requirements. We
present both a theoretical and experimental characterization of such pumps. In
order to examine volume flow rate, we have developed a mathemati- cal model
describing membrane motion under external pressure. The viscoelasticity of the
membrane and hydrodynamic resistance of the microfluidic channel have been
taken into account. Unlike other models, the developed model includes only the
physical parameters of the pump and allows the estimation of their impact on
the resulting flow. The model has been validated experimentally.
",0,1,0,0,0,0
4975,A theoretical analysis of extending frequency-bin entanglement from photon-photon to atom-photon hybrid systems,"  Inspired by the recent developments in the research of atom-photon quantum
interface and energy-time entanglement between single photon pulses, we propose
to establish the concept of a special energy-time entanglement between a single
photon pulse and internal states of a single atom, which is analogous to the
frequency-bin entanglement between single photon pulses. We show that this type
of entanglement arises naturally in the interaction between frequency-bin
entangled single photon pulse pair and a single atom, via straightforward
atom-photon phase gate operations. We also discuss the properties of this type
of entanglement and show a preliminary example of its potential application in
quantum networking. Moreover, a quantum entanglement witness is constructed to
detect such entanglement from a reasonably large set of separable states.
",0,1,0,0,0,0
12988,Convolutional Graph Auto-encoder: A Deep Generative Neural Architecture for Probabilistic Spatio-temporal Solar Irradiance Forecasting,"  Machine Learning on graph-structured data is an important and omnipresent
task for a vast variety of applications including anomaly detection and dynamic
network analysis. In this paper, a deep generative model is introduced to
capture continuous probability densities corresponding to the nodes of an
arbitrary graph. In contrast to all learning formulations in the area of
discriminative pattern recognition, we propose a scalable generative
optimization/algorithm theoretically proved to capture distributions at the
nodes of a graph. Our model is able to generate samples from the probability
densities learned at each node. This probabilistic data generation model, i.e.
convolutional graph auto-encoder (CGAE), is devised based on the localized
first-order approximation of spectral graph convolutions, deep learning, and
the variational Bayesian inference. We apply our CGAE to a new problem, the
spatio-temporal probabilistic solar irradiance prediction. Multiple solar
radiation measurement sites in a wide area in northern states of the US are
modeled as an undirected graph. Using our proposed model, the distribution of
future irradiance given historical radiation observations is estimated for
every site/node. Numerical results on the National Solar Radiation Database
show state-of-the-art performance for probabilistic radiation prediction on
geographically distributed irradiance data in terms of reliability, sharpness,
and continuous ranked probability score.
",0,0,0,1,0,0
7221,Simultaneous smoothness and simultaneous stability of a $C^\infty$ strictly convex integrand and its dual,"  In this paper, we investigate simultaneous properties of a convex integrand
$\gamma$ and its dual $\delta$. The main results are the following three.
(1) For a $C^\infty$ convex integrand $\gamma: S^n\to \mathbb{R}_+$, its dual
convex integrand $\delta: S^n\to \mathbb{R}_+$ is of class $C^\infty$ if and
only if $\gamma$ is a strictly convex integrand.
(2) Let $\gamma: S^n\to \mathbb{R}_+$ be a $C^\infty$ strictly convex
integrand. Then, $\gamma$ is stable if and only if its dual convex integrand
$\delta: S^n\to \mathbb{R}_+$ is stable.
(3) Let $\gamma: S^n\to \mathbb{R}_+$ be a $C^\infty$ strictly convex
integrand. Suppose that $\gamma$ is stable. Then, for any $i$ $(0\le i\le n)$,
a point $\theta_0\in S^n$ is a non-degenerate critical point of $\gamma$ with
Morse index $i$ if and only if its antipodal point $-\theta_0\in S^n$ is a
non-degenerate critical point of the dual convex integrand $\delta$ with Morse
index $(n-i)$.
",0,0,1,0,0,0
6307,Private Information Retrieval from MDS Coded Data with Colluding Servers: Settling a Conjecture by Freij-Hollanti et al.,"  A $(K, N, T, K_c)$ instance of the MDS-TPIR problem is comprised of $K$
messages and $N$ distributed servers. Each message is separately encoded
through a $(K_c, N)$ MDS storage code. A user wishes to retrieve one message,
as efficiently as possible, while revealing no information about the desired
message index to any colluding set of up to $T$ servers. The fundamental limit
on the efficiency of retrieval, i.e., the capacity of MDS-TPIR is known only at
the extremes where either $T$ or $K_c$ belongs to $\{1,N\}$. The focus of this
work is a recent conjecture by Freij-Hollanti, Gnilke, Hollanti and Karpuk
which offers a general capacity expression for MDS-TPIR. We prove that the
conjecture is false by presenting as a counterexample a PIR scheme for the
setting $(K, N, T, K_c) = (2,4,2,2)$, which achieves the rate $3/5$, exceeding
the conjectured capacity, $4/7$. Insights from the counterexample lead us to
capacity characterizations for various instances of MDS-TPIR including all
cases with $(K, N, T, K_c) = (2,N,T,N-1)$, where $N$ and $T$ can be arbitrary.
",1,0,0,0,0,0
13098,SIFM: A network architecture for seamless flow mobility between LTE and WiFi networks - Analysis and Testbed Implementation,"  This paper deals with cellular (e.g. LTE) networks that selectively offload
the mobile data traffic onto WiFi (IEEE 802.11) networks to improve network
performance. We propose the Seamless Internetwork Flow Mobility (SIFM)
architecture that provides seamless flow-mobility support using concepts of
Software Defined Networking (SDN). The SDN paradigm decouples the control and
data plane, leading to a centralized network intelligence and state. The SIFM
architecture utilizes this aspect of SDN and moves the mobility decisions to a
centralized Flow Controller (FC). This provides a global network view while
making mobility decisions and also reduces the complexity at the PGW. We
implement and evaluate both basic PMIPv6 and the SIFM architectures by
incorporating salient LTE and WiFi network features in the ns-3 simulator.
Performance experiments validate that seamless mobility is achieved. Also, the
SIFM architecture shows an improved network performance when compared to the
base PMIPv6 architecture. A proof-of-concept prototype of the SIFM architecture
has been implemented on an experimental testbed. The LTE network is emulated by
integrating USRP B210x with the OpenLTE eNodeB and OpenLTE EPC. The WiFi
network is emulated using hostapd and dnsmasq daemons running on Ubuntu 12.04.
An off-the-shelf LG G2 mobile phone running Android 4.2.2 is used as the user
equipment. We demonstrate seamless mobility between the LTE network and the
WiFi network with the help of ICMP ping and a TCP chat application.
",1,0,0,0,0,0
1333,Prospects for gravitational wave astronomy with next generation large-scale pulsar timing arrays,"  Next generation radio telescopes, namely the Five-hundred-meter Aperture
Spherical Telescope (FAST) and the Square Kilometer Array (SKA), will
revolutionize the pulsar timing arrays (PTAs) based gravitational wave (GW)
searches. We review some of the characteristics of FAST and SKA, and the
resulting PTAs, that are pertinent to the detection of gravitational wave
signals from individual supermassive black hole binaries.
",0,1,0,0,0,0
18597,Bayesian Boolean Matrix Factorisation,"  Boolean matrix factorisation aims to decompose a binary data matrix into an
approximate Boolean product of two low rank, binary matrices: one containing
meaningful patterns, the other quantifying how the observations can be
expressed as a combination of these patterns. We introduce the OrMachine, a
probabilistic generative model for Boolean matrix factorisation and derive a
Metropolised Gibbs sampler that facilitates efficient parallel posterior
inference. On real world and simulated data, our method outperforms all
currently existing approaches for Boolean matrix factorisation and completion.
This is the first method to provide full posterior inference for Boolean Matrix
factorisation which is relevant in applications, e.g. for controlling false
positive rates in collaborative filtering and, crucially, improves the
interpretability of the inferred patterns. The proposed algorithm scales to
large datasets as we demonstrate by analysing single cell gene expression data
in 1.3 million mouse brain cells across 11 thousand genes on commodity
hardware.
",1,0,0,1,0,0
15680,Templated ligation can create a hypercycle replication network,"  The stability of sequence replication was crucial for the emergence of
molecular evolution and early life. Exponential replication with a first-order
growth dynamics show inherent instabilities such as the error catastrophe and
the dominance by the fastest replicators. This favors less structured and short
sequences. The theoretical concept of hypercycles has been proposed to solve
these problems. Their higher-order growth kinetics leads to frequency-dependent
selection and stabilizes the replication of majority molecules. However, many
implementations of hypercycles are unstable or require special sequences with
catalytic activity. Here, we demonstrate the spontaneous emergence of
higher-order cooperative replication from a network of simple ligation chain
reactions (LCR). We performed long-term LCR experiments from a mixture of
sequences under molecule degrading conditions with a ligase protein. At the
chosen temperature cycling, a network of positive feedback loops arose from
both the cooperative ligation of matching sequences and the emerging increase
in sequence length. It generated higher-order replication with
frequency-dependent selection. The experiments matched a complete simulation
using experimentally determined ligation rates and the hypercycle mechanism was
also confirmed by abstracted modeling. Since templated ligation is a most basic
reaction of oligonucleotides, the described mechanism could have been
implemented under microthermal convection on early Earth.
",0,0,0,0,1,0
2963,IDK Cascades: Fast Deep Learning by Learning not to Overthink,"  Advances in deep learning have led to substantial increases in prediction
accuracy but have been accompanied by increases in the cost of rendering
predictions. We conjecture that fora majority of real-world inputs, the recent
advances in deep learning have created models that effectively ""overthink"" on
simple inputs. In this paper, we revisit the classic question of building model
cascades that primarily leverage class asymmetry to reduce cost. We introduce
the ""I Don't Know""(IDK) prediction cascades framework, a general framework to
systematically compose a set of pre-trained models to accelerate inference
without a loss in prediction accuracy. We propose two search based methods for
constructing cascades as well as a new cost-aware objective within this
framework. The proposed IDK cascade framework can be easily adopted in the
existing model serving systems without additional model re-training. We
evaluate the proposed techniques on a range of benchmarks to demonstrate the
effectiveness of the proposed framework.
",1,0,0,0,0,0
11087,Coverage Analysis in Millimeter Wave Cellular Networks with Reflections,"  The coverage probability of a user in a mmwave system depends on the
availability of line-of-sight paths or reflected paths from any base station.
Many prior works modelled blockages using random shape theory and analyzed the
SIR distribution with and without interference. While, it is intuitive that the
reflected paths do not significantly contribute to the coverage (because of
longer path lengths), there are no works which provide a model and study the
coverage with reflections. In this paper, we model and analyze the impact of
reflectors using stochastic geometry. We observe that the reflectors have very
little impact on the coverage probability.
",1,0,0,1,0,0
2336,Fractal curves from prime trigonometric series,"  We study the convergence of the parameter family of series
$$V_{\alpha,\beta}(t)=\sum_{p}p^{-\alpha}\exp(2\pi i p^{\beta}t),\quad
\alpha,\beta \in \mathbb{R}_{>0},\; t \in [0,1)$$ defined over prime numbers
$p$, and subsequently, their differentiability properties. The visible fractal
nature of the graphs as a function of $\alpha,\beta$ is analyzed in terms of
Hölder continuity, self similarity and fractal dimension, backed with
numerical results. We also discuss the link of this series to random walks and
consequently, explore numerically its random properties.
",0,0,1,0,0,0
10941,A Diversified Multi-Start Algorithm for Unconstrained Binary Quadratic Problems Leveraging the Graphics Processor Unit,"  Multi-start algorithms are a common and effective tool for metaheuristic
searches. In this paper we amplify multi-start capabilities by employing the
parallel processing power of the graphics processer unit (GPU) to quickly
generate a diverse starting set of solutions for the Unconstrained Binary
Quadratic Optimization Problem which are evaluated and used to implement
screening methods to select solutions for further optimization. This method is
implemented as an initial high quality solution generation phase prior to a
secondary steepest ascent search and a comparison of results to best known
approaches on benchmark unconstrained binary quadratic problems demonstrates
that GPU-enabled diversified multi-start with screening quickly yields very
good results.
",1,0,0,0,0,0
6437,A unified theory of adaptive stochastic gradient descent as Bayesian filtering,"  We formulate stochastic gradient descent (SGD) as a Bayesian filtering
problem. Inference in the Bayesian setting naturally gives rise to BRMSprop and
BAdam: Bayesian variants of RMSprop and Adam. Remarkably, the Bayesian approach
recovers many features of state-of-the-art adaptive SGD methods, including
amoungst others root-mean-square normalization, Nesterov acceleration and
AdamW. As such, the Bayesian approach provides one explanation for the
empirical effectiveness of state-of-the-art adaptive SGD algorithms.
Empirically comparing BRMSprop and BAdam with naive RMSprop and Adam on MNIST,
we find that Bayesian methods have the potential to considerably reduce test
loss and classification error.
",0,0,0,1,0,0
20493,Using Human Brain Activity to Guide Machine Learning,"  Machine learning is a field of computer science that builds algorithms that
learn. In many cases, machine learning algorithms are used to recreate a human
ability like adding a caption to a photo, driving a car, or playing a game.
While the human brain has long served as a source of inspiration for machine
learning, little effort has been made to directly use data collected from
working brains as a guide for machine learning algorithms. Here we demonstrate
a new paradigm of ""neurally-weighted"" machine learning, which takes fMRI
measurements of human brain activity from subjects viewing images, and infuses
these data into the training process of an object recognition learning
algorithm to make it more consistent with the human brain. After training,
these neurally-weighted classifiers are able to classify images without
requiring any additional neural data. We show that our neural-weighting
approach can lead to large performance gains when used with traditional machine
vision features, as well as to significant improvements with already
high-performing convolutional neural network features. The effectiveness of
this approach points to a path forward for a new class of hybrid machine
learning algorithms which take both inspiration and direct constraints from
neuronal data.
",1,0,0,0,0,0
4060,Enumeration of complementary-dual cyclic $\mathbb{F}_{q}$-linear $\mathbb{F}_{q^t}$-codes,"  Let $\mathbb{F}_q$ denote the finite field of order $q,$ $n$ be a positive
integer coprime to $q$ and $t \geq 2$ be an integer. In this paper, we
enumerate all the complementary-dual cyclic $\mathbb{F}_q$-linear
$\mathbb{F}_{q^t}$-codes of length $n$ by placing $\ast$, ordinary and
Hermitian trace bilinear forms on $\mathbb{F}_{q^t}^n.$
",0,0,1,0,0,0
4606,The Effects of Protostellar Disk Turbulence on CO Emission Lines: A Comparison Study of Disks with Constant CO Abundance vs. Chemically Evolving Disks,"  Turbulence is the leading candidate for angular momentum transport in
protoplanetary disks and therefore influences disk lifetimes and planet
formation timescales. However, the turbulent properties of protoplanetary disks
are poorly constrained observationally. Recent studies have found turbulent
speeds smaller than what fully-developed MRI would produce (Flaherty et al.
2015, 2017). However, existing studies assumed a constant CO/H2 ratio of 0.0001
in locations where CO is not frozen-out or photo-dissociated. Our previous
studies of evolving disk chemistry indicate that CO is depleted by
incorporation into complex organic molecules well inside the freeze-out radius
of CO. We consider the effects of this chemical depletion on measurements of
turbulence. Simon et al. (2015) suggested that the ratio of the peak line flux
to the flux at line center of the CO J=3-2 transition is a reasonable
diagnostic of turbulence, so we focus on that metric, while adding some
analysis of the more complex effects on spatial distribution. We simulate the
emission lines of CO based on chemical evolution models presented in Yu et al.
(2016), and find that the peak-to-trough ratio changes as a function of time as
CO is destroyed. Specifically, a CO-depleted disk with high turbulent velocity
mimics the peak-to-trough ratios of a non-CO-depleted disk with lower turbulent
velocity. We suggest that disk observers and modelers take into account the
possibility of CO depletion when using line peak-to-trough ratios to constrain
the degree of turbulence in disks. Assuming that CO/H2 = 0.0001 at all disk
radii can lead to underestimates of turbulent speeds in the disk by at least
0.2 km/s.
",0,1,0,0,0,0
6752,A Debris Backwards Flow Simulation System for Malaysia Airlines Flight 370,"  This paper presents a system based on a Two-Way Particle-Tracking Model to
analyze possible crash positions of flight MH370. The particle simulator
includes a simple flow simulation of the debris based on a Lagrangian approach
and a module to extract appropriated ocean current data from netCDF files. The
influence of wind, waves, immersion depth and hydrodynamic behavior are not
considered in the simulation.
",1,1,0,0,0,0
11546,Effect of Anodizing Parameters on Corrosion Resistance of Coated Purified Magnesium,"  Magnesium and its alloys are being considered for biodegradable biomaterials.
However, high and uncontrollable corrosion rates have limited the use of
magnesium and its alloys in biological environments. In this research, high
purified magnesium (HP-Mg) was coated with stearic acid in order to improve the
corrosion resistance of magnesium. Anodization and immersion in stearic acid
were used to form a hydrophobic layer on magnesium substrate. Different DC
voltages, times, electrolytes, and temperatures were tested. Electrochemical
impedance spectroscopy and potentiodynamic polarization were used to measure
the corrosion rates of the coated HP-Mg. The results showed that optimum
corrosion resistance occurred for specimens anodized at +4 volts for 4 minutes
at 70°C in borate benzoate. The corrosion resistance was temporarily
enhanced by 1000x.
",0,1,0,0,0,0
15106,An FPT algorithm for planar multicuts with sources and sinks on the outer face,"  Given a list of k source-sink pairs in an edge-weighted graph G, the minimum
multicut problem consists in selecting a set of edges of minimum total weight
in G, such that removing these edges leaves no path from each source to its
corresponding sink. To the best of our knowledge, no non-trivial FPT result for
special cases of this problem, which is APX-hard in general graphs for any
fixed k>2, is known with respect to k only. When the graph G is planar, this
problem is known to be polynomial-time solvable if k=O(1), but cannot be FPT
with respect to k under the Exponential Time Hypothesis.
In this paper, we show that, if G is planar and in addition all sources and
sinks lie on the outer face, then this problem does admit an FPT algorithm when
parameterized by k (although it remains APX-hard when k is part of the input,
even in stars). To do this, we provide a new characterization of optimal
solutions in this case, and then use it to design a ""divide-and-conquer""
approach: namely, some edges that are part of any such solution actually define
an optimal solution for a polynomial-time solvable multiterminal variant of the
problem on some of the sources and sinks (which can be identified thanks to a
reduced enumeration phase). Removing these edges from the graph cuts it into
several smaller instances, which can then be solved recursively.
",1,0,0,0,0,0
16832,Diophantine approximation by special primes,"  We show that whenever $\delta>0$, $\eta$ is real and constants $\lambda_i$
satisfy some necessary conditions, there are infinitely many prime triples
$p_1,\, p_2,\, p_3$ satisfying the inequality $|\lambda_1p_1 + \lambda_2p_2 +
\lambda_3p_3+\eta|<(\max p_j)^{-1/12+\delta}$ and such that, for each
$i\in\{1,2,3\}$, $p_i+2$ has at most $28$ prime factors.
",0,0,1,0,0,0
16357,Self-consistent DFT+U method for real-space time-dependent density functional theory calculations,"  We implemented various DFT+U schemes, including the ACBN0 self-consistent
density-functional version of the DFT+U method [Phys. Rev. X 5, 011006 (2015)]
within the massively parallel real-space time-dependent density functional
theory (TDDFT) code Octopus. We further extended the method to the case of the
calculation of response functions with real-time TDDFT+U and to the description
of non-collinear spin systems. The implementation is tested by investigating
the ground-state and optical properties of various transition metal oxides,
bulk topological insulators, and molecules. Our results are found to be in good
agreement with previously published results for both the electronic band
structure and structural properties. The self consistent calculated values of U
and J are also in good agreement with the values commonly used in the
literature. We found that the time-dependent extension of the self-consistent
DFT+U method yields improved optical properties when compared to the empirical
TDDFT+U scheme. This work thus opens a different theoretical framework to
address the non equilibrium properties of correlated systems.
",0,1,0,0,0,0
555,State-dependent Priority Scheduling for Networked Control Systems,"  Networked control systems (NCS) have attracted considerable attention in
recent years. While the stabilizability and optimal control of NCS for a given
communication system has already been studied extensively, the design of the
communication system for NCS has recently seen an increase in more thorough
investigation. In this paper, we address an optimal scheduling problem for a
set of NCS sharing a dedicated communication channel, providing performance
bounds and asymptotic stability. We derive a suboptimal scheduling policy with
dynamic state-based priorities calculated at the sensors, which are then used
for stateless priority queuing in the network, making it both scalable and
efficient to implement on routers or multi-layer switches. These properties are
beneficial towards leveraging existing IP networks for control, which will be a
crucial factor for the proliferation of wide-area NCS applications. By allowing
for an arbitrary number of concurrent transmissions, we are able to investigate
the relationship between available bandwidth, transmission rate, and delay. To
demonstrate the feasibility of our approach, we provide a proof-of-concept
implementation of the priority scheduler using real networking hardware.
",1,0,0,0,0,0
14559,A Multimodal Corpus of Expert Gaze and Behavior during Phonetic Segmentation Tasks,"  Phonetic segmentation is the process of splitting speech into distinct
phonetic units. Human experts routinely perform this task manually by analyzing
auditory and visual cues using analysis software, which is an extremely
time-consuming process. Methods exist for automatic segmentation, but these are
not always accurate enough. In order to improve automatic segmentation, we need
to model it as close to the manual segmentation as possible. This corpus is an
effort to capture the human segmentation behavior by recording experts
performing a segmentation task. We believe that this data will enable us to
highlight the important aspects of manual segmentation, which can be used in
automatic segmentation to improve its accuracy.
",1,0,0,0,0,0
15678,Deep Convolutional Neural Network Inference with Floating-point Weights and Fixed-point Activations,"  Deep convolutional neural network (CNN) inference requires significant amount
of memory and computation, which limits its deployment on embedded devices. To
alleviate these problems to some extent, prior research utilize low precision
fixed-point numbers to represent the CNN weights and activations. However, the
minimum required data precision of fixed-point weights varies across different
networks and also across different layers of the same network. In this work, we
propose using floating-point numbers for representing the weights and
fixed-point numbers for representing the activations. We show that using
floating-point representation for weights is more efficient than fixed-point
representation for the same bit-width and demonstrate it on popular large-scale
CNNs such as AlexNet, SqueezeNet, GoogLeNet and VGG-16. We also show that such
a representation scheme enables compact hardware multiply-and-accumulate (MAC)
unit design. Experimental results show that the proposed scheme reduces the
weight storage by up to 36% and power consumption of the hardware multiplier by
up to 50%.
",1,0,0,0,0,0
8762,An Investigation of Newton-Sketch and Subsampled Newton Methods,"  The concepts of sketching and subsampling have recently received much
attention by the optimization and statistics communities. In this paper, we
study Newton-Sketch and Subsampled Newton (SSN) methods for the finite-sum
optimization problem. We consider practical versions of the two methods in
which the Newton equations are solved approximately using the conjugate
gradient (CG) method or a stochastic gradient iteration. We establish new
complexity results for the SSN-CG method that exploit the spectral properties
of CG. Controlled numerical experiments compare the relative strengths of
Newton-Sketch and SSN methods and show that for many finite-sum problems, they
are far more efficient than SVRG, a popular first-order method.
",1,0,1,1,0,0
20842,Supervised Learning of Labeled Pointcloud Differences via Cover-Tree Entropy Reduction,"  We introduce a new algorithm, called CDER, for supervised machine learning
that merges the multi-scale geometric properties of Cover Trees with the
information-theoretic properties of entropy. CDER applies to a training set of
labeled pointclouds embedded in a common Euclidean space. If typical
pointclouds corresponding to distinct labels tend to differ at any scale in any
sub-region, CDER can identify these differences in (typically) linear time,
creating a set of distributional coordinates which act as a feature extraction
mechanism for supervised learning. We describe theoretical properties and
implementation details of CDER, and illustrate its benefits on several
synthetic examples.
",1,0,0,1,0,0
8260,Learning across scales - A multiscale method for Convolution Neural Networks,"  In this work we establish the relation between optimal control and training
deep Convolution Neural Networks (CNNs). We show that the forward propagation
in CNNs can be interpreted as a time-dependent nonlinear differential equation
and learning as controlling the parameters of the differential equation such
that the network approximates the data-label relation for given training data.
Using this continuous interpretation we derive two new methods to scale CNNs
with respect to two different dimensions. The first class of multiscale methods
connects low-resolution and high-resolution data through prolongation and
restriction of CNN parameters. We demonstrate that this enables classifying
high-resolution images using CNNs trained with low-resolution images and vice
versa and warm-starting the learning process. The second class of multiscale
methods connects shallow and deep networks and leads to new training strategies
that gradually increase the depths of the CNN while re-using parameters for
initializations.
",1,0,0,0,0,0
1270,Fast-neutron and gamma-ray imaging with a capillary liquid xenon converter coupled to a gaseous photomultiplier,"  Gamma-ray and fast-neutron imaging was performed with a novel liquid xenon
(LXe) scintillation detector read out by a Gaseous Photomultiplier (GPM). The
100 mm diameter detector prototype comprised a capillary-filled LXe
converter/scintillator, coupled to a triple-THGEM imaging-GPM, with its first
electrode coated by a CsI UV-photocathode, operated in Ne/5%CH4 cryogenic
temperatures. Radiation localization in 2D was derived from
scintillation-induced photoelectron avalanches, measured on the GPM's segmented
anode. The localization properties of Co-60 gamma-rays and a mixed
fast-neutron/gamma-ray field from an AmBe neutron source were derived from
irradiation of a Pb edge absorber. Spatial resolutions of 12+/-2 mm and 10+/-2
mm (FWHM) were reached with Co-60 and AmBe sources, respectively. The
experimental results are in good agreement with GEANT4 simulations. The
calculated ultimate expected resolutions for our application-relevant 4.4 and
15.1 MeV gamma-rays and 1-15 MeV neutrons are 2-4 mm and ~2 mm (FWHM),
respectively. These results indicate the potential applicability of the new
detector concept to Fast-Neutron Resonance Radiography (FNRR) and
Dual-Discrete-Energy Gamma Radiography (DDEGR) of large objects.
",0,1,0,0,0,0
16126,Hybrid integration of solid-state quantum emitters on a silicon photonic chip,"  Scalable quantum photonic systems require efficient single photon sources
coupled to integrated photonic devices. Solid-state quantum emitters can
generate single photons with high efficiency, while silicon photonic circuits
can manipulate them in an integrated device structure. Combining these two
material platforms could, therefore, significantly increase the complexity of
integrated quantum photonic devices. Here, we demonstrate hybrid integration of
solid-state quantum emitters to a silicon photonic device. We develop a
pick-and-place technique that can position epitaxially grown InAs/InP quantum
dots emitting at telecom wavelengths on a silicon photonic chip
deterministically with nanoscale precision. We employ an adiabatic tapering
approach to transfer the emission from the quantum dots to the waveguide with
high efficiency. We also incorporate an on-chip silicon-photonic beamsplitter
to perform a Hanbury-Brown and Twiss measurement. Our approach could enable
integration of pre-characterized III-V quantum photonic devices into
large-scale photonic structures to enable complex devices composed of many
emitters and photons.
",0,1,0,0,0,0
12062,Degrees of Freedom in Cached MIMO Relay Networks With Multiple Base Stations,"  The ability of physical layer relay caching to increase the degrees of
freedom (DoF) of a single cell was recently illustrated. In this paper, we
extend this result to the case of multiple cells in which a caching relay is
shared among multiple non-cooperative base stations (BSs). In particular, we
show that a large DoF gain can be achieved by exploiting the benefits of having
a shared relay that cooperates with the BSs. We first propose a cache-assisted
relaying protocol that improves the cooperation opportunity between the BSs and
the relay. Next, we consider the cache content placement problem that aims to
design the cache content at the relay such that the DoF gain is maximized. We
propose an optimal algorithm and a near-optimal low-complexity algorithm for
the cache content placement problem. Simulation results show significant
improvement in the DoF gain using the proposed relay-caching protocol.
",1,0,0,0,0,0
18228,The light pollution as a surrogate for urban population of the US cities,"  We show that the definition of the city boundaries can have a dramatic
influence on the scaling behavior of the night-time light (NTL) as a function
of population (POP) in the US. Precisely, our results show that the arbitrary
geopolitical definition based on the Metropolitan/Consolidated Metropolitan
Statistical Areas (MSA/CMSA) leads to a sublinear power-law growth of NTL with
POP. On the other hand, when cities are defined according to a more natural
agglomeration criteria, namely, the City Clustering Algorithm (CCA), an
isometric relation emerges between NTL and population. This discrepancy is
compatible with results from previous works showing that the scaling behaviors
of various urban indicators with population can be substantially different for
distinct definitions of city boundaries. Moreover, considering the CCA
definition as more adequate than the MSA/CMSA one because the former does not
violate the expected extensivity between land population and area of their
generated clusters, we conclude that, without loss of generality, the CCA
measures of light pollution and population could be interchangeably utilized in
future studies.
",0,1,0,0,0,0
9232,On the generalization of Erdős-Vincze's theorem about the approximation of closed convex plane curves by polyellipses,"  A polyellipse is a curve in the Euclidean plane all of whose points have the
same sum of distances from finitely many given points (focuses). The classical
version of Erdős-Vincze's theorem states that regular triangles can not be
presented as the Hausdorff limit of polyellipses even if the number of the
focuses can be arbitrary large. In other words the topological closure of the
set of polyellipses with respect to the Hausdorff distance does not contain any
regular triangle and we have a negative answer to the problem posed by E.
Vázsonyi (Weissfeld) about the approximation of closed convex plane curves by
polyellipses. It is the additive version of the approximation of simple closed
plane curves by polynomial lemniscates all of whose points have the same
product of distances from finitely many given points (focuses). Here we are
going to generalize the classical version of Erdős-Vincze's theorem for
regular polygons in the plane. We will conclude that the error of the
approximation tends to zero as the number of the vertices of the regular
polygon tends to the infinity. The decreasing tendency of the approximation
error gives the idea to construct curves in the topological closure of the set
of polyellipses. If we use integration to compute the average distance of a
point from a given (focal) set in the plane then the curves all of whose points
have the same average distance from the focal set can be given as the Hausdorff
limit of polyellipses corresponding to partial sums.
",0,0,1,0,0,0
12993,A remark on the disorienting of species due to the fluctuating environment,"  In this article we study the stabilizing of a primitive pattern of behaviour
for the two-species community with chemotaxis due to the short-wavelength
external signal. We use a system of Patlak-Keller-Segel type as a model of the
community. It is well-known that such systems can produce complex unsteady
patterns of behaviour which are usually explained mathematically by
bifurcations of some basic solutions that describe simpler patterns. As far as
we aware, all such bifurcations in the models of the Patlak-Keller-Segel type
had been found for homogeneous (i.e. translationally invariant) systems where
the basic solutions are equilibria with homogeneous distributions of all
species. The model considered in the present paper does not possess the
translational invariance: one of species (the predators) is assumed to be
capable of moving in response to a signal produced externally in addition to
the signal emitted by another species (the prey). For instance, the external
signal may arise from the inhomogeneity of the distribution of an environmental
characteristic such as temperature, salinity, terrain relief, etc. Our goal is
to examine the effect of short-wavelength inhomogeneity. To do this, we employ
a certain homogenization procedure. We separate the short-wavelength and smooth
components of the system response and derive a slow system governing the latter
one. Analysing the slow system and comparing it with the case of homogeneous
environment shows that, generically, a short-wavelength inhomogeneity results
in an exponential decrease in the motility of the predators. The loss of
motility prevents, to a great extent, the occurrence of complex unsteady
patterns and dramatically stabilizes the primitive basic solution. In some
sense, the necessity of dealing with intensive small-scale changes of the
environment makes the system unable to respond to other challenges.
",0,0,0,0,1,0
18717,Adversarial Discriminative Sim-to-real Transfer of Visuo-motor Policies,"  Various approaches have been proposed to learn visuo-motor policies for
real-world robotic applications. One solution is first learning in simulation
then transferring to the real world. In the transfer, most existing approaches
need real-world images with labels. However, the labelling process is often
expensive or even impractical in many robotic applications. In this paper, we
propose an adversarial discriminative sim-to-real transfer approach to reduce
the cost of labelling real data. The effectiveness of the approach is
demonstrated with modular networks in a table-top object reaching task where a
7 DoF arm is controlled in velocity mode to reach a blue cuboid in clutter
through visual observations. The adversarial transfer approach reduced the
labelled real data requirement by 50%. Policies can be transferred to real
environments with only 93 labelled and 186 unlabelled real images. The
transferred visuo-motor policies are robust to novel (not seen in training)
objects in clutter and even a moving target, achieving a 97.8% success rate and
1.8 cm control accuracy.
",1,0,0,0,0,0
1125,"Neural-Network Quantum States, String-Bond States, and Chiral Topological States","  Neural-Network Quantum States have been recently introduced as an Ansatz for
describing the wave function of quantum many-body systems. We show that there
are strong connections between Neural-Network Quantum States in the form of
Restricted Boltzmann Machines and some classes of Tensor-Network states in
arbitrary dimensions. In particular we demonstrate that short-range Restricted
Boltzmann Machines are Entangled Plaquette States, while fully connected
Restricted Boltzmann Machines are String-Bond States with a nonlocal geometry
and low bond dimension. These results shed light on the underlying architecture
of Restricted Boltzmann Machines and their efficiency at representing many-body
quantum states. String-Bond States also provide a generic way of enhancing the
power of Neural-Network Quantum States and a natural generalization to systems
with larger local Hilbert space. We compare the advantages and drawbacks of
these different classes of states and present a method to combine them
together. This allows us to benefit from both the entanglement structure of
Tensor Networks and the efficiency of Neural-Network Quantum States into a
single Ansatz capable of targeting the wave function of strongly correlated
systems. While it remains a challenge to describe states with chiral
topological order using traditional Tensor Networks, we show that
Neural-Network Quantum States and their String-Bond States extension can
describe a lattice Fractional Quantum Hall state exactly. In addition, we
provide numerical evidence that Neural-Network Quantum States can approximate a
chiral spin liquid with better accuracy than Entangled Plaquette States and
local String-Bond States. Our results demonstrate the efficiency of neural
networks to describe complex quantum wave functions and pave the way towards
the use of String-Bond States as a tool in more traditional machine-learning
applications.
",0,1,0,1,0,0
5193,The meet operation in the imbalance lattice of maximal instantaneous codes: alternative proof of existence,"  An alternative proof is given of the existence of greatest lower bounds in
the imbalance order of binary maximal instantaneous codes of a given size.
These codes are viewed as maximal antichains of a given size in the infinite
binary tree of 0-1 words. The proof proposed makes use of a single balancing
operation instead of expansion and contraction as in the original proof of the
existence of glb.
",0,0,1,0,0,0
15606,Differentially Private Learning of Undirected Graphical Models using Collective Graphical Models,"  We investigate the problem of learning discrete, undirected graphical models
in a differentially private way. We show that the approach of releasing noisy
sufficient statistics using the Laplace mechanism achieves a good trade-off
between privacy, utility, and practicality. A naive learning algorithm that
uses the noisy sufficient statistics ""as is"" outperforms general-purpose
differentially private learning algorithms. However, it has three limitations:
it ignores knowledge about the data generating process, rests on uncertain
theoretical foundations, and exhibits certain pathologies. We develop a more
principled approach that applies the formalism of collective graphical models
to perform inference over the true sufficient statistics within an
expectation-maximization framework. We show that this learns better models than
competing approaches on both synthetic data and on real human mobility data
used as a case study.
",1,0,0,1,0,0
6652,A Strongly Consistent Finite Difference Scheme for Steady Stokes Flow and its Modified Equations,"  We construct and analyze a strongly consistent second-order finite difference
scheme for the steady two-dimensional Stokes flow. The pressure Poisson
equation is explicitly incorporated into the scheme. Our approach suggested by
the first two authors is based on a combination of the finite volume method,
difference elimination, and numerical integration. We make use of the
techniques of the differential and difference Janet/Groebner bases. In order to
prove strong consistency of the generated scheme we correlate the differential
ideal generated by the polynomials in the Stokes equations with the difference
ideal generated by the polynomials in the constructed difference scheme.
Additionally, we compute the modified differential system of the obtained
scheme and analyze the scheme's accuracy and strong consistency by considering
this system. An evaluation of our scheme against the established
marker-and-cell method is carried out.
",1,0,0,0,0,0
18481,New cardinality estimation algorithms for HyperLogLog sketches,"  This paper presents new methods to estimate the cardinalities of data sets
recorded by HyperLogLog sketches. A theoretically motivated extension to the
original estimator is presented that eliminates the bias for small and large
cardinalities. Based on the maximum likelihood principle a second unbiased
method is derived together with a robust and efficient numerical algorithm to
calculate the estimate. The maximum likelihood approach can also be applied to
more than a single HyperLogLog sketch. In particular, it is shown that it gives
more precise cardinality estimates for union, intersection, or relative
complements of two sets that are both represented by HyperLogLog sketches
compared to the conventional technique using the inclusion-exclusion principle.
All the new methods are demonstrated and verified by extensive simulations.
",1,0,0,0,0,0
5350,A Liouville Theorem for Mean Curvature Flow,"  Ancient solutions arise in the study of parabolic blow-ups. If we can
categorize ancient solutions, we can better understand blow-up limits. Based on
an argument of Giga and Kohn, we give a Liouville-type theorem restricting
ancient, type-I, non-collapsing two- dimensional mean curvature flows to either
spheres or cylinders.
",0,0,1,0,0,0
19940,Learning to Plan Chemical Syntheses,"  From medicines to materials, small organic molecules are indispensable for
human well-being. To plan their syntheses, chemists employ a problem solving
technique called retrosynthesis. In retrosynthesis, target molecules are
recursively transformed into increasingly simpler precursor compounds until a
set of readily available starting materials is obtained. Computer-aided
retrosynthesis would be a highly valuable tool, however, past approaches were
slow and provided results of unsatisfactory quality. Here, we employ Monte
Carlo Tree Search (MCTS) to efficiently discover retrosynthetic routes. MCTS
was combined with an expansion policy network that guides the search, and an
""in-scope"" filter network to pre-select the most promising retrosynthetic
steps. These deep neural networks were trained on 12 million reactions, which
represents essentially all reactions ever published in organic chemistry. Our
system solves almost twice as many molecules and is 30 times faster in
comparison to the traditional search method based on extracted rules and
hand-coded heuristics. Finally after a 60 year history of computer-aided
synthesis planning, chemists can no longer distinguish between routes generated
by a computer system and real routes taken from the scientific literature. We
anticipate that our method will accelerate drug and materials discovery by
assisting chemists to plan better syntheses faster, and by enabling fully
automated robot synthesis.
",1,1,0,0,0,0
14841,Superposition of p-superharmonic functions,"  The Dominative $p$-Laplace Operator is introduced. This operator is a
relative to the $p$-Laplacian, but with the distinguishing property of being
sublinear. It explains the superposition principle in the $p$-Laplace Equation.
",0,0,1,0,0,0
15827,"The Theory is Predictive, but is it Complete? An Application to Human Perception of Randomness","  When we test a theory using data, it is common to focus on correctness: do
the predictions of the theory match what we see in the data? But we also care
about completeness: how much of the predictable variation in the data is
captured by the theory? This question is difficult to answer, because in
general we do not know how much ""predictable variation"" there is in the
problem. In this paper, we consider approaches motivated by machine learning
algorithms as a means of constructing a benchmark for the best attainable level
of prediction.
We illustrate our methods on the task of predicting human-generated random
sequences. Relative to an atheoretical machine learning algorithm benchmark, we
find that existing behavioral models explain roughly 15 percent of the
predictable variation in this problem. This fraction is robust across several
variations on the problem. We also consider a version of this approach for
analyzing field data from domains in which human perception and generation of
randomness has been used as a conceptual framework; these include sequential
decision-making and repeated zero-sum games. In these domains, our framework
for testing the completeness of theories provides a way of assessing their
effectiveness over different contexts; we find that despite some differences,
the existing theories are fairly stable across our field domains in their
performance relative to the benchmark. Overall, our results indicate that (i)
there is a significant amount of structure in this problem that existing models
have yet to capture and (ii) there are rich domains in which machine learning
may provide a viable approach to testing completeness.
",1,0,0,1,0,0
13408,Modified mean curvature flow of entire locally Lipschitz radial graphs in hyperbolic space,"  In a previous joint work of Xiao and the second author, the modified mean
curvature flow (MMCF) in hyperbolic space $\mathbb{H}^{n+1}$: $$\frac{\partial
\mathbf{F}}{\partial t} = (H-\sigma)\,\vnu\,,\quad \quad \sigma\in (-n,n)$$ was
first introduced and the flow starting from an entire Lipschitz continuous
radial graph with uniform local ball condition on the asymptotic boundary was
shown to exist for all time and converge to a complete hypersurface of constant
mean curvature with prescribed asymptotic boundary at infinity. In this paper,
we remove the uniform local ball condition on the asymptotic boundary of the
initial hypersurface, and prove that the MMCF starting from an entire locally
Lipschitz continuous radial graph exists and stays radially graphic for all
time.
",0,0,1,0,0,0
17597,Semantically Enhanced Dynamic Bayesian Network for Detecting Sepsis Mortality Risk in ICU Patients with Infection,"  Although timely sepsis diagnosis and prompt interventions in Intensive Care
Unit (ICU) patients are associated with reduced mortality, early clinical
recognition is frequently impeded by non-specific signs of infection and
failure to detect signs of sepsis-induced organ dysfunction in a constellation
of dynamically changing physiological data. The goal of this work is to
identify patient at risk of life-threatening sepsis utilizing a data-centered
and machine learning-driven approach. We derive a mortality risk predictive
dynamic Bayesian network (DBN) guided by a customized sepsis knowledgebase and
compare the predictive accuracy of the derived DBN with the Sepsis-related
Organ Failure Assessment (SOFA) score, the Quick SOFA (qSOFA) score, the
Simplified Acute Physiological Score (SAPS-II) and the Modified Early Warning
Score (MEWS) tools.
A customized sepsis ontology was used to derive the DBN node structure and
semantically characterize temporal features derived from both structured
physiological data and unstructured clinical notes. We assessed the performance
in predicting mortality risk of the DBN predictive model and compared
performance to other models using Receiver Operating Characteristic (ROC)
curves, area under curve (AUROC), calibration curves, and risk distributions.
The derived dataset consists of 24,506 ICU stays from 19,623 patients with
evidence of suspected infection, with 2,829 patients deceased at discharge. The
DBN AUROC was found to be 0.91, which outperformed the SOFA (0.843), qSOFA
(0.66), MEWS (0.73), and SAPS-II (0.77) scoring tools. Continuous Net
Reclassification Index and Integrated Discrimination Improvement analysis
supported the superiority DBN. Compared with conventional rule-based risk
scoring tools, the sepsis knowledgebase-driven DBN algorithm offers improved
performance for predicting mortality of infected patients in ICUs.
",0,0,0,1,0,0
7298,A Nonlinear Dimensionality Reduction Framework Using Smooth Geodesics,"  Existing dimensionality reduction methods are adept at revealing hidden
underlying manifolds arising from high-dimensional data and thereby producing a
low-dimensional representation. However, the smoothness of the manifolds
produced by classic techniques over sparse and noisy data is not guaranteed. In
fact, the embedding generated using such data may distort the geometry of the
manifold and thereby produce an unfaithful embedding. Herein, we propose a
framework for nonlinear dimensionality reduction that generates a manifold in
terms of smooth geodesics that is designed to treat problems in which manifold
measurements are either sparse or corrupted by noise. Our method generates a
network structure for given high-dimensional data using a nearest neighbors
search and then produces piecewise linear shortest paths that are defined as
geodesics. Then, we fit points in each geodesic by a smoothing spline to
emphasize the smoothness. The robustness of this approach for sparse and noisy
datasets is demonstrated by the implementation of the method on synthetic and
real-world datasets.
",1,0,0,1,0,0
6367,Local Symmetry and Global Structure in Adaptive Voter Models,"  ""Coevolving"" or ""adaptive"" voter models (AVMs) are natural systems for
modeling the emergence of mesoscopic structure from local networked processes
driven by conflict and homophily. Because of this, many methods for
approximating the long-run behavior of AVMs have been proposed over the last
decade. However, most such methods are either restricted in scope, expensive in
computation, or inaccurate in predicting important statistics. In this work, we
develop a novel, second-order moment closure approximation method for studying
the equilibrium mesoscopic structure of AVMs and apply it to binary-state
rewire-to-random and rewire-to-same model variants with random state-switching.
This framework exploits an asymmetry in voting events that enables us to derive
analytic approximations for the fast-timescale dynamics. The resulting
numerical approximations enable the computation of key properties of the model
behavior, such as the location of the fragmentation transition and the
equilibrium active edge density, across the entire range of state densities.
Numerically, they are nearly exact for the rewire-to-random model, and
competitive with other current approaches for the rewire-to-same model. We
conclude with suggestions for model refinement and extensions to more complex
models.
",1,0,0,0,0,0
17748,Probing the possibility of hotspots on the central neutron star in HESS J1731-347,"  The X-ray spectra of the neutron stars located in the centers of supernova
remnants Cas A and HESS J1731-347 are well fit with carbon atmosphere models.
These fits yield plausible neutron star sizes for the known or estimated
distances to these supernova remnants. The evidence in favor of the presence of
a pure carbon envelope at the neutron star surface is rather indirect and is
based on the assumption that the emission is generated uniformly by the entire
stellar surface. Although this assumption is supported by the absence of
pulsations, the observational upper limit on the pulsed fraction is not very
stringent. In an attempt to quantify this evidence, we investigate the
possibility that the observed spectrum of the neutron star in HESS J1731-347 is
a combination of the spectra produced in a hydrogen atmosphere of the hotspots
and of the cooler remaining part of the neutron star surface. The lack of
pulsations in this case has to be explained either by a sufficiently small
angle between the neutron star spin axis and the line of sight, or by a
sufficiently small angular distance between the hotspots and the neutron star
rotation poles. As the observed flux from a non-uniformly emitting neutron star
depends on the angular distribution of the radiation emerging from the
atmosphere, we have computed two new grids of pure carbon and pure hydrogen
atmosphere model spectra accounting for Compton scattering. Using new hydrogen
models, we have evaluated the probability of a geometry that leads to a pulsed
fraction below the observed upper limit to be about 8.2 %. Such a geometry thus
seems to be rather improbable but cannot be excluded at this stage.
",0,1,0,0,0,0
7734,Generative Adversarial Network based Autoencoder: Application to fault detection problem for closed loop dynamical systems,"  Fault detection problem for closed loop uncertain dynamical systems, is
investigated in this paper, using different deep learning based methods.
Traditional classifier based method does not perform well, because of the
inherent difficulty of detecting system level faults for closed loop dynamical
system. Specifically, acting controller in any closed loop dynamical system,
works to reduce the effect of system level faults. A novel Generative
Adversarial based deep Autoencoder is designed to classify datasets under
normal and faulty operating conditions. This proposed network performs
significantly well when compared to any available classifier based methods, and
moreover, does not require labeled fault incorporated datasets for training
purpose. Finally, this aforementioned network's performance is tested on a high
complexity building energy system dataset.
",0,0,0,1,0,0
2405,SG1120-1202: Mass-Quenching as Tracked by UV Emission in the Group Environment at z=0.37,"  We use the Hubble Space Telescope to obtain WFC3/F390W imaging of the
supergroup SG1120-1202 at z=0.37, mapping the UV emission of 138
spectroscopically confirmed members. We measure total (F390W-F814W) colors and
visually classify the UV morphology of individual galaxies as ""clumpy"" or
""smooth."" Approximately 30% of the members have pockets of UV emission (clumpy)
and we identify for the first time in the group environment galaxies with UV
morphologies similar to the jellyfish galaxies observed in massive clusters. We
stack the clumpy UV members and measure a shallow internal color gradient,
which indicates unobscured star formation is occurring throughout these
galaxies. We also stack the four galaxy groups and measure a strong trend of
decreasing UV emission with decreasing projected group distance ($R_{proj}$).
We find that the strong correlation between decreasing UV emission and
increasing stellar mass can fully account for the observed trend in
(F390W-F814W) - $R_{proj}$, i.e., mass-quenching is the dominant mechanism for
extinguishing UV emission in group galaxies. Our extensive multi-wavelength
analysis of SG1120-1202 indicates that stellar mass is the primary predictor of
UV emission, but that the increasing fraction of massive (red/smooth) galaxies
at $R_{proj}$ < 2$R_{200}$ and existence of jellyfish candidates is due to the
group environment.
",0,1,0,0,0,0
19684,Turbulent Mass Inhomogeneities induced by a point-source,"  We describe how turbulence distributes tracers away from a localized source
of injection, and analyse how the spatial inhomogeneities of the concentration
field depend on the amount of randomness in the injection mechanism. For that
purpose, we contrast the mass correlations induced by purely random injections
with those induced by continuous injections in the environment. Using the
Kraichnan model of turbulent advection, whereby the underlying velocity field
is assumed to be shortly correlated in time, we explicitly identify scaling
regions for the statistics of the mass contained within a shell of radius $r$
and located at a distance $\rho$ away from the source. The two key parameters
are found to be (i) the ratio $s^2$ between the absolute and the relative
timescales of dispersion and (ii) the ratio $\Lambda$ between the size of the
cloud and its distance away from the source. When the injection is random, only
the former is relevant, as previously shown by Celani, Martins-Afonso $\&$
Mazzino, $J. Fluid. Mech$, 2007 in the case of an incompressible fluid. It is
argued that the space partition in terms of $s^2$ and $\Lambda$ is a robust
feature of the injection mechanism itself, which should remain relevant beyond
the Kraichnan model. This is for instance the case in a generalised version of
the model, where the absolute dispersion is prescribed to be ballistic rather
than diffusive.
",0,1,0,0,0,0
12785,The Cosmic V-Web,"  The network of filaments with embedded clusters surrounding voids seen in
maps derived from redshift surveys and reproduced in simulations has been
referred to as the cosmic web. A complementary description is provided by
considering the shear in the velocity field of galaxies. The eigenvalues of the
shear provide information on whether a region is collapsing in three
dimensions, the condition for a knot, expanding in three-dimensions, the
condition for a void, or in the intermediate condition of a filament or sheet.
The structures that are quantitatively defined by the eigenvalues can be
approximated by iso-contours that provide a visual representation of the cosmic
velocity (V) web. The current application is based on radial peculiar
velocities from the Cosmicflows-2 collection of distances. The
three-dimensional velocity field is constructed using the Wiener filter
methodology in the linear approximation. Eigenvalues of the velocity shear are
calculated at each point on a grid. Here, knots and filaments are visualized
across a local domain of diameter ~0.1c.
",0,1,0,0,0,0
2796,SUBIC: A Supervised Bi-Clustering Approach for Precision Medicine,"  Traditional medicine typically applies one-size-fits-all treatment for the
entire patient population whereas precision medicine develops tailored
treatment schemes for different patient subgroups. The fact that some factors
may be more significant for a specific patient subgroup motivates clinicians
and medical researchers to develop new approaches to subgroup detection and
analysis, which is an effective strategy to personalize treatment. In this
study, we propose a novel patient subgroup detection method, called Supervised
Biclustring (SUBIC) using convex optimization and apply our approach to detect
patient subgroups and prioritize risk factors for hypertension (HTN) in a
vulnerable demographic subgroup (African-American). Our approach not only finds
patient subgroups with guidance of a clinically relevant target variable but
also identifies and prioritizes risk factors by pursuing sparsity of the input
variables and encouraging similarity among the input variables and between the
input and target variables
",1,0,0,1,0,0
19455,Magnetoresistance in the superconducting state at the (111) LaAlO$_3$/SrTiO$_3$ interface,"  Condensed matter systems that simultaneously exhibit superconductivity and
ferromagnetism are rare due the antagonistic relationship between conventional
spin-singlet superconductivity and ferromagnetic order. In materials in which
superconductivity and magnetic order is known to coexist (such as some
heavy-fermion materials), the superconductivity is thought to be of an
unconventional nature. Recently, the conducting gas that lives at the interface
between the perovskite band insulators LaAlO$_3$ (LAO) and SrTiO$_3$ (STO) has
also been shown to host both superconductivity and magnetism. Most previous
research has focused on LAO/STO samples in which the interface is in the (001)
crystal plane. Relatively little work has focused on the (111) crystal
orientation, which has hexagonal symmetry at the interface, and has been
predicted to have potentially interesting topological properties, including
unconventional superconducting pairing states. Here we report measurements of
the magnetoresistance of (111) LAO/STO heterostructures at temperatures at
which they are also superconducting. As with the (001) structures, the
magnetoresistance is hysteretic, indicating the coexistence of magnetism and
superconductivity, but in addition, we find that this magnetoresistance is
anisotropic. Such an anisotropic response is completely unexpected in the
superconducting state, and suggests that (111) LAO/STO heterostructures may
support unconventional superconductivity.
",0,1,0,0,0,0
11161,Learning Convolutional Text Representations for Visual Question Answering,"  Visual question answering is a recently proposed artificial intelligence task
that requires a deep understanding of both images and texts. In deep learning,
images are typically modeled through convolutional neural networks, and texts
are typically modeled through recurrent neural networks. While the requirement
for modeling images is similar to traditional computer vision tasks, such as
object recognition and image classification, visual question answering raises a
different need for textual representation as compared to other natural language
processing tasks. In this work, we perform a detailed analysis on natural
language questions in visual question answering. Based on the analysis, we
propose to rely on convolutional neural networks for learning textual
representations. By exploring the various properties of convolutional neural
networks specialized for text data, such as width and depth, we present our
""CNN Inception + Gate"" model. We show that our model improves question
representations and thus the overall accuracy of visual question answering
models. We also show that the text representation requirement in visual
question answering is more complicated and comprehensive than that in
conventional natural language processing tasks, making it a better task to
evaluate textual representation methods. Shallow models like fastText, which
can obtain comparable results with deep learning models in tasks like text
classification, are not suitable in visual question answering.
",1,0,0,1,0,0
12095,A new scenario for gravity detection in plants: the position sensor hypothesis,"  The detection of gravity plays a fundamental role during the growth and
evolution of plants. Although progress has been made in our understanding of
the molecular, cellular and physical mechanisms involved in the gravity
detection, a coherent scenario consistent with all the observations is still
lacking. In this perspective paper we discuss recent experiments showing that
the response to inclination of shoots is independent of the gravity intensity,
meaning that the gravity sensor detects an inclination and not a force. This
result questions some of the commonly accepted hypotheses and leads to propose
a new ""position sensor hypothesis"". The implications of this new scenario are
discussed in the light of different observations available in the literature.
",0,1,0,0,0,0
1448,Levitation of non-magnetizable droplet inside ferrofluid,"  The central theme of this work is that a stable levitation of a denser
non-magnetizable liquid droplet, against gravity, inside a relatively lighter
ferrofluid -- a system barely considered in ferrohydrodynamics -- is possible,
and exhibits unique interfacial features; the stability of the levitation
trajectory, however, is subject to an appropriate magnetic field modulation. We
explore the shapes and the temporal dynamics of a plane non-magnetizable
droplet levitating inside ferrofluid against gravity due to a spatially
complex, but systematically generated, magnetic field in two dimensions. The
effect of the viscosity ratio, the stability of the levitation path and the
possibility of existence of multiple-stable equilibrium states is investigated.
We find, for certain conditions on the viscosity ratio, that there can be
developments of cusps and singularities at the droplet surface; this phenomenon
we also observe experimentally and compared with the simulations. Our
simulations closely replicate the singular projection on the surface of the
levitating droplet. Finally, we present an dynamical model for the vertical
trajectory of the droplet. This model reveals a condition for the onset of
levitation and the relation for the equilibrium levitation height. The
linearization of the model around the steady state captures that the nature of
the equilibrium point goes under a transition from being a spiral to a node
depending upon the control parameters, which essentially means that the
temporal route to the equilibrium can be either monotonic or undulating. The
analytical model for the droplet trajectory is in close agreement with the
detailed simulations. (See draft for full abstract).
",0,1,0,0,0,0
15073,On Abruptly-Changing and Slowly-Varying Multiarmed Bandit Problems,"  We study the non-stationary stochastic multiarmed bandit (MAB) problem and
propose two generic algorithms, namely, the limited memory deterministic
sequencing of exploration and exploitation (LM-DSEE) and the Sliding-Window
Upper Confidence Bound# (SW-UCB#). We rigorously analyze these algorithms in
abruptly-changing and slowly-varying environments and characterize their
performance. We show that the expected cumulative regret for these algorithms
under either of the environments is upper bounded by sublinear functions of
time, i.e., the time average of the regret asymptotically converges to zero. We
complement our analytic results with numerical illustrations.
",0,0,0,1,0,0
2888,Some algebraic invariants of edge ideal of circulant graphs,"  Let $G$ be the circulant graph $C_n(S)$ with $S\subseteq\{ 1,\ldots,\left
\lfloor\frac{n}{2}\right \rfloor\}$ and let $I(G)$ be its edge ideal in the
ring $K[x_0,\ldots,x_{n-1}]$. Under the hypothesis that $n$ is prime we : 1)
compute the regularity index of $R/I(G)$; 2) compute the Castelnuovo-Mumford
regularity when $R/I(G)$ is Cohen-Macaulay; 3) prove that the circulant graphs
with $S=\{1,\ldots,s\}$ are sequentially $S_2$ . We end characterizing the
Cohen-Macaulay circulant graphs of Krull dimension $2$ and computing their
Cohen-Macaulay type and Castelnuovo-Mumford regularity.
",0,0,1,0,0,0
18782,Gradient-based Representational Similarity Analysis with Searchlight for Analyzing fMRI Data,"  Representational Similarity Analysis (RSA) aims to explore similarities
between neural activities of different stimuli. Classical RSA techniques employ
the inverse of the covariance matrix to explore a linear model between the
neural activities and task events. However, calculating the inverse of a
large-scale covariance matrix is time-consuming and can reduce the stability
and robustness of the final analysis. Notably, it becomes severe when the
number of samples is too large. For facing this shortcoming, this paper
proposes a novel RSA method called gradient-based RSA (GRSA). Moreover, the
proposed method is not restricted to a linear model. In fact, there is a
growing interest in finding more effective ways of using multi-subject and
whole-brain fMRI data. Searchlight technique can extend RSA from the localized
brain regions to the whole-brain regions with smaller memory footprint in each
process. Based on Searchlight, we propose a new method called Spatiotemporal
Searchlight GRSA (SSL-GRSA) that generalizes our ROI-based GRSA algorithm to
the whole-brain data. Further, our approach can handle some computational
challenges while dealing with large-scale, multi-subject fMRI data.
Experimental studies on multi-subject datasets confirm that both proposed
approaches achieve superior performance to other state-of-the-art RSA
algorithms.
",0,0,0,1,1,0
5344,Topology reveals universal features for network comparison,"  The topology of any complex system is key to understanding its structure and
function. Fundamentally, algebraic topology guarantees that any system
represented by a network can be understood through its closed paths. The length
of each path provides a notion of scale, which is vitally important in
characterizing dominant modes of system behavior. Here, by combining topology
with scale, we prove the existence of universal features which reveal the
dominant scales of any network. We use these features to compare several
canonical network types in the context of a social media discussion which
evolves through the sharing of rumors, leaks and other news. Our analysis
enables for the first time a universal understanding of the balance between
loops and tree-like structure across network scales, and an assessment of how
this balance interacts with the spreading of information online. Crucially, our
results allow networks to be quantified and compared in a purely model-free way
that is theoretically sound, fully automated, and inherently scalable.
",1,0,1,1,0,0
2581,Genetic and Memetic Algorithm with Diversity Equilibrium based on Greedy Diversification,"  The lack of diversity in a genetic algorithm's population may lead to a bad
performance of the genetic operators since there is not an equilibrium between
exploration and exploitation. In those cases, genetic algorithms present a fast
and unsuitable convergence.
In this paper we develop a novel hybrid genetic algorithm which attempts to
obtain a balance between exploration and exploitation. It confronts the
diversity problem using the named greedy diversification operator. Furthermore,
the proposed algorithm applies a competition between parent and children so as
to exploit the high quality visited solutions. These operators are complemented
by a simple selection mechanism designed to preserve and take advantage of the
population diversity.
Additionally, we extend our proposal to the field of memetic algorithms,
obtaining an improved model with outstanding results in practice.
The experimental study shows the validity of the approach as well as how
important is taking into account the exploration and exploitation concepts when
designing an evolutionary algorithm.
",1,0,0,0,0,0
427,Why Condorcet Consistency is Essential,"  In a single winner election with several candidates and ranked choice or
rating scale ballots, a Condorcet winner is one who wins all their two way
races by majority rule or MR. A voting system has Condorcet consistency or CC
if it names any Condorcet winner the winner. Many voting systems lack CC, but a
three step line of reasoning is used here to show why it is necessary. In step
1 we show that we can dismiss all the electoral criteria which conflict with
CC. In step 2 we point out that CC follows almost automatically if we can agree
that MR is the only acceptable system for elections with two candidates. In
step 3 we make that argument for MR. This argument itself has three parts.
First, in races with two candidates, the only well known alternatives to MR can
sometimes name as winner a candidate who is preferred over their opponent by
only one voter, with all others preferring the opponent. That is unacceptable.
Second, those same systems are also extremely susceptible to strategic
insincere voting. Third, in simulation studies using spatial models with two
candidates, the best known alternative to MR picks the best or most centrist
candidate significantly less often than MR does.
",0,0,0,1,0,0
18624,Real-Time Background Subtraction Using Adaptive Sampling and Cascade of Gaussians,"  Background-Foreground classification is a fundamental well-studied problem in
computer vision. Due to the pixel-wise nature of modeling and processing in the
algorithm, it is usually difficult to satisfy real-time constraints. There is a
trade-off between the speed (because of model complexity) and accuracy.
Inspired by the rejection cascade of Viola-Jones classifier, we decompose the
Gaussian Mixture Model (GMM) into an adaptive cascade of classifiers. This way
we achieve a good improvement in speed without compensating for accuracy. In
the training phase, we learn multiple KDEs for different durations to be used
as strong prior distribution and detect probable oscillating pixels which
usually results in misclassifications. We propose a confidence measure for the
classifier based on temporal consistency and the prior distribution. The
confidence measure thus derived is used to adapt the learning rate and the
thresholds of the model, to improve accuracy. The confidence measure is also
employed to perform temporal and spatial sampling in a principled way. We
demonstrate a speed-up factor of 5x to 10x and 17 percent average improvement
in accuracy over several standard videos.
",1,0,0,1,0,0
13193,Lagrangians of hypergraphs: The Frankl-Füredi conjecture holds almost everywhere,"  Frankl and Füredi conjectured in 1989 that the maximum Lagrangian of all
$r$-uniform hypergraphs of fixed size $m$ is realised by the initial segment of
the colexicographic order. In particular, in the principal case
$m=\binom{t}{r}$ their conjecture states that every $H\subseteq
\mathbb{N}^{(r)}$ of size $\binom{t}{r}$ satisfies \begin{align*} \max
\{\sum_{A \in H}\prod_{i\in A} y_i \ \colon \ y_1,y_2,\ldots \geq 0; \sum_{i\in
\mathbb{N}} y_i=1 \}&\leq \frac{1}{t^r}\binom{t}{r}. \end{align*}
We prove the above statement for all $r\geq 4$ and large values of $t$ (the
case $r=3$ was settled by Talbot in 2002). More generally, we show for any
$r\geq 4$ that the Frankl-Füredi conjecture holds whenever $\binom{t-1}{r}
\leq m \leq \binom{t}{r}- \gamma_r t^{r-2}$ for a constant $\gamma_r>0$,
thereby verifying it for `most' $m\in \mathbb{N}$.
Furthermore, for $r=3$ we make an improvement on the results of
Talbot~\cite{Tb} and Tang, Peng, Zhang and Zhao~\cite{TPZZ}.
",0,0,1,0,0,0
4821,Unusual behavior of cuprates explained by heterogeneous charge localization,"  The cuprate high-temperature superconductors are among the most intensively
studied materials, yet essential questions regarding their principal phases and
the transitions between them remain unanswered. Generally thought of as doped
charge-transfer insulators, these complex lamellar oxides exhibit pseudogap,
strange-metal, superconducting and Fermi-liquid behaviour with increasing
hole-dopant concentration. Here we propose a simple inhomogeneous Mott-like
(de)localization model wherein exactly one hole per copper-oxygen unit is
gradually delocalized with increasing doping and temperature. The model is
percolative in nature, with parameters that are experimentally constrained. It
comprehensively captures pivotal unconventional experimental results, including
the temperature and doping dependence of the pseudogap phenomenon, the
strange-metal linear temperature dependence of the planar resistivity, and the
doping dependence of the superfluid density. The success and simplicity of our
model greatly demystify the cuprate phase diagram and point to a local
superconducting pairing mechanism involving the (de)localized hole.
",0,1,0,0,0,0
473,"Time-Reversal Breaking in QCD$_4$, Walls, and Dualities in 2+1 Dimensions","  We study $SU(N)$ Quantum Chromodynamics (QCD) in 3+1 dimensions with $N_f$
degenerate fundamental quarks with mass $m$ and a $\theta$-parameter. For
generic $m$ and $\theta$ the theory has a single gapped vacuum. However, as
$\theta$ is varied through $\theta=\pi$ for large $m$ there is a first order
transition. For $N_f=1$ the first order transition line ends at a point with a
massless $\eta'$ particle (for all $N$) and for $N_f>1$ the first order
transition ends at $m=0$, where, depending on the value of $N_f$, the IR theory
has free Nambu-Goldstone bosons, an interacting conformal field theory, or a
free gauge theory. Even when the $4d$ bulk is smooth, domain walls and
interfaces can have interesting phase transitions separating different $3d$
phases. These turn out to be the phases of the recently studied $3d$
Chern-Simons matter theories, thus relating the dynamics of QCD$_4$ and
QCD$_3$, and, in particular, making contact with the recently discussed
dualities in 2+1 dimensions. For example, when the massless $4d$ theory has an
$SU(N_f)$ sigma model, the domain wall theory at low (nonzero) mass supports a
$3d$ massless $CP^{N_f-1}$ nonlinear $\sigma$-model with a Wess-Zumino term, in
agreement with the conjectured dynamics in 2+1 dimensions.
",0,1,0,0,0,0
14349,ALFABURST: A commensal search for Fast Radio Bursts with Arecibo,"  ALFABURST has been searching for Fast Radio Bursts (FRBs) commensally with
other projects using the Arecibo L-band Feed Array (ALFA) receiver at the
Arecibo Observatory since July 2015. We describe the observing system and
report on the non-detection of any FRBs from that time until August 2017 for a
total observing time of 518 hours. With current FRB rate models, along with
measurements of telescope sensitivity and beam size, we estimate that this
survey probed redshifts out to about 3.4 with an effective survey volume of
around 600,000 Mpc$^3$. Based on this, we would expect, at the 99% confidence
level, to see at most two FRBs. We discuss the implications of this
non-detection in the context of results from other telescopes and the
limitation of our search pipeline. During the survey, single pulses from 17
known pulsars were detected. We also report the discovery of a Galactic radio
transient with a pulse width of 3 ms and dispersion measure of 281 pc
cm$^{-3}$, which was detected while the telescope was slewing between fields.
",0,1,0,0,0,0
4507,Evolutionary Acyclic Graph Partitioning,"  Directed graphs are widely used to model data flow and execution dependencies
in streaming applications. This enables the utilization of graph partitioning
algorithms for the problem of parallelizing computation for multiprocessor
architectures. However due to resource restrictions, an acyclicity constraint
on the partition is necessary when mapping streaming applications to an
embedded multiprocessor. Here, we contribute a multi-level algorithm for the
acyclic graph partitioning problem. Based on this, we engineer an evolutionary
algorithm to further reduce communication cost, as well as to improve load
balancing and the scheduling makespan on embedded multiprocessor architectures.
",1,0,0,0,0,0
12387,The fundamental factor of optical interference,"  It has been widely accepted that electric field alone is the fundamental
factor for optical interference, since Wiener's experiments in 1890 proved that
the electric field plays such a dominant role. A group of experiments were
demonstrated against Wiener's experiments under the condition that the
interference fringes made by optical standing waves could have been
distinguished from the fringes of equal thickness between the inner surface of
emulsion and the plane mirror used to build the optical standing waves. It was
found that the Bragg diffraction from the interference fringes formed by the
standing waves did not exist. This means optical standing waves did not blacken
the photographic emulsion, or the electric field did not play such a dominant
role. Therefore, instead of the electric-field energy density solely in
proportion to the electric-field square, Energy Flux in Interference was
proposed to represent the intensity of optical interference-field and approved
in the derivation of equations for the interference. The derived equations
indicate that both the electric-field vector and the magnetic-field vector are
in phase and have equal amount of energy densities at the interference maxima
of two light beams. Thus, the magnetic-field vector acts the same role as the
electric-field vector on light interacting with substance. The fundamental
factor of optical interference is electromagnetic energy flux densities rather
than electric-field alone, or the intensity of optical interference fringes
should be the energy flux density, not electric-field energy density.
",0,1,0,0,0,0
1178,On Evaluation of Embodied Navigation Agents,"  Skillful mobile operation in three-dimensional environments is a primary
topic of study in Artificial Intelligence. The past two years have seen a surge
of creative work on navigation. This creative output has produced a plethora of
sometimes incompatible task definitions and evaluation protocols. To coordinate
ongoing and future research in this area, we have convened a working group to
study empirical methodology in navigation research. The present document
summarizes the consensus recommendations of this working group. We discuss
different problem statements and the role of generalization, present evaluation
measures, and provide standard scenarios that can be used for benchmarking.
",1,0,0,0,0,0
16579,Einstein's accelerated reference systems and Fermi-Walker coordinates,"  We show that the uniformly accelerated reference systems proposed by Einstein
when introducing acceleration in the theory of relativity are Fermi-Walker
coordinate systems. We then consider more general accelerated motions and, on
the one hand we obtain Thomas precession and, on the other, we prove that the
only accelerated reference systems that at any time admit an instantaneously
comoving inertial system belong necessarily to the Fermi-Walker class.
",0,1,0,0,0,0
19613,Learning Latent Features with Pairwise Penalties in Matrix Completion,"  Low-rank matrix completion (MC) has achieved great success in many real-world
data applications. A latent feature model formulation is usually employed and,
to improve prediction performance, the similarities between latent variables
can be exploited by pairwise learning, e.g., the graph regularized matrix
factorization (GRMF) method. However, existing GRMF approaches often use a
squared L2 norm to measure the pairwise difference, which may be overly
influenced by dissimilar pairs and lead to inferior prediction. To fully
empower pairwise learning for matrix completion, we propose a general
optimization framework that allows a rich class of (non-)convex pairwise
penalty functions. A new and efficient algorithm is further developed to
uniformly solve the optimization problem, with a theoretical convergence
guarantee. In an important situation where the latent variables form a small
number of subgroups, its statistical guarantee is also fully characterized. In
particular, we theoretically characterize the complexity-regularized maximum
likelihood estimator, as a special case of our framework. It has a better error
bound when compared to the standard trace-norm regularized matrix completion.
We conduct extensive experiments on both synthetic and real datasets to
demonstrate the superior performance of this general framework.
",0,0,0,1,0,0
12520,Spherical Planetary Robot for Rugged Terrain Traversal,"  Wheeled planetary rovers such as the Mars Exploration Rovers (MERs) and Mars
Science Laboratory (MSL) have provided unprecedented, detailed images of the
Mars surface. However, these rovers are large and are of high-cost as they need
to carry sophisticated instruments and science laboratories. We propose the
development of low-cost planetary rovers that are the size and shape of
cantaloupes and that can be deployed from a larger rover. The rover named
SphereX is 2 kg in mass, is spherical, holonomic and contains a hopping
mechanism to jump over rugged terrain. A small low-cost rover complements a
larger rover, particularly to traverse rugged terrain or roll down a canyon,
cliff or crater to obtain images and science data. While it may be a one-way
journey for these small robots, they could be used tactically to obtain
high-reward science data. The robot is equipped with a pair of stereo cameras
to perform visual navigation and has room for a science payload. In this paper,
we analyze the design and development of a laboratory prototype. The results
show a promising pathway towards development of a field system.
",1,1,0,0,0,0
17526,Evaluating (and improving) the correspondence between deep neural networks and human representations,"  Decades of psychological research have been aimed at modeling how people
learn features and categories. The empirical validation of these theories is
often based on artificial stimuli with simple representations. Recently, deep
neural networks have reached or surpassed human accuracy on tasks such as
identifying objects in natural images. These networks learn representations of
real-world stimuli that can potentially be leveraged to capture psychological
representations. We find that state-of-the-art object classification networks
provide surprisingly accurate predictions of human similarity judgments for
natural images, but fail to capture some of the structure represented by
people. We show that a simple transformation that corrects these discrepancies
can be obtained through convex optimization. We use the resulting
representations to predict the difficulty of learning novel categories of
natural images. Our results extend the scope of psychological experiments and
computational modeling by enabling tractable use of large natural stimulus
sets.
",1,0,0,0,0,0
12956,Joint User Selection and Energy Minimization for Ultra-Dense Multi-channel C-RAN with Incomplete CSI,"  This paper provides a unified framework to deal with the challenges arising
in dense cloud radio access networks (C-RAN), which include huge power
consumption, limited fronthaul capacity, heavy computational complexity,
unavailability of full channel state information (CSI), etc. Specifically, we
aim to jointly optimize the remote radio head (RRH) selection, user equipment
(UE)-RRH associations and beam-vectors to minimize the total network power
consumption (NPC) for dense multi-channel downlink C-RAN with incomplete CSI
subject to per-RRH power constraints, each UE's total rate requirement, and
fronthaul link capacity constraints. This optimization problem is NP-hard. In
addition, due to the incomplete CSI, the exact expression of UEs' rate
expression is intractable. We first conservatively replace UEs' rate expression
with its lower-bound. Then, based on the successive convex approximation (SCA)
technique and the relationship between the data rate and the mean square error
(MSE), we propose a single-layer iterative algorithm to solve the NPC
minimization problem with convergence guarantee. In each iteration of the
algorithm, the Lagrange dual decomposition method is used to derive the
structure of the optimal beam-vectors, which facilitates the parallel
computations at the Baseband unit (BBU) pool. Furthermore, a bisection UE
selection algorithm is proposed to guarantee the feasibility of the problem.
Simulation results show the benefits of the proposed algorithms and the fact
that a limited amount of CSI is sufficient to achieve performance close to that
obtained when perfect CSI is possessed.
",1,0,0,0,0,0
19922,An intuitive approach to the unified theory of spin-relaxation,"  Spin-relaxation is conventionally discussed using two different approaches
for materials with and without inversion symmetry. The former is known as the
Elliott-Yafet (EY) theory and for the latter the D'yakonov-Perel' (DP) theory
applies, respectively. We discuss herein a simple and intuitive approach to
demonstrate that the two seemingly disparate mechanisms are closely related. A
compelling analogy between the respective Hamiltonian is presented and that the
usual derivation of spin-relaxation times, in the respective frameworks of the
two theories, can be performed. The result also allows to obtain the less
canonical spin-relaxation regimes; the generalization of the EY when the
material has a large quasiparticle broadening and the DP mechanism in ultrapure
semiconductors. The method also allows a practical and intuitive numerical
implementation of the spin-relaxation calculation, which is demonstrated for
MgB$_2$ that has anomalous spin-relaxation properties.
",0,1,0,0,0,0
14263,On hyperballeans of bounded geometry,"  A ballean (or coarse structure) is a set endowed with some family of subsets,
the balls, is such a way that balleans with corresponding morphisms can be
considered as asymptotic counterparts of uniform topological spaces. For a
ballean $\mathcal{B}$ on a set $X$, the hyperballean $\mathcal{B}^{\flat}$ is a
ballean naturally defined on the set $X^{\flat}$ of all bounded subsets of $X$.
We describe all balleans with hyperballeans of bounded geometry and analyze the
structure of these hyperballeans.
",0,0,1,0,0,0
19005,Rapid processing of 85Kr/Kr ratios using Atom Trap Trace Analysis,"  We report a methodology for measuring 85Kr/Kr isotopic abundances using Atom
Trap Trace Analysis (ATTA) that increases sample measurement throughput by over
an order of magnitude to 6 samples per 24 hours. The noble gas isotope 85Kr
(half-life = 10.7 yr) is a useful tracer for young groundwater in the age range
of 5-50 years. ATTA, an efficient and selective laser-based atom counting
method, has recently been applied to 85Kr/Kr isotopic abundance measurements,
requiring 5-10 microliters of krypton gas at STP extracted from 50-100 L of
water. Previously a single such measurement required 48 hours. Our new method
demonstrates that we can measure 85Kr/Kr ratios with 3-5% relative uncertainty
every 4 hours, on average, with the same sample requirements.
",0,1,0,0,0,0
1219,The adaptive zero-error capacity for a class of channels with noisy feedback,"  The adaptive zero-error capacity of discrete memoryless channels (DMC) with
noiseless feedback has been shown to be positive whenever there exists at least
one channel output ""disprover"", i.e. a channel output that cannot be reached
from at least one of the inputs. Furthermore, whenever there exists a
disprover, the adaptive zero-error capacity attains the Shannon (small-error)
capacity. Here, we study the zero-error capacity of a DMC when the channel
feedback is noisy rather than perfect. We show that the adaptive zero-error
capacity with noisy feedback is lower bounded by the forward channel's
zero-undetected error capacity, and show that under certain conditions this is
tight.
",1,0,0,0,0,0
12116,Multiuser Communication Based on the DFT Eigenstructure,"  The eigenstructure of the discrete Fourier transform (DFT) is examined and
new systematic procedures to generate eigenvectors of the unitary DFT are
proposed. DFT eigenvectors are suggested as user signatures for data
communication over the real adder channel (RAC). The proposed multiuser
communication system over the 2-user RAC is detailed.
",1,0,0,1,0,0
12926,Estimation of lactate threshold with machine learning techniques in recreational runners,"  Lactate threshold is considered an essential parameter when assessing
performance of elite and recreational runners and prescribing training
intensities in endurance sports. However, the measurement of blood lactate
concentration requires expensive equipment and the extraction of blood samples,
which are inconvenient for frequent monitoring. Furthermore, most recreational
runners do not have access to routine assessment of their physical fitness by
the aforementioned equipment so they are not able to calculate the lactate
threshold without resorting to an expensive and specialized centre. Therefore,
the main objective of this study is to create an intelligent system capable of
estimating the lactate threshold of recreational athletes participating in
endurance running sports. The solution here proposed is based on a machine
learning system which models the lactate evolution using recurrent neural
networks and includes the proposal of standardization of the temporal axis as
well as a modification of the stratified sampling method. The results show that
the proposed system accurately estimates the lactate threshold of 89.52% of the
athletes and its correlation with the experimentally measured lactate threshold
is very high (R=0,89). Moreover, its behaviour with the test dataset is as good
as with the training set, meaning that the generalization power of the model is
high. Therefore, in this study a machine learning based system is proposed as
alternative to the traditional invasive lactate threshold measurement tests for
recreational runners.
",0,0,0,1,0,0
14248,GraphGAN: Graph Representation Learning with Generative Adversarial Nets,"  The goal of graph representation learning is to embed each vertex in a graph
into a low-dimensional vector space. Existing graph representation learning
methods can be classified into two categories: generative models that learn the
underlying connectivity distribution in the graph, and discriminative models
that predict the probability of edge existence between a pair of vertices. In
this paper, we propose GraphGAN, an innovative graph representation learning
framework unifying above two classes of methods, in which the generative model
and discriminative model play a game-theoretical minimax game. Specifically,
for a given vertex, the generative model tries to fit its underlying true
connectivity distribution over all other vertices and produces ""fake"" samples
to fool the discriminative model, while the discriminative model tries to
detect whether the sampled vertex is from ground truth or generated by the
generative model. With the competition between these two models, both of them
can alternately and iteratively boost their performance. Moreover, when
considering the implementation of generative model, we propose a novel graph
softmax to overcome the limitations of traditional softmax function, which can
be proven satisfying desirable properties of normalization, graph structure
awareness, and computational efficiency. Through extensive experiments on
real-world datasets, we demonstrate that GraphGAN achieves substantial gains in
a variety of applications, including link prediction, node classification, and
recommendation, over state-of-the-art baselines.
",1,0,0,1,0,0
12605,The origin and early evolution of life in chemical complexity space,"  Life can be viewed as a localized chemical system that sits on, or in the
basin of attraction of, a metastable dynamical attractor state that remains out
of equilibrium with the environment. Such a view of life allows that new living
states can arise through chance changes in local chemical concentration
(=mutations) that move points in space into the basin of attraction of a life
state - the attractor being an autocatalytic sets whose essential (=keystone)
species are produced at a higher rate than they are lost to the environment by
diffusion, such that growth in expected. This conception of life yields several
new insights and conjectures. (1) This framework suggests that the first new
life states to arise are likely at interfaces where the rate of diffusion of
keystone species is tied to a low-diffusion regime, while precursors and waste
products diffuse at a higher rate. (2) There are reasons to expect that once
the first life state arises, most likely on a mineral surface, additional
mutations will generate derived life states with which the original state will
compete. (3) I propose that in the resulting adaptive process there is a
general tendency for higher complexity life states (i.e., ones that are further
from being at equilibrium with the environment) to dominate a given mineral
surface. (4) The framework suggests a simple and predictable path by which
cells evolve and provides pointers on why such cells are likely to acquire
particulate inheritance. Overall, the dynamical systems theoretical framework
developed provides an integrated view of the origin and early evolution of life
and supports novel empirical approaches.
",0,0,0,0,1,0
13996,Categorical relations between Langlands dual quantum affine algebras: Doubly laced types,"  We prove that the Grothendieck rings of category $\mathcal{C}^{(t)}_Q$ over
quantum affine algebras $U_q'(\g^{(t)})$ $(t=1,2)$ associated to each Dynkin
quiver $Q$ of finite type $A_{2n-1}$ (resp. $D_{n+1}$) is isomorphic to one of
category $\mathcal{C}_{\mQ}$ over the Langlands dual $U_q'({^L}\g^{(2)})$ of
$U_q'(\g^{(2)})$ associated to any twisted adapted class $[\mQ]$ of $A_{2n-1}$
(resp. $D_{n+1}$). This results provide partial answers of conjectures of
Frenkel-Hernandez on Langlands duality for finite-dimensional representation of
quantum affine algebras.
",0,0,1,0,0,0
3322,Numerical prediction of the piezoelectric transducer response in the acoustic nearfield using a one-dimensional electromechanical finite difference approach,"  We present a simple electromechanical finite difference model to study the
response of a piezoelectric polyvinylidenflourid (PVDF) transducer to
optoacoustic (OA) pressure waves in the acoustic nearfield prior to thermal
relaxation of the OA source volume. The assumption of nearfield conditions,
i.e. the absence of acoustic diffraction, allows to treat the problem using a
one-dimensional numerical approach. Therein, the computational domain is
modeled as an inhomogeneous elastic medium, characterized by its local wave
velocities and densities, allowing to explore the effect of stepwise impedance
changes on the stress wave propagation. The transducer is modeled as a thin
piezoelectric sensing layer and the electromechanical coupling is accomplished
by means of the respective linear constituting equations. Considering a
low-pass characteristic of the full experimental setup, we obtain the resulting
transducer signal. Complementing transducer signals measured in a controlled
laboratory experiment with numerical simulations that result from a model of
the experimental setup, we find that, bearing in mind the apparent limitations
of the one-dimensional approach, the simulated transducer signals can be used
very well to predict and interpret the experimental findings.
",0,1,0,0,0,0
19783,Houdini: Fooling Deep Structured Prediction Models,"  Generating adversarial examples is a critical step for evaluating and
improving the robustness of learning machines. So far, most existing methods
only work for classification and are not designed to alter the true performance
measure of the problem at hand. We introduce a novel flexible approach named
Houdini for generating adversarial examples specifically tailored for the final
performance measure of the task considered, be it combinatorial and
non-decomposable. We successfully apply Houdini to a range of applications such
as speech recognition, pose estimation and semantic segmentation. In all cases,
the attacks based on Houdini achieve higher success rate than those based on
the traditional surrogates used to train the models while using a less
perceptible adversarial perturbation.
",1,0,0,1,0,0
17794,Tunable Weyl and Dirac states in the nonsymmorphic compound $\rm\mathbf{CeSbTe}$,"  Recent interest in topological semimetals has lead to the proposal of many
new topological phases that can be realized in real materials. Next to Dirac
and Weyl systems, these include more exotic phases based on manifold band
degeneracies in the bulk electronic structure. The exotic states in topological
semimetals are usually protected by some sort of crystal symmetry and the
introduction of magnetic order can influence these states by breaking time
reversal symmetry. Here we show that we can realize a rich variety of different
topological semimetal states in a single material, $\rm CeSbTe$. This compound
can exhibit different types of magnetic order that can be accessed easily by
applying a small field. It allows, therefore, for tuning the electronic
structure and can drive it through a manifold of topologically distinct phases,
such as the first nonsymmorphic magnetic topological material with an
eight-fold band crossing at a high symmetry point. Our experimental results are
backed by a full magnetic group theory analysis and ab initio calculations.
This discovery introduces a realistic and promising platform for studying the
interplay of magnetism and topology.
",0,1,0,0,0,0
4209,Polynomial Relations Between Matrices of Graphs,"  We derive a correspondence between the eigenvalues of the adjacency matrix
$A$ and the signless Laplacian matrix $Q$ of a graph $G$ when $G$ is
$(d_1,d_2)$-biregular by using the relation $A^2=(Q-d_1I)(Q-d_2I)$. This
motivates asking when it is possible to have $X^r=f(Y)$ for $f$ a polynomial,
$r>0$, and $X,\ Y$ matrices associated to a graph $G$. It turns out that,
essentially, this can only happen if $G$ is either regular or biregular.
",0,0,1,0,0,0
1259,"Scaling up the software development process, a case study highlighting the complexities of large team software development","  Diamond Light Source is the UK's National Synchrotron Facility and as such
provides access to world class experimental services for UK and international
researchers. As a user facility, that is one that focuses on providing a good
user experience to our varied visitors, Diamond invests heavily in software
infrastructure and staff. Over 100 members of the 600 strong workforce consider
software development as a significant tool to help them achieve their primary
role. These staff work on a diverse number of different software packages,
providing support for installation and configuration, maintenance and bug
fixing, as well as additional research and development of software when
required.
This talk focuses on one of the software projects undertaken to unify and
improve the user experience of several experiments. The ""mapping project"" is a
large 2 year, multi group project targeting the collection and processing
experiments which involve scanning an X-ray beam over a sample and building up
an image of that sample, similar to the way that google maps bring together
small pieces of information to produce a full map of the world. The project
itself is divided into several work packages, ranging from teams of one to 5 or
6 in size, with varying levels of time commitment to the project. This paper
aims to explore one of these work packages as a case study, highlighting the
experiences of the project team, the methodologies employed, their outcomes,
and the lessons learnt from the experience.
",1,0,0,0,0,0
19144,Correlating Cell Shape and Cellular Stress in Motile Confluent Tissues,"  Collective cell migration is a highly regulated process involved in wound
healing, cancer metastasis and morphogenesis. Mechanical interactions among
cells provide an important regulatory mechanism to coordinate such collective
motion. Using a Self-Propelled Voronoi (SPV) model that links cell mechanics to
cell shape and cell motility, we formulate a generalized mechanical inference
method to obtain the spatio-temporal distribution of cellular stresses from
measured traction forces in motile tissues and show that such traction-based
stresses match those calculated from instantaneous cell shapes. We additionally
use stress information to characterize the rheological properties of the
tissue. We identify a motility-induced swim stress that adds to the interaction
stress to determine the global contractility or extensibility of epithelia. We
further show that the temporal correlation of the interaction shear stress
determines an effective viscosity of the tissue that diverges at the
liquid-solid transition, suggesting the possibility of extracting rheological
information directly from traction data.
",0,1,0,0,0,0
16088,From arteries to boreholes: Transient response of a poroelastic cylinder to fluid injection,"  The radially outward flow of fluid through a porous medium occurs in many
practical problems, from transport across vascular walls to the pressurisation
of boreholes in the subsurface. When the driving pressure is non-negligible
relative to the stiffness of the solid structure, the poromechanical coupling
between the fluid and the solid can control both the steady-state and the
transient mechanics of the system. Very large pressures or very soft materials
lead to large deformations of the solid skeleton, which introduce kinematic and
constitutive nonlinearity that can have a nontrivial impact on these mechanics.
Here, we study the transient response of a poroelastic cylinder to sudden fluid
injection. We consider the impacts of kinematic and constitutive nonlinearity,
both separately and in combination, and we highlight the central role of
driving method in the evolution of the response. We show that the various
facets of nonlinearity may either accelerate or decelerate the transient
response relative to linear poroelasticity, depending on the boundary
conditions and the initial geometry, and that an imposed fluid pressure leads
to a much faster response than an imposed fluid flux.
",0,1,0,0,0,0
15696,An Ensemble Classification Algorithm Based on Information Entropy for Data Streams,"  Data stream mining problem has caused widely concerns in the area of machine
learning and data mining. In some recent studies, ensemble classification has
been widely used in concept drift detection, however, most of them regard
classification accuracy as a criterion for judging whether concept drift
happening or not. Information entropy is an important and effective method for
measuring uncertainty. Based on the information entropy theory, a new algorithm
using information entropy to evaluate a classification result is developed. It
uses ensemble classification techniques, and the weight of each classifier is
decided through the entropy of the result produced by an ensemble classifiers
system. When the concept in data streams changing, the classifiers' weight
below a threshold value will be abandoned to adapt to a new concept in one
time. In the experimental analysis section, six databases and four proposed
algorithms are executed. The results show that the proposed method can not only
handle concept drift effectively, but also have a better classification
accuracy and time performance than the contrastive algorithms.
",1,0,0,0,0,0
17965,Suppressing correlations in massively parallel simulations of lattice models,"  For lattice Monte Carlo simulations parallelization is crucial to make
studies of large systems and long simulation time feasible, while sequential
simulations remain the gold-standard for correlation-free dynamics. Here,
various domain decomposition schemes are compared, concluding with one which
delivers virtually correlation-free simulations on GPU Extensive simulations of
the octahedron model for $2+1$ dimensional Karda--Parisi--Zhang surface growth,
which is very sensitive to correlation in the site-selection dynamics, were
performed to show self-consistency of the parallel runs and agreement with the
sequential algorithm. We present a GPU implementation providing a speedup of
about $30\times$ over a parallel CPU implementation on a single socket and at
least $180\times$ with respect to the sequential reference.
",0,1,0,0,0,0
15407,The Sample Complexity of Online One-Class Collaborative Filtering,"  We consider the online one-class collaborative filtering (CF) problem that
consists of recommending items to users over time in an online fashion based on
positive ratings only. This problem arises when users respond only occasionally
to a recommendation with a positive rating, and never with a negative one. We
study the impact of the probability of a user responding to a recommendation,
p_f, on the sample complexity, i.e., the number of ratings required to make
`good' recommendations, and ask whether receiving positive and negative
ratings, instead of positive ratings only, improves the sample complexity. Both
questions arise in the design of recommender systems. We introduce a simple
probabilistic user model, and analyze the performance of an online user-based
CF algorithm. We prove that after an initial cold start phase, where
recommendations are invested in exploring the user's preferences, this
algorithm makes---up to a fraction of the recommendations required for updating
the user's preferences---perfect recommendations. The number of ratings
required for the cold start phase is nearly proportional to 1/p_f, and that for
updating the user's preferences is essentially independent of p_f. As a
consequence we find that, receiving positive and negative ratings instead of
only positive ones improves the number of ratings required for initial
exploration by a factor of 1/p_f, which can be significant.
",1,0,0,1,0,0
350,Social media mining for identification and exploration of health-related information from pregnant women,"  Widespread use of social media has led to the generation of substantial
amounts of information about individuals, including health-related information.
Social media provides the opportunity to study health-related information about
selected population groups who may be of interest for a particular study. In
this paper, we explore the possibility of utilizing social media to perform
targeted data collection and analysis from a particular population group --
pregnant women. We hypothesize that we can use social media to identify cohorts
of pregnant women and follow them over time to analyze crucial health-related
information. To identify potentially pregnant women, we employ simple
rule-based searches that attempt to detect pregnancy announcements with
moderate precision. To further filter out false positives and noise, we employ
a supervised classifier using a small number of hand-annotated data. We then
collect their posts over time to create longitudinal health timelines and
attempt to divide the timelines into different pregnancy trimesters. Finally,
we assess the usefulness of the timelines by performing a preliminary analysis
to estimate drug intake patterns of our cohort at different trimesters. Our
rule-based cohort identification technique collected 53,820 users over thirty
months from Twitter. Our pregnancy announcement classification technique
achieved an F-measure of 0.81 for the pregnancy class, resulting in 34,895 user
timelines. Analysis of the timelines revealed that pertinent health-related
information, such as drug-intake and adverse reactions can be mined from the
data. Our approach to using user timelines in this fashion has produced very
encouraging results and can be employed for other important tasks where
cohorts, for which health-related information may not be available from other
sources, are required to be followed over time to derive population-based
estimates.
",1,0,0,0,0,0
11535,Analysis and optimal individual pitch control decoupling by inclusion of an azimuth offset in the multi-blade coordinate transformation,"  With the trend of increasing wind turbine rotor diameters, the mitigation of
blade fatigue loadings is of special interest to extend the turbine lifetime.
Fatigue load reductions can be partly accomplished using Individual Pitch
Control (IPC) facilitated by the so-called Multi-Blade Coordinate (MBC)
transformation. This operation transforms and decouples the blade load signals
in a yaw- and tilt-axis. However, in practical scenarios, the resulting
transformed system still shows coupling between the axes, posing a need for
more advanced Multiple-Input Multiple-Output (MIMO) control architectures. This
paper presents a novel analysis and design framework for decoupling of the
non-rotating axes by the inclusion of an azimuth offset in the reverse MBC
transformation, enabling the application of simple Single-Input Single-Output
(SISO) controllers. A thorough analysis is given by including the azimuth
offset in a frequency-domain representation. The result is evaluated on
simplified blade models, as well as linearizations obtained from the
NREL~5\nobreakdash-MW reference wind turbine. A sensitivity and decoupling
assessment justify the application of decentralized SISO control loops for IPC.
Furthermore, closed-loop high-fidelity simulations show beneficial effects on
pitch actuation and blade fatigue load reductions.
",1,0,0,0,0,0
16586,Phase-Resolved Two-Dimensional Spectroscopy of Electronic Wavepackets by Laser-Induced XUV Free Induction Decay,"  We present a novel time- and phase-resolved, background-free scheme to study
the extreme ultraviolet dipole emission of a bound electronic wavepacket,
without the use of any extreme ultraviolet exciting pulse. Using multiphoton
transitions, we populate a superposition of quantum states which coherently
emit extreme ultraviolet radiation through free induction decay. This emission
is probed and controlled, both in amplitude and phase, by a time-delayed
infrared femtosecond pulse. We directly measure the laser-induced dephasing of
the emission by using a simple heterodyne detection scheme based on two-source
interferometry. This technique provides rich information about the interplay
between the laser field and the Coulombic potential on the excited electron
dynamics. Its background-free nature enables us to use a large range of gas
pressures and to reveal the influence of collisions in the relaxation process.
",0,1,0,0,0,0
19149,Using highly uniform and smooth Selenium colloids as low-loss magnetodielectric building blocks of optical metafluids,"  We systematically analyzed magnetodielectric resonances of Se colloids for
the first time to exploit the possibility for use as building blocks of
all-dielectric optical metafluids. By taking synergistic advantages of Se
colloids, including (i) high-refractive-index at optical frequencies, (ii)
unprecedented structural uniformity, and (iii) versatile access to copious
quantities, the Kerker-type directional light scattering resulting from
efficient coupling between strong electric and magnetic resonances were
observed directly from Se colloidal suspension. Thus, the use of Se colloid as
a generic magnetodielectric building block highlights an opportunity for the
fluidic low-loss optical antenna, which can be processed via spin-coating and
painting.
",0,1,0,0,0,0
8775,New neutrino physics and the altered shapes of solar neutrino spectra,"  Neutrinos coming from the Sun's core are now measured with a high precision,
and fundamental neutrino oscillations parameters are determined with a good
accuracy. In this work, we estimate the impact that a new neutrino physics
model, the so-called generalized Mikheyev-Smirnov-Wolfenstein (MSW) oscillation
mechanism, has on the shape of some of leading solar neutrino spectra, some of
which will be partially tested by the next generation of solar neutrino
experiments. In these calculations, we use a high-precision standard solar
model in good agreement with helioseismology data. We found that the neutrino
spectra of the different solar nuclear reactions of the proton-proton chains
and carbon-nitrogen-oxygen cycle have quite distinct sensitivities to the new
neutrino physics. The $HeP$ and $^8B$ neutrino spectra are the ones for which
their shapes are more affected when neutrinos interact with quarks in addition
to electrons. The shape of the $^{15}O$ and $^{17}F$ neutrino spectra are also
modified, although in these cases the impact is much smaller. Finally, the
impact in the shape of the $PP$ and $^{13}N$ neutrino spectra is practically
negligible.
",0,1,0,0,0,0
13835,Refining Trace Abstraction using Abstract Interpretation,"  The CEGAR loop in software model checking notoriously diverges when the
abstraction refinement procedure does not derive a loop invariant. An
abstraction refinement procedure based on an SMT solver is applied to a trace,
i.e., a restricted form of a program (without loops). In this paper, we present
a new abstraction refinement procedure that aims at circumventing this
restriction whenever possible. We apply abstract interpretation to a program
that we derive from the given trace. If the program contains a loop, we are
guaranteed to obtain a loop invariant. We call an SMT solver only in the case
where the abstract interpretation returns an indefinite answer. That is, the
idea is to use abstract interpretation and an SMT solver in tandem. An
experimental evaluation in the setting of trace abstraction indicates the
practical potential of this idea.
",1,0,0,0,0,0
16080,Rank conditional coverage and confidence intervals in high dimensional problems,"  Confidence interval procedures used in low dimensional settings are often
inappropriate for high dimensional applications. When a large number of
parameters are estimated, marginal confidence intervals associated with the
most significant estimates have very low coverage rates: They are too small and
centered at biased estimates. The problem of forming confidence intervals in
high dimensional settings has previously been studied through the lens of
selection adjustment. In this framework, the goal is to control the proportion
of non-covering intervals formed for selected parameters.
In this paper we approach the problem by considering the relationship between
rank and coverage probability. Marginal confidence intervals have very low
coverage rates for significant parameters and high rates for parameters with
more boring estimates. Many selection adjusted intervals display the same
pattern. This connection motivates us to propose a new coverage criterion for
confidence intervals in multiple testing/covering problems --- the rank
conditional coverage (RCC). This is the expected coverage rate of an interval
given the significance ranking for the associated estimator. We propose
interval construction via bootstrapping which produces small intervals and have
a rank conditional coverage close to the nominal level. These methods are
implemented in the R package rcc.
",0,0,0,1,0,0
11811,Affine-Gradient Based Local Binary Pattern Descriptor for Texture Classiffication,"  We present a novel Affine-Gradient based Local Binary Pattern (AGLBP)
descriptor for texture classification. It is very hard to describe complicated
texture using single type information, such as Local Binary Pattern (LBP),
which just utilizes the sign information of the difference between the pixel
and its local neighbors. Our descriptor has three characteristics: 1) In order
to make full use of the information contained in the texture, the
Affine-Gradient, which is different from Euclidean-Gradient and invariant to
affine transformation is incorporated into AGLBP. 2) An improved method is
proposed for rotation invariance, which depends on the reference direction
calculating respect to local neighbors. 3) Feature selection method,
considering both the statistical frequency and the intraclass variance of the
training dataset, is also applied to reduce the dimensionality of descriptors.
Experiments on three standard texture datasets, Outex12, Outex10 and KTH-TIPS2,
are conducted to evaluate the performance of AGLBP. The results show that our
proposed descriptor gets better performance comparing to some state-of-the-art
rotation texture descriptors in texture classification.
",1,0,0,0,0,0
5682,Combining Information from Multiple Forecasters: Inefficiency of Central Tendency,"  Even though the forecasting literature agrees that aggregating multiple
predictions of some future outcome typically outperforms the individual
predictions, there is no general consensus about the right way to do this. Most
common aggregators are means, defined loosely as aggregators that always remain
between the smallest and largest predictions. Examples include the arithmetic
mean, trimmed means, median, mid-range, and many other measures of central
tendency. If the forecasters use different information, the aggregator ideally
combines their information into a consensus without losing or distorting any of
it. An aggregator that achieves this is considered efficient. Unfortunately,
our results show that if the forecasters use their information accurately, an
aggregator that always remains strictly between the smallest and largest
predictions is never efficient in practice. A similar result holds even if the
ideal predictions are distorted with random error that is centered at zero. If
these noisy predictions are aggregated with a similar notion of centrality,
then, under some mild conditions, the aggregator is asymptotically inefficient.
",0,0,1,1,0,0
18242,Evolving to Non-round Weingarten Spheres: Integer Linear Hopf Flows,"  In the 1950's Hopf gave examples of non-round convex 2-spheres in Euclidean
3-space with rotational symmetry that satisfy a linear relationship between
their principal curvatures. In this paper we investigate conditions under which
evolving a smooth rotationally symmetric sphere by a linear combination of its
radii of curvature yields a Hopf sphere. When the coefficients of the flow have
certain integer values, the fate of an initial sphere is entirely determined by
the local geometry of its isolated umbilic points. A surprising variety of
behaviours is uncovered: convergence to round spheres and non-round Hopf
spheres, as well as divergence to infinity.
The critical quantity is the rate of vanishing of the astigmatism - the
difference of the radii of curvature - at the isolated umbilic points. It is
proven that the size of this quantity versus the coefficient in the flow
function determines the fate of the evolution.
The geometric setting for the equation is Radius of Curvature space, viewed
as a pair of hyperbolic/AdS half-planes joined along their boundary, the
umbilic horizon. A rotationally symmetric sphere determines a parameterized
curve in this plane with end-points on the umbilic horizon. The slope of the
curve at the umbilic horizon is linked by the Codazzi-Mainardi equations to the
rate of vanishing of astigmatism, and for generic initial conditions can be
used to determine the outcome of the flow.
The slope can jump during the flow, and a number of examples are given:
instant jumps of the initial slope, as well as umbilic circles that contract to
points in finite time and 'pop' the slope. Finally, we present soliton-like
solutions: curves that evolve under linear flows by mutual hyperbolic/AdS
isometries (dilation and translation) of Radius of Curvature space. A
forthcoming paper will apply these geometric ideas to non-linear curvature
flows.
",0,0,1,0,0,0
20927,An Overview on Application of Machine Learning Techniques in Optical Networks,"  Today's telecommunication networks have become sources of enormous amounts of
widely heterogeneous data. This information can be retrieved from network
traffic traces, network alarms, signal quality indicators, users' behavioral
data, etc. Advanced mathematical tools are required to extract meaningful
information from these data and take decisions pertaining to the proper
functioning of the networks from the network-generated data. Among these
mathematical tools, Machine Learning (ML) is regarded as one of the most
promising methodological approaches to perform network-data analysis and enable
automated network self-configuration and fault management. The adoption of ML
techniques in the field of optical communication networks is motivated by the
unprecedented growth of network complexity faced by optical networks in the
last few years. Such complexity increase is due to the introduction of a huge
number of adjustable and interdependent system parameters (e.g., routing
configurations, modulation format, symbol rate, coding schemes, etc.) that are
enabled by the usage of coherent transmission/reception technologies, advanced
digital signal processing and compensation of nonlinear effects in optical
fiber propagation. In this paper we provide an overview of the application of
ML to optical communications and networking. We classify and survey relevant
literature dealing with the topic, and we also provide an introductory tutorial
on ML for researchers and practitioners interested in this field. Although a
good number of research papers have recently appeared, the application of ML to
optical networks is still in its infancy: to stimulate further work in this
area, we conclude the paper proposing new possible research directions.
",0,0,0,1,0,0
19163,Coordinate Descent with Bandit Sampling,"  Coordinate descent methods usually minimize a cost function by updating a
random decision variable (corresponding to one coordinate) at a time. Ideally,
we would update the decision variable that yields the largest decrease in the
cost function. However, finding this coordinate would require checking all of
them, which would effectively negate the improvement in computational
tractability that coordinate descent is intended to afford. To address this, we
propose a new adaptive method for selecting a coordinate. First, we find a
lower bound on the amount the cost function decreases when a coordinate is
updated. We then use a multi-armed bandit algorithm to learn which coordinates
result in the largest lower bound by interleaving this learning with
conventional coordinate descent updates except that the coordinate is selected
proportionately to the expected decrease. We show that our approach improves
the convergence of coordinate descent methods both theoretically and
experimentally.
",1,0,0,1,0,0
6030,Connecting Software Metrics across Versions to Predict Defects,"  Accurate software defect prediction could help software practitioners
allocate test resources to defect-prone modules effectively and efficiently. In
the last decades, much effort has been devoted to build accurate defect
prediction models, including developing quality defect predictors and modeling
techniques. However, current widely used defect predictors such as code metrics
and process metrics could not well describe how software modules change over
the project evolution, which we believe is important for defect prediction. In
order to deal with this problem, in this paper, we propose to use the
Historical Version Sequence of Metrics (HVSM) in continuous software versions
as defect predictors. Furthermore, we leverage Recurrent Neural Network (RNN),
a popular modeling technique, to take HVSM as the input to build software
prediction models. The experimental results show that, in most cases, the
proposed HVSM-based RNN model has a significantly better effort-aware ranking
effectiveness than the commonly used baseline models.
",1,0,0,0,0,0
14861,Carleman estimates for forward and backward stochastic fourth order Schrödinger equations and their applications,"  In this paper, we establish the Carleman estimates for forward and backward
stochastic fourth order Schrödinger equations, on basis of which, we can
obtain the observability, unique continuation property and the exact
controllability for the forward and backward stochastic fourth order
Schrödinger equations.
",0,0,1,0,0,0
4798,Transição de fase no sistema de Hénon-Heiles (Phase transition in the Henon-Heiles system),"  The Henon-Heiles system was originally proposed to describe the dynamical
behavior of galaxies, but this system has been widely applied in dynamical
systems by exhibit great details in phase space. This work presents the
formalism to describe Henon-Heiles system and a qualitative approach of
dynamics behavior. The growth of chaotic region in phase space is observed by
Poincare Surface of Section when the total energy increases. Island of
regularity remain around stable points and relevants phenomena appear, such as
sticky.
",0,1,0,0,0,0
12576,Transient photon echoes from donor-bound excitons in ZnO epitaxial layers,"  The coherent optical response from 140~nm and 65~nm thick ZnO epitaxial
layers is studied using transient four-wave-mixing spectroscopy with picosecond
temporal resolution. Resonant excitation of neutral donor-bound excitons
results in two-pulse and three-pulse photon echoes. For the donor-bound A
exciton (D$^0$X$_\text{A}$) at temperature of 1.8~K we evaluate optical
coherence times $T_2=33-50$~ps corresponding to homogeneous linewidths of
$13-19~\mu$eV, about two orders of magnitude smaller as compared with the
inhomogeneous broadening of the optical transitions. The coherent dynamics is
determined mainly by the population decay with time $T_1=30-40$~ps, while pure
dephasing is negligible in the studied high quality samples even for strong
optical excitation. Temperature increase leads to a significant shortening of
$T_2$ due to interaction with acoustic phonons. In contrast, the loss of
coherence of the donor-bound B exciton (D$^0$X$_\text{B}$) is significantly
faster ($T_2=3.6$~ps) and governed by pure dephasing processes.
",0,1,0,0,0,0
62,Timed Automata with Polynomial Delay and their Expressiveness,"  We consider previous models of Timed, Probabilistic and Stochastic Timed
Automata, we introduce our model of Timed Automata with Polynomial Delay and we
characterize the expressiveness of these models relative to each other.
",1,0,0,0,0,0
15909,Theoretical investigation of excitonic magnetism in LaSrCoO$_{4}$,"  We use the LDA+U approach to search for possible ordered ground states of
LaSrCoO$_4$. We find a staggered arrangement of magnetic multipoles to be
stable over a broad range of Co $3d$ interaction parameters. This ordered state
can be described as a spin-denity-wave-type condensate of $d_{xy} \otimes
d_{x^2-y^2}$ excitons carrying spin $S=1$. Further, we construct an effective
strong-coupling model, calculate the exciton dispersion and investigate closing
of the exciton gap, which marks the exciton condensation instability. Comparing
the layered LaSrCoO$_4$ with its pseudo cubic analog LaCoO$_3$, we find that
for the same interaction parameters the excitonic gap is smaller (possibly
vanishing) in the layered cobaltite.
",0,1,0,0,0,0
4075,Active matrix completion with uncertainty quantification,"  The noisy matrix completion problem, which aims to recover a low-rank matrix
$\mathbf{X}$ from a partial, noisy observation of its entries, arises in many
statistical, machine learning, and engineering applications. In this paper, we
present a new, information-theoretic approach for active sampling (or
designing) of matrix entries for noisy matrix completion, based on the maximum
entropy design principle. One novelty of our method is that it implicitly makes
use of uncertainty quantification (UQ) -- a measure of uncertainty for
unobserved matrix entries -- to guide the active sampling procedure. The
proposed framework reveals several novel insights on the role of compressive
sensing (e.g., coherence) and coding design (e.g., Latin squares) on the
sampling performance and UQ for noisy matrix completion. Using such insights,
we develop an efficient posterior sampler for UQ, which is then used to guide a
closed-form sampling scheme for matrix entries. Finally, we illustrate the
effectiveness of this integrated sampling / UQ methodology in simulation
studies and two applications to collaborative filtering.
",0,0,0,1,0,0
11282,Phase-Aware Single-Channel Speech Enhancement with Modulation-Domain Kalman Filtering,"  We present a single-channel phase-sensitive speech enhancement algorithm that
is based on modulation-domain Kalman filtering and on tracking the speech phase
using circular statistics. With Kalman filtering, using that speech and noise
are additive in the complex STFT domain, the algorithm tracks the speech
log-spectrum, the noise log-spectrum and the speech phase. Joint amplitude and
phase estimation of speech is performed. Given the noisy speech signal,
conventional algorithms use the noisy phase for signal reconstruction
approximating the speech phase with the noisy phase. In the proposed Kalman
filtering algorithm, the speech phase posterior is used to create an enhanced
speech phase spectrum for signal reconstruction. The Kalman filter prediction
models the temporal/inter-frame correlation of the speech and noise log-spectra
and of the speech phase, while the Kalman filter update models their nonlinear
relations. With the proposed algorithm, speech is tracked and estimated both in
the log-spectral and spectral phase domains. The algorithm is evaluated in
terms of speech quality and different algorithm configurations, dependent on
the signal model, are compared in different noise types. Experimental results
show that the proposed algorithm outperforms traditional enhancement algorithms
over a range of SNRs for various noise types.
",1,0,0,0,0,0
16788,Greater data science at baccalaureate institutions,"  Donoho's JCGS (in press) paper is a spirited call to action for
statisticians, who he points out are losing ground in the field of data science
by refusing to accept that data science is its own domain. (Or, at least, a
domain that is becoming distinctly defined.) He calls on writings by John
Tukey, Bill Cleveland, and Leo Breiman, among others, to remind us that
statisticians have been dealing with data science for years, and encourages
acceptance of the direction of the field while also ensuring that statistics is
tightly integrated.
As faculty at baccalaureate institutions (where the growth of undergraduate
statistics programs has been dramatic), we are keen to ensure statistics has a
place in data science and data science education. In his paper, Donoho is
primarily focused on graduate education. At our undergraduate institutions, we
are considering many of the same questions.
",0,0,0,1,0,0
20322,Adhesion-induced Discontinuous Transitions and Classifying Social Networks,"  Transition points mark qualitative changes in the macroscopic properties of
large complex systems. Explosive transitions, exhibiting properties of both
continuous and discontinuous phase transitions, have recently been uncovered in
network growth processes. Real networks not only grow but often also
restructure, yet common network restructuring processes, such as small world
rewiring, do not exhibit phase transitions. Here, we uncover a class of
intrinsically discontinuous transitions emerging in network restructuring
processes controlled by \emph{adhesion} -- the preference of a chosen link to
remain connected to its end node. Deriving a master equation for the temporal
network evolution and working out an analytic solution, we identify genuinely
discontinuous transitions in non-growing networks, separating qualitatively
distinct phases with monotonic and with peaked degree distributions.
Intriguingly, our analysis of heuristic data indicates a separation between the
same two forms of degree distributions distinguishing abstract from
face-to-face social networks.
",1,0,0,0,0,0
10798,Text-Independent Speaker Verification Using 3D Convolutional Neural Networks,"  In this paper, a novel method using 3D Convolutional Neural Network (3D-CNN)
architecture has been proposed for speaker verification in the text-independent
setting. One of the main challenges is the creation of the speaker models. Most
of the previously-reported approaches create speaker models based on averaging
the extracted features from utterances of the speaker, which is known as the
d-vector system. In our paper, we propose an adaptive feature learning by
utilizing the 3D-CNNs for direct speaker model creation in which, for both
development and enrollment phases, an identical number of spoken utterances per
speaker is fed to the network for representing the speakers' utterances and
creation of the speaker model. This leads to simultaneously capturing the
speaker-related information and building a more robust system to cope with
within-speaker variation. We demonstrate that the proposed method significantly
outperforms the traditional d-vector verification system. Moreover, the
proposed system can also be an alternative to the traditional d-vector system
which is a one-shot speaker modeling system by utilizing 3D-CNNs.
",1,0,0,0,0,0
17292,Stein-like Estimators for Causal Mediation Analysis in Randomized Trials,"  Causal mediation analysis aims to estimate the natural direct and indirect
effects under clearly specified assumptions. Traditional mediation analysis
based on Ordinary Least Squares (OLS) relies on the absence of unmeasured
causes of the putative mediator and outcome. When this assumption cannot be
justified, Instrumental Variables (IV) estimators can be used in order to
produce an asymptotically unbiased estimator of the mediator-outcome link.
However, provided that valid instruments exist, bias removal comes at the cost
of variance inflation for standard IV procedures such as Two-Stage Least
Squares (TSLS). A Semi-Parametric Stein-Like (SPSL) estimator has been proposed
in the literature that strikes a natural trade-off between the unbiasedness of
the TSLS procedure and the relatively small variance of the OLS estimator.
Moreover, the SPSL has the advantage that its shrinkage parameter can be
directly estimated from the data. In this paper, we demonstrate how this
Stein-like estimator can be implemented in the context of the estimation of
natural direct and natural indirect effects of treatments in randomized
controlled trials. The performance of the competing methods is studied in a
simulation study, in which both the strength of hidden confounding and the
strength of the instruments are independently varied. These considerations are
motivated by a trial in mental health evaluating the impact of a primary
care-based intervention to reduce depression in the elderly.
",0,0,0,1,0,0
16378,YUI and HANA: Control and Visualization Programs for HRC in J-PARC,"  We developed control and visualization programs, YUI and HANA, for High-
Resolution Chopper spectrometer (HRC) installed at BL12 in MLF, J-PARC. YUI is
a comprehensive program to control DAQ-middleware, the accessories, and sample
environment devices. HANA is a program for the data transformation and
visualization of inelastic neutron scattering spectra. In this paper, we
describe the basic system structures and unique functions of these programs
from the viewpoint of users.
",0,1,0,0,0,0
20400,Suspension-thermal noise in spring-antispring systems for future gravitational-wave detectors,"  Spring-antispring systems have been investigated as possible low-frequency
seismic isolation in high-precision optical experiments. These systems provide
the possibility to tune the fundamental resonance frequency to, in principle,
arbitrarily low values, and at the same time maintain a compact design of the
isolation system. It was argued though that thermal noise in spring-antispring
systems would not be as small as one may naively expect from lowering the
fundamental resonance frequency. In this paper, we present a detailed
calculation of the suspension thermal noise for a specific spring-antispring
system, namely the Roberts linkage. We find a concise expression of the
suspension thermal noise spectrum, which assumes a form very similar to the
well-known expression for a simple pendulum. It is found that while the Roberts
linkage can provide strong seismic isolation due to a very low fundamental
resonance frequency, its thermal noise is rather determined by the dimension of
the system. We argue that this is true for all horizontal mechanical isolation
systems with spring-antispring dynamics. This imposes strict requirements on
mechanical spring-antispring systems for the seismic isolation in potential
future low-frequency gravitational-wave detectors as we discuss for the four
main concepts: atom-interferometric, superconducting, torsion-bars, and
conventional laser interferometer.
",0,1,0,0,0,0
8375,Second descent and rational points on Kummer varieties,"  A powerful method pioneered by Swinnerton-Dyer allows one to study rational
points on pencils of curves of genus 1 by combining the fibration method with a
sophisticated form of descent. A variant of this method, first used by
Skorobogatov and Swinnerton-Dyer in 2005, can be applied to study rational
points on Kummer varieties. In this paper we extend the method to include an
additional step of second descent. Assuming finiteness of the relevant
Tate-Shafarevich groups, we use the extended method to show that the
Brauer-Manin obstruction is the only obstruction to the Hasse principle on
Kummer varieties associated to abelian varieties with all rational 2-torsion,
under mild additional hypotheses.
",0,0,1,0,0,0
4971,Non-commutative Discretize-then-Optimize Algorithms for Elliptic PDE-Constrained Optimal Control Problems,"  In this paper, we analyze the convergence of several discretize-then-optimize
algorithms, based on either a second-order or a fourth-order finite difference
discretization, for solving elliptic PDE-constrained optimization or optimal
control problems. To ensure the convergence of a discretize-then-optimize
algorithm, one well-accepted criterion is to choose or redesign the
discretization scheme such that the resultant discretize-then-optimize
algorithm commutes with the corresponding optimize-then-discretize algorithm.
In other words, both types of algorithms would give rise to exactly the same
discrete optimality system. However, such an approach is not trivial. In this
work, by investigating a simple distributed elliptic optimal control problem,
we first show that enforcing such a stringent condition of commutative property
is only sufficient but not necessary for achieving the desired convergence. We
then propose to add some suitable $H_1$ semi-norm penalty/regularization terms
to recover the lost convergence due to the inconsistency caused by the loss of
commutativity. Numerical experiments are carried out to verify our theoretical
analysis and also validate the effectiveness of our proposed regularization
techniques.
",0,0,1,0,0,0
12298,Position-sensitive propagation of information on social media using social physics approach,"  The excitement and convergence of tweets on specific topics are well studied.
However, by utilizing the position information of Tweet, it is also possible to
analyze the position-sensitive tweet. In this research, we focus on bomb
terrorist attacks and propose a method for separately analyzing the number of
tweets at the place where the incident occurred, nearby, and far. We made
measurements of position-sensitive tweets and suggested a theory to explain it.
This theory is an extension of the mathematical model of the hit phenomenon.
",1,1,0,0,0,0
17546,Applications of an algorithm for solving Fredholm equations of the first kind,"  In this paper we use an iterative algorithm for solving Fredholm equations of
the first kind. The basic algorithm is known and is based on an EM algorithm
when involved functions are non-negative and integrable. With this algorithm we
demonstrate two examples involving the estimation of a mixing density and a
first passage time density function involving Brownian motion. We also develop
the basic algorithm to include functions which are not necessarily non-negative
and again present illustrations under this scenario. A self contained proof of
convergence of all the algorithms employed is presented.
",0,0,1,1,0,0
20339,Detecting Changes in Time Series Data using Volatility Filters,"  This work develops techniques for the sequential detection and location
estimation of transient changes in the volatility (standard deviation) of time
series data. In particular, we introduce a class of change detection algorithms
based on the windowed volatility filter. The first method detects changes by
employing a convex combination of two such filters with differing window sizes,
such that the adaptively updated convex weight parameter is then used as an
indicator for the detection of instantaneous power changes. Moreover, the
proposed adaptive filtering based method is readily extended to the
multivariate case by using recent advances in distributed adaptive filters,
thereby using cooperation between the data channels for more effective
detection of change points. Furthermore, this work also develops a novel change
point location estimator based on the differenced output of the volatility
filter. Finally, the performance of the proposed methods were evaluated on both
synthetic and real world data.
",1,0,0,0,0,0
7833,Fractional integrals and Fourier transforms,"  This paper gives a short survey of some basic results related to estimates of
fractional integrals and Fourier transforms. It is closely adjoint to our
previous survey papers \cite{K1998} and \cite{K2007}. The main methods used in
the paper are based on nonincreasing rearrangements. We give alternative proofs
of some results.
We observe also that the paper represents the mini-course given by the author
at Barcelona University in October, 2014.
",0,0,1,0,0,0
15558,"Refractive index measurements of single, spherical cells using digital holographic microscopy","  In this chapter, we introduce digital holographic microscopy (DHM) as a
marker-free method to determine the refractive index of single, spherical cells
in suspension. The refractive index is a conclusive measure in a biological
context. Cell conditions, such as differentiation or infection, are known to
yield significant changes in the refractive index. Furthermore, the refractive
index of biological tissue determines the way it interacts with light. Besides
the biological relevance of this interaction in the retina, a lot of methods
used in biology, including microscopy, rely on light-tissue or light-cell
interactions. Hence, determining the refractive index of cells using DHM is
valuable in many biological applications. This chapter covers the main topics
which are important for the implementation of DHM: setup, sample preparation
and analysis. First, the optical setup is described in detail including notes
and suggestions for the implementation. Following that, a protocol for the
sample and measurement preparation is explained. In the analysis section, an
algorithm for the determination of the quantitative phase map is described.
Subsequently, all intermediate steps for the calculation of the refractive
index of suspended cells are presented, exploiting their spherical shape. In
the last section, a discussion of possible extensions to the setup, further
measurement configurations and additional analysis methods are given.
Throughout this chapter, we describe a simple, robust, and thus easily
reproducible implementation of DHM. The different possibilities for extensions
show the diverse fields of application for this technique.
",0,0,0,0,1,0
13754,Evidence of Significant Energy Input in the Late Phase of a Solar Flare from NuSTAR X-Ray Observations,"  We present observations of the occulted active region AR12222 during the
third {\em NuSTAR} solar campaign on 2014 December 11, with concurrent {\em
SDO/}AIA and {\em FOXSI-2} sounding rocket observations. The active region
produced a medium size solar flare one day before the observations, at
$\sim18$UT on 2014 December 10, with the post-flare loops still visible at the
time of {\em NuSTAR} observations. The time evolution of the source emission in
the {\em SDO/}AIA $335\textrm{\AA}$ channel reveals the characteristics of an
extreme-ultraviolet late phase event, caused by the continuous formation of new
post-flare loops that arch higher and higher in the solar corona. The spectral
fitting of {\em NuSTAR} observations yields an isothermal source, with
temperature $3.8-4.6$ MK, emission measure $0.3-1.8 \times 10^{46}\textrm{
cm}^{-3}$, and density estimated at $2.5-6.0 \times 10^8 \textrm{ cm}^{-3}$.
The observed AIA fluxes are consistent with the derived {\em NuSTAR}
temperature range, favoring temperature values in the range $4.0-4.3$ MK. By
examining the post-flare loops' cooling times and energy content, we estimate
that at least 12 sets of post-flare loops were formed and subsequently cooled
between the onset of the flare and {\em NuSTAR} observations, with their total
thermal energy content an order of magnitude larger than the energy content at
flare peak time. This indicates that the standard approach of using only the
flare peak time to derive the total thermal energy content of a flare can lead
to a large underestimation of its value.
",0,1,0,0,0,0
6401,Generalized weighted Ostrowski and Ostrowski-Grüss type inequalities on time scales via a parameter function,"  We prove generalized weighted Ostrowski and Ostrowski--Grüss type
inequalities on time scales via a parameter function. In particular, our result
extends a result of Dragomir and Barnett. Furthermore, we apply our results to
the continuous, discrete, and quantum cases, to obtain some interesting new
inequalities.
",0,0,1,0,0,0
17079,Variable-Length Resolvability for General Sources and Channels,"  We introduce the problem of variable-length source resolvability, where a
given target probability distribution is approximated by encoding a
variable-length uniform random number, and the asymptotically minimum average
length rate of the uniform random numbers, called the (variable-length)
resolvability, is investigated. We first analyze the variable-length
resolvability with the variational distance as an approximation measure. Next,
we investigate the case under the divergence as an approximation measure. When
the asymptotically exact approximation is required, it is shown that the
resolvability under the two kinds of approximation measures coincides. We then
extend the analysis to the case of channel resolvability, where the target
distribution is the output distribution via a general channel due to the fixed
general source as an input. The obtained characterization of the channel
resolvability is fully general in the sense that when the channel is just the
identity mapping, the characterization reduces to the general formula for the
source resolvability. We also analyze the second-order variable-length
resolvability.
",1,0,0,0,0,0
14465,Transient phenomena in a three-layer waveguide and the analytical structure of the dispersion diagram,"  Excitation of waves in a three-layer acoustic wavegide is studied. The wave
field is presented as a sum of integrals. The summation is held over all
waveguide modes. The integration is performed over the temporal frequency axis.
The dispersion diagram of the waveguide is analytically continued, and the
integral is transformed by deformation of the integration contour into the
domain of complex frequencies. As the result, the expression for the fast
components of the signal (i.e. for the transient fields) is simplified.
The structure of the Riemann surface of the dispersion diagram of the
waveguide is studied. For this, a family of auxiliary problems indexed by the
parameters describing the links between layers is introduced. The family
depends on the linking parameters analytically, and the limiting case of weak
links can be solved analytically.
",0,0,1,0,0,0
20631,Subspace Tracking Algorithms for Millimeter Wave MIMO Channel Estimation with Hybrid Beamforming,"  This paper proposes the use of subspace tracking algorithms for performing
MIMO channel estimation at millimeter wave (mmWave) frequencies. Using a
subspace approach, we develop a protocol enabling the estimation of the right
(resp. left) singular vectors at the transmitter (resp. receiver) side; then,
we adapt the projection approximation subspace tracking with deflation (PASTd)
and the orthogonal Oja (OOJA) algorithms to our framework and obtain two
channel estimation algorithms. The hybrid analog/digital nature of the
beamformer is also explicitly taken into account at the algorithm design stage.
Numerical results show that the proposed estimation algorithms are effective,
and that they perform better than two relevant competing alternatives available
in the open literature.
",1,0,0,0,0,0
19956,Instabilities in Interacting Binary Stars,"  The types of instability in the interacting binary stars are reviewed. The
project ""Inter-Longitude Astronomy"" is a series of smaller projects on concrete
stars or groups of stars. It has no special funds, and is supported from
resources and grants of participating organizations, when informal working
groups are created. Totally we studied 1900+ variable stars of different types.
The characteristic timescale is from seconds to decades and (extrapolating)
even more. The monitoring of the first star of our sample AM Her was initiated
by Prof. V.P. Tsesevich (1907-1983). Since more than 358 ADS papers were
published. Some highlights of our photometric and photo-polarimetric monitoring
and mathematical modelling of interacting binary stars of different types are
presented: classical, asynchronous, intermediate polars and magnetic dwarf
novae (DO Dra) with 25 timescales corresponding to different physical
mechanisms and their combinations (part ""Polar""); negative and positive
superhumpers in nova-like and many dwarf novae stars (""Superhumper""); eclipsing
""non-magnetic"" cataclysmic variables; symbiotic systems (""Symbiosis"");
super-soft sources (SSS, QR And); spotted (and not spotted) eclipsing variables
with (and without) evidence for a current mass transfer (""Eclipser"") with a
special emphasis on systems with a direct impact of the stream into the gainer
star's atmosphere, or V361 Lyr-type stars. Other parts of the ILA project are
""Stellar Bell"" (interesting pulsating variables of different types and periods
- M, SR, RV Tau, RR Lyr, Delta Sct) and ""Novice""(=""New Variable"") discoveries
and classification with a subsequent monitoring for searching and studying
possible multiple components of variability. Special mathematical methods have
been developed to create a set of complementary software for statistically
optimal modelling of variable stars of different types.
",0,1,0,0,0,0
17444,Estimation Considerations in Contextual Bandits,"  Contextual bandit algorithms are sensitive to the estimation method of the
outcome model as well as the exploration method used, particularly in the
presence of rich heterogeneity or complex outcome models, which can lead to
difficult estimation problems along the path of learning. We study a
consideration for the exploration vs. exploitation framework that does not
arise in multi-armed bandits but is crucial in contextual bandits; the way
exploration and exploitation is conducted in the present affects the bias and
variance in the potential outcome model estimation in subsequent stages of
learning. We develop parametric and non-parametric contextual bandits that
integrate balancing methods from the causal inference literature in their
estimation to make it less prone to problems of estimation bias. We provide the
first regret bound analyses for contextual bandits with balancing in the domain
of linear contextual bandits that match the state of the art regret bounds. We
demonstrate the strong practical advantage of balanced contextual bandits on a
large number of supervised learning datasets and on a synthetic example that
simulates model mis-specification and prejudice in the initial training data.
Additionally, we develop contextual bandits with simpler assignment policies by
leveraging sparse model estimation methods from the econometrics literature and
demonstrate empirically that in the early stages they can improve the rate of
learning and decrease regret.
",1,0,0,1,0,0
4972,Gate-Variants of Gated Recurrent Unit (GRU) Neural Networks,"  The paper evaluates three variants of the Gated Recurrent Unit (GRU) in
recurrent neural networks (RNN) by reducing parameters in the update and reset
gates. We evaluate the three variant GRU models on MNIST and IMDB datasets and
show that these GRU-RNN variant models perform as well as the original GRU RNN
model while reducing the computational expense.
",1,0,0,1,0,0
7769,How Wrong Am I? - Studying Adversarial Examples and their Impact on Uncertainty in Gaussian Process Machine Learning Models,"  Machine learning models are vulnerable to Adversarial Examples: minor
perturbations to input samples intended to deliberately cause
misclassification. Current defenses against adversarial examples, especially
for Deep Neural Networks (DNN), are primarily derived from empirical
developments, and their security guarantees are often only justified
retroactively. Many defenses therefore rely on hidden assumptions that are
subsequently subverted by increasingly elaborate attacks. This is not
surprising: deep learning notoriously lacks a comprehensive mathematical
framework to provide meaningful guarantees.
In this paper, we leverage Gaussian Processes to investigate adversarial
examples in the framework of Bayesian inference. Across different models and
datasets, we find deviating levels of uncertainty reflect the perturbation
introduced to benign samples by state-of-the-art attacks, including novel
white-box attacks on Gaussian Processes. Our experiments demonstrate that even
unoptimized uncertainty thresholds already reject adversarial examples in many
scenarios.
Comment: Thresholds can be broken in a modified attack, which was done in
arXiv:1812.02606 (The limitations of model uncertainty in adversarial
settings).
",1,0,0,1,0,0
4282,The careless use of language in quantum information,"  An imperative aspect of modern science is that scientific institutions act
for the benefit of a common scientific enterprise, rather than for the personal
gain of individuals within them. This implies that science should not
perpetuate existing or historical unequal social orders. Some scientific
terminology, though, gives a very different impression. I will give two
examples of terminology invented recently for the field of quantum information
which use language associated with subordination, slavery, and racial
segregation: 'ancilla qubit' and 'quantum supremacy'.
",0,1,0,0,0,0
12158,Classifying Time-Varying Complex Networks on the Tensor Manifold,"  At the core of understanding dynamical systems is the ability to maintain and
control the systems behavior that includes notions of robustness,
heterogeneity, and/or regime-shift detection. Recently, to explore such
functional properties, a convenient representation has been to model such
dynamical systems as a weighted graph consisting of a finite, but very large
number of interacting agents. This said, there exists very limited relevant
statistical theory that is able cope with real-life data, i.e., how does
perform simple analysis and/or statistics over a family of networks as opposed
to a specific network or network-to-network variation. Here, we are interested
in the analysis of network families whereby each network represents a point on
an underlying statistical manifold. From this, we explore the Riemannian
structure of the statistical (tensor) manifold in order to define notions of
geodesics or shortest distance amongst such points as well as a statistical
framework for time-varying complex networks for which we can utilize in higher
order classification tasks.
",1,0,0,0,0,0
70,Characterizations of quasitrivial symmetric nondecreasing associative operations,"  In this paper we are interested in the class of n-ary operations on an
arbitrary chain that are quasitrivial, symmetric, nondecreasing, and
associative. We first provide a description of these operations. We then prove
that associativity can be replaced with bisymmetry in the definition of this
class. Finally we investigate the special situation where the chain is finite.
",0,0,1,0,0,0
18184,An effective algorithm for hyperparameter optimization of neural networks,"  A major challenge in designing neural network (NN) systems is to determine
the best structure and parameters for the network given the data for the
machine learning problem at hand. Examples of parameters are the number of
layers and nodes, the learning rates, and the dropout rates. Typically, these
parameters are chosen based on heuristic rules and manually fine-tuned, which
may be very time-consuming, because evaluating the performance of a single
parametrization of the NN may require several hours. This paper addresses the
problem of choosing appropriate parameters for the NN by formulating it as a
box-constrained mathematical optimization problem, and applying a
derivative-free optimization tool that automatically and effectively searches
the parameter space. The optimization tool employs a radial basis function
model of the objective function (the prediction accuracy of the NN) to
accelerate the discovery of configurations yielding high accuracy. Candidate
configurations explored by the algorithm are trained to a small number of
epochs, and only the most promising candidates receive full training. The
performance of the proposed methodology is assessed on benchmark sets and in
the context of predicting drug-drug interactions, showing promising results.
The optimization tool used in this paper is open-source.
",1,0,0,0,0,0
7463,In-situ Optical Characterization of Noble Metal Thin Film Deposition and Development of a High-performance Plasmonic Sensor,"  The present work addressed in this thesis introduces, for the first time, the
use of tilted fiber Bragg grating (TFBG) sensors for accurate, real-time, and
in-situ characterization of CVD and ALD processes for noble metals, but with a
particular focus on gold due to its desirable optical and plasmonic properties.
Through the use of orthogonally-polarized transverse electric (TE) and
transverse magnetic (TM) resonance modes imposed by a boundary condition at the
cladding-metal interface of the optical fiber, polarization-dependent
resonances excited by the TFBG are easily decoupled. It was found that for
ultrathin thicknesses of gold films from CVD (~6-65 nm), the anisotropic
property of these films made it non-trivial to characterize their effective
optical properties such as the real component of the permittivity.
Nevertheless, the TFBG introduces a new sensing platform to the ALD and CVD
community for extremely sensitive in-situ process monitoring. We later also
demonstrate thin film growth at low (<10 cycle) numbers for the well-known
Al2O3 thermal ALD process, as well as the plasma-enhanced gold ALD process.
Finally, the use of ALD-grown gold coatings has been employed for the
development of a plasmonic TFBG-based sensor with ultimate refractometric
sensitivity (~550 nm/RIU).
",0,1,0,0,0,0
10714,Optimisation approach for the Monge-Ampere equation,"  This paper studies the numerical approximation of solution of the Dirichlet
problem for the fully nonlinear Monge-Ampere equation. In this approach, we
take the advantage of reformulation the Monge-Ampere problem as an optimization
problem, to which we associate a well defined functional whose minimum provides
us with the solution to the Monge-Ampere problem after resolving a Poisson
problem by the finite element Galerkin method. We present some numerical
examples, for which a good approximation is obtained in 68 iterations.
",0,0,1,0,0,0
6398,General Bayesian Updating and the Loss-Likelihood Bootstrap,"  In this paper we revisit the weighted likelihood bootstrap, a method that
generates samples from an approximate Bayesian posterior of a parametric model.
We show that the same method can be derived, without approximation, under a
Bayesian nonparametric model with the parameter of interest defined as
minimising an expected negative log-likelihood under an unknown sampling
distribution. This interpretation enables us to extend the weighted likelihood
bootstrap to posterior sampling for parameters minimizing an expected loss. We
call this method the loss-likelihood bootstrap. We make a connection between
this and general Bayesian updating, which is a way of updating prior belief
distributions without needing to construct a global probability model, yet
requires the calibration of two forms of loss function. The loss-likelihood
bootstrap is used to calibrate the general Bayesian posterior by matching
asymptotic Fisher information. We demonstrate the methodology on a number of
examples.
",0,0,0,1,0,0
4397,Probing magnetism in the vortex phase of PuCoGa$_5$ by X-ray magnetic circular dichroism,"  We have measured X-ray magnetic circular dichroism (XMCD) spectra at the Pu
$M_{4,5}$ absorption edges from a newly-prepared high-quality single crystal of
the heavy fermion superconductor $^{242}$PuCoGa$_{5}$, exhibiting a critical
temperature $T_{c} = 18.7~{\rm K}$. The experiment probes the vortex phase
below $T_{c}$ and shows that an external magnetic field induces a Pu 5$f$
magnetic moment at 2 K equal to the temperature-independent moment measured in
the normal phase up to 300 K by a SQUID device. This observation is in
agreement with theoretical models claiming that the Pu atoms in PuCoGa$_{5}$
have a nonmagnetic singlet ground state resulting from the hybridization of the
conduction electrons with the intermediate-valence 5$f$ electronic shell.
Unexpectedly, XMCD spectra show that the orbital component of the $5f$ magnetic
moment increases significantly between 30 and 2 K; the antiparallel spin
component increases as well, leaving the total moment practically constant. We
suggest that this indicates a low-temperature breakdown of the complete
Kondo-like screening of the local 5$f$ moment.
",0,1,0,0,0,0
15345,"A New Convolutional Network-in-Network Structure and Its Applications in Skin Detection, Semantic Segmentation, and Artifact Reduction","  The inception network has been shown to provide good performance on image
classification problems, but there are not much evidences that it is also
effective for the image restoration or pixel-wise labeling problems. For image
restoration problems, the pooling is generally not used because the decimated
features are not helpful for the reconstruction of an image as the output.
Moreover, most deep learning architectures for the restoration problems do not
use dense prediction that need lots of training parameters. From these
observations, for enjoying the performance of inception-like structure on the
image based problems we propose a new convolutional network-in-network
structure. The proposed network can be considered a modification of inception
structure where pool projection and pooling layer are removed for maintaining
the entire feature map size, and a larger kernel filter is added instead.
Proposed network greatly reduces the number of parameters on account of removed
dense prediction and pooling, which is an advantage, but may also reduce the
receptive field in each layer. Hence, we add a larger kernel than the original
inception structure for not increasing the depth of layers. The proposed
structure is applied to typical image-to-image learning problems, i.e., the
problems where the size of input and output are same such as skin detection,
semantic segmentation, and compression artifacts reduction. Extensive
experiments show that the proposed network brings comparable or better results
than the state-of-the-art convolutional neural networks for these problems.
",1,0,0,0,0,0
20428,Imbalanced Malware Images Classification: a CNN based Approach,"  Deep convolutional neural networks (CNNs) can be applied to malware binary
detection through images classification. The performance, however, is degraded
due to the imbalance of malware families (classes). To mitigate this issue, we
propose a simple yet effective weighted softmax loss which can be employed as
the final layer of deep CNNs. The original softmax loss is weighted, and the
weight value can be determined according to class size. A scaling parameter is
also included in computing the weight. Proper selection of this parameter has
been studied and an empirical option is given. The weighted loss aims at
alleviating the impact of data imbalance in an end-to-end learning fashion. To
validate the efficacy, we deploy the proposed weighted loss in a pre-trained
deep CNN model and fine-tune it to achieve promising results on malware images
classification. Extensive experiments also indicate that the new loss function
can fit other typical CNNs with an improved classification performance.
",1,0,0,1,0,0
6008,Synthesizing Bijective Lenses,"  Bidirectional transformations between different data representations occur
frequently in modern software systems. They appear as serializers and
deserializers, as database views and view updaters, and more. Manually building
bidirectional transformations---by writing two separate functions that are
intended to be inverses---is tedious and error prone. A better approach is to
use a domain-specific language in which both directions can be written as a
single expression. However, these domain-specific languages can be difficult to
program in, requiring programmers to manage fiddly details while working in a
complex type system.
To solve this, we present Optician, a tool for type-directed synthesis of
bijective string transformers. The inputs to Optician are two ordinary regular
expressions representing two data formats and a few concrete examples for
disambiguation. The output is a well-typed program in Boomerang (a
bidirectional language based on the theory of lenses). The main technical
challenge involves navigating the vast program search space efficiently enough.
Unlike most prior work on type-directed synthesis, our system operates in the
context of a language with a rich equivalence relation on types (the theory of
regular expressions). We synthesize terms of a equivalent language and convert
those generated terms into our lens language. We prove the correctness of our
synthesis algorithm. We also demonstrate empirically that our new language
changes the synthesis problem from one that admits intractable solutions to one
that admits highly efficient solutions. We evaluate Optician on a benchmark
suite of 39 examples including both microbenchmarks and realistic examples
derived from other data management systems including Flash Fill, a tool for
synthesizing string transformations in spreadsheets, and Augeas, a tool for
bidirectional processing of Linux system configuration files.
",1,0,0,0,0,0
2102,Resilience: A Criterion for Learning in the Presence of Arbitrary Outliers,"  We introduce a criterion, resilience, which allows properties of a dataset
(such as its mean or best low rank approximation) to be robustly computed, even
in the presence of a large fraction of arbitrary additional data. Resilience is
a weaker condition than most other properties considered so far in the
literature, and yet enables robust estimation in a broader variety of settings.
We provide new information-theoretic results on robust distribution learning,
robust estimation of stochastic block models, and robust mean estimation under
bounded $k$th moments. We also provide new algorithmic results on robust
distribution learning, as well as robust mean estimation in $\ell_p$-norms.
Among our proof techniques is a method for pruning a high-dimensional
distribution with bounded $1$st moments to a stable ""core"" with bounded $2$nd
moments, which may be of independent interest.
",1,0,0,1,0,0
11305,Fast and Accurate Semantic Mapping through Geometric-based Incremental Segmentation,"  We propose an efficient and scalable method for incrementally building a
dense, semantically annotated 3D map in real-time. The proposed method assigns
class probabilities to each region, not each element (e.g., surfel and voxel),
of the 3D map which is built up through a robust SLAM framework and
incrementally segmented with a geometric-based segmentation method. Differently
from all other approaches, our method has a capability of running at over 30Hz
while performing all processing components, including SLAM, segmentation, 2D
recognition, and updating class probabilities of each segmentation label at
every incoming frame, thanks to the high efficiency that characterizes the
computationally intensive stages of our framework. By utilizing a specifically
designed CNN to improve the frame-wise segmentation result, we can also achieve
high accuracy. We validate our method on the NYUv2 dataset by comparing with
the state of the art in terms of accuracy and computational efficiency, and by
means of an analysis in terms of time and space complexity.
",1,0,0,0,0,0
9064,AI4AI: Quantitative Methods for Classifying Host Species from Avian Influenza DNA Sequence,"  Avian Influenza breakouts cause millions of dollars in damage each year
globally, especially in Asian countries such as China and South Korea. The
impact magnitude of a breakout directly correlates to time required to fully
understand the influenza virus, particularly the interspecies pathogenicity.
The procedure requires laboratory tests that require resources typically
lacking in a breakout emergency. In this study, we propose new quantitative
methods utilizing machine learning and deep learning to correctly classify host
species given raw DNA sequence data of the influenza virus, and provide
probabilities for each classification. The best deep learning models achieve
top-1 classification accuracy of 47%, and top-3 classification accuracy of 82%,
on a dataset of 11 host species classes.
",0,0,0,1,1,0
819,On the K-theory of C*-algebras for substitution tilings (a pedestrian version),"  Under suitable conditions, a substitution tiling gives rise to a Smale space,
from which three equivalence relations can be constructed, namely the stable,
unstable, and asymptotic equivalence relations. We denote with $S$, $U$, and
$A$ their corresponding $C^*$-algebras in the sense of Renault. In this article
we show that the $K$-theories of $S$ and $U$ can be computed from the
cohomology and homology of a single cochain complex with connecting maps for
tilings of the line and of the plane. Moreover, we provide formulas to compute
the $K$-theory for these three $C^*$-algebras. Furthermore, we show that the
$K$-theory groups for tilings of dimension 1 are always torsion free. For
tilings of dimension 2, only $K_0(U)$ and $K_1(S)$ can contain torsion.
",0,0,1,0,0,0
19098,A Computational Approach to Extinction Events in Chemical Reaction Networks with Discrete State Spaces,"  Recent work of M.D. Johnston et al. has produced sufficient conditions on the
structure of a chemical reaction network which guarantee that the corresponding
discrete state space system exhibits an extinction event. The conditions
consist of a series of systems of equalities and inequalities on the edges of a
modified reaction network called a domination-expanded reaction network. In
this paper, we present a computational implementation of these conditions
written in Python and apply the program on examples drawn from the biochemical
literature, including a model of polyamine metabolism in mammals and a model of
the pentose phosphate pathway in Trypanosoma brucei. We also run the program on
458 models from the European Bioinformatics Institute's BioModels Database and
report our results.
",0,0,1,0,0,0
18209,Classifying the Correctness of Generated White-Box Tests: An Exploratory Study,"  White-box test generator tools rely only on the code under test to select
test inputs, and capture the implementation's output as assertions. If there is
a fault in the implementation, it could get encoded in the generated tests.
Tool evaluations usually measure fault-detection capability using the number of
such fault-encoding tests. However, these faults are only detected, if the
developer can recognize that the encoded behavior is faulty. We designed an
exploratory study to investigate how developers perform in classifying
generated white-box test as faulty or correct. We carried out the study in a
laboratory setting with 54 graduate students. The tests were generated for two
open-source projects with the help of the IntelliTest tool. The performance of
the participants were analyzed using binary classification metrics and by
coding their observed activities. The results showed that participants
incorrectly classified a large number of both fault-encoding and correct tests
(with median misclassification rate 33% and 25% respectively). Thus the real
fault-detection capability of test generators could be much lower than
typically reported, and we suggest to take this human factor into account when
evaluating generated white-box tests.
",1,0,0,0,0,0
14673,ZOO: Zeroth Order Optimization based Black-box Attacks to Deep Neural Networks without Training Substitute Models,"  Deep neural networks (DNNs) are one of the most prominent technologies of our
time, as they achieve state-of-the-art performance in many machine learning
tasks, including but not limited to image classification, text mining, and
speech processing. However, recent research on DNNs has indicated
ever-increasing concern on the robustness to adversarial examples, especially
for security-critical tasks such as traffic sign identification for autonomous
driving. Studies have unveiled the vulnerability of a well-trained DNN by
demonstrating the ability of generating barely noticeable (to both human and
machines) adversarial images that lead to misclassification. Furthermore,
researchers have shown that these adversarial images are highly transferable by
simply training and attacking a substitute model built upon the target model,
known as a black-box attack to DNNs.
Similar to the setting of training substitute models, in this paper we
propose an effective black-box attack that also only has access to the input
(images) and the output (confidence scores) of a targeted DNN. However,
different from leveraging attack transferability from substitute models, we
propose zeroth order optimization (ZOO) based attacks to directly estimate the
gradients of the targeted DNN for generating adversarial examples. We use
zeroth order stochastic coordinate descent along with dimension reduction,
hierarchical attack and importance sampling techniques to efficiently attack
black-box models. By exploiting zeroth order optimization, improved attacks to
the targeted DNN can be accomplished, sparing the need for training substitute
models and avoiding the loss in attack transferability. Experimental results on
MNIST, CIFAR10 and ImageNet show that the proposed ZOO attack is as effective
as the state-of-the-art white-box attack and significantly outperforms existing
black-box attacks via substitute models.
",1,0,0,1,0,0
20559,Statistical Speech Enhancement Based on Probabilistic Integration of Variational Autoencoder and Non-Negative Matrix Factorization,"  This paper presents a statistical method of single-channel speech enhancement
that uses a variational autoencoder (VAE) as a prior distribution on clean
speech. A standard approach to speech enhancement is to train a deep neural
network (DNN) to take noisy speech as input and output clean speech. Although
this supervised approach requires a very large amount of pair data for
training, it is not robust against unknown environments. Another approach is to
use non-negative matrix factorization (NMF) based on basis spectra trained on
clean speech in advance and those adapted to noise on the fly. This
semi-supervised approach, however, causes considerable signal distortion in
enhanced speech due to the unrealistic assumption that speech spectrograms are
linear combinations of the basis spectra. Replacing the poor linear generative
model of clean speech in NMF with a VAE---a powerful nonlinear deep generative
model---trained on clean speech, we formulate a unified probabilistic
generative model of noisy speech. Given noisy speech as observed data, we can
sample clean speech from its posterior distribution. The proposed method
outperformed the conventional DNN-based method in unseen noisy environments.
",1,0,0,1,0,0
7167,On a class of integrable systems of Monge-Ampère type,"  We investigate a class of multi-dimensional two-component systems of
Monge-Ampère type that can be viewed as generalisations of heavenly-type
equations appearing in self-dual Ricci-flat geometry. Based on the
Jordan-Kronecker theory of skew-symmetric matrix pencils, a classification of
normal forms of such systems is obtained. All two-component systems of
Monge-Ampère type turn out to be integrable, and can be represented as the
commutativity conditions of parameter-dependent vector fields. Geometrically,
systems of Monge-Ampère type are associated with linear sections of the
Grassmannians. This leads to an invariant differential-geometric
characterisation of the Monge-Ampère property.
",0,1,1,0,0,0
7605,An Empirical Study on Team Formation in Online Games,"  Online games provide a rich recording of interactions that can contribute to
our understanding of human behavior. One potential lesson is to understand what
motivates people to choose their teammates and how their choices leadto
performance. We examine several hypotheses about team formation using a large,
longitudinal dataset from a team-based online gaming environment. Specifically,
we test how positive familiarity, homophily, and competence determine team
formationin Battlefield 4, a popular team-based game in which players choose
one of two competing teams to play on. Our dataset covers over two months of
in-game interactions between over 380,000 players. We show that familiarity is
an important factorin team formation, while homophily is not. Competence
affects team formation in more nuanced ways: players with similarly high
competence team-up repeatedly, but large variations in competence discourage
repeated interactions.
",1,0,0,0,0,0
361,XFlow: 1D-2D Cross-modal Deep Neural Networks for Audiovisual Classification,"  We propose two multimodal deep learning architectures that allow for
cross-modal dataflow (XFlow) between the feature extractors, thereby extracting
more interpretable features and obtaining a better representation than through
unimodal learning, for the same amount of training data. These models can
usefully exploit correlations between audio and visual data, which have a
different dimensionality and are therefore nontrivially exchangeable. Our work
improves on existing multimodal deep learning metholodogies in two essential
ways: (1) it presents a novel method for performing cross-modality (before
features are learned from individual modalities) and (2) extends the previously
proposed cross-connections, which only transfer information between streams
that process compatible data. Both cross-modal architectures outperformed their
baselines (by up to 7.5%) when evaluated on the AVletters dataset.
",1,0,0,1,0,0
6314,On Detecting Adversarial Perturbations,"  Machine learning and deep learning in particular has advanced tremendously on
perceptual tasks in recent years. However, it remains vulnerable against
adversarial perturbations of the input that have been crafted specifically to
fool the system while being quasi-imperceptible to a human. In this work, we
propose to augment deep neural networks with a small ""detector"" subnetwork
which is trained on the binary classification task of distinguishing genuine
data from data containing adversarial perturbations. Our method is orthogonal
to prior work on addressing adversarial perturbations, which has mostly focused
on making the classification network itself more robust. We show empirically
that adversarial perturbations can be detected surprisingly well even though
they are quasi-imperceptible to humans. Moreover, while the detectors have been
trained to detect only a specific adversary, they generalize to similar and
weaker adversaries. In addition, we propose an adversarial attack that fools
both the classifier and the detector and a novel training procedure for the
detector that counteracts this attack.
",1,0,0,1,0,0
6861,General Dynamics of Spinors,"  In this paper, we consider a general twisted-curved space-time hosting Dirac
spinors and we take into account the Lorentz covariant polar decomposition of
the Dirac spinor field: the corresponding decomposition of the Dirac spinor
field equation leads to a set of field equations that are real and where
spinorial components have disappeared while still maintaining Lorentz
covariance. We will see that the Dirac spinor will contain two real scalar
degrees of freedom, the module and the so-called Yvon-Takabayashi angle, and we
will display their field equations. This will permit us to study the coupling
of curvature and torsion respectively to the module and the YT angle.
",0,1,0,0,0,0
3030,Introspection: Accelerating Neural Network Training By Learning Weight Evolution,"  Neural Networks are function approximators that have achieved
state-of-the-art accuracy in numerous machine learning tasks. In spite of their
great success in terms of accuracy, their large training time makes it
difficult to use them for various tasks. In this paper, we explore the idea of
learning weight evolution pattern from a simple network for accelerating
training of novel neural networks. We use a neural network to learn the
training pattern from MNIST classification and utilize it to accelerate
training of neural networks used for CIFAR-10 and ImageNet classification. Our
method has a low memory footprint and is computationally efficient. This method
can also be used with other optimizers to give faster convergence. The results
indicate a general trend in the weight evolution during training of neural
networks.
",1,0,0,0,0,0
16840,Network Flow Based Post Processing for Sales Diversity,"  Collaborative filtering is a broad and powerful framework for building
recommendation systems that has seen widespread adoption. Over the past decade,
the propensity of such systems for favoring popular products and thus creating
echo chambers have been observed. This has given rise to an active area of
research that seeks to diversify recommendations generated by such algorithms.
We address the problem of increasing diversity in recommendation systems that
are based on collaborative filtering that use past ratings to predicting a
rating quality for potential recommendations. Following our earlier work, we
formulate recommendation system design as a subgraph selection problem from a
candidate super-graph of potential recommendations where both diversity and
rating quality are explicitly optimized: (1) On the modeling side, we define a
new flexible notion of diversity that allows a system designer to prescribe the
number of recommendations each item should receive, and smoothly penalizes
deviations from this distribution. (2) On the algorithmic side, we show that
minimum-cost network flow methods yield fast algorithms in theory and practice
for designing recommendation subgraphs that optimize this notion of diversity.
(3) On the empirical side, we show the effectiveness of our new model and
method to increase diversity while maintaining high rating quality in standard
rating data sets from Netflix and MovieLens.
",1,0,0,0,0,0
13627,Design and performance of dual-polarization lumped-element kinetic inductance detectors for millimeter-wave polarimetry,"  Lumped-element kinetic inductance detectors (LEKIDs) are an attractive
technology for millimeter-wave observations that require large arrays of
extremely low-noise detectors. We designed, fabricated and characterized
64-element (128 LEKID) arrays of horn-coupled, dual-polarization LEKIDs
optimized for ground-based CMB polarimetry. Our devices are sensitive to two
orthogonal polarizations in a single spectral band centered on 150 GHz with
$\Delta\nu/\nu=0.2$. The $65\times 65$ mm square arrays are designed to be
tiled into the focal plane of an optical system. We demonstrate the viability
of these dual-polarization LEKIDs with laboratory measurements. The LEKID
modules are tested with an FPGA-based readout system in a sub-kelvin cryostat
that uses a two-stage adiabatic demagnetization refrigerator. The devices are
characterized using a blackbody and a millimeter-wave source. The polarization
properties are measured with a cryogenic stepped half-wave plate. We measure
the resonator parameters and the detector sensitivity, noise spectrum, dynamic
range, and polarization response. The resonators have internal quality factors
approaching $1\times 10^{6}$. The detectors have uniform response between
orthogonal polarizations and a large dynamic range. The detectors are
photon-noise limited above 1 pW of absorbed power. The noise-equivalent
temperatures under a 3.4 K blackbody load are $<100~\mu\mathrm{K\sqrt{s}}$. The
polarization fractions of detectors sensitive to orthogonal polarizations are
>80%. The entire array is multiplexed on a single readout line, demonstrating a
multiplexing factor of 128. The array and readout meet the requirements for 4
arrays to be read out simultaneously for a multiplexing factor of 512. This
laboratory study demonstrates the first dual-polarization LEKID array optimized
for CMB polarimetry and shows the readiness of the detectors for on-sky
observations.
",0,1,0,0,0,0
11044,Temporal Multimodal Fusion for Video Emotion Classification in the Wild,"  This paper addresses the question of emotion classification. The task
consists in predicting emotion labels (taken among a set of possible labels)
best describing the emotions contained in short video clips. Building on a
standard framework -- lying in describing videos by audio and visual features
used by a supervised classifier to infer the labels -- this paper investigates
several novel directions. First of all, improved face descriptors based on 2D
and 3D Convo-lutional Neural Networks are proposed. Second, the paper explores
several fusion methods, temporal and multimodal, including a novel hierarchical
method combining features and scores. In addition, we carefully reviewed the
different stages of the pipeline and designed a CNN architecture adapted to the
task; this is important as the size of the training set is small compared to
the difficulty of the problem, making generalization difficult. The so-obtained
model ranked 4th at the 2017 Emotion in the Wild challenge with the accuracy of
58.8 %.
",1,0,0,0,0,0
20124,Online codes for analog signals,"  We revisit a classical scenario in communication theory: a source is
generating a waveform which we sample at regular intervals; we wish to
transform the signal in such a way as to minimize distortion in its
reconstruction, despite noise. The transformation must be online (also called
causal), in order to enable real-time signaling. The noise model we consider is
adversarial $\ell_1$-bounded; this is the ""atomic norm"" convex relaxation of
the standard adversary model in discrete-alphabet communications, namely
sparsity (low Hamming weight). We require that our encoding not increase the
power of the original signal.
In the ""block coding"" setting such encoding is possible due to the existence
of large almost-Euclidean sections in $\ell_1$ spaces (established in the work
of Dvoretzky, Milman, Kašin, and Figiel, Lindenstrauss and Milman).
Our main result is that an analogous result is achievable even online.
Equivalently, we show a ""lower triangular"" version of $\ell_1$ Dvoretzky
theorems. In terms of communication, the result has the following form: If the
signal is a stream of reals $x_1,\ldots$, one per unit time, which we encode
causally into $\rho$ (a constant) reals per unit time (forming altogether an
output stream $\mathcal{E}(x)$), and if the adversarial noise added to this
encoded stream up to time $s$ is a vector $\vec{y}$, then at time $s$ the
decoder's reconstruction of the input prefix $x_{[s]}$ is accurate in a
time-weighted $\ell_2$ norm, to within $s^{-1/2+\delta}$ (any $\delta>0$) times
the adversary's noise as measured in a time-weighted $\ell_1$ norm. The
time-weighted decoding norm forces increasingly accurate reconstruction of the
distant past, while the time-weighted noise norm permits only vanishing effect
from noise in the distant past.
Encoding is linear, and decoding is performed by an LP analogous to those
used in compressed sensing.
",1,0,0,0,0,0
450,The Tu--Deng Conjecture holds almost surely,"  The Tu--Deng Conjecture is concerned with the sum of digits $w(n)$ of $n$ in
base~$2$ (the Hamming weight of the binary expansion of $n$) and states the
following: assume that $k$ is a positive integer and $1\leq t<2^k-1$. Then
\[\Bigl \lvert\Bigl\{(a,b)\in\bigl\{0,\ldots,2^k-2\bigr\}^2:a+b\equiv t\bmod
2^k-1, w(a)+w(b)<k\Bigr\}\Bigr \rvert\leq 2^{k-1}.\]
We prove that the Tu--Deng Conjecture holds almost surely in the following
sense: the proportion of $t\in[1,2^k-2]$ such that the above inequality holds
approaches $1$ as $k\rightarrow\infty$.
Moreover, we prove that the Tu--Deng Conjecture implies a conjecture due to
T.~W.~Cusick concerning the sum of digits of $n$ and $n+t$.
",1,0,1,0,0,0
12315,Appropriate conditions to realize a $p$-wave superfluid state starting from a spin-orbit coupled $s$-wave superfluid Fermi gas,"  We theoretically investigate a spin-orbit coupled $s$-wave superfluid Fermi
gas, to examine the time evolution of the system, after an $s$-wave pairing
interaction is replaced by a $p$-wave one at $t=0$. In our recent paper, we
proposed that this manipulation may realize a $p$-wave superfluid Fermi gas,
because the $p$-wave pair amplitude that is induced in the $s$-wave superfluid
state by a parity-broken antisymmetric spin-orbit interaction gives a
non-vanishing $p$-wave superfluid order parameter, immediately after the
$p$-wave interaction is turned on. In this paper, using a time-dependent
Bogoliubov-de Gennes theory, we assess this idea under various conditions with
respect to the $s$-wave and $p$-wave interaction strengths, as well as the
spin-orbit coupling strength. From these, we clarify that the momentum
distribution of Fermi atoms in the initial $s$-wave state ($t<0$) is a key to
produce a large $p$-wave superfluid order parameter. Since the realization of a
$p$-wave superfluid state is one of the most exciting and difficult challenges
in cold Fermi gas physics, our results may provide a possible way to accomplish
this.
",0,1,0,0,0,0
8141,Titanium dioxide hole-blocking layer in ultra-thin-film crystalline silicon solar cells,"  One of the remaining obstacles to approaching the theoretical efficiency
limit of crystalline silicon (c-Si) solar cells is the exceedingly high
interface recombination loss for minority carriers at the Ohmic contacts. In
ultra-thin-film c-Si solar cells, this contact recombination loss is far more
severe than for traditional thick cells due to the smaller volume and higher
minority carrier concentration of the former. This paper presents a novel
design of an electron passing (Ohmic) contact to n-type Si that is
hole-blocking with significantly reduced hole recombination. This contact is
formed by depositing a thin titanium dioxide (TiO2) layer to form a silicon
metal-insulator-semiconductor (MIS) contact. A 2 {\mu}m thick Si cell with this
TiO2 MIS contact achieved an open circuit voltage (Voc) of 645 mV, which is 10
mV higher than that of an ultra-thin cell with a metal contact. This MIS
contact demonstrates a new path for ultra-thin-film c-Si solar cells to achieve
high efficiencies as high as traditional thick cells, and enables the
fabrication of high-efficiency c-Si solar cells at a lower cost.
",0,1,0,0,0,0
18046,Volume Dependence of N-Body Bound States,"  We derive the finite-volume correction to the binding energy of an N-particle
quantum bound state in a cubic periodic volume. Our results are applicable to
bound states with arbitrary composition and total angular momentum, and in any
number of spatial dimensions. The only assumptions are that the interactions
have finite range. The finite-volume correction is a sum of contributions from
all possible breakup channels. In the case where the separation is into two
bound clusters, our result gives the leading volume dependence up to
exponentially small corrections. If the separation is into three or more
clusters, there is a power-law factor that is beyond the scope of this work,
however our result again determines the leading exponential dependence. We also
present two independent methods that use finite-volume data to determine
asymptotic normalization coefficients. The coefficients are useful to determine
low-energy capture reactions into weakly bound states relevant for nuclear
astrophysics. Using the techniques introduced here, one can even extract the
infinite-volume energy limit using data from a single-volume calculation. The
derived relations are tested using several exactly solvable systems and
numerical examples. We anticipate immediate applications to lattice
calculations of hadronic, nuclear, and cold atomic systems.
",0,1,1,0,0,0
15143,Generative Adversarial Privacy,"  We present a data-driven framework called generative adversarial privacy
(GAP). Inspired by recent advancements in generative adversarial networks
(GANs), GAP allows the data holder to learn the privatization mechanism
directly from the data. Under GAP, finding the optimal privacy mechanism is
formulated as a constrained minimax game between a privatizer and an adversary.
We show that for appropriately chosen adversarial loss functions, GAP provides
privacy guarantees against strong information-theoretic adversaries. We also
evaluate the performance of GAP on multi-dimensional Gaussian mixture models
and the GENKI face database.
",0,0,0,1,0,0
4515,Quantum oscillations and Dirac-Landau levels in Weyl superconductors,"  When magnetic field is applied to metals and semimetals quantum oscillations
appear as individual Landau levels cross the Fermi level. Quantum oscillations
generally do not occur in superconductors (SC) because magnetic field is either
expelled from the sample interior or, if strong enough, drives the material
into the normal state. In addition, elementary excitations of a superconductor
-- Bogoliubov quasiparticles -- do not carry a well defined electric charge and
therefore do not couple in a simple way to the applied magnetic field. We
predict here that in Weyl superconductors certain types of elastic strain have
the ability to induce chiral pseudo-magnetic field which can reorganize the
electronic states into Dirac-Landau levels with linear band crossings at low
energy. The resulting quantum oscillations in the quasiparticle density of
states and thermal conductivity can be experimentally observed under a bending
deformation of a thin film Weyl SC and provide new insights into this
fascinating family of materials.
",0,1,0,0,0,0
10042,A Novel Bayesian Multiple Testing Approach to Deregulated miRNA Discovery Harnessing Positional Clustering,"  MicroRNAs (miRNAs) are small non-coding RNAs that function as regulators of
gene expression. In recent years, there has been a tremendous and growing
interest among researchers to investigate the role of miRNAs in normal cellular
as well as in disease processes. Thus to investigate the role of miRNAs in oral
cancer, we analyse the expression levels of miRNAs to identify miRNAs with
statistically significant differential expression in cancer tissues.
In this article, we propose a novel Bayesian hierarchical model of miRNA
expression data. Compelling evidences have demonstrated that the transcription
process of miRNAs in human genome is a latent process instrumental for the
observed expression levels. We take into account positional clustering of the
miRNAs in the analysis and model the latent transcription phenomenon
nonparametrically by an appropriate Gaussian process.
For the testing purpose we employ a novel Bayesian multiple testing method
where we mainly focus on utilizing the dependence structure between the
hypotheses for better results, while also ensuring optimality in many respects.
Indeed, our non-marginal method yielded results in accordance with the
underlying scientific knowledge which are found to be missed by the very
popular Benjamini-Hochberg method.
",0,0,0,1,0,0
10738,Quantum-continuum simulation of underpotential deposition at electrified metal-solution interfaces,"  The underpotential deposition of transition metal ions is a critical step in
many electrosynthetic approaches. While underpotential deposition has been
intensively studied at the atomic level, first-principles calculations in
vacuum can strongly underestimate the stability of underpotentially deposited
metals. It has been shown recently that the consideration of co-adsorbed anions
can deliver more reliable descriptions of underpotential deposition reactions;
however, the influence of additional key environmental factors such as the
electrification of the interface under applied voltage and the activities of
the ions in solution have yet to be investigated. In this work, copper
underpotential deposition on gold is studied under realistic electrochemical
conditions using a quantum-continuum model of the electrochemical interface. We
report here on the influence of surface electrification, concentration effects,
and anion co-adsorption on the stability of the copper underpotential
deposition layer on the gold (100) surface.
",0,1,0,0,0,0
20569,Commutativity of integral quasi-arithmetic means on measure spaces,"  Let $(X, \mathscr{L}, \lambda)$ and $(Y, \mathscr{M}, \mu)$ be finite measure
spaces for which there exist $A \in \mathscr{L}$ and $B \in \mathscr{M}$ with
$0 < \lambda(A) < \lambda(X)$ and $0 < \mu(B) < \mu(Y)$, and let $I\subseteq
\mathbf{R}$ be a non-empty interval. We prove that, if $f$ and $g$ are
continuous bijections $I \to \mathbf{R}^+$, then the equation $$
f^{-1}\!\left(\int_X f\!\left(g^{-1}\!\left(\int_Y g \circ
h\;d\mu\right)\right)d \lambda\right)\! = g^{-1}\!\left(\int_Y
g\!\left(f^{-1}\!\left(\int_X f \circ h\;d\lambda\right)\right)d \mu\right)$$
is satisfied by every $\mathscr{L} \otimes \mathscr{M}$-measurable simple
function $h: X \times Y \to I$ if and only if $f=c g$ for some $c \in
\mathbf{R}^+$ (it is easy to see that the equation is well posed). An
analogous, but essentially different, result, with $f$ and $g$ replaced by
continuous injections $I \to \mathbf R$ and $\lambda(X)=\mu(Y)=1$, was recently
obtained in [Indag. Math. 27 (2016), 945-953].
",0,0,1,0,0,0
14225,Nonparametric inference for continuous-time event counting and link-based dynamic network models,"  A flexible approach for modeling both dynamic event counting and dynamic
link-based networks based on counting processes is proposed, and estimation in
these models is studied. We consider nonparametric likelihood based estimation
of parameter functions via kernel smoothing. The asymptotic behavior of these
estimators is rigorously analyzed by allowing the number of nodes to tend to
infinity. The finite sample performance of the estimators is illustrated
through an empirical analysis of bike share data.
",0,0,1,1,0,0
16818,The Price of Differential Privacy For Online Learning,"  We design differentially private algorithms for the problem of online linear
optimization in the full information and bandit settings with optimal
$\tilde{O}(\sqrt{T})$ regret bounds. In the full-information setting, our
results demonstrate that $\epsilon$-differential privacy may be ensured for
free -- in particular, the regret bounds scale as
$O(\sqrt{T})+\tilde{O}\left(\frac{1}{\epsilon}\right)$. For bandit linear
optimization, and as a special case, for non-stochastic multi-armed bandits,
the proposed algorithm achieves a regret of
$\tilde{O}\left(\frac{1}{\epsilon}\sqrt{T}\right)$, while the previously known
best regret bound was
$\tilde{O}\left(\frac{1}{\epsilon}T^{\frac{2}{3}}\right)$.
",1,0,0,1,0,0
6286,"The universal DAHA of type $(C_1^\vee,C_1)$ and Leonard pairs of $q$-Racah type","  A Leonard pair is a pair of diagonalizable linear transformations of a
finite-dimensional vector space, each of which acts in an irreducible
tridiagonal fashion on an eigenbasis for the other one. Let $\mathbb F$ denote
an algebraically closed field, and fix a nonzero $q \in \mathbb F$ that is not
a root of unity. The universal double affine Hecke algebra (DAHA) $\hat{H}_q$
of type $(C_1^\vee,C_1)$ is the associative $\mathbb F$-algebra defined by
generators $\lbrace t_i^{\pm 1}\rbrace_{i=0}^3$ and relations (i)
$t_it_i^{-1}=t_i^{-1}t_i=1$; (ii) $t_i+t_i^{-1}$ is central; (iii)
$t_0t_1t_2t_3 = q^{-1}$. We consider the elements $X=t_3t_0$ and $Y=t_0t_1$ of
$\hat{H}_q$. Let $\mathcal V$ denote a finite-dimensional irreducible
$\hat{H}_q$-module on which each of $X$, $Y$ is diagonalizable and $t_0$ has
two distinct eigenvalues. Then $\mathcal V$ is a direct sum of the two
eigenspaces of $t_0$. We show that the pair $X+X^{-1}$, $Y+Y^{-1}$ acts on each
eigenspace as a Leonard pair, and each of these Leonard pairs falls into a
class said to have $q$-Racah type. Thus from $\mathcal V$ we obtain a pair of
Leonard pairs of $q$-Racah type. It is known that a Leonard pair of $q$-Racah
type is determined up to isomorphism by a parameter sequence $(a,b,c,d)$ called
its Huang data. Given a pair of Leonard pairs of $q$-Racah type, we find
necessary and sufficient conditions on their Huang data for that pair to come
from the above construction.
",0,0,1,0,0,0
13046,Cryogenic readout for multiple VUV4 Multi-Pixel Photon Counters in liquid xenon,"  We present the performances and characterization of an array made of
S13370-3050CN (VUV4 generation) Multi-Pixel Photon Counters manufactured by
Hamamatsu and equipped with a low power consumption preamplifier operating at
liquid xenon temperature (~ 175 K). The electronics is designed for the readout
of a matrix of maximum dimension of 8 x 8 individual photosensors and it is
based on a single operational amplifier. The detector prototype presented in
this paper utilizes the Analog Devices AD8011 current feedback operational
amplifier, but other models can be used depending on the application. A biasing
correction circuit has been implemented for the gain equalization of
photosensors operating at different voltages. The results show single photon
detection capability making this device a promising choice for future
generation of large scale dark matter detectors based on liquid xenon, such as
DARWIN.
",0,1,0,0,0,0
7626,On the joint asymptotic distribution of the restricted estimators in multivariate regression model,"  The main Theorem of Jain et al.[Jain, K., Singh, S., and Sharma, S. (2011),
Re- stricted estimation in multivariate measurement error regression model;
JMVA, 102, 2, 264-280] is established in its full generality. Namely, we derive
the joint asymp- totic normality of the unrestricted estimator (UE) and the
restricted estimators of the matrix of the regression coefficients. The derived
result holds under the hypothesized restriction as well as under the sequence
of alternative restrictions. In addition, we establish Asymptotic
Distributional Risk for the estimators and compare their relative performance.
It is established that near the restriction, the restricted estimators (REs)
perform better than the UE. But the REs perform worse than the unrestricted
estimator when one moves far away from the restriction.
",0,0,1,1,0,0
840,Stable representations of posets,"  The purpose of this paper is to study stable representations of partially
ordered sets (posets) and compare it to the well known theory for quivers. In
particular, we prove that every indecomposable representation of a poset of
finite type is stable with respect to some weight and construct that weight
explicitly in terms of the dimension vector. We show that if a poset is
primitive then Coxeter transformations preserve stable representations. When
the base field is the field of complex numbers we establish the connection
between the polystable representations and the unitary $\chi$-representations
of posets. This connection explains the similarity of the results obtained in
the series of papers.
",0,0,1,0,0,0
20005,A Deep Learning Interpretable Classifier for Diabetic Retinopathy Disease Grading,"  Deep neural network models have been proven to be very successful in image
classification tasks, also for medical diagnosis, but their main concern is its
lack of interpretability. They use to work as intuition machines with high
statistical confidence but unable to give interpretable explanations about the
reported results. The vast amount of parameters of these models make difficult
to infer a rationale interpretation from them. In this paper we present a
diabetic retinopathy interpretable classifier able to classify retine images
into the different levels of disease severity and of explaining its results by
assigning a score for every point in the hidden and input space, evaluating its
contribution to the final classification in a linear way. The generated visual
maps can be interpreted by an expert in order to compare its own knowledge with
the interpretation given by the model.
",1,0,0,1,0,0
15686,A new algorithm for constraint satisfaction problems with few subpowers templates,"  In this article, we provide a new algorithm for solving constraint
satisfaction problems over templates with few subpowers, by reducing the
problem to the combination of solvability of a polynomial number of systems of
linear equations over finite fields and reductions via absorbing subuniverses.
",0,0,1,0,0,0
6424,F-pure threshold and height of quasi-homogeneous polynomials,"  We consider a quasi-homogeneous polynomial $f \in \mathbb{Z}[x_0, \ldots,
x_N]$ of degree $w$ equal to the degree of $x_0 \cdots x_N$ and show that the
$F$-pure threshold of the reduction $f_p \in \mathbb{F}_p[x_0, \ldots, x_N]$ is
equal to the log canonical threshold if and only if the height of the
Artin-Mazur formal group associated to $H^{N-1}\left( X, {\mathbb{G}}_{m,X}
\right)$, where $X$ is the hypersurface given by $f$, is equal to 1. We also
prove that a similar result holds for Fermat hypersurfaces of degree $>N+1$.
Furthermore, we give examples of weighted Delsarte surfaces which show that
other values of the $F$-pure threshold of a quasi-homogeneous polynomial of
degree $w$ cannot be characterized by the height.
",0,0,1,0,0,0
7019,Design and implementation of lighting control system using battery-less wireless human detection sensor networks,"  Artificial lighting is responsible for a large portion of total energy
consumption and has great potential for energy saving. This paper designs an
LED light control algorithm based on users' localization using multiple
battery-less binary human detection sensors. The proposed lighting control
system focuses on reducing office lighting energy consumption and satisfying
users' illumination requirement. Most current lighting control systems use
infrared human detection sensors, but the poor detection probability,
especially for a static user, makes it difficult to realize comfortable and
effective lighting control. To improve the detection probability of each
sensor, we proposed to locate sensors as close to each user as possible by
using a battery-less wireless sensor network, in which all sensors can be
placed freely in the space with high energy stability. We also proposed to use
a multi-sensor-based user localization algorithm to capture user's position
more accurately and realize fine lighting control which works even with static
users. The system is actually implemented in an indoor office environment in a
pilot project. A verification experiment is conducted by measuring the
practical illumination and power consumption. The performance agrees with
design expectations. It shows that the proposed LED lighting control system
reduces the energy consumption significantly, 57% compared to the batch control
scheme, and satisfies user's illumination requirement with 100% probability.
",1,0,0,0,0,0
11711,GM-PHD Filter for Searching and Tracking an Unknown Number of Targets with a Mobile Sensor with Limited FOV,"  We study the problem of searching for and tracking a collection of moving
targets using a robot with a limited Field-Of-View (FOV) sensor. The actual
number of targets present in the environment is not known a priori. We propose
a search and tracking framework based on the concept of Bayesian Random Finite
Sets (RFSs). Specifically, we generalize the Gaussian Mixture Probability
Hypothesis Density (GM-PHD) filter which was previously applied for tracking
problems to allow for simultaneous search and tracking with a limited FOV
sensor. The proposed framework can extract individual target tracks as well as
estimate the number and the spatial density of targets. We also show how to use
the Gaussian Process (GP) regression to extract and predict non-linear target
trajectories in this framework. We demonstrate the efficacy of our techniques
through representative simulations and a real data collected from an aerial
robot.
",1,0,0,0,0,0
11357,Short-term Motion Prediction of Traffic Actors for Autonomous Driving using Deep Convolutional Networks,"  Despite its ubiquity in our daily lives, AI is only just starting to make
advances in what may arguably have the largest societal impact thus far, the
nascent field of autonomous driving. In this work we discuss this important
topic and address one of crucial aspects of the emerging area, the problem of
predicting future state of autonomous vehicle's surrounding necessary for safe
and efficient operations. We introduce a deep learning-based approach that
takes into account current world state and produces rasterized representations
of each actor's vicinity. The raster images are then used by deep convolutional
models to infer future movement of actors while accounting for inherent
uncertainty of the prediction task. Extensive experiments on real-world data
strongly suggest benefits of the proposed approach. Moreover, following
successful tests the system was deployed to a fleet of autonomous vehicles.
",1,0,0,1,0,0
7419,On links between horocyclic and geodesic orbits on geometrically infinite surfaces,"  We study the topological dynamics of the horocycle flow $h_\mathbb{R}$ on a
geometrically infinite hyperbolic surface S. Let u be a non-periodic vector for
$h_\mathbb{R}$ in T^1 S. Suppose that the half-geodesic $u(\mathbb{R}^+)$ is
almost minimizing and that the injectivity radius along $u(\mathbb{R}^+)$ has a
finite inferior limit $Inj(u(\mathbb{R}^+))$. We prove that the closure of
$h_\mathbb{R} u$ meets the geodesic orbit along un unbounded sequence of points
$g_{t_n} u$. Moreover, if $Inj(u(\mathbb{R}^+)) = 0$, the whole half-orbit
$g_{\mathbb{R}^+} u$ is contained in $h_\mathbb{R} u$. When
$Inj(u(\mathbb{R}^+)) > 0$, it is known that in general $g_{\mathbb{R}^+} u
\subset h_\mathbb{R} u$. Yet, we give a construction where
$Inj(u(\mathbb{R}^+)) > 0$ and $g_{\mathbb{R}^+} u \subset h_\mathbb{R} u$,
which also constitutes a counterexample to Proposition 3 of [Led97].
",0,0,1,0,0,0
6031,Semiparametric panel data models using neural networks,"  This paper presents an estimator for semiparametric models that uses a
feed-forward neural network to fit the nonparametric component. Unlike many
methodologies from the machine learning literature, this approach is suitable
for longitudinal/panel data. It provides unbiased estimation of the parametric
component of the model, with associated confidence intervals that have
near-nominal coverage rates. Simulations demonstrate (1) efficiency, (2) that
parametric estimates are unbiased, and (3) coverage properties of estimated
intervals. An application section demonstrates the method by predicting
county-level corn yield using daily weather data from the period 1981-2015,
along with parametric time trends representing technological change. The method
is shown to out-perform linear methods such as OLS and ridge/lasso, as well as
random forest. The procedures described in this paper are implemented in the R
package panelNNET.
",0,0,0,1,0,0
4690,Temperature fluctuations in a changing climate: an ensemble-based experimental approach,"  There is an ongoing debate in the literature about whether the present global
warming is increasing local and global temperature variability. The central
methodological issues of this debate relate to the proper treatment of
normalised temperature anomalies and trends in the studied time series which
may be difficult to separate from time-evolving fluctuations. Some argue that
temperature variability is indeed increasing globally, whereas others conclude
it is decreasing or remains practically unchanged. Meanwhile, a consensus
appears to emerge that local variability in certain regions (e.g. Western
Europe and North America) has indeed been increasing in the past 40 years. Here
we investigate the nature of connections between external forcing and climate
variability conceptually by using a laboratory-scale minimal model of
mid-latitude atmospheric thermal convection subject to continuously decreasing
`equator-to-pole' temperature contrast, mimicking climate change. The analysis
of temperature records from an ensemble of experimental runs (`realisations')
all driven by identical time-dependent external forcing reveals that the
collective variability of the ensemble and that of individual realisations may
be markedly different -- a property to be considered when interpreting climate
records.
",0,1,0,0,0,0
5232,Quarnet inference rules for level-1 networks,"  An important problem in phylogenetics is the construction of phylogenetic
trees. One way to approach this problem, known as the supertree method,
involves inferring a phylogenetic tree with leaves consisting of a set $X$ of
species from a collection of trees, each having leaf-set some subset of $X$. In
the 1980's characterizations, certain inference rules were given for when a
collection of 4-leaved trees, one for each 4-element subset of $X$, can all be
simultaneously displayed by a single supertree with leaf-set $X$. Recently, it
has become of interest to extend such results to phylogenetic networks. These
are a generalization of phylogenetic trees which can be used to represent
reticulate evolution (where species can come together to form a new species).
It has been shown that a certain type of phylogenetic network, called a level-1
network, can essentially be constructed from 4-leaved trees. However, the
problem of providing appropriate inference rules for such networks remains
unresolved. Here we show that by considering 4-leaved networks, called
quarnets, as opposed to 4-leaved trees, it is possible to provide such rules.
In particular, we show that these rules can be used to characterize when a
collection of quarnets, one for each 4-element subset of $X$, can all be
simultaneously displayed by a level-1 network with leaf-set $X$. The rules are
an intriguing mixture of tree inference rules, and an inference rule for
building up a cyclic ordering of $X$ from orderings on subsets of $X$ of size
4. This opens up several new directions of research for inferring phylogenetic
networks from smaller ones, which could yield new algorithms for solving the
supernetwork problem in phylogenetics.
",1,0,0,0,0,0
2913,Flexible Deep Neural Network Processing,"  The recent success of Deep Neural Networks (DNNs) has drastically improved
the state of the art for many application domains. While achieving high
accuracy performance, deploying state-of-the-art DNNs is a challenge since they
typically require billions of expensive arithmetic computations. In addition,
DNNs are typically deployed in ensemble to boost accuracy performance, which
further exacerbates the system requirements. This computational overhead is an
issue for many platforms, e.g. data centers and embedded systems, with tight
latency and energy budgets. In this article, we introduce flexible DNNs
ensemble processing technique, which achieves large reduction in average
inference latency while incurring small to negligible accuracy drop. Our
technique is flexible in that it allows for dynamic adaptation between quality
of results (QoR) and execution runtime. We demonstrate the effectiveness of the
technique on AlexNet and ResNet-50 using the ImageNet dataset. This technique
can also easily handle other types of networks.
",0,0,0,1,0,0
14331,Fine-tuning deep CNN models on specific MS COCO categories,"  Fine-tuning of a deep convolutional neural network (CNN) is often desired.
This paper provides an overview of our publicly available py-faster-rcnn-ft
software library that can be used to fine-tune the VGG_CNN_M_1024 model on
custom subsets of the Microsoft Common Objects in Context (MS COCO) dataset.
For example, we improved the procedure so that the user does not have to look
for suitable image files in the dataset by hand which can then be used in the
demo program. Our implementation randomly selects images that contain at least
one object of the categories on which the model is fine-tuned.
",1,0,0,0,0,0
602,Subset Labeled LDA for Large-Scale Multi-Label Classification,"  Labeled Latent Dirichlet Allocation (LLDA) is an extension of the standard
unsupervised Latent Dirichlet Allocation (LDA) algorithm, to address
multi-label learning tasks. Previous work has shown it to perform in par with
other state-of-the-art multi-label methods. Nonetheless, with increasing label
sets sizes LLDA encounters scalability issues. In this work, we introduce
Subset LLDA, a simple variant of the standard LLDA algorithm, that not only can
effectively scale up to problems with hundreds of thousands of labels but also
improves over the LLDA state-of-the-art. We conduct extensive experiments on
eight data sets, with label sets sizes ranging from hundreds to hundreds of
thousands, comparing our proposed algorithm with the previously proposed LLDA
algorithms (Prior--LDA, Dep--LDA), as well as the state of the art in extreme
multi-label classification. The results show a steady advantage of our method
over the other LLDA algorithms and competitive results compared to the extreme
multi-label classification algorithms.
",0,0,0,1,0,0
9681,A Deep Reinforcement Learning Chatbot,"  We present MILABOT: a deep reinforcement learning chatbot developed by the
Montreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize
competition. MILABOT is capable of conversing with humans on popular small talk
topics through both speech and text. The system consists of an ensemble of
natural language generation and retrieval models, including template-based
models, bag-of-words models, sequence-to-sequence neural network and latent
variable neural network models. By applying reinforcement learning to
crowdsourced data and real-world user interactions, the system has been trained
to select an appropriate response from the models in its ensemble. The system
has been evaluated through A/B testing with real-world users, where it
performed significantly better than many competing systems. Due to its machine
learning architecture, the system is likely to improve with additional data.
",1,0,0,1,0,0
18975,Gaussian Processes Over Graphs,"  We propose Gaussian processes for signals over graphs (GPG) using the apriori
knowledge that the target vectors lie over a graph. We incorporate this
information using a graph- Laplacian based regularization which enforces the
target vectors to have a specific profile in terms of graph Fourier transform
coeffcients, for example lowpass or bandpass graph signals. We discuss how the
regularization affects the mean and the variance in the prediction output. In
particular, we prove that the predictive variance of the GPG is strictly
smaller than the conventional Gaussian process (GP) for any non-trivial graph.
We validate our concepts by application to various real-world graph signals.
Our experiments show that the performance of the GPG is superior to GP for
small training data sizes and under noisy training.
",0,0,0,1,0,0
17060,Boosting Adversarial Attacks with Momentum,"  Deep neural networks are vulnerable to adversarial examples, which poses
security concerns on these algorithms due to the potentially severe
consequences. Adversarial attacks serve as an important surrogate to evaluate
the robustness of deep learning models before they are deployed. However, most
of existing adversarial attacks can only fool a black-box model with a low
success rate. To address this issue, we propose a broad class of momentum-based
iterative algorithms to boost adversarial attacks. By integrating the momentum
term into the iterative process for attacks, our methods can stabilize update
directions and escape from poor local maxima during the iterations, resulting
in more transferable adversarial examples. To further improve the success rates
for black-box attacks, we apply momentum iterative algorithms to an ensemble of
models, and show that the adversarially trained models with a strong defense
ability are also vulnerable to our black-box attacks. We hope that the proposed
methods will serve as a benchmark for evaluating the robustness of various deep
models and defense methods. With this method, we won the first places in NIPS
2017 Non-targeted Adversarial Attack and Targeted Adversarial Attack
competitions.
",1,0,0,1,0,0
13275,Uniform convergence for the incompressible limit of a tumor growth model,"  We study a model introduced by Perthame and Vauchelet that describes the
growth of a tumor governed by Brinkman's Law, which takes into account friction
between the tumor cells. We adopt the viscosity solution approach to establish
an optimal uniform convergence result of the tumor density as well as the
pressure in the incompressible limit. The system lacks standard maximum
principle, and thus modification of the usual approach is necessary.
",0,0,1,0,0,0
16811,A brain signature highly predictive of future progression to Alzheimer's dementia,"  Early prognosis of Alzheimer's dementia is hard. Mild cognitive impairment
(MCI) typically precedes Alzheimer's dementia, yet only a fraction of MCI
individuals will progress to dementia, even when screened using biomarkers. We
propose here to identify a subset of individuals who share a common brain
signature highly predictive of oncoming dementia. This signature was composed
of brain atrophy and functional dysconnectivity and discovered using a machine
learning model in patients suffering from dementia. The model recognized the
same brain signature in MCI individuals, 90% of which progressed to dementia
within three years. This result is a marked improvement on the state-of-the-art
in prognostic precision, while the brain signature still identified 47% of all
MCI progressors. We thus discovered a sizable MCI subpopulation which
represents an excellent recruitment target for clinical trials at the prodromal
stage of Alzheimer's disease.
",0,0,0,1,0,0
7638,Binary systems with an RR Lyrae component - progress in 2016,"  In this contribution, we summarize the progress made in the investigation of
binary candidates with an RR Lyrae component in 2016. We also discuss the
actual status of the RRLyrBinCan database.
",0,1,0,0,0,0
16147,A causal approach to analysis of censored medical costs in the presence of time-varying treatment,"  There has recently been a growing interest in the development of statistical
methods to compare medical costs between treatment groups. When cumulative cost
is the outcome of interest, right-censoring poses the challenge of informative
missingness due to heterogeneity in the rates of cost accumulation across
subjects. Existing approaches seeking to address the challenge of informative
cost trajectories typically rely on inverse probability weighting and target a
net ""intent-to-treat"" effect. However, no approaches capable of handling
time-dependent treatment and confounding in this setting have been developed to
date. A method to estimate the joint causal effect of a treatment regime on
cost would be of value to inform public policy when comparing interventions. In
this paper, we develop a nested g-computation approach to cost analysis in
order to accommodate time-dependent treatment and repeated outcome measures. We
demonstrate that our procedure is reasonably robust to departures from its
distributional assumptions and can provide unique insights into fundamental
differences in average cost across time-dependent treatment regimes.
",0,0,0,1,0,0
12977,Poisson Structures and Potentials,"  We introduce a notion of weakly log-canonical Poisson structures on positive
varieties with potentials. Such a Poisson structure is log-canonical up to
terms dominated by the potential. To a compatible real form of a weakly
log-canonical Poisson variety we assign an integrable system on the product of
a certain real convex polyhedral cone (the tropicalization of the variety) and
a compact torus.
We apply this theory to the dual Poisson-Lie group $G^*$ of a
simply-connected semisimple complex Lie group $G$. We define a positive
structure and potential on $G^*$ and show that the natural Poisson-Lie
structure on $G^*$ is weakly log-canonical with respect to this positive
structure and potential.
For $K \subset G$ the compact real form, we show that the real form $K^*
\subset G^*$ is compatible and prove that the corresponding integrable system
is defined on the product of the decorated string cone and the compact torus of
dimension $\frac{1}{2}({\rm dim} \, G - {\rm rank} \, G)$.
",0,0,1,0,0,0
8880,Rgtsvm: Support Vector Machines on a GPU in R,"  Rgtsvm provides a fast and flexible support vector machine (SVM)
implementation for the R language. The distinguishing feature of Rgtsvm is that
support vector classification and support vector regression tasks are
implemented on a graphical processing unit (GPU), allowing the libraries to
scale to millions of examples with >100-fold improvement in performance over
existing implementations. Nevertheless, Rgtsvm retains feature parity and has
an interface that is compatible with the popular e1071 SVM package in R.
Altogether, Rgtsvm enables large SVM models to be created by both experienced
and novice practitioners.
",1,0,0,1,0,0
15712,Eventness: Object Detection on Spectrograms for Temporal Localization of Audio Events,"  In this paper, we introduce the concept of Eventness for audio event
detection, which can, in part, be thought of as an analogue to Objectness from
computer vision. The key observation behind the eventness concept is that audio
events reveal themselves as 2-dimensional time-frequency patterns with specific
textures and geometric structures in spectrograms. These time-frequency
patterns can then be viewed analogously to objects occurring in natural images
(with the exception that scaling and rotation invariance properties do not
apply). With this key observation in mind, we pose the problem of detecting
monophonic or polyphonic audio events as an equivalent visual object(s)
detection problem under partial occlusion and clutter in spectrograms. We adapt
a state-of-the-art visual object detection model to evaluate the audio event
detection task on publicly available datasets. The proposed network has
comparable results with a state-of-the-art baseline and is more robust on
minority events. Provided large-scale datasets, we hope that our proposed
conceptual model of eventness will be beneficial to the audio signal processing
community towards improving performance of audio event detection.
",1,0,0,0,0,0
18685,Uniform confidence bands for nonparametric errors-in-variables regression,"  This paper develops a method to construct uniform confidence bands for a
nonparametric regression function where a predictor variable is subject to a
measurement error. We allow for the distribution of the measurement error to be
unknown, but assume that there is an independent sample from the measurement
error distribution. The sample from the measurement error distribution need not
be independent from the sample on response and predictor variables. The
availability of a sample from the measurement error distribution is satisfied
if, for example, either 1) validation data or 2) repeated measurements (panel
data) on the latent predictor variable with measurement errors, one of which is
symmetrically distributed, are available. The proposed confidence band builds
on the deconvolution kernel estimation and a novel application of the
multiplier (or wild) bootstrap method. We establish asymptotic validity of the
proposed confidence band under ordinary smooth measurement error densities,
showing that the proposed confidence band contains the true regression function
with probability approaching the nominal coverage probability. To the best of
our knowledge, this is the first paper to derive asymptotically valid uniform
confidence bands for nonparametric errors-in-variables regression. We also
propose a novel data-driven method to choose a bandwidth, and conduct
simulation studies to verify the finite sample performance of the proposed
confidence band. Applying our method to a combination of two empirical data
sets, we draw confidence bands for nonparametric regressions of medical costs
on the body mass index (BMI), accounting for measurement errors in BMI.
Finally, we discuss extensions of our results to specification testing, cases
with additional error-free regressors, and confidence bands for conditional
distribution functions.
",0,0,1,1,0,0
13361,QWIRE Practice: Formal Verification of Quantum Circuits in Coq,"  We describe an embedding of the QWIRE quantum circuit language in the Coq
proof assistant. This allows programmers to write quantum circuits using
high-level abstractions and to prove properties of those circuits using Coq's
theorem proving features. The implementation uses higher-order abstract syntax
to represent variable binding and provides a type-checking algorithm for linear
wire types, ensuring that quantum circuits are well-formed. We formalize a
denotational semantics that interprets QWIRE circuits as superoperators on
density matrices, and prove the correctness of some simple quantum programs.
",1,0,0,0,0,0
13370,Criteria for the Absence and Existence of Bounded Solutions at the Threshold Frequency in a Junction of Quantum Waveguides,"  In the junction $\Omega$ of several semi-infinite cylindrical waveguides we
consider the Dirichlet Laplacian whose continuous spectrum is the ray
$[\lambda_\dagger, +\infty)$ with a positive cut-off value $\lambda_\dagger$.
We give two different criteria for the threshold resonance generated by
nontrivial bounded solutions to the Dirichlet problem for the Helmholtz
equation $-\Delta u=\lambda_\dagger u$ in $\Omega$. The first criterion is
quite simple and is convenient to disprove the existence of bounded solutions.
The second criterion is rather involved but can help to detect concrete shapes
supporting the resonance. Moreover, the latter distinguishes in a natural way
between stabilizing, i.e., bounded but non-descending solutions and trapped
modes with exponential decay at infinity.
",0,0,1,0,0,0
13883,Analytic evaluation of some three- and four- electron atomic integrals involving s STO's and exponential correlation with unlinked $r_{ij}$'s,"  The method of evaluation outlined in a previous work has been utilized here
to evaluate certain other three- electron and four- electron atomic integrals
involving s Slater-type orbitals and exponential correlation with unlinked
$r_{ij}$'s. Limiting expressions for various such integrals have been derived,
which has not been done earlier. Closed-form expressions for $<r_{12} r_{13} /
r_{14}>$, $<r_{12}r_{34}/r_{23}>$, $<r_{12}r_{23}/r_{34}>$,
$<r_{12}r_{13}/r_{34}>$ and $<r_{12}r_{34}/r_{13}>$ have been obtained.
",0,1,0,0,0,0
16626,Cosmological solutions in generalized hybrid metric-Palatini gravity,"  We construct exact solutions representing a
Friedmann-Lemaître-Robsertson-Walker (FLRW) universe in a generalized hybrid
metric-Palatini theory. By writing the gravitational action in a scalar-tensor
representation, the new solutions are obtained by either making an ansatz on
the scale factor or on the effective potential. Among other relevant results,
we show that it is possible to obtain exponentially expanding solutions for
flat universes even when the cosmology is not purely vacuum. We then derive the
classes of actions for the original theory which generate these solutions.
",0,1,0,0,0,0
19857,On the synthesis of acoustic sources with controllable near fields,"  In this paper we present a strategy for the the synthesis of acoustic sources
with controllable near fields in free space and finite depth homogeneous ocean
environments. We first present the theoretical results at the basis of our
discussion and then, to illustrate our findings we focus on the following three
particular examples:
1. acoustic source approximating a prescribed field pattern in a given
bounded sub- region of its near field. 2. acoustic source approximating
different prescribed field patterns in given disjoint bounded near field
sub-regions. 3. acoustic source approximating a prescribed back-propagating
field in a given bounded near field sub-region while maintaining a very low far
field signature.
For each of these three examples, we discuss the optimization scheme used to
approx- imate their solutions and support our claims through relevant numerical
simulations.
",0,0,1,0,0,0
8933,Integrable $sl(\infty)$-modules and Category $\mathcal O$ for $\mathfrak{gl}(m|n)$,"  We introduce and study new categories T(g,k)of integrable sl(\infty)-modules
which depend on the choice of a certain reductive subalgebra k in g=sl(\infty).
The simple objects of these categories are tensor modules as in the previously
studied category, however, the choice of k provides more flexibility of
nonsimple modules. We then choose k to have two infinite-dimensional diagonal
blocks, and show that a certain injective object K(m|n) in T(g,k) realizes a
categorical sl(\infty)-action on the integral category O(m|n) of the Lie
superalgebra gl(m|n). We show that the socle of K(m|n) is generated by the
projective modules in O(m|n), and compute the socle filtration of K(m|n)
explicitly. We conjecture that the socle filtration of K(m|n) reflects a
""degree of atypicality filtration"" on the category O(m|n). We also conjecture
that a natural tensor filtration on K(m|n) arises via the Duflo--Serganova
functor sending the category O(m|n) to O(m-1|n-1). We prove this latter
conjecture for a direct summand of K(m|n) corresponding to the
finite-dimensional gl(m|n)-modules.
",0,0,1,0,0,0
20872,Bidirectional Nested Weighted Automata,"  Nested weighted automata (NWA) present a robust and convenient
automata-theoretic formalism for quantitative specifications. Previous works
have considered NWA that processed input words only in the forward direction.
It is natural to allow the automata to process input words backwards as well,
for example, to measure the maximal or average time between a response and the
preceding request. We therefore introduce and study bidirectional NWA that can
process input words in both directions. First, we show that bidirectional NWA
can express interesting quantitative properties that are not expressible by
forward-only NWA. Second, for the fundamental decision problems of emptiness
and universality, we establish decidability and complexity results for the new
framework which match the best-known results for the special case of
forward-only NWA. Thus, for NWA, the increased expressiveness of
bidirectionality is achieved at no additional computational complexity. This is
in stark contrast to the unweighted case, where bidirectional finite automata
are no more expressive but exponentially more succinct than their forward-only
counterparts.
",1,0,0,0,0,0
18452,Mass-Imbalanced Ionic Hubbard Chain,"  A repulsive Hubbard model with both spin-asymmetric hopping (${t_\uparrow\neq
t_\downarrow}$) and a staggered potential (of strength $\Delta$) is studied in
one dimension. The model is a compound of the mass-imbalanced (${t_\uparrow\neq
t_\downarrow}$, ${\Delta=0}$) and ionic (${t_\uparrow = t_\downarrow}$,
${\Delta>0}$) Hubbard models, and may be realized by cold atoms in engineered
optical lattices. We use mostly mean-field theory to determine the phases and
phase transitions in the ground state for a half-filled band (one particle per
site). We find that a period-two modulation of the particle (or charge) density
and an alternating spin density coexist for arbitrary Hubbard interaction
strength, ${U\geqslant 0}$. The amplitude of the charge modulation is largest
at ${U=0}$, decreases with increasing $U$ and tends to zero for
${U\rightarrow\infty}$. The amplitude for spin alternation increases with $U$
and tends to saturation for ${U\rightarrow\infty}$. Charge order dominates
below a critical value $U_c$, whereas magnetic order dominates above. The
mean-field Hamiltonian has two gap parameters, $\Delta_\uparrow$ and
$\Delta_\downarrow$, which have to be determined self-consistently. For
${U<U_c}$ both parameters are positive, for ${U>U_c}$ they have different
signs, and for ${U=U_c}$ one gap parameter jumps from a positive to a negative
value. The weakly first-order phase transition at $U_c$ can be interpreted in
terms of an avoided criticality (or metallicity). The system is reluctant to
restore a symmetry that has been broken explicitly.
",0,1,0,0,0,0
9415,Lipschitz perturbations of Morse-Smale semigroups,"  In this paper we will deal with Lipschitz continuous perturbations of
Morse-Smale semigroups with only equilibrium points as critical elements. We
study the behavior of the structure of equilibrium points and their connections
when subjected to non-differentiable perturbations. To this end we define more
general notions of \emph{hyperbolicity} and \emph{transversality}, which do not
require differentiability.
",0,0,1,0,0,0
13827,Dirichlet Mixture Model based VQ Performance Prediction for Line Spectral Frequency,"  In this paper, we continue our previous work on the Dirichlet mixture model
(DMM)-based VQ to derive the performance bound of the LSF VQ. The LSF
parameters are transformed into the $\Delta$LSF domain and the underlying
distribution of the $\Delta$LSF parameters are modelled by a DMM with finite
number of mixture components. The quantization distortion, in terms of the mean
squared error (MSE), is calculated with the high rate theory. The mapping
relation between the perceptually motivated log spectral distortion (LSD) and
the MSE is empirically approximated by a polynomial. With this mapping
function, the minimum required bit rate for transparent coding of the LSF is
estimated.
",1,0,0,1,0,0
11243,Query-Efficient Black-box Adversarial Examples (superceded),"  Note that this paper is superceded by ""Black-Box Adversarial Attacks with
Limited Queries and Information.""
Current neural network-based image classifiers are susceptible to adversarial
examples, even in the black-box setting, where the attacker is limited to query
access without access to gradients. Previous methods --- substitute networks
and coordinate-based finite-difference methods --- are either unreliable or
query-inefficient, making these methods impractical for certain problems.
We introduce a new method for reliably generating adversarial examples under
more restricted, practical black-box threat models. First, we apply natural
evolution strategies to perform black-box attacks using two to three orders of
magnitude fewer queries than previous methods. Second, we introduce a new
algorithm to perform targeted adversarial attacks in the partial-information
setting, where the attacker only has access to a limited number of target
classes. Using these techniques, we successfully perform the first targeted
adversarial attack against a commercially deployed machine learning system, the
Google Cloud Vision API, in the partial information setting.
",1,0,0,1,0,0
19778,On the Application of ISO 26262 in Control Design for Automated Vehicles,"  Research on automated vehicles has experienced an explosive growth over the
past decade. A main obstacle to their practical realization, however, is a
convincing safety concept. This question becomes ever more important as more
sophisticated algorithms are used and the vehicle automation level increases.
The field of functional safety offers a systematic approach to identify
possible sources of risk and to improve the safety of a vehicle. It is based on
practical experience across the aerospace, process and other industries over
multiple decades. This experience is compiled in the functional safety standard
for the automotive domain, ISO 26262, which is widely adopted throughout the
automotive industry. However, its applicability and relevance for highly
automated vehicles is subject to a controversial debate. This paper takes a
critical look at the discussion and summarizes the main steps of ISO 26262 for
a safe control design for automated vehicles.
",1,0,0,0,0,0
6777,The Minimal Resolution Conjecture on a general quartic surface in $\mathbb P^3$,"  Mustaţă has given a conjecture for the graded Betti numbers in the
minimal free resolution of the ideal of a general set of points on an
irreducible projective algebraic variety. For surfaces in $\mathbb P^3$ this
conjecture has been proven for points on quadric surfaces and on general cubic
surfaces. In the latter case, Gorenstein liaison was the main tool. Here we
prove the conjecture for general quartic surfaces. Gorenstein liaison continues
to be a central tool, but to prove the existence of our links we make use of
certain dimension computations. We also discuss the higher degree case, but now
the dimension count does not force the existence of our links.
",0,0,1,0,0,0
12984,On a free boundary problem and minimal surfaces,"  From minimal surfaces such as Simons' cone and catenoids, using refined
Lyapunov-Schmidt reduction method, we construct new solutions for a free
boundary problem whose free boundary has two components. In dimension $8$,
using variational arguments, we also obtain solutions which are global
minimizers of the corresponding energy functional. This shows that Savin's
theorem is optimal.
",0,0,1,0,0,0
16647,Markov State Models from short non-Equilibrium Simulations - Analysis and Correction of Estimation Bias,"  Many state of the art methods for the thermodynamic and kinetic
characterization of large and complex biomolecular systems by simulation rely
on ensemble approaches, where data from large numbers of relatively short
trajectories are integrated. In this context, Markov state models (MSMs) are
extremely popular because they can be used to compute stationary quantities and
long-time kinetics from ensembles of short simulations, provided that these
short simulations are in ""local equilibrium"" within the MSM states. However, in
the last over 15 years since the inception of MSMs, it has been controversially
discussed and not yet been answered how deviations from local equilibrium can
be detected, whether these deviations induce a practical bias in MSM
estimation, and how to correct for them. In this paper, we address these
issues: We systematically analyze the estimation of Markov state models (MSMs)
from short non-equilibrium simulations, and we provide an expression for the
error between unbiased transition probabilities and the expected estimate from
many short simulations. We show that the unbiased MSM estimate can be obtained
even from relatively short non-equilibrium simulations in the limit of long lag
times and good discretization. Further, we exploit observable operator model
(OOM) theory to derive an unbiased estimator for the MSM transition matrix that
corrects for the effect of starting out of equilibrium, even when short lag
times are used. Finally, we show how the OOM framework can be used to estimate
the exact eigenvalues or relaxation timescales of the system without estimating
an MSM transition matrix, which allows us to practically assess the
discretization quality of the MSM. Applications to model systems and molecular
dynamics simulation data of alanine dipeptide are included for illustration.
The improved MSM estimator is implemented in PyEMMA as of version 2.3.
",0,1,1,0,0,0
9136,Light in Power: A General and Parameter-free Algorithm for Caustic Design,"  We present in this paper a generic and parameter-free algorithm to
efficiently build a wide variety of optical components, such as mirrors or
lenses, that satisfy some light energy constraints. In all of our problems, one
is given a collimated or point light source and a desired illumination after
reflection or refraction and the goal is to design the geometry of a mirror or
lens which transports exactly the light emitted by the source onto the target.
We first propose a general framework and show that eight different optical
component design problems amount to solving a light energy conservation
equation that involves the computation of visibility diagrams. We then show
that these diagrams all have the same structure and can be obtained by
intersecting a 3D Power diagram with a planar or spherical domain. This allows
us to propose an efficient and fully generic algorithm capable to solve these
eight optical component design problems. The support of the prescribed target
illumination can be a set of directions or a set of points located at a finite
distance. Our solutions satisfy design constraints such as convexity or
concavity. We show the effectiveness of our algorithm on simulated and
fabricated examples.
",1,0,0,0,0,0
6214,Étale groupoids and their $C^*$-algebras,"  These notes were written as supplementary material for a five-hour lecture
series presented at the Centre de Recerca Mathemàtica at the Universitat
Autònoma de Barcelona from the 13th to the 17th of March 2017. The intention
of these notes is to give a brief overview of some key topics in the area of
$C^*$-algebras associated to étale groupoids. The scope has been deliberately
contained to the case of étale groupoids with the intention that much of the
representation-theoretic technology and measure-theoretic analysis required to
handle general groupoids can be suppressed in this simpler setting.
A published version of these notes will appear in the volume tentatively
titled ""Operator algebras and dynamics: groupoids, crossed products and Rokhlin
dimension"" by Gabor Szabo, Dana P. Williams and myself, and edited by Francesc
Perera, in the series ""Advanced Courses in Mathematics. CRM Barcelona."" The
pagination of this arXiv version is not identical to Birkhäuser's style, but
I have tried to make it close. The theorem numbering should be correct. I'm
grateful to the CRM and Birkhäuser for allowing me to post a version on
arXiv.
",0,0,1,0,0,0
13415,Fine Selmer Groups and Isogeny Invariance,"  We investigate fine Selmer groups for elliptic curves and for Galois
representations over a number field. More specifically, we discuss Conjecture
A, which states that the fine Selmer group of an elliptic curve over the
cyclotomic extension is a finitely generated $\mathbb{Z}_p$-module. The
relationship between this conjecture and Iwasawa's classical $\mu=0$ conjecture
is clarified. We also present some partial results towards the question whether
Conjecture A is invariant under isogenies.
",0,0,1,0,0,0
8089,Exact Diffusion for Distributed Optimization and Learning --- Part II: Convergence Analysis,"  Part I of this work [2] developed the exact diffusion algorithm to remove the
bias that is characteristic of distributed solutions for deterministic
optimization problems. The algorithm was shown to be applicable to a larger set
of combination policies than earlier approaches in the literature. In
particular, the combination matrices are not required to be doubly stochastic,
which impose stringent conditions on the graph topology and communications
protocol. In this Part II, we examine the convergence and stability properties
of exact diffusion in some detail and establish its linear convergence rate. We
also show that it has a wider stability range than the EXTRA consensus
solution, meaning that it is stable for a wider range of step-sizes and can,
therefore, attain faster convergence rates. Analytical examples and numerical
simulations illustrate the theoretical findings.
",0,0,1,0,0,0
17157,Stratified surgery and K-theory invariants of the signature operator,"  In work of Higson-Roe the fundamental role of the signature as a homotopy and
bordism invariant for oriented manifolds is made manifest in how it and related
secondary invariants define a natural transformation between the
(Browder-Novikov-Sullivan-Wall) surgery exact sequence and a long exact
sequence of C*-algebra K-theory groups.
In recent years the (higher) signature invariants have been extended from
closed oriented manifolds to a class of stratified spaces known as L-spaces or
Cheeger spaces. In this paper we show that secondary invariants, such as the
rho-class, also extend from closed manifolds to Cheeger spaces. We revisit a
surgery exact sequence for stratified spaces originally introduced by
Browder-Quinn and obtain a natural transformation analogous to that of
Higson-Roe. We also discuss geometric applications.
",0,0,1,0,0,0
14883,Local reservoir model for choice-based learning,"  Decision making based on behavioral and neural observations of living systems
has been extensively studied in brain science, psychology, and other
disciplines. Decision-making mechanisms have also been experimentally
implemented in physical processes, such as single photons and chaotic lasers.
The findings of these experiments suggest that there is a certain common basis
in describing decision making, regardless of its physical realizations. In this
study, we propose a local reservoir model to account for choice-based learning
(CBL). CBL describes decision consistency as a phenomenon where making a
certain decision increases the possibility of making that same decision again
later, which has been intensively investigated in neuroscience, psychology,
etc. Our proposed model is inspired by the viewpoint that a decision is
affected by its local environment, which is referred to as a local reservoir.
If the size of the local reservoir is large enough, consecutive decision making
will not be affected by previous decisions, thus showing lower degrees of
decision consistency in CBL. In contrast, if the size of the local reservoir
decreases, a biased distribution occurs within it, which leads to higher
degrees of decision consistency in CBL. In this study, an analytical approach
on local reservoirs is presented, as well as several numerical demonstrations.
Furthermore, a physical architecture for CBL based on single photons is
discussed, and the effects of local reservoirs is numerically demonstrated.
Decision consistency in human decision-making tasks and in recruiting empirical
data are evaluated based on local reservoir. In summary, the proposed local
reservoir model paves a path toward establishing a foundation for computational
mechanisms and the systematic analysis of decision making on different levels.
",0,0,0,1,1,0
16733,Optimal control of two qubits via a single cavity drive in circuit quantum electrodynamics,"  Optimization of the fidelity of control operations is of critical importance
in the pursuit of fault-tolerant quantum computation. We apply optimal control
techniques to demonstrate that a single drive via the cavity in circuit quantum
electrodynamics can implement a high-fidelity two-qubit all-microwave gate that
directly entangles the qubits via the mutual qubit-cavity couplings. This is
performed by driving at one of the qubits' frequencies which generates a
conditional two-qubit gate, but will also generate other spurious interactions.
These optimal control techniques are used to find pulse shapes that can perform
this two-qubit gate with high fidelity, robust against errors in the system
parameters. The simulations were all performed using experimentally relevant
parameters and constraints.
",0,1,0,0,0,0
11238,Presentations of the saturated cluster modular groups of finite mutation type $X_6$ and $X_7$,"  We give finite presentations of the saturated cluster modular groups of type
$X_6$ and $X_7$. We compute the first homology of these groups and conclude
that they are different from Artin-Tits braid groups and mapping class groups
of surfaces. We verify that the cluster modular group of type $X_7$ is
generated by cluster Dehn twists. Further we discuss several relations between
these cluster modular groups and the mapping class group of an annulus.
",0,0,1,0,0,0
6682,"Robust, high brightness, degenerate entangled photon source at room temperature","  We report on a compact, simple and robust high brightness entangled photon
source at room temperature. Based on a 30 mm long periodically poled potassium
titanyl phosphate (PPKTP), the source produces non-collinear, type0 phase
matched, degenerate photons at 810 nm with pair production rate as high 39.13
MHz per mW at room temperature. To the best of our knowledge, this is the
highest photon pair rate generated using bulk crystals pump with
continuous-wave laser. Combined with the inherently stable polarization Sagnac
interferometer, the source produces entangled state violating the Bells
inequality by nearly 10 standard deviations and a Bell state fidelity of 0.96.
The compact footprint, simple and robust experimental design and room
temperature operation, make our source ideal for various quantum communication
experiments including long distance free space and satellite communications.
",0,1,0,0,0,0
10946,Temperature induced transition from p-n to n-n electronic behavior in Ni0.07Zn0.93O/Mg0.21Zn0.79O heterojunction,"  The transport characteristics across the pulsed laser deposited
Ni0.07Zn0.93O/Mg0.21Zn0.79O heterojunction exhibits p-n type semiconducting
properties at 10 K while at 100 K, its characteristics become similar to that
of an n-n junction. The reason for the same is attributed to the role of larger
electronegativity of Ni as compared to Mg at 10 K and ionization of impurity
states at 100 K. The above behavior is confirmed by performing the Hall
measurements.
",0,1,0,0,0,0
7518,Distant Supervision for Topic Classification of Tweets in Curated Streams,"  We tackle the challenge of topic classification of tweets in the context of
analyzing a large collection of curated streams by news outlets and other
organizations to deliver relevant content to users. Our approach is novel in
applying distant supervision based on semi-automatically identifying curated
streams that are topically focused (for example, on politics, entertainment, or
sports). These streams provide a source of labeled data to train topic
classifiers that can then be applied to categorize tweets from more
topically-diffuse streams. Experiments on both noisy labels and human
ground-truth judgments demonstrate that our approach yields good topic
classifiers essentially ""for free"", and that topic classifiers trained in this
manner are able to dynamically adjust for topic drift as news on Twitter
evolves.
",1,0,0,0,0,0
9259,"Imaginary time, shredded propagator method for large-scale GW calculations","  The GW method is a many-body approach capable of providing quasiparticle
bands for realistic systems spanning physics, chemistry, and materials science.
Despite its power, GW is not routinely applied to large complex materials due
to its computational expense. We perform an exact recasting of the GW
polarizability and the self-energy as Laplace integrals over imaginary time
propagators. We then ""shred"" the propagators (via energy windowing) and
approximate them in a controlled manner by using Gauss-Laguerre quadrature and
discrete variable methods to treat the imaginary time propagators in real
space. The resulting cubic scaling GW method has a sufficiently small prefactor
to outperform standard quartic scaling methods on small systems (>=10 atoms)
and also represents a substantial improvement over several other cubic methods
tested. This approach is useful for evaluating quantum mechanical response
function involving large sums containing energy (difference) denominators.
",0,1,0,0,0,0
17189,Products of topological groups in which all closed subgroups are separable,"  We prove that if $H$ is a topological group such that all closed subgroups of
$H$ are separable, then the product $G\times H$ has the same property for every
separable compact group $G$.
Let $c$ be the cardinality of the continuum. Assuming $2^{\omega_1} = c$, we
show that there exist:
(1) pseudocompact topological abelian groups $G$ and $H$ such that all closed
subgroups of $G$ and $H$ are separable, but the product $G\times H$ contains a
closed non-separable $\sigma$-compact subgroup;
(2) pseudocomplete locally convex vector spaces $K$ and $L$ such that all
closed vector subspaces of $K$ and $L$ are separable, but the product $K\times
L$ contains a closed non-separable $\sigma$-compact vector subspace.
",0,0,1,0,0,0
6110,"Fluid Communities: A Competitive, Scalable and Diverse Community Detection Algorithm","  We introduce a community detection algorithm (Fluid Communities) based on the
idea of fluids interacting in an environment, expanding and contracting as a
result of that interaction. Fluid Communities is based on the propagation
methodology, which represents the state-of-the-art in terms of computational
cost and scalability. While being highly efficient, Fluid Communities is able
to find communities in synthetic graphs with an accuracy close to the current
best alternatives. Additionally, Fluid Communities is the first
propagation-based algorithm capable of identifying a variable number of
communities in network. To illustrate the relevance of the algorithm, we
evaluate the diversity of the communities found by Fluid Communities, and find
them to be significantly different from the ones found by alternative methods.
",1,1,0,0,0,0
3567,Relaxing Integrity Requirements for Attack-Resilient Cyber-Physical Systems,"  The increase in network connectivity has also resulted in several
high-profile attacks on cyber-physical systems. An attacker that manages to
access a local network could remotely affect control performance by tampering
with sensor measurements delivered to the controller. Recent results have shown
that with network-based attacks, such as Man-in-the-Middle attacks, the
attacker can introduce an unbounded state estimation error if measurements from
a suitable subset of sensors contain false data when delivered to the
controller. While these attacks can be addressed with the standard
cryptographic tools that ensure data integrity, their continuous use would
introduce significant communication and computation overhead. Consequently, we
study effects of intermittent data integrity guarantees on system performance
under stealthy attacks. We consider linear estimators equipped with a general
type of residual-based intrusion detectors (including $\chi^2$ and SPRT
detectors), and show that even when integrity of sensor measurements is
enforced only intermittently, the attack impact is significantly limited;
specifically, the state estimation error is bounded or the attacker cannot
remain stealthy. Furthermore, we present methods to: (1) evaluate the effects
of any given integrity enforcement policy in terms of reachable
state-estimation errors for any type of stealthy attacks, and (2) design an
enforcement policy that provides the desired estimation error guarantees under
attack. Finally, on three automotive case studies we show that even with less
than 10% of authenticated messages we can ensure satisfiable control
performance in the presence of attacks.
",1,0,1,0,0,0
19608,Constraining the giant planets' initial configuration from their evolution: implications for the timing of the planetary instability,"  Recent works on planetary migration show that the orbital structure of the
Kuiper belt can be very well reproduced if before the onset of the planetary
instability Neptune underwent a long-range planetesimal-driven migration up to
$\sim$28 au. However, considering that all giant planets should have been
captured in mean motion resonances among themselves during the gas-disk phase,
it is not clear whether such a very specific evolution for Neptune is possible,
nor whether the instability could have happened at late times. Here, we first
investigate which initial resonant configuration of the giant planets can be
compatible with Neptune being extracted from the resonant chain and migrating
to $\sim$28 au before that the planetary instability happened. We address the
late instability issue by investigating the conditions where the planets can
stay in resonance for about 400 My. Our results indicate that this can happen
only in the case where the planetesimal disk is beyond a specific minimum
distance $\delta_{stab}$ from Neptune. Then, if there is a sufficient amount of
dust produced in the planetesimal disk, that drifts inwards, Neptune can enter
in a slow dust-driven migration phase for hundreds of Mys until it reaches a
critical distance $\delta_{mig}$ from the disk. From that point, faster
planetesimal-driven migration takes over and Neptune continues migrating
outward until the instability happens. We conclude that, although an early
instability reproduces more easily the evolution of Neptune required to explain
the structure of the Kuiper belt, such evolution is also compatible with a late
instability.
",0,1,0,0,0,0
12374,Exploring patterns of demand in bike sharing systems via replicated point process models,"  Understanding patterns of demand is fundamental for fleet management of bike
sharing systems. In this paper we analyze data from the Divvy system of the
city of Chicago. We show that the demand of bicycles can be modeled as a
multivariate temporal point process, with each dimension corresponding to a
bike station in the network. The availability of daily replications of the
process allows nonparametric estimation of the intensity functions, even for
stations with low daily counts, and straightforward estimation of pairwise
correlations between stations. These correlations are then used for clustering,
revealing different patterns of bike usage.
",0,0,0,1,0,0
9416,Semi-Supervised Learning for Detecting Human Trafficking,"  Human trafficking is one of the most atrocious crimes and among the
challenging problems facing law enforcement which demands attention of global
magnitude. In this study, we leverage textual data from the website ""Backpage""-
used for classified advertisement- to discern potential patterns of human
trafficking activities which manifest online and identify advertisements of
high interest to law enforcement. Due to the lack of ground truth, we rely on a
human analyst from law enforcement, for hand-labeling a small portion of the
crawled data. We extend the existing Laplacian SVM and present S3VM-R, by
adding a regularization term to exploit exogenous information embedded in our
feature space in favor of the task at hand. We train the proposed method using
labeled and unlabeled data and evaluate it on a fraction of the unlabeled data,
herein referred to as unseen data, with our expert's further verification.
Results from comparisons between our method and other semi-supervised and
supervised approaches on the labeled data demonstrate that our learner is
effective in identifying advertisements of high interest to law enforcement
",1,0,0,0,0,0
2973,Generalised Reichenbachian Common Cause Systems,"  The principle of the common cause claims that if an improbable coincidence
has occurred, there must exist a common cause. This is generally taken to mean
that positive correlations between non-causally related events should disappear
when conditioning on the action of some underlying common cause. The extended
interpretation of the principle, by contrast, urges that common causes should
be called for in order to explain positive deviations between the estimated
correlation of two events and the expected value of their correlation. The aim
of this paper is to provide the extended reading of the principle with a
general probabilistic model, capturing the simultaneous action of a system of
multiple common causes. To this end, two distinct models are elaborated, and
the necessary and sufficient conditions for their existence are determined.
",1,0,0,1,0,0
6017,Lattice Boltzmann simulation of viscous fingering of immiscible displacement in a channel using an improved wetting scheme,"  An improved wetting boundary implementation strategy is proposed based on
lattice Boltzmann color-gradient model in this paper. In this strategy, an
extra interface force condition is demonstrated based on the diffuse interface
assumption and is employed in contact line region. It has been validated by
three benchmark problems: static droplet wetting on a flat surface and a curved
surface, and dynamic capillary filling. Good performances are shown in all
three cases. Relied on the strict validation to our scheme, the viscous
fingering phenomenon of immiscible fluids displacement in a two-dimensional
channel has been restudied in this paper. High viscosity ratio, wide range
contact angle, accurate moving contact line and mutual independence between
surface tension and viscosity are the obvious advantages of our model. We find
the linear relationship between the contact angle and displacement velocity or
variation of finger length. When the viscosity ratio is smaller than 20, the
displacement velocity is increasing with increasing viscosity ratio and
reducing capillary number, and when the viscosity ratio is larger than 20, the
displacement velocity tends to a specific constant. A similar conclusion is
obtained on the variation of finger length.
",0,1,0,0,0,0
2510,The Social Bow Tie,"  Understanding tie strength in social networks, and the factors that influence
it, have received much attention in a myriad of disciplines for decades.
Several models incorporating indicators of tie strength have been proposed and
used to quantify relationships in social networks, and a standard set of
structural network metrics have been applied to predominantly online social
media sites to predict tie strength. Here, we introduce the concept of the
""social bow tie"" framework, a small subgraph of the network that consists of a
collection of nodes and ties that surround a tie of interest, forming a
topological structure that resembles a bow tie. We also define several
intuitive and interpretable metrics that quantify properties of the bow tie. We
use random forests and regression models to predict categorical and continuous
measures of tie strength from different properties of the bow tie, including
nodal attributes. We also investigate what aspects of the bow tie are most
predictive of tie strength in two distinct social networks: a collection of 75
rural villages in India and a nationwide call network of European mobile phone
users. Our results indicate several of the bow tie metrics are highly
predictive of tie strength, and we find the more the social circles of two
individuals overlap, the stronger their tie, consistent with previous findings.
However, we also find that the more tightly-knit their non-overlapping social
circles, the weaker the tie. This new finding complements our current
understanding of what drives the strength of ties in social networks.
",0,0,0,1,0,0
1638,Forming short-period Wolf-Rayet X-ray binaries and double black holes through stable mass transfer,"  We show that black-hole High-Mass X-ray Binaries (HMXBs) with O- or B-type
donor stars and relatively short orbital periods, of order one week to several
months may survive spiral in, to then form Wolf-Rayet (WR) X-ray binaries with
orbital periods of order a day to a few days; while in systems where the
compact star is a neutron star, HMXBs with these orbital periods never survive
spiral-in. We therefore predict that WR X-ray binaries can only harbor black
holes. The reason why black-hole HMXBs with these orbital periods may survive
spiral in is: the combination of a radiative envelope of the donor star, and a
high mass of the compact star. In this case, when the donor begins to overflow
its Roche lobe, the systems are able to spiral in slowly with stable Roche-lobe
overflow, as is shown by the system SS433. In this case the transferred mass is
ejected from the vicinity of the compact star (so-called ""isotropic
re-emission"" mass loss mode, or ""SS433-like mass loss""), leading to gradual
spiral-in. If the mass ratio of donor and black hole is $>3.5$, these systems
will go into CE evolution and are less likely to survive. If they survive, they
produce WR X-ray binaries with orbital periods of a few hours to one day.
Several of the well-known WR+O binaries in our Galaxy and the Magellanic
Clouds, with orbital periods in the range between a week and several months,
are expected to evolve into close WR-Black-Hole binaries,which may later
produce close double black holes. The galactic formation rate of double black
holes resulting from such systems is still uncertain, as it depends on several
poorly known factors in this evolutionary picture. It might possibly be as high
as $\sim 10^{-5}$ per year.
",0,1,0,0,0,0
1401,Facial Keypoints Detection,"  Detect facial keypoints is a critical element in face recognition. However,
there is difficulty to catch keypoints on the face due to complex influences
from original images, and there is no guidance to suitable algorithms. In this
paper, we study different algorithms that can be applied to locate keyponits.
Specifically: our framework (1)prepare the data for further investigation
(2)Using PCA and LBP to process the data (3) Apply different algorithms to
analysis data, including linear regression models, tree based model, neural
network and convolutional neural network, etc. Finally we will give our
conclusion and further research topic. A comprehensive set of experiments on
dataset demonstrates the effectiveness of our framework.
",1,0,0,1,0,0
8513,RankDCG: Rank-Ordering Evaluation Measure,"  Ranking is used for a wide array of problems, most notably information
retrieval (search). There are a number of popular approaches to the evaluation
of ranking such as Kendall's $\tau$, Average Precision, and nDCG. When dealing
with problems such as user ranking or recommendation systems, all these
measures suffer from various problems, including an inability to deal with
elements of the same rank, inconsistent and ambiguous lower bound scores, and
an inappropriate cost function. We propose a new measure, rankDCG, that
addresses these problems. This is a modification of the popular nDCG algorithm.
We provide a number of criteria for any effective ranking algorithm and show
that only rankDCG satisfies all of them. Results are presented on constructed
and real data sets. We release a publicly available rankDCG evaluation package.
",1,0,0,0,0,0
18484,Triangular Decomposition of Matrices in a Domain,"  Deterministic recursive algorithms for the computation of matrix triangular
decompositions with permutations like LU and Bruhat decomposition are presented
for the case of commutative domains. This decomposition can be considered as a
generalization of LU and Bruhat decompositions, because they both may be easily
obtained from this triangular decomposition. Algorithms have the same
complexity as the algorithm of matrix multiplication.
",1,0,0,0,0,0
17245,A quantum Mirković-Vybornov isomorphism,"  We present a quantization of an isomorphism of Mirković and Vybornov which
relates the intersection of a Slodowy slice and a nilpotent orbit closure in
$\mathfrak{gl}_N$ , to a slice between spherical Schubert varieties in the
affine Grassmannian of $PGL_n$ (with weights encoded by the Jordan types of the
nilpotent orbits). A quantization of the former variety is provided by a
parabolic W-algebra and of the latter by a truncated shifted Yangian. Building
on earlier work of Brundan and Kleshchev, we define an explicit isomorphism
between these non-commutative algebras, and show that its classical limit is a
variation of the original isomorphism of Mirković and Vybornov. As a
corollary, we deduce that the W-algebra is free as a left (or right) module
over its Gelfand-Tsetlin subalgebra, as conjectured by Futorny, Molev, and
Ovsienko.
",0,0,1,0,0,0
492,Bayesian Joint Topic Modelling for Weakly Supervised Object Localisation,"  We address the problem of localisation of objects as bounding boxes in images
with weak labels. This weakly supervised object localisation problem has been
tackled in the past using discriminative models where each object class is
localised independently from other classes. We propose a novel framework based
on Bayesian joint topic modelling. Our framework has three distinctive
advantages over previous works: (1) All object classes and image backgrounds
are modelled jointly together in a single generative model so that ""explaining
away"" inference can resolve ambiguity and lead to better learning and
localisation. (2) The Bayesian formulation of the model enables easy
integration of prior knowledge about object appearance to compensate for
limited supervision. (3) Our model can be learned with a mixture of weakly
labelled and unlabelled data, allowing the large volume of unlabelled images on
the Internet to be exploited for learning. Extensive experiments on the
challenging VOC dataset demonstrate that our approach outperforms the
state-of-the-art competitors.
",1,0,0,0,0,0
6921,On the scaling of entropy viscosity in high order methods,"  In this work, we outline the entropy viscosity method and discuss how the
choice of scaling influences the size of viscosity for a simple shock problem.
We present examples to illustrate the performance of the entropy viscosity
method under two distinct scalings.
",0,0,1,0,0,0
2330,Correcting rural building annotations in OpenStreetMap using convolutional neural networks,"  Rural building mapping is paramount to support demographic studies and plan
actions in response to crisis that affect those areas. Rural building
annotations exist in OpenStreetMap (OSM), but their quality and quantity are
not sufficient for training models that can create accurate rural building
maps. The problems with these annotations essentially fall into three
categories: (i) most commonly, many annotations are geometrically misaligned
with the updated imagery; (ii) some annotations do not correspond to buildings
in the images (they are misannotations or the buildings have been destroyed);
and (iii) some annotations are missing for buildings in the images (the
buildings were never annotated or were built between subsequent image
acquisitions). First, we propose a method based on Markov Random Field (MRF) to
align the buildings with their annotations. The method maximizes the
correlation between annotations and a building probability map while enforcing
that nearby buildings have similar alignment vectors. Second, the annotations
with no evidence in the building probability map are removed. Third, we present
a method to detect non-annotated buildings with predefined shapes and add their
annotation. The proposed methodology shows considerable improvement in accuracy
of the OSM annotations for two regions of Tanzania and Zimbabwe, being more
accurate than state-of-the-art baselines.
",1,0,0,0,0,0
11773,Learning to Detect Human-Object Interactions,"  We study the problem of detecting human-object interactions (HOI) in static
images, defined as predicting a human and an object bounding box with an
interaction class label that connects them. HOI detection is a fundamental
problem in computer vision as it provides semantic information about the
interactions among the detected objects. We introduce HICO-DET, a new large
benchmark for HOI detection, by augmenting the current HICO classification
benchmark with instance annotations. To solve the task, we propose Human-Object
Region-based Convolutional Neural Networks (HO-RCNN). At the core of our
HO-RCNN is the Interaction Pattern, a novel DNN input that characterizes the
spatial relations between two bounding boxes. Experiments on HICO-DET
demonstrate that our HO-RCNN, by exploiting human-object spatial relations
through Interaction Patterns, significantly improves the performance of HOI
detection over baseline approaches.
",1,0,0,0,0,0
4928,Phase diagram of a generalized off-diagonal Aubry-André model with p-wave pairing,"  Off-diagonal Aubry-André (AA) model has recently attracted a great deal
of attention as they provide condensed matter realization of topological
phases. We numerically study a generalized off-diagonal AA model with p-wave
superfluid pairing in the presence of both commensurate and incommensurate
hopping modulations. The phase diagram as functions of the modulation strength
of incommensurate hopping and the strength of the p-wave pairing is obtained by
using the multifractal analysis. We show that with the appearance of the p-wave
pairing, the system exhibits mobility-edge phases and critical phases with
various number of topologically protected zero-energy modes. Predicted
topological nature of these exotic phases can be realized in a cold atomic
system of incommensurate bichromatic optical lattice with induced p-wave
superfluid pairing by using a Raman laser in proximity to a molecular
Bose-Einstein condensation.
",0,1,0,0,0,0
16806,Scalable Generalized Dynamic Topic Models,"  Dynamic topic models (DTMs) model the evolution of prevalent themes in
literature, online media, and other forms of text over time. DTMs assume that
word co-occurrence statistics change continuously and therefore impose
continuous stochastic process priors on their model parameters. These dynamical
priors make inference much harder than in regular topic models, and also limit
scalability. In this paper, we present several new results around DTMs. First,
we extend the class of tractable priors from Wiener processes to the generic
class of Gaussian processes (GPs). This allows us to explore topics that
develop smoothly over time, that have a long-term memory or are temporally
concentrated (for event detection). Second, we show how to perform scalable
approximate inference in these models based on ideas around stochastic
variational inference and sparse Gaussian processes. This way we can train a
rich family of DTMs to massive data. Our experiments on several large-scale
datasets show that our generalized model allows us to find interesting patterns
that were not accessible by previous approaches.
",0,0,0,1,0,0
16544,Distributed Exact Shortest Paths in Sublinear Time,"  The distributed single-source shortest paths problem is one of the most
fundamental and central problems in the message-passing distributed computing.
Classical Bellman-Ford algorithm solves it in $O(n)$ time, where $n$ is the
number of vertices in the input graph $G$. Peleg and Rubinovich (FOCS'99)
showed a lower bound of $\tilde{\Omega}(D + \sqrt{n})$ for this problem, where
$D$ is the hop-diameter of $G$.
Whether or not this problem can be solved in $o(n)$ time when $D$ is
relatively small is a major notorious open question. Despite intensive research
\cite{LP13,N14,HKN15,EN16,BKKL16} that yielded near-optimal algorithms for the
approximate variant of this problem, no progress was reported for the original
problem.
In this paper we answer this question in the affirmative. We devise an
algorithm that requires $O((n \log n)^{5/6})$ time, for $D = O(\sqrt{n \log
n})$, and $O(D^{1/3} \cdot (n \log n)^{2/3})$ time, for larger $D$. This
running time is sublinear in $n$ in almost the entire range of parameters,
specifically, for $D = o(n/\log^2 n)$. For the all-pairs shortest paths
problem, our algorithm requires $O(n^{5/3} \log^{2/3} n)$ time, regardless of
the value of $D$.
We also devise the first algorithm with non-trivial complexity guarantees for
computing exact shortest paths in the multipass semi-streaming model of
computation.
From the technical viewpoint, our algorithm computes a hopset $G""$ of a
skeleton graph $G'$ of $G$ without first computing $G'$ itself. We then conduct
a Bellman-Ford exploration in $G' \cup G""$, while computing the required edges
of $G'$ on the fly. As a result, our algorithm computes exactly those edges of
$G'$ that it really needs, rather than computing approximately the entire $G'$.
",1,0,0,0,0,0
5296,Depth Separation for Neural Networks,"  Let $f:\mathbb{S}^{d-1}\times \mathbb{S}^{d-1}\to\mathbb{S}$ be a function of
the form $f(\mathbf{x},\mathbf{x}') = g(\langle\mathbf{x},\mathbf{x}'\rangle)$
for $g:[-1,1]\to \mathbb{R}$. We give a simple proof that shows that poly-size
depth two neural networks with (exponentially) bounded weights cannot
approximate $f$ whenever $g$ cannot be approximated by a low degree polynomial.
Moreover, for many $g$'s, such as $g(x)=\sin(\pi d^3x)$, the number of neurons
must be $2^{\Omega\left(d\log(d)\right)}$. Furthermore, the result holds
w.r.t.\ the uniform distribution on $\mathbb{S}^{d-1}\times \mathbb{S}^{d-1}$.
As many functions of the above form can be well approximated by poly-size depth
three networks with poly-bounded weights, this establishes a separation between
depth two and depth three networks w.r.t.\ the uniform distribution on
$\mathbb{S}^{d-1}\times \mathbb{S}^{d-1}$.
",1,0,0,1,0,0
13577,Stability Conditions and Lagrangian Cobordisms,"  In this paper we study the interplay between Lagrangian cobordisms and
stability conditions. We show that any stability condition on the derived
Fukaya category $D\mathcal{F}uk(M)$ of a symplectic manifold $(M,\omega)$
induces a stability condition on the derived Fukaya category of Lagrangian
cobordisms $D\mathcal{F}uk(\mathbb{C} \times M)$. In addition, using stability
conditions, we provide general conditions under which the homomorphism $\Theta:
\Omega_{Lag}(M)\to K_0(D\mathcal{F}uk(M))$, introduced by Biran and Cornea, is
an isomorphism. This yields a better understanding of how stability conditions
affect $\Theta$ and it allows us to elucidate Haug's result, that the
Lagrangian cobordism group of $T^2$ is isomorphic to
$K_0(D\mathcal{F}uk(T^2))$.
",0,0,1,0,0,0
13796,It's Like Python But: Towards Supporting Transfer of Programming Language Knowledge,"  Expertise in programming traditionally assumes a binary novice-expert divide.
Learning resources typically target programmers who are learning programming
for the first time, or expert programmers for that language. An
underrepresented, yet important group of programmers are those that are
experienced in one programming language, but desire to author code in a
different language. For this scenario, we postulate that an effective form of
feedback is presented as a transfer from concepts in the first language to the
second. Current programming environments do not support this form of feedback.
In this study, we apply the theory of learning transfer to teach a language
that programmers are less familiar with--such as R--in terms of a programming
language they already know--such as Python. We investigate learning transfer
using a new tool called Transfer Tutor that presents explanations for R code in
terms of the equivalent Python code. Our study found that participants
leveraged learning transfer as a cognitive strategy, even when unprompted.
Participants found Transfer Tutor to be useful across a number of affordances
like stepping through and highlighting facts that may have been missed or
misunderstood. However, participants were reluctant to accept facts without
code execution or sometimes had difficulty reading explanations that are
verbose or complex. These results provide guidance for future designs and
research directions that can support learning transfer when learning new
programming languages.
",1,0,0,0,0,0
5935,Sampling a Network to Find Nodes of Interest,"  The focus of the current research is to identify people of interest in social
networks. We are especially interested in studying dark networks, which
represent illegal or covert activity. In such networks, people are unlikely to
disclose accurate information when queried. We present REDLEARN, an algorithm
for sampling dark networks with the goal of identifying as many nodes of
interest as possible. We consider two realistic lying scenarios, which describe
how individuals in a dark network may attempt to conceal their connections. We
test and present our results on several real-world multilayered networks, and
show that REDLEARN achieves up to a 340% improvement over the next best
strategy.
",1,1,0,0,0,0
5579,Volume growth in the component of fibered twists,"  For a Liouville domain $W$ whose boundary admits a periodic Reeb flow, we can
consider the connected component $[\tau] \in \pi_0(\text{Symp}^c(\widehat W))$
of fibered twists. In this paper, we investigate an entropy-type invariant,
called the slow volume growth, of the component $[\tau]$ and give a uniform
lower bound of the growth using wrapped Floer homology. We also show that
$[\tau]$ has infinite order in $\pi_0(\text{Symp}^c(\widehat W))$ if there is
an admissible Lagrangian $L$ in $W$ whose wrapped Floer homology is infinite
dimensional. We apply our results to fibered twists coming from the Milnor
fibers of $A_k$-type singularities and complements of a symplectic hypersurface
in a real symplectic manifold. They admit so-called real Lagrangians, and we
can explicitly compute wrapped Floer homology groups using a version of
Morse-Bott spectral sequences.
",0,0,1,0,0,0
1718,Faddeev-Jackiw approach of the noncommutative spacetime Podolsky electromagnetic theory,"  The interest in higher derivatives field theories has its origin mainly in
their influence concerning the renormalization properties of physical models
and to remove ultraviolet divergences. The noncommutative Podolsky theory is a
constrained system that cannot by directly quantized by the canonical way. In
this work we have used the Faddeev-Jackiw method in order to obtain the Dirac
brackets of the NC Podolsky theory.
",0,1,0,0,0,0
14503,Centralized Network Utility Maximization over Aggregate Flows,"  We study a network utility maximization (NUM) decomposition in which the set
of flow rates is grouped by source-destination pairs. We develop theorems for
both single-path and multipath cases, which relate an arbitrary NUM problem
involving all flow rates to a simpler problem involving only the aggregate
rates for each source-destination pair. The optimal aggregate flows are then
apportioned among the constituent flows of each pair. This apportionment is
simple for the case of $\alpha$-fair utility functions. We also show how the
decomposition can be implemented with the alternating direction method of
multipliers (ADMM) algorithm.
",1,0,1,0,0,0
9362,Towards security defect prediction with AI,"  In this study, we investigate the limits of the current state of the art AI
system for detecting buffer overflows and compare it with current static
analysis tools. To do so, we developed a code generator, s-bAbI, capable of
producing an arbitrarily large number of code samples of controlled complexity.
We found that the static analysis engines we examined have good precision, but
poor recall on this dataset, except for a sound static analyzer that has good
precision and recall. We found that the state of the art AI system, a memory
network modeled after Choi et al. [1], can achieve similar performance to the
static analysis engines, but requires an exhaustive amount of training data in
order to do so. Our work points towards future approaches that may solve these
problems; namely, using representations of code that can capture appropriate
scope information and using deep learning methods that are able to perform
arithmetic operations.
",0,0,0,1,0,0
13051,Discovery of water at high spectral resolution in the atmosphere of 51 Peg b,"  We report the detection of water absorption features in the dayside spectrum
of the first-known hot Jupiter, 51 Peg b, confirming the star-planet system to
be a double-lined spectroscopic binary. We used high-resolution (R~100,000),
3.2 micron spectra taken with CRIRES/VLT to trace the radial-velocity shift of
the water features in the planet's dayside atmosphere during 4 hours of its
4.23-day orbit after superior conjunction. We detect the signature of molecular
absorption by water at a significance of 5.6 sigma at a systemic velocity of
Vsys=-33+/-2 km/s, coincident with the host star, with a corresponding orbital
velocity Kp = 133^+4.3_-3.5 km/s. This translates directly to a planet mass of
Mp=0.476^+0.032_-0.031MJ, placing it at the transition boundary between Jovian
and Neptunian worlds. We determine upper and lower limits on the orbital
inclination of the system of 70<i (deg)<82.2. We also provide an updated
orbital solution for 51 Peg b, using an extensive set of 639 stellar radial
velocities measured between 1994 and 2013, finding no significant evidence of
an eccentric orbit. We find no evidence of significant absorption or emission
from other major carbon-bearing molecules of the planet, including methane and
carbon dioxide. The atmosphere is non-inverted in the temperature-pressure
region probed by these observations. The deepest absorption lines reach an
observed relative contrast of 0.9x10^-3 with respect to the host star continuum
flux, at an angular separation of 3 milliarcseconds. This work is consistent
with a previous tentative report of K-band molecular absorption for 51 Peg b by
Brogi et al. (2013).
",0,1,0,0,0,0
3104,Investigating Enactive Learning for Autonomous Intelligent Agents,"  The enactive approach to cognition is typically proposed as a viable
alternative to traditional cognitive science. Enactive cognition displaces the
explanatory focus from the internal representations of the agent to the direct
sensorimotor interaction with its environment. In this paper, we investigate
enactive learning through means of artificial agent simulations. We compare the
performances of the enactive agent to an agent operating on classical
reinforcement learning in foraging tasks within maze environments. The
characteristics of the agents are analysed in terms of the accessibility of the
environmental states, goals, and exploration/exploitation tradeoffs. We confirm
that the enactive agent can successfully interact with its environment and
learn to avoid unfavourable interactions using intrinsically defined goals. The
performance of the enactive agent is shown to be limited by the number of
affordable actions.
",1,0,0,0,0,0
12451,Minimal hard surface-unlink and classical unlink diagrams,"  We describe a method for generating minimal hard prime surface-link diagrams.
We extend the known examples of minimal hard prime classical unknot and unlink
diagrams up to three components and generate figures of all minimal hard prime
surface-unknot and surface-unlink diagrams with prime base surface components
up to ten crossings.
",0,0,1,0,0,0
6081,Online Learning with Diverse User Preferences,"  In this paper, we investigate the impact of diverse user preference on
learning under the stochastic multi-armed bandit (MAB) framework. We aim to
show that when the user preferences are sufficiently diverse and each arm can
be optimal for certain users, the O(log T) regret incurred by exploring the
sub-optimal arms under the standard stochastic MAB setting can be reduced to a
constant. Our intuition is that to achieve sub-linear regret, the number of
times an optimal arm being pulled should scale linearly in time; when all arms
are optimal for certain users and pulled frequently, the estimated arm
statistics can quickly converge to their true values, thus reducing the need of
exploration dramatically. We cast the problem into a stochastic linear bandits
model, where both the users preferences and the state of arms are modeled as
{independent and identical distributed (i.i.d)} d-dimensional random vectors.
After receiving the user preference vector at the beginning of each time slot,
the learner pulls an arm and receives a reward as the linear product of the
preference vector and the arm state vector. We also assume that the state of
the pulled arm is revealed to the learner once its pulled. We propose a
Weighted Upper Confidence Bound (W-UCB) algorithm and show that it can achieve
a constant regret when the user preferences are sufficiently diverse. The
performance of W-UCB under general setups is also completely characterized and
validated with synthetic data.
",1,0,0,1,0,0
11586,"Particles, Cutoffs and Inequivalent Representations. Fraser andWallace on Quantum Field Theory","  We critically review the recent debate between Doreen Fraser and David
Wallace on the interpretation of quantum field theory, with the aim of
identifying where the core of the disagreement lies. We show that, despite
appearances, their conflict does not concern the existence of particles or the
occurrence of unitarily inequivalent representations. Instead, the dispute
ultimately turns on the very definition of what a quantum field theory is. We
further illustrate the fundamental differences between the two approaches by
comparing them both to the Bohmian program in quantum field theory.
",0,1,0,0,0,0
7227,Envy-free Matchings with Lower Quotas,"  While every instance of the Hospitals/Residents problem admits a stable
matching, the problem with lower quotas (HR-LQ) has instances with no stable
matching. For such an instance, we expect the existence of an envy-free
matching, which is a relaxation of a stable matching preserving a kind of
fairness property. In this paper, we investigate the existence of an envy-free
matching in several settings, in which hospitals have lower quotas and not all
doctor-hospital pairs are acceptable. We first show that, for an HR-LQ
instance, we can efficiently decide the existence of an envy-free matching.
Then, we consider envy-freeness in the Classified Stable Matching model due to
Huang (2010), i.e., each hospital has lower and upper quotas on subsets of
doctors. We show that, for this model, deciding the existence of an envy-free
matching is NP-hard in general, but solvable in polynomial time if quotas are
paramodular.
",1,0,0,0,0,0
14926,APSYNSIM: An Interactive Tool To Learn Interferometry,"  The APerture SYNthesis SIMulator is a simple interactive tool to help the
students visualize and understand the basics of the Aperture Synthesis
technique, applied to astronomical interferometers. The users can load many
different interferometers and source models (and also create their own), change
the observing parameters (e.g., source coordinates, observing wavelength,
antenna location, integration time, etc.), and even deconvolve the
interferometric images and corrupt the data with gain errors (amplitude and
phase). The program is fully interactive and all the figures are updated in
real time. APSYNSIM has already been used in several interferometry schools and
has got very positive feedback from the students.
",0,1,0,0,0,0
11315,"Cosmological discordances II: Hubble constant, Planck and large-scale-structure data sets","  We examine systematically the (in)consistency between cosmological
constraints as obtained from various current data sets of the expansion
history, Large Scale Structure (LSS), and Cosmic Microwave Background (CMB)
from Planck. We run (dis)concordance tests within each set and across the sets
using a recently introduced index of inconsistency (IOI) capable of dissecting
inconsistencies between two or more data sets. First, we compare the
constraints on $H_0$ from five different methods and find that the IOI drops
from 2.85 to 0.88 (on Jeffreys' scales) when the local $H_0$ measurements is
removed. This seems to indicate that the local measurement is an outlier, thus
favoring a systematics-based explanation. We find a moderate inconsistency
(IOI=2.61) between Planck temperature and polarization. We find that current
LSS data sets including WiggleZ, SDSS RSD, CFHTLenS, CMB lensing and SZ cluster
count, are consistent one with another and when all combined. However, we find
a persistent moderate inconsistency between Planck and individual or combined
LSS probes. For Planck TT+lowTEB versus individual LSS probes, the IOI spans
the range 2.92--3.72 and increases to 3.44--4.20 when the polarization data is
added in. The joint LSS versus the combined Planck temperature and polarization
has an IOI of 2.83 in the most conservative case. But if Planck lowTEB is added
to the joint LSS to constrain $\tau$ and break degeneracies, the inconsistency
between Planck and joint LSS data increases to the high-end of the moderate
range with IOI=4.81. Whether due to systematic effects in the data or to the
underlying model, these inconsistencies need to be resolved. Finally, we
perform forecast calculations using LSST and find that the discordance between
Planck and future LSS data, if it persists as present, can rise up to a high
IOI of 17, thus falling in the strong range of inconsistency. (Abridged).
",0,1,0,0,0,0
8468,Learning Vertex Representations for Bipartite Networks,"  Recent years have witnessed a widespread increase of interest in network
representation learning (NRL). By far most research efforts have focused on NRL
for homogeneous networks like social networks where vertices are of the same
type, or heterogeneous networks like knowledge graphs where vertices (and/or
edges) are of different types. There has been relatively little research
dedicated to NRL for bipartite networks. Arguably, generic network embedding
methods like node2vec and LINE can also be applied to learn vertex embeddings
for bipartite networks by ignoring the vertex type information. However, these
methods are suboptimal in doing so, since real-world bipartite networks concern
the relationship between two types of entities, which usually exhibit different
properties and patterns from other types of network data. For example,
E-Commerce recommender systems need to capture the collaborative filtering
patterns between customers and products, and search engines need to consider
the matching signals between queries and webpages. This work addresses the
research gap of learning vertex representations for bipartite networks. We
present a new solution BiNE, short for Bipartite Network Embedding}, which
accounts for two special properties of bipartite networks: long-tail
distribution of vertex degrees and implicit connectivity relations between
vertices of the same type. Technically speaking, we make three contributions:
(1) We design a biased random walk generator to generate vertex sequences that
preserve the long-tail distribution of vertices; (2) We propose a new
optimization framework by simultaneously modeling the explicit relations (i.e.,
observed links) and implicit relations (i.e., unobserved but transitive links);
(3) We explore the theoretical foundations of BiNE to shed light on how it
works, proving that BiNE can be interpreted as factorizing multiple matrices.
",1,0,0,1,0,0
3355,On the Uplink Achievable Rate of Massive MIMO System With Low-Resolution ADC and RF Impairments,"  This paper considers channel estimation and uplink achievable rate of the
coarsely quantized massive multiple-input multiple-output (MIMO) system with
radio frequency (RF) impairments. We utilize additive quantization noise model
(AQNM) and extended error vector magnitude (EEVM) model to analyze the impacts
of low-resolution analog-to-digital converters (ADCs) and RF impairments
respectively. We show that hardware impairments cause a nonzero floor on the
channel estimation error, which contraries to the conventional case with ideal
hardware. The maximal-ratio combining (MRC) technique is then used at the
receiver, and an approximate tractable expression for the uplink achievable
rate is derived. The simulation results illustrate the appreciable
compensations between ADCs' resolution and RF impairments. The proposed studies
support the feasibility of equipping economical coarse ADCs and economical
imperfect RF components in practical massive MIMO systems.
",1,0,0,0,0,0
15655,Berry-Esséen bounds for parameter estimation of general Gaussian processes,"  We study rates of convergence in central limit theorems for the partial sum
of squares of general Gaussian sequences, using tools from analysis on Wiener
space. No assumption of stationarity, asymptotically or otherwise, is made. The
main theoretical tool is the so-called Optimal Fourth Moment Theorem
\cite{NP2015}, which provides a sharp quantitative estimate of the total
variation distance on Wiener chaos to the normal law. The only assumptions made
on the sequence are the existence of an asymptotic variance, that a
least-squares-type estimator for this variance parameter has a bias and a
variance which can be controlled, and that the sequence's auto-correlation
function, which may exhibit long memory, has a no-worse memory than that of
fractional Brownian motion with Hurst parameter }$H<3/4$.{\ \ Our main result
is explicit, exhibiting the trade-off between bias, variance, and memory. We
apply our result to study drift parameter estimation problems for subfractional
Ornstein-Uhlenbeck and bifractional Ornstein-Uhlenbeck processes with
fixed-time-step observations. These are processes which fail to be stationary
or self-similar, but for which detailed calculations result in explicit
formulas for the estimators' asymptotic normality.
",0,0,1,1,0,0
1719,Spin Transport and Accumulation in 2D Weyl Fermion System,"  In this work, we study the spin Hall effect and Rashba-Edelstein effect of a
2D Weyl fermion system in the clean limit using the Kubo formalism. Spin
transport is solely due to the spin-torque current in this strongly spin-orbit
coupled (SOC) system, and chiral spin-flip scattering off non-SOC scalar
impurities, with potential strength $V$ and size $a$, gives rise to a
skew-scattering mechanism for the spin Hall effect. The key result is that the
resultant spin-Hall angle has a fixed sign, with $\theta^{SH} \sim O
\left(\tfrac{V^2}{v_F^2/a^2} (k_F a)^4 \right)$ being a strongly-dependent
function of $k_F a$, with $k_F$ and $v_F$ being the Fermi wave-vector and Fermi
velocity respectively. This, therefore, allows for the possibility of tuning
the SHE by adjusting the Fermi energy or impurity size.
",0,1,0,0,0,0
192,Identitas: A Better Way To Be Meaningless,"  It is often recommended that identifiers for ontology terms should be
semantics-free or meaningless. In practice, ontology developers tend to use
numeric identifiers, starting at 1 and working upwards. In this paper we
present a critique of current ontology semantics-free identifiers;
monotonically increasing numbers have a number of significant usability flaws
which make them unsuitable as a default option, and we present a series of
alternatives. We have provide an implementation of these alternatives which can
be freely combined.
",1,0,0,0,0,0
13181,Nonvanishing theorems for twisted L-functions on unitary groups,"  We prove the nonvanishing of the twisted central critical values of a class
of automorphic L-functions for twists by all but finitely many unitary
characters in particular infinite families. The methods build on a
non-archimedean approach introduced by Greenberg in the context of the Birch
and Swinnerton-Dyer Conjecture. While this paper focuses on L-functions
associated to certain automorphic representations of unitary groups, it
illustrates how decades-old methods from Iwasawa theory can be combined with
the output of new machinery to achieve broader nonvanishing results.
",0,0,1,0,0,0
14147,Efficient cold outflows driven by cosmic rays in high redshift galaxies and their global effects on the IGM,"  We present semi-analytical models of galactic outflows in high redshift
galaxies driven by both hot thermal gas and non-thermal cosmic rays. Thermal
pressure alone may not sustain a large scale outflow in low mass galaxies (i.e
$M\sim 10^8$~M$_\odot$), in the presence of supernovae (SNe) feedback with
large mass loading. We show that inclusion of cosmic ray pressure allows
outflow solutions even in these galaxies. In massive galaxies for the same
energy efficiency, cosmic ray driven winds can propagate to larger distances
compared to pure thermally driven winds. On an average gas in the cosmic ray
driven winds has a lower temperature which could aid detecting it through
absorption lines in the spectra of background sources. Using our constrained
semi-analytical models of galaxy formation (that explains the observed UV
luminosity functions of galaxies) we study the influence of cosmic ray driven
winds on the properties of the intergalactic medium (IGM) at different
redshifts. In particular, we study the volume filling factor, average
metallicity, cosmic ray and magnetic field energy densities for models invoking
atomic cooled and molecular cooled halos. We show that the cosmic rays in the
IGM could have enough energy that can be transferred to the thermal gas in
presence of magnetic fields to influence the thermal history of the
intergalactic medium. The significant volume filling and resulting strength of
IGM magnetic fields can also account for recent $\gamma$-ray observations of
blazars.
",0,1,0,0,0,0
16426,Data-driven Approach to Measuring the Level of Press Freedom Using Media Attention Diversity from Unfiltered News,"  Published by Reporters Without Borders every year, the Press Freedom Index
(PFI) reflects the fear and tension in the newsroom pushed by the government
and private sectors. While the PFI is invaluable in monitoring media
environments worldwide, the current survey-based method has inherent
limitations to updates in terms of cost and time. In this work, we introduce an
alternative way to measure the level of press freedom using media attention
diversity compiled from Unfiltered News.
",1,0,0,0,0,0
4346,Resolution enhancement in in-line holography by numerical compensation of vibrations,"  Mechanical vibrations of components of the optical system is one of the
sources of blurring of interference pattern in coherent imaging systems. The
problem is especially important in holography where the resolution of the
reconstructed objects depends on the effective size of the hologram, that is on
the extent of the interference pattern, and on the contrast of the interference
fringes. We discuss the mathematical relation between the vibrations, the
hologram contrast and the reconstructed object. We show how vibrations can be
post-filtered out from the hologram or from the reconstructed object assuming a
Gaussian distribution of the vibrations. We also provide a numerical example of
compensation for directional motion blur. We demonstrate our approach for light
optical and electron holograms, acquired with both, plane- as well as
spherical-waves. As a result of such hologram deblurring, the resolution of the
reconstructed objects is enhanced by almost a factor of 2. We believe that our
approach opens up a new venue of post-experimental resolution enhancement in
in-line holography by adapting the rich database/catalogue of motion deblurring
algorithms developed for photography and image restoration applications.
",0,1,0,0,0,0
16389,On the secrecy gain of $\ell$-modular lattices,"  We show that for every $\ell>1$, there is a counterexample to the
$\ell$-modular secrecy function conjecture by Oggier, Solé and Belfiore.
These counterexamples all satisfy the modified conjecture by Ernvall-Hytönen
and Sethuraman. Furthermore, we provide a method to prove or disprove the
modified conjecture for any given $\ell$-modular lattice rationally equivalent
to a suitable amount of copies of $\mathbb{Z}\oplus \sqrt{\ell}\,\mathbb{Z}$
with $\ell \in \{3,5,7,11,23\}$. We also provide a variant of the method for
strongly $\ell$-modular lattices when $\ell\in \{6,14,15\}$.
",0,0,1,0,0,0
12211,Homological subsets of Spec,"  We investigate homological subsets of the prime spectrum of a ring, defined
by the help of the Ext-family $\{\Ext^i_R(-,R)\}$. We extend Grothendieck's
calculation of $\dim(\Ext^g_R(M,R))$. We compute support of $\Ext^i_R(M,R)$ in
many cases. Also, we answer a low-dimensional case of a problem posed by
Vasconcelos on the finiteness of associated prime ideals of
$\{\Ext^i_R(M,R)\}$. An application is given.
",0,0,1,0,0,0
15471,An Optimal Control Formulation of Pulse-Based Control Using Koopman Operator,"  In many applications, and in systems/synthetic biology, in particular, it is
desirable to compute control policies that force the trajectory of a bistable
system from one equilibrium (the initial point) to another equilibrium (the
target point), or in other words to solve the switching problem. It was
recently shown that, for monotone bistable systems, this problem admits
easy-to-implement open-loop solutions in terms of temporal pulses (i.e., step
functions of fixed length and fixed magnitude). In this paper, we develop this
idea further and formulate a problem of convergence to an equilibrium from an
arbitrary initial point. We show that this problem can be solved using a static
optimization problem in the case of monotone systems. Changing the initial
point to an arbitrary state allows to build closed-loop, event-based or
open-loop policies for the switching/convergence problems. In our derivations
we exploit the Koopman operator, which offers a linear infinite-dimensional
representation of an autonomous nonlinear system. One of the main advantages of
using the Koopman operator is the powerful computational tools developed for
this framework. Besides the presence of numerical solutions, the
switching/convergence problem can also serve as a building block for solving
more complicated control problems and can potentially be applied to
non-monotone systems. We illustrate this argument on the problem of
synchronizing cardiac cells by defibrillation. Potentially, our approach can be
extended to problems with different parametrizations of control signals since
the only fundamental limitation is the finite time application of the control
signal.
",1,0,1,0,0,0
2117,ZigZag: A new approach to adaptive online learning,"  We develop a novel family of algorithms for the online learning setting with
regret against any data sequence bounded by the empirical Rademacher complexity
of that sequence. To develop a general theory of when this type of adaptive
regret bound is achievable we establish a connection to the theory of
decoupling inequalities for martingales in Banach spaces. When the hypothesis
class is a set of linear functions bounded in some norm, such a regret bound is
achievable if and only if the norm satisfies certain decoupling inequalities
for martingales. Donald Burkholder's celebrated geometric characterization of
decoupling inequalities (1984) states that such an inequality holds if and only
if there exists a special function called a Burkholder function satisfying
certain restricted concavity properties. Our online learning algorithms are
efficient in terms of queries to this function.
We realize our general theory by giving novel efficient algorithms for
classes including lp norms, Schatten p-norms, group norms, and reproducing
kernel Hilbert spaces. The empirical Rademacher complexity regret bound implies
--- when used in the i.i.d. setting --- a data-dependent complexity bound for
excess risk after online-to-batch conversion. To showcase the power of the
empirical Rademacher complexity regret bound, we derive improved rates for a
supervised learning generalization of the online learning with low rank experts
task and for the online matrix prediction task.
In addition to obtaining tight data-dependent regret bounds, our algorithms
enjoy improved efficiency over previous techniques based on Rademacher
complexity, automatically work in the infinite horizon setting, and are
scale-free. To obtain such adaptive methods, we introduce novel machinery, and
the resulting algorithms are not based on the standard tools of online convex
optimization.
",1,0,1,1,0,0
810,A Coherent vorticity preserving eddy viscosity correction for Large-Eddy Simulation,"  This paper introduces a new approach to Large-Eddy Simulation (LES) where
subgrid-scale (SGS) dissipation is applied proportionally to the degree of
local spectral broadening, hence mitigated or deactivated in regions dominated
by large-scale and/or laminar vortical motion. The proposed Coherent vorticity
preserving (CvP) LES methodology is based on the evaluation of the ratio of the
test-filtered to resolved (or grid-filtered) enstrophy $\sigma$. Values of
$\sigma$ close to 1 indicate low sub-test-filter turbulent activity, justifying
local deactivation of the SGS dissipation. The intensity of the SGS dissipation
is progressively increased for $\sigma < 1$ which corresponds to a small-scale
spectral broadening. The SGS dissipation is then fully activated in developed
turbulence characterized by $\sigma \le \sigma_{eq}$, where the value
$\sigma_{eq}$ is derived assuming a Kolmogorov spectrum. The proposed approach
can be applied to any eddy-viscosity model, is algorithmically simple and
computationally inexpensive. LES of Taylor-Green vortex breakdown demonstrates
that the CvP methodology improves the performance of traditional, non-dynamic
dissipative SGS models, capturing the peak of total turbulent kinetic energy
dissipation during transition. Similar accuracy is obtained by adopting
Germano's dynamic procedure albeit at more than twice the computational
overhead. A CvP-LES of a pair of unstable periodic helical vortices is shown to
predict accurately the experimentally observed growth rate using coarse
resolutions. The ability of the CvP methodology to dynamically sort the
coherent, large-scale motion from the smaller, broadband scales during
transition is demonstrated via flow visualizations. LES of compressible channel
are carried out and show a good match with a reference DNS.
",1,1,0,0,0,0
154,Flatness results for nonlocal minimal cones and subgraphs,"  We show that nonlocal minimal cones which are non-singular subgraphs outside
the origin are necessarily halfspaces.
The proof is based on classical ideas of~\cite{DG1} and on the computation of
the linearized nonlocal mean curvature operator, which is proved to satisfy a
suitable maximum principle.
With this, we obtain new, and somehow simpler, proofs of the Bernstein-type
results for nonlocal minimal surfaces which have been recently established
in~\cite{FV}. In addition, we establish a new nonlocal Bernstein-Moser-type
result which classifies Lipschitz nonlocal minimal subgraphs outside a ball.
",0,0,1,0,0,0
8521,Isotope Shifts in the 7s$\rightarrow$8s Transition of Francium: Measurements and Comparison to \textit{ab initio} Theory,"  We observe the electric-dipole forbidden $7s\rightarrow8s$ transition in the
francium isotopes $^{208-211}$Fr and $^{213}$Fr using a two-photon excitation
scheme. We collect the atoms online from an accelerator and confine them in a
magneto optical trap for the measurements. In combination with previous
measurements of the $7s\rightarrow7p_{1/2}$ transition we perform a King Plot
analysis. We compare the thus determined ratio of the field shift constants
(1.230 $\pm$ 0.019) to results obtained from new ab initio calculations (1.234
$\pm$ 0.010) and find excellent agreement.
",0,1,0,0,0,0
5759,Oxidative species-induced excitonic transport in tubulin aromatic networks: Potential implications for neurodegenerative disease,"  Oxidative stress is a pathological hallmark of neurodegenerative tauopathic
disorders such as Alzheimer's disease and Parkinson's disease-related dementia,
which are characterized by altered forms of the microtubule-associated protein
(MAP) tau. MAP tau is a key protein in stabilizing the microtubule architecture
that regulates neuron morphology and synaptic strength. The precise role of
reactive oxygen species (ROS) in the tauopathic disease process, however, is
poorly understood. It is known that the production of ROS by mitochondria can
result in ultraweak photon emission (UPE) within cells. One likely absorber of
these photons is the microtubule cytoskeleton, as it forms a vast network
spanning neurons, is highly co-localized with mitochondria, and shows a high
density of aromatic amino acids. Functional microtubule networks may traffic
this ROS-generated endogenous photon energy for cellular signaling, or they may
serve as dissipaters/conduits of such energy. Experimentally, after in vitro
exposure to exogenous photons, microtubules have been shown to reorient and
reorganize in a dose-dependent manner with the greatest effect being observed
around 280 nm, in the tryptophan and tyrosine absorption range. In this paper,
recent modeling efforts based on ambient temperature experiment are presented,
showing that tubulin polymers can feasibly absorb and channel these
photoexcitations via resonance energy transfer, on the order of dendritic
length scales. Since microtubule networks are compromised in tauopathic
diseases, patients with these illnesses would be unable to support effective
channeling of these photons for signaling or dissipation. Consequent emission
surplus due to increased UPE production or decreased ability to absorb and
transfer may lead to increased cellular oxidative damage, thus hastening the
neurodegenerative process.
",0,1,0,0,0,0
13272,Localizing virtual structure sheaves by cosections,"  We construct a cosection localized virtual structure sheaf when a
Deligne-Mumford stack is equipped with a perfect obstruction theory and a
cosection of the obstruction sheaf.
",0,0,1,0,0,0
5482,A Nash Type result for Divergence Parabolic Equation related to Hormander's vector fields,"  In this paper we consider the divergence parabolic equation with bounded and
measurable coefficients related to Hormander's vector fields and establish a
Nash type result, i.e., the local Holder regularity for weak solutions. After
deriving the parabolic Sobolev inequality, (1,1) type Poincaré inequality of
Hormander's vector fields and a De Giorgi type Lemma, the Holder regularity
of weak solutions to the equation is proved based on the estimates of
oscillations of solutions and the isomorphism between parabolic Campanato space
and parabolic Holder space. As a consequence, we give the Harnack inequality
of weak solutions by showing an extension property of positivity for functions
in the De Giorgi class.
",0,0,1,0,0,0
1786,Statistical PT-symmetric lasing in an optical fiber network,"  PT-symmetry in optics is a condition whereby the real and imaginary parts of
the refractive index across a photonic structure are deliberately balanced.
This balance can lead to a host of novel optical phenomena, such as
unidirectional invisibility, loss-induced lasing, single-mode lasing from
multimode resonators, and non-reciprocal effects in conjunction with
nonlinearities. Because PT-symmetry has been thought of as fragile,
experimental realizations to date have been usually restricted to on-chip
micro-devices. Here, we demonstrate that certain features of PT-symmetry are
sufficiently robust to survive the statistical fluctuations associated with a
macroscopic optical cavity. We construct optical-fiber-based coupled-cavities
in excess of a kilometer in length (the free spectral range is less than 0.8
fm) with balanced gain and loss in two sub-cavities and examine the lasing
dynamics. In such a macroscopic system, fluctuations can lead to a
cavity-detuning exceeding the free spectral range. Nevertheless, by varying the
gain-loss contrast, we observe that both the lasing threshold and the growth of
the laser power follow the predicted behavior of a stable PT-symmetric
structure. Furthermore, a statistical symmetry-breaking point is observed upon
varying the cavity loss. These findings indicate that PT-symmetry is a more
robust optical phenomenon than previously expected, and points to potential
applications in optical fiber networks and fiber lasers.
",0,1,0,0,0,0
6966,"Combined analysis of galaxy cluster number count, thermal Sunyaev-Zel'dovich power spectrum, and bispectrum","  The Sunyaev-Zel'dovich (SZ) effect is a powerful probe of the evolution of
structures in the universe, and is thus highly sensitive to cosmological
parameters $\sigma_8$ and $\Omega_m$, though its power is hampered by the
current uncertainties on the cluster mass calibration. In this analysis we
revisit constraints on these cosmological parameters as well as the hydrostatic
mass bias, by performing (i) a robust estimation of the tSZ power-spectrum,
(ii) a complete modeling and analysis of the tSZ bispectrum, and (iii) a
combined analysis of galaxy clusters number count, tSZ power spectrum, and tSZ
bispectrum. From this analysis, we derive as final constraints $\sigma_8 = 0.79
\pm 0.02$, $\Omega_{\rm m} = 0.29 \pm 0.02$, and $(1-b) = 0.71 \pm 0.07$. These
results favour a high value for the hydrostatic mass bias compared to numerical
simulations and weak-lensing based estimations. They are furthermore consistent
with both previous tSZ analyses, CMB derived cosmological parameters, and
ancillary estimations of the hydrostatic mass bias.
",0,1,0,0,0,0
20905,Robust Photometric Stereo Using Learned Image and Gradient Dictionaries,"  Photometric stereo is a method for estimating the normal vectors of an object
from images of the object under varying lighting conditions. Motivated by
several recent works that extend photometric stereo to more general objects and
lighting conditions, we study a new robust approach to photometric stereo that
utilizes dictionary learning. Specifically, we propose and analyze two
approaches to adaptive dictionary regularization for the photometric stereo
problem. First, we propose an image preprocessing step that utilizes an
adaptive dictionary learning model to remove noise and other non-idealities
from the image dataset before estimating the normal vectors. We also propose an
alternative model where we directly apply the adaptive dictionary
regularization to the normal vectors themselves during estimation. We study the
practical performance of both methods through extensive simulations, which
demonstrate the state-of-the-art performance of both methods in the presence of
noise.
",0,0,0,1,0,0
12708,From Abstract Entities in Mathematics to Superposition States in Quantum Mechanics,"  Given an equivalence relation ~ on a set U, there are two abstract notions of
an element of the quotient set U/~. The #1 abstract notion is a set S=[u] of
equivalent elements of U (an equivalence class); the #2 notion is an abstract
entity u_{S} that is definite on what is common to the elements of the
equivalence class S but is otherwise indefinite on the differences between
those elements. For instance, the #1 interpretation of a homotopy type is an
equivalence class of homotopic spaces, but the #2 interpretation, e.g., as
developed in homotopy type theory, is an abstract space (without points) that
has the properties that are in common to the spaces in the equivalence class
but is otherwise indefinite. In philosophy, the #2 abstract entities might be
called paradigm-universals, e.g., `the white thing' as opposed to the #1
abstract notion of ""the set of white things"" (out of some given collection U).
The paper shows how this #2 notion of a paradigm may be mathematically modeled
using incidence matrices in Boolean logic and density matrices in probability
theory. Then we cross the bridge to the density matrix treatment of the
indefinite superposition states in quantum mechanics (QM). This connection
between the #2 abstracts in mathematics and ontic indefinite states in QM
elucidates Abner Shimony's literal or objective indefiniteness interpretation
of QM.
",0,1,1,0,0,0
7477,Multiband Electronic Structure of Magnetic Quantum Dots: Numerical Studies,"  Semiconductor quantum dots (QDs) doped with magnetic impurities have been a
focus of continuous research for a couple of decades. A significant effort has
been devoted to studies of magnetic polarons (MP) in these nanostructures.
These collective states arise through exchange interaction between a carrier
confined in a QD and localized spins of the magnetic impurities (typically:
Mn). We discuss our theoretical description of various MP properties in
self-assembled QDs. We present a self-consistent, temperature-dependent
approach to MPs formed by a valence band hole. We use the Luttinger-Kohn k.p
Hamiltonian to account for the important effects of spin-orbit interaction.
",0,1,0,0,0,0
19039,Stability criteria for the 2D $α$-Euler equations,"  We derive analogues of the classical Rayleigh, Fjortoft and Arnold stability
and instability theorems in the context of the 2D $\alpha$-Euler equations.
",0,1,0,0,0,0
19835,High resolution ion trap time-of-flight mass spectrometer for cold trapped ion experiments,"  Trapping molecular ions that have been sympathetically cooled with
laser-cooled atomic ions is a useful platform for exploring cold ion chemistry.
We designed and characterized a new experimental apparatus for probing chemical
reaction dynamics between molecular cations and neutral radicals at
temperatures below 1 K. The ions are trapped in a linear quadrupole
radio-frequency trap and sympathetically cooled by co-trapped, laser-cooled,
atomic ions. The ion trap is coupled to a time-of-flight mass spectrometer to
readily identify product ion species, as well as to accurately determine
trapped ion numbers. We discuss, and present in detail, the design of this ion
trap time-of-flight mass spectrometer, as well as the electronics required for
driving the trap and mass spectrometer. Furthermore, we measure the performance
of this system, which yields mass resolutions of $m/\Delta{}m \geq 1100$ over a
wide mass range, and discuss its relevance for future measurements in chemical
reaction kinetics and dynamics.
",0,1,0,0,0,0
12113,Integrable Floquet dynamics,"  We discuss several classes of integrable Floquet systems, i.e. systems which
do not exhibit chaotic behavior even under a time dependent perturbation. The
first class is associated with finite-dimensional Lie groups and
infinite-dimensional generalization thereof. The second class is related to the
row transfer matrices of the 2D statistical mechanics models. The third class
of models, called here ""boost models"", is constructed as a periodic interchange
of two Hamiltonians - one is the integrable lattice model Hamiltonian, while
the second is the boost operator. The latter for known cases coincides with the
entanglement Hamiltonian and is closely related to the corner transfer matrix
of the corresponding 2D statistical models. We present several explicit
examples. As an interesting application of the boost models we discuss a
possibility of generating periodically oscillating states with the period
different from that of the driving field. In particular, one can realize an
oscillating state by performing a static quench to a boost operator. We term
this state a ""Quantum Boost Clock"". All analyzed setups can be readily realized
experimentally, for example in cod atoms.
",0,1,1,0,0,0
19140,Singly-Thermostated Ergodicity in Gibbs' Canonical Ensemble and the 2016 Ian Snook Prize Award,"  The 2016 Snook Prize has been awarded to Diego Tapias, Alessandro Bravetti,
and David Sanders for their paper -- Ergodicity of One-Dimensional Systems
Coupled to the Logistic Thermostat. They introduced a relatively stiff
hyperbolic tangent thermostat force and successfully tested its ability to
reproduce Gibbs' canonical distribution for the harmonic oscillator, the
quartic oscillator, and the Mexican Hat potentials. Their work constitutes an
effective response to the 2016 Ian Snook Prize Award goal -- Finding ergodic
algorithms for Gibbs' canonical ensemble using a single thermostat variable. We
confirm their work here and highlight an interesting feature of the Mexican Hat
problem when it is solved with an adaptive integrator.
",0,1,0,0,0,0
8900,Tug-of-War: Observations on Unified Content Handling,"  Modern applications and Operating Systems vary greatly with respect to how
they register and identify different types of content. These discrepancies lead
to exploits and inconsistencies in user experience. In this paper, we highlight
the issues arising in the modern content handling ecosystem, and examine how
the operating system can be used to achieve unified and consistent content
identification.
",1,0,0,0,0,0
16613,Uncertainty quantification for kinetic models in socio-economic and life sciences,"  Kinetic equations play a major rule in modeling large systems of interacting
particles. Recently the legacy of classical kinetic theory found novel
applications in socio-economic and life sciences, where processes characterized
by large groups of agents exhibit spontaneous emergence of social structures.
Well-known examples are the formation of clusters in opinion dynamics, the
appearance of inequalities in wealth distributions, flocking and milling
behaviors in swarming models, synchronization phenomena in biological systems
and lane formation in pedestrian traffic. The construction of kinetic models
describing the above processes, however, has to face the difficulty of the lack
of fundamental principles since physical forces are replaced by empirical
social forces. These empirical forces are typically constructed with the aim to
reproduce qualitatively the observed system behaviors, like the emergence of
social structures, and are at best known in terms of statistical information of
the modeling parameters. For this reason the presence of random inputs
characterizing the parameters uncertainty should be considered as an essential
feature in the modeling process. In this survey we introduce several examples
of such kinetic models, that are mathematically described by nonlinear Vlasov
and Fokker--Planck equations, and present different numerical approaches for
uncertainty quantification which preserve the main features of the kinetic
solution.
",0,1,0,0,0,0
9716,Nematic phase with colossal magnetoresistance and orbital polarons in manganite La$_{1-x}$Sr$_x$MnO$_3$,"  The origin of colossal magnetoresistance (CMR) is still controversial. The
spin dynamics of La$_{1-x}$Sr$_x$MnO$_3$ is revisited along the Mn-O-Mn
direction at $x\leq 0.5$, $T\leq T_C$ with a new study at $x$=0.4. A new
lattice dynamics study is also reported at $x_0$=0.2,representative of the
optimal doping for CMR. In large-$q$ wavevector range, typical of the scale of
polarons, spin dynamics exhibits a discrete spectrum, $E^n_{\rm mag}$ with $n$
equal to the degeneracy of orbital-pseudospin transitions and energy values in
coincidence with the phonon ones. It corresponds to the spin-orbital excitation
spectrum of short life-time polarons, in which the orbital pseudospin
degeneracy is lift by phonons. For $x\neq x_0$, its q-range reveals a $\ell
\approx 1.7a$ size of polarons with a dimension $2d$ at $x=1/8$ partly
increasing to $\approx$ $3d$ at $x=0.3$. At $x_0=0.2$ ($T<T_C$) two distinct
$q$ and energy ranges appear separated by $\Delta E(q_0\approx 0.35)=3meV$. The
same $\Delta E(q_0)$ value separates two unusual transverse acoustic branches
($T>T_C$). Both characterize a nematic-phase defined by chains of ""orbital
polarons"" of $2a$ size, distant from $3a$, typical of $x_0=1/6$. It could
explain CMR.
",0,1,0,0,0,0
2098,Uniform Rates of Convergence of Some Representations of Extremes : a first approach,"  Uniform convergence rates are provided for asymptotic representations of
sample extremes. These bounds which are universal in the sense that they do not
depend on the extreme value index are meant to be extended to arbitrary samples
extremes in coming papers.
",0,0,0,1,0,0
20149,Restriction of Odd Degree Characters of $\mathfrak{S}_n$,"  Let $n$ and $k$ be natural numbers such that $2^k < n$. We study the
restriction to $\mathfrak{S}_{n-2^k}$ of odd-degree irreducible characters of
the symmetric group $\mathfrak{S}_n$. This analysis completes the study begun
in [Ayyer A., Prasad A., Spallone S., Sem. Lothar. Combin. 75 (2015), Art.
B75g, 13 pages] and recently developed in [Isaacs I.M., Navarro G., Olsson
J.B., Tiep P.H., J. Algebra 478 (2017), 271-282].
",0,0,1,0,0,0
1213,Ricci solitons on Ricci pseudosymmetric $(LCS)_n$-manifolds,"  The object of the present paper is to study some types of Ricci
pseudosymmetric $(LCS)_n$-manifolds whose metric is Ricci soliton. We found the
conditions when Ricci soliton on concircular Ricci pseudosymmetric, projective
Ricci pseudosymmetric, $W_{3}$-Ricci pseudosymmetric, conharmonic Ricci
pseudosymmetric, conformal Ricci pseudosymmetric $(LCS)_n$-manifolds to be
shrinking, steady and expanding. We also construct an example of concircular
Ricci pseudosymmetric $(LCS)_3$-manifold whose metric is Ricci soliton.
",0,0,1,0,0,0
10612,Third-Person Imitation Learning,"  Reinforcement learning (RL) makes it possible to train agents capable of
achiev- ing sophisticated goals in complex and uncertain environments. A key
difficulty in reinforcement learning is specifying a reward function for the
agent to optimize. Traditionally, imitation learning in RL has been used to
overcome this problem. Unfortunately, hitherto imitation learning methods tend
to require that demonstra- tions are supplied in the first-person: the agent is
provided with a sequence of states and a specification of the actions that it
should have taken. While powerful, this kind of imitation learning is limited
by the relatively hard problem of collect- ing first-person demonstrations.
Humans address this problem by learning from third-person demonstrations: they
observe other humans perform tasks, infer the task, and accomplish the same
task themselves.
In this paper, we present a method for unsupervised third-person imitation
learn- ing. Here third-person refers to training an agent to correctly achieve
a simple goal in a simple environment when it is provided a demonstration of a
teacher achieving the same goal but from a different viewpoint; and
unsupervised refers to the fact that the agent receives only these third-person
demonstrations, and is not provided a correspondence between teacher states and
student states. Our methods primary insight is that recent advances from domain
confusion can be utilized to yield domain agnostic features which are crucial
during the training process. To validate our approach, we report successful
experiments on learning from third-person demonstrations in a pointmass domain,
a reacher domain, and inverted pendulum.
",1,0,0,0,0,0
6744,A Hierarchical Bayes Approach to Adjust for Selection Bias in Before-After Analyses of Vision Zero Policies,"  American cities devote significant resources to the implementation of traffic
safety countermeasures that prevent pedestrian fatalities. However, the
before-after comparisons typically used to evaluate the success of these
countermeasures often suffer from selection bias. This paper motivates the
tendency for selection bias to overestimate the benefits of traffic safety
policy, using New York City's Vision Zero strategy as an example. The NASS
General Estimates System, Fatality Analysis Reporting System and other
databases are combined into a Bayesian hierarchical model to calculate a more
realistic before-after comparison. The results confirm the before-after
analysis of New York City's Vision Zero policy did in fact overestimate the
effect of the policy, and a more realistic estimate is roughly two-thirds the
size.
",0,0,0,1,0,0
8129,Dynamics of the scenery flow and conical density theorems,"  Conical density theorems are used in the geometric measure theory to derive
geometric information from given metric information. The idea is to examine how
a measure is distributed in small balls. Finding conditions that guarantee the
measure to be effectively spread out in different directions is a classical
question going back to Besicovitch (1938) and Marstrand (1954). Classically,
conical density theorems deal with the distribution of the Hausdorff measure.
The process of taking blow-ups of a measure around a point induces a natural
dynamical system called the scenery flow. Relying on this dynamics makes it
possible to apply ergodic-theoretical methods to understand the statistical
behavior of tangent measures. This approach was initiated by Furstenberg (1970,
2008) and greatly developed by Hochman (2010). The scenery flow is a
well-suited tool to address problems concerning conical densities.
In this survey, we demonstrate how to develop the ergodic-theoretical
machinery around the scenery flow and use it to study conical density theorems.
",0,0,1,0,0,0
6369,"Poverty Mapping Using Convolutional Neural Networks Trained on High and Medium Resolution Satellite Images, With an Application in Mexico","  Mapping the spatial distribution of poverty in developing countries remains
an important and costly challenge. These ""poverty maps"" are key inputs for
poverty targeting, public goods provision, political accountability, and impact
evaluation, that are all the more important given the geographic dispersion of
the remaining bottom billion severely poor individuals. In this paper we train
Convolutional Neural Networks (CNNs) to estimate poverty directly from high and
medium resolution satellite images. We use both Planet and Digital Globe
imagery with spatial resolutions of 3-5 sq. m. and 50 sq. cm. respectively,
covering all 2 million sq. km. of Mexico. Benchmark poverty estimates come from
the 2014 MCS-ENIGH combined with the 2015 Intercensus and are used to estimate
poverty rates for 2,456 Mexican municipalities. CNNs are trained using the 896
municipalities in the 2014 MCS-ENIGH. We experiment with several architectures
(GoogleNet, VGG) and use GoogleNet as a final architecture where weights are
fine-tuned from ImageNet. We find that 1) the best models, which incorporate
satellite-estimated land use as a predictor, explain approximately 57% of the
variation in poverty in a validation sample of 10 percent of MCS-ENIGH
municipalities; 2) Across all MCS-ENIGH municipalities explanatory power
reduces to 44% in a CNN prediction and landcover model; 3) Predicted poverty
from the CNN predictions alone explains 47% of the variation in poverty in the
validation sample, and 37% over all MCS-ENIGH municipalities; 4) In urban areas
we see slight improvements from using Digital Globe versus Planet imagery,
which explain 61% and 54% of poverty variation respectively. We conclude that
CNNs can be trained end-to-end on satellite imagery to estimate poverty,
although there is much work to be done to understand how the training process
influences out of sample validation.
",1,0,0,1,0,0
12025,Discrete diffusion Lyman-alpha radiative transfer,"  Due to its accuracy and generality, Monte Carlo radiative transfer (MCRT) has
emerged as the prevalent method for Ly$\alpha$ radiative transfer in arbitrary
geometries. The standard MCRT encounters a significant efficiency barrier in
the high optical depth, diffusion regime. Multiple acceleration schemes have
been developed to improve the efficiency of MCRT but the noise from photon
packet discretization remains a challenge. The discrete diffusion Monte Carlo
(DDMC) scheme has been successfully applied in state-of-the-art radiation
hydrodynamics (RHD) simulations. Still, the established framework is not
optimal for resonant line transfer. Inspired by the DDMC paradigm, we present a
novel extension to resonant DDMC (rDDMC) in which diffusion in space and
frequency are treated on equal footing. We explore the robustness of our new
method and demonstrate a level of performance that justifies incorporating the
method into existing Ly$\alpha$ codes. We present computational speedups of
$\sim 10^2$-$10^6$ relative to contemporary MCRT implementations with schemes
that skip scattering in the core of the line profile. This is because the rDDMC
runtime scales with the spatial and frequency resolution rather than the number
of scatterings - the latter is typically $\propto \tau_0$ for static media, or
$\propto (a \tau_0)^{2/3}$ with core-skipping. We anticipate new frontiers in
which on-the-fly Ly$\alpha$ radiative transfer calculations are feasible in 3D
RHD. More generally, rDDMC is transferable to any computationally demanding
problem amenable to a Fokker-Planck approximation of frequency redistribution.
",0,1,0,0,0,0
14757,The Recommendation System to SNS Community for Tourists by Using Altruistic Behaviors,"  We have already developed the recommendation system of sightseeing
information on SNS by using smartphone based user participatory sensing system.
The system can post the attractive information for tourists to the specified
Facebook page by our developed smartphone application. The users in Facebook,
who are interested in sightseeing, can come flocking through information space
from far and near. However, the activities in the community on SNS are only
supported by the specified people called a hub. We proposed the method of
vitalization of tourist behaviors to give a stimulus to the people. We
developed the simulation system for multi agent system with altruistic
behaviors inspired by the Army Ants. The army ant takes feeding action with
altruistic behaviors to suppress selfish behavior to a common object used by a
plurality of users in common. In this paper, we introduced the altruism
behavior determined by some simulation to vitalize the SNS community. The
efficiency of the revitalization process of the community was investigated by
some experimental simulation results.
",1,0,0,0,0,0
16380,Reconstruction of stochastic 3-D signals with symmetric statistics from 2-D projection images motivated by cryo-electron microscopy,"  Cryo-electron microscopy provides 2-D projection images of the 3-D electron
scattering intensity of many instances of the particle under study (e.g., a
virus). Both symmetry (rotational point groups) and heterogeneity are important
aspects of biological particles and both aspects can be combined by describing
the electron scattering intensity of the particle as a stochastic process with
a symmetric probability law and therefore symmetric moments. A maximum
likelihood estimator implemented by an expectation-maximization algorithm is
described which estimates the unknown statistics of the electron scattering
intensity stochastic process from images of instances of the particle. The
algorithm is demonstrated on the bacteriophage HK97 and the virus N$\omega$V.
The results are contrasted with existing algorithms which assume that each
instance of the particle has the symmetry rather than the less restrictive
assumption that the probability law has the symmetry.
",0,1,0,1,0,0
20174,Critical behavior of quasi-two-dimensional semiconducting ferromagnet CrGeTe$_3$,"  The critical properties of the single-crystalline semiconducting ferromagnet
CrGeTe$_3$ were investigated by bulk dc magnetization around the paramagnetic
to ferromagnetic phase transition. Critical exponents $\beta = 0.200\pm0.003$
with critical temperature $T_c = 62.65\pm0.07$ K and $\gamma = 1.28\pm0.03$
with $T_c = 62.75\pm0.06$ K are obtained by the Kouvel-Fisher method whereas
$\delta = 7.96\pm0.01$ is obtained by the critical isotherm analysis at $T_c =
62.7$ K. These critical exponents obey the Widom scaling relation $\delta =
1+\gamma/\beta$, indicating self-consistency of the obtained values. With these
critical exponents the isotherm $M(H)$ curves below and above the critical
temperatures collapse into two independent universal branches, obeying the
single scaling equation $m = f_\pm(h)$, where $m$ and $h$ are renormalized
magnetization and field, respectively. The determined exponents match well with
those calculated from the results of renormalization group approach for a
two-dimensional Ising system coupled with long-range interaction between spins
decaying as $J(r)\approx r^{-(d+\sigma)}$ with $\sigma=1.52$.
",0,1,0,0,0,0
8590,An Extension of Averaged-Operator-Based Algorithms,"  Many of the algorithms used to solve minimization problems with
sparsity-inducing regularizers are generic in the sense that they do not take
into account the sparsity of the solution in any particular way. However,
algorithms known as semismooth Newton are able to take advantage of this
sparsity to accelerate their convergence. We show how to extend these
algorithms in different directions, and study the convergence of the resulting
algorithms by showing that they are a particular case of an extension of the
well-known Krasnosel'ski\u{\i}--Mann scheme.
",0,0,0,1,0,0
6739,On the $L^p$ boundedness of wave operators for two-dimensional Schrödinger operators with threshold obstructions,"  Let $H=-\Delta+V$ be a Schrödinger operator on $L^2(\mathbb R^2)$ with
real-valued potential $V$, and let $H_0=-\Delta$. If $V$ has sufficient
pointwise decay, the wave operators $W_{\pm}=s-\lim_{t\to \pm\infty}
e^{itH}e^{-itH_0}$ are known to be bounded on $L^p(\mathbb R^2)$ for all $1< p<
\infty$ if zero is not an eigenvalue or resonance. We show that if there is an
s-wave resonance or an eigenvalue only at zero, then the wave operators are
bounded on $L^p(\mathbb R^2)$ for $1 < p<\infty$. This result stands in
contrast to results in higher dimensions, where the presence of zero energy
obstructions is known to shrink the range of valid exponents $p$.
",0,0,1,0,0,0
14611,Arrow calculus for welded and classical links,"  We develop a calculus for diagrams of knotted objects. We define Arrow
presentations, which encode the crossing informations of a diagram into arrows
in a way somewhat similar to Gauss diagrams, and more generally w-tree
presentations, which can be seen as `higher order Gauss diagrams'. This Arrow
calculus is used to develop an analogue of Habiro's clasper theory for welded
knotted objects, which contain classical link diagrams as a subset. This
provides a 'realization' of Polyak's algebra of arrow diagrams at the welded
level, and leads to a characterization of finite type invariants of welded
knots and long knots. As a corollary, we recover several topological results
due to K. Habiro and A. Shima and to T. Watanabe on knotted surfaces in
4-space. We also classify welded string links up to homotopy, thus recovering a
result of the first author with B. Audoux, P. Bellingeri and E. Wagner.
",0,0,1,0,0,0
14313,On the maximal directional Hilbert transform in three dimensions,"  We establish the sharp growth rate, in terms of cardinality, of the $L^p$
norms of the maximal Hilbert transform $H_\Omega$ along finite subsets of a
finite order lacunary set of directions $\Omega \subset \mathbb R^3$, answering
a question of Parcet and Rogers in dimension $n=3$. Our result is the first
sharp estimate for maximal directional singular integrals in dimensions greater
than 2.
The proof relies on a representation of the maximal directional Hilbert
transform in terms of a model maximal operator associated to compositions of
two-dimensional angular multipliers, as well as on the usage of weighted norm
inequalities, and their extrapolation, in the directional setting.
",0,0,1,0,0,0
20854,Data-driven Analytics for Business Architectures: Proposed Use of Graph Theory,"  Business Architecture (BA) plays a significant role in helping organizations
understand enterprise structures and processes, and align them with strategic
objectives. However, traditional BAs are represented in fixed structure with
static model elements and fail to dynamically capture business insights based
on internal and external data. To solve this problem, this paper introduces the
graph theory into BAs with aim of building extensible data-driven analytics and
automatically generating business insights. We use IBM's Component Business
Model (CBM) as an example to illustrate various ways in which graph theory can
be leveraged for data-driven analytics, including what and how business
insights can be obtained. Future directions for applying graph theory to
business architecture analytics are discussed.
",0,0,0,1,0,0
20856,Phase-type distributions in population genetics,"  Probability modelling for DNA sequence evolution is well established and
provides a rich framework for understanding genetic variation between samples
of individuals from one or more populations. We show that both classical and
more recent models for coalescence (with or without recombination) can be
described in terms of the so-called phase-type theory, where complicated and
tedious calculations are circumvented by the use of matrices. The application
of phase-type theory consists of describing the stochastic model as a Markov
model by appropriately setting up a state space and calculating the
corresponding intensity and reward matrices. Formulae of interest are then
expressed in terms of these aforementioned matrices. We illustrate this by a
few examples calculating the mean, variance and even higher order moments of
the site frequency spectrum in the multiple merger coalescent models, and by
analysing the mean and variance for the number of segregating sites for
multiple samples in the two-locus ancestral recombination graph. We believe
that phase-type theory has great potential as a tool for analysing probability
models in population genetics. The compact matrix notation is useful for
clarification of current models, in particular their formal manipulation
(calculation), but also for further development or extensions.
",0,0,0,1,1,0
1591,Persuasive Technology For Human Development: Review and Case Study,"  Technology is an extremely potent tool that can be leveraged for human
development and social good. Owing to the great importance of environment and
human psychology in driving human behavior, and the ubiquity of technology in
modern life, there is a need to leverage the insights and capabilities of both
fields together for nudging people towards a behavior that is optimal in some
sense (personal or social). In this regard, the field of persuasive technology,
which proposes to infuse technology with appropriate design and incentives
using insights from psychology, behavioral economics, and human-computer
interaction holds a lot of promise. Whilst persuasive technology is already
being developed and is at play in many commercial applications, it can have the
great social impact in the field of Information and Communication Technology
for Development (ICTD) which uses Information and Communication Technology
(ICT) for human developmental ends such as education and health. In this paper
we will explore what persuasive technology is and how it can be used for the
ends of human development. To develop the ideas in a concrete setting, we
present a case study outlining how persuasive technology can be used for human
development in Pakistan, a developing South Asian country, that suffers from
many of the problems that plague typical developing country.
",1,0,0,0,0,0
4285,Generalizing the first-difference correlated random walk for marine animal movement data,"  Animal telemetry data are often analysed with discrete time movement models
assuming rotation in the movement. These models are defined with equidistant
distant time steps. However, telemetry data from marine animals are observed
irregularly. To account for irregular data, a time-irregularised
first-difference correlated random walk model with drift is introduced. The
model generalizes the commonly used first-difference correlated random walk
with regular time steps by allowing irregular time steps, including a drift
term, and by allowing different autocorrelation in the two coordinates. The
model is applied to data from a ringed seal collected through the Argos
satellite system, and is compared to related movement models through
simulations. Accounting for irregular data in the movement model results in
accurate parameter estimates and reconstruction of movement paths. Measured by
distance, the introduced model can provide more accurate movement paths than
the regular time counterpart. Extracting accurate movement paths from uncertain
telemetry data is important for evaluating space use patterns for marine
animals, which in turn is crucial for management. Further, handling irregular
data directly in the movement model allows efficient simultaneous analysis of
several animals.
",0,0,0,0,1,0
747,Input-to-State Stability of a Clamped-Free Damped String in the Presence of Distributed and Boundary Disturbances,"  This note establishes the input-to-state stability (ISS) property for a
clamped-free damped string with respect to distributed and boundary
disturbances. While efficient methods for establishing ISS properties for
distributed parameter systems with respect to distributed disturbances have
been developed during the last decades, establishing ISS properties with
respect to boundary disturbances remains challenging. One of the well-known
methods for well-posedness analysis of systems with boundary inputs is to use
an adequate lifting operator, which transfers the boundary disturbance to a
distributed one. However, the resulting distributed disturbance involves time
derivatives of the boundary perturbation. Thus, the subsequent ISS estimate
depends on its amplitude, and may not be expressed in the strict form of ISS
properties. To solve this problem, we show for a clamped-free damped string
equation that the projection of the original system trajectories in an adequate
Riesz basis can be used to establish the desired ISS property.
",1,0,0,0,0,0
15587,Baryon acoustic oscillations from the complete SDSS-III Ly$α$-quasar cross-correlation function at $z=2.4$,"  We present a measurement of baryon acoustic oscillations (BAO) in the
cross-correlation of quasars with the Ly$\alpha$-forest flux-transmission at a
mean redshift $z=2.40$. The measurement uses the complete SDSS-III data sample:
168,889 forests and 234,367 quasars from the SDSS Data Release DR12. In
addition to the statistical improvement on our previous study using DR11, we
have implemented numerous improvements at the analysis level allowing a more
accurate measurement of this cross-correlation. We also developed the first
simulations of the cross-correlation allowing us to test different aspects of
our data analysis and to search for potential systematic errors in the
determination of the BAO peak position. We measure the two ratios
$D_{H}(z=2.40)/r_{d} = 9.01 \pm 0.36$ and $D_{M}(z=2.40)/r_{d} = 35.7 \pm 1.7$,
where the errors include marginalization over the non-linear velocity of
quasars and the metal - quasar cross-correlation contribution, among other
effects. These results are within $1.8\sigma$ of the prediction of the
flat-$\Lambda$CDM model describing the observed CMB anisotropies. We combine
this study with the Ly$\alpha$-forest auto-correlation function
[2017A&A...603A..12B], yielding $D_{H}(z=2.40)/r_{d} = 8.94 \pm 0.22$ and
$D_{M}(z=2.40)/r_{d} = 36.6 \pm 1.2$, within $2.3\sigma$ of the same
flat-$\Lambda$CDM model.
",0,1,0,0,0,0
9620,Control of Ultracold Photodissociation with Magnetic Fields,"  Photodissociation of a molecule produces a spatial distribution of
photofragments determined by the molecular structure and the characteristics of
the dissociating light. Performing this basic chemical reaction at ultracold
temperatures allows its quantum mechanical features to dominate. In this
regime, weak applied fields can be used to control the reaction. Here, we
photodissociate ultracold diatomic strontium in magnetic fields below 10 G and
observe striking changes in photofragment angular distributions. The
observations are in excellent qualitative agreement with a multichannel quantum
chemistry model that includes nonadiabatic effects and predicts strong mixing
of partial waves in the photofragment energy continuum. The experiment is
enabled by precise quantum-state control of the molecules.
",0,1,0,0,0,0
11763,Detection and Tracking of General Movable Objects in Large 3D Maps,"  This paper studies the problem of detection and tracking of general objects
with long-term dynamics, observed by a mobile robot moving in a large
environment. A key problem is that due to the environment scale, it can only
observe a subset of the objects at any given time. Since some time passes
between observations of objects in different places, the objects might be moved
when the robot is not there. We propose a model for this movement in which the
objects typically only move locally, but with some small probability they jump
longer distances, through what we call global motion. For filtering, we
decompose the posterior over local and global movements into two linked
processes. The posterior over the global movements and measurement associations
is sampled, while we track the local movement analytically using Kalman
filters. This novel filter is evaluated on point cloud data gathered
autonomously by a mobile robot over an extended period of time. We show that
tracking jumping objects is feasible, and that the proposed probabilistic
treatment outperforms previous methods when applied to real world data. The key
to efficient probabilistic tracking in this scenario is focused sampling of the
object posteriors.
",1,0,0,0,0,0
53,"Parallelism, Concurrency and Distribution in Constraint Handling Rules: A Survey","  Constraint Handling Rules is an effective concurrent declarative programming
language and a versatile computational logic formalism. CHR programs consist of
guarded reactive rules that transform multisets of constraints. One of the main
features of CHR is its inherent concurrency. Intuitively, rules can be applied
to parts of a multiset in parallel. In this comprehensive survey, we give an
overview of concurrent and parallel as well as distributed CHR semantics,
standard and more exotic, that have been proposed over the years at various
levels of refinement. These semantics range from the abstract to the concrete.
They are related by formal soundness results. Their correctness is established
as correspondence between parallel and sequential computations. We present
common concise sample CHR programs that have been widely used in experiments
and benchmarks. We review parallel CHR implementations in software and
hardware. The experimental results obtained show a consistent parallel speedup.
Most implementations are available online. The CHR formalism can also be used
to implement and reason with models for concurrency. To this end, the Software
Transaction Model, the Actor Model, Colored Petri Nets and the Join-Calculus
have been faithfully encoded in CHR. Under consideration in Theory and Practice
of Logic Programming (TPLP).
",1,0,0,0,0,0
15072,Properties of Hydrogen Bonds in the Protic Ionic Liquid Ethylammonium Nitrate. DFT versus DFTB Molecular Dynamics,"  Comparative molecular dynamics simulations of a hexamer cluster of the protic
ionic liquid ethylammonium nitrate are performed using density functional
theory (DFT) and density functional-based tight binding (DFTB) methods. The
focus is on assessing the performance of the DFTB approach to describe the
dynamics and infrared spectroscopic signatures of hydrogen bonding between the
ions. Average geometries and geometric correlations are found to be rather
similar. The same holds true for the far-infrared spectral region. Differences
are more pronounced for the NH- and CH-stretching band, where DFTB predicts a
broader intensity distribution. DFTB completely fails to describe the
fingerprint range shaped by nitrate anion vibrations. Finally, charge
fluctuations within the H-bonds are characterized yielding moderate
dependencies on geometry. On the basis of these results, DFTB is recommend for
the simulation of H-bond properties of this type of ionic liquids.
",0,1,0,0,0,0
14066,Orbital Graphs,"  We introduce orbital graphs and discuss some of their basic properties. Then
we focus on their usefulness for search algorithms for permutation groups,
including finding the intersection of groups and the stabilizer of sets in a
group.
",1,0,1,0,0,0
14332,On the Throughput of Channels that Wear Out,"  This work investigates the fundamental limits of communication over a noisy
discrete memoryless channel that wears out, in the sense of signal-dependent
catastrophic failure. In particular, we consider a channel that starts as a
memoryless binary-input channel and when the number of transmitted ones causes
a sufficient amount of damage, the channel ceases to convey signals. Constant
composition codes are adopted to obtain an achievability bound and the
left-concave right-convex inequality is then refined to obtain a converse bound
on the log-volume throughput for channels that wear out. Since infinite
blocklength codes will always wear out the channel for any finite threshold of
failure and therefore cannot convey information at positive rates, we analyze
the performance of finite blocklength codes to determine the maximum expected
transmission volume at a given level of average error probability. We show that
this maximization problem has a recursive form and can be solved by dynamic
programming. Numerical results demonstrate that a sequence of block codes is
preferred to a single block code for streaming sources.
",1,0,1,0,0,0
14721,Free constructions and coproducts of d-frames,"  A general theory of presentations for d-frames does not yet exist. We review
the difficulties and give sufficient conditions for when they can be overcome.
As an application we prove that the category of d-frames is closed under
coproducts.
",1,0,1,0,0,0
20796,Provenance and Pseudo-Provenance for Seeded Learning-Based Automated Test Generation,"  Many methods for automated software test generation, including some that
explicitly use machine learning (and some that use ML more broadly conceived)
derive new tests from existing tests (often referred to as seeds). Often, the
seed tests from which new tests are derived are manually constructed, or at
least simpler than the tests that are produced as the final outputs of such
test generators. We propose annotation of generated tests with a provenance
(trail) showing how individual generated tests of interest (especially failing
tests) derive from seed tests, and how the population of generated tests
relates to the original seed tests. In some cases, post-processing of generated
tests can invalidate provenance information, in which case we also propose a
method for attempting to construct ""pseudo-provenance"" describing how the tests
could have been (partly) generated from seeds.
",1,0,0,1,0,0
19075,Agile Software Engineering and Systems Engineering at SKA Scale,"  Systems Engineering (SE) is the set of processes and documentation required
for successfully realising large-scale engineering projects, but the classical
approach is not a good fit for software-intensive projects, especially when the
needs of the different stakeholders are not fully known from the beginning, and
requirement priorities might change. The SKA is the ultimate software-enabled
telescope, with enormous amounts of computing hardware and software required to
perform its data reduction. We give an overview of the system and software
engineering processes in the SKA1 development, and the tension between
classical and agile SE.
",1,1,0,0,0,0
5656,Interior transmission eigenvalue problems on compact manifolds with boundary conductivity parameters,"  In this paper, we consider an interior transmission eigenvalue (ITE) problem
on some compact $C^{\infty }$-Riemannian manifolds with a common smooth
boundary. In particular, these manifolds may have different topologies, but we
impose some conditions of Riemannian metrics, indices of refraction and
boundary conductivity parameters on the boundary. Then we prove the
discreteness of the set of ITEs, the existence of infinitely many ITEs, and its
Weyl type lower bound. For our settings, we can adopt the argument by
Lakshtanov and Vainberg, considering the Dirichlet-to-Neumann map. As an
application, we derive the existence of non-scattering energies for
time-harmonic acoustic equations. For the sake of simplicity, we consider the
scattering theory on the Euclidean space. However, the argument is applicable
for certain kinds of non-compact manifolds with ends on which we can define the
scattering matrix.
",0,0,1,0,0,0
2282,New Generalized Fixed Point Results on $S_{b}$-Metric Spaces,"  Recently $S_{b}$-metric spaces have been introduced as the generalizations of
metric and $S$-metric spaces. In this paper we investigate some basic
properties of this new space. We generalize the classical Banach's contraction
principle using the theory of a complete $S_{b}$-metric space. Also we give an
application to linear equation systems using the $S_{b}$-metric which is
generated by a metric.
",0,0,1,0,0,0
12628,Expansion of pinched hypersurfaces of the Euclidean and hyperbolic space by high powers of curvature,"  We prove convergence results for expanding curvature flows in the Euclidean
and hyperbolic space. The flow speeds have the form $F^{-p}$, where $p>1$ and
$F$ is a positive, strictly monotone and 1-homogeneous curvature function. In
particular this class includes the mean curvature $F=H$. We prove that a
certain initial pinching condition is preserved and the properly rescaled
hypersurfaces converge smoothly to the unit sphere. We show that an example due
to Andrews-McCoy-Zheng can be used to construct strictly convex initial
hypersurfaces, for which the inverse mean curvature flow to the power $p>1$
loses convexity, justifying the necessity to impose a certain pinching
condition on the initial hypersurface.
",0,0,1,0,0,0
20769,When is a Network a Network? Multi-Order Graphical Model Selection in Pathways and Temporal Networks,"  We introduce a framework for the modeling of sequential data capturing
pathways of varying lengths observed in a network. Such data are important,
e.g., when studying click streams in information networks, travel patterns in
transportation systems, information cascades in social networks, biological
pathways or time-stamped social interactions. While it is common to apply graph
analytics and network analysis to such data, recent works have shown that
temporal correlations can invalidate the results of such methods. This raises a
fundamental question: when is a network abstraction of sequential data
justified? Addressing this open question, we propose a framework which combines
Markov chains of multiple, higher orders into a multi-layer graphical model
that captures temporal correlations in pathways at multiple length scales
simultaneously. We develop a model selection technique to infer the optimal
number of layers of such a model and show that it outperforms previously used
Markov order detection techniques. An application to eight real-world data sets
on pathways and temporal networks shows that it allows to infer graphical
models which capture both topological and temporal characteristics of such
data. Our work highlights fallacies of network abstractions and provides a
principled answer to the open question when they are justified. Generalizing
network representations to multi-order graphical models, it opens perspectives
for new data mining and knowledge discovery algorithms.
",1,1,0,0,0,0
20729,Representing de Rham cohomology classes on an open Riemann surface by holomorphic forms,"  Let $X$ be a connected open Riemann surface. Let $Y$ be an Oka domain in the
smooth locus of an analytic subvariety of $\mathbb C^n$, $n\geq 1$, such that
the convex hull of $Y$ is all of $\mathbb C^n$. Let $\mathscr O_*(X, Y)$ be the
space of nondegenerate holomorphic maps $X\to Y$. Take a holomorphic $1$-form
$\theta$ on $X$, not identically zero, and let $\pi:\mathscr O_*(X,Y) \to
H^1(X,\mathbb C^n)$ send a map $g$ to the cohomology class of $g\theta$. Our
main theorem states that $\pi$ is a Serre fibration. This result subsumes the
1971 theorem of Kusunoki and Sainouchi that both the periods and the divisor of
a holomorphic form on $X$ can be prescribed arbitrarily. It also subsumes two
parametric h-principles in minimal surface theory proved by Forstneric and
Larusson in 2016.
",0,0,1,0,0,0
12728,A Fast Numerical Scheme for the Godunov-Peshkov-Romenski Model of Continuum Mechanics,"  A new second-order numerical scheme based on an operator splitting is
proposed for the Godunov-Peshkov-Romenski model of continuum mechanics. The
homogeneous part of the system is solved with a finite volume method based on a
WENO reconstruction, and the temporal ODEs are solved using some analytic
results presented here. Whilst it is not possible to attain arbitrary-order
accuracy with this scheme (as with ADER-WENO schemes used previously), the
attainable order of accuracy is often sufficient, and solutions are
computationally cheap when compared with other available schemes. The new
scheme is compared with an ADER-WENO scheme for various test cases, and a
convergence study is undertaken to demonstrate its order of accuracy.
",0,1,1,0,0,0
4398,Joint secrecy over the K-Transmitter Multiple Access Channel,"  This paper studies the problem of secure communication over a K-transmitter
multiple access channel in the presence of an external eavesdropper, subject to
a joint secrecy constraint (i.e., information leakage rate from the collection
of K messages to an eavesdropper is made vanishing). As a result, we establish
the joint secrecy achievable rate region. To this end, our results build upon
two techniques in addition to the standard information-theoretic methods. The
first is a generalization of Chia-El Gamal's lemma on entropy bound for a set
of codewords given partial information. The second is to utilize a compact
representation of a list of sets that, together with properties of mutual
information, leads to an efficient Fourier-Motzkin elimination. These two
approaches could also be of independent interests in other contexts.
",1,0,0,0,0,0
20663,Minimax Optimal Estimators for Additive Scalar Functionals of Discrete Distributions,"  In this paper, we consider estimators for an additive functional of $\phi$,
which is defined as $\theta(P;\phi)=\sum_{i=1}^k\phi(p_i)$, from $n$ i.i.d.
random samples drawn from a discrete distribution $P=(p_1,...,p_k)$ with
alphabet size $k$. We propose a minimax optimal estimator for the estimation
problem of the additive functional. We reveal that the minimax optimal rate is
characterized by the divergence speed of the fourth derivative of $\phi$ if the
divergence speed is high. As a result, we show there is no consistent estimator
if the divergence speed of the fourth derivative of $\phi$ is larger than
$p^{-4}$. Furthermore, if the divergence speed of the fourth derivative of
$\phi$ is $p^{4-\alpha}$ for $\alpha \in (0,1)$, the minimax optimal rate is
obtained within a universal multiplicative constant as $\frac{k^2}{(n\ln
n)^{2\alpha}} + \frac{k^{2-2\alpha}}{n}$.
",1,0,1,1,0,0
6563,Dropout is a special case of the stochastic delta rule: faster and more accurate deep learning,"  Multi-layer neural networks have lead to remarkable performance on many kinds
of benchmark tasks in text, speech and image processing. Nonlinear parameter
estimation in hierarchical models is known to be subject to overfitting. One
approach to this overfitting and related problems (local minima, colinearity,
feature discovery etc.) is called dropout (Srivastava, et al 2014, Baldi et al
2016). This method removes hidden units with a Bernoulli random variable with
probability $p$ over updates. In this paper we will show that Dropout is a
special case of a more general model published originally in 1990 called the
stochastic delta rule ( SDR, Hanson, 1990). SDR parameterizes each weight in
the network as a random variable with mean $\mu_{w_{ij}}$ and standard
deviation $\sigma_{w_{ij}}$. These random variables are sampled on each forward
activation, consequently creating an exponential number of potential networks
with shared weights. Both parameters are updated according to prediction error,
thus implementing weight noise injections that reflect a local history of
prediction error and efficient model averaging. SDR therefore implements a
local gradient-dependent simulated annealing per weight converging to a bayes
optimal network. Tests on standard benchmarks (CIFAR) using a modified version
of DenseNet shows the SDR outperforms standard dropout in error by over 50% and
in loss by over 50%. Furthermore, the SDR implementation converges on a
solution much faster, reaching a training error of 5 in just 15 epochs with
DenseNet-40 compared to standard DenseNet-40's 94 epochs.
",0,0,0,1,0,0
5439,Ensemble of Neural Classifiers for Scoring Knowledge Base Triples,"  This paper describes our approach for the triple scoring task at the WSDM Cup
2017. The task required participants to assign a relevance score for each pair
of entities and their types in a knowledge base in order to enhance the ranking
results in entity retrieval tasks. We propose an approach wherein the outputs
of multiple neural network classifiers are combined using a supervised machine
learning model. The experimental results showed that our proposed method
achieved the best performance in one out of three measures (i.e., Kendall's
tau), and performed competitively in the other two measures (i.e., accuracy and
average score difference).
",1,0,0,0,0,0
13976,Sparse Poisson Regression with Penalized Weighted Score Function,"  We proposed a new penalized method in this paper to solve sparse Poisson
Regression problems. Being different from $\ell_1$ penalized log-likelihood
estimation, our new method can be viewed as penalized weighted score function
method. We show that under mild conditions, our estimator is $\ell_1$
consistent and the tuning parameter can be pre-specified, which shares the same
good property of the square-root Lasso.
",0,0,1,1,0,0
19909,Language as a matrix product state,"  We propose a statistical model for natural language that begins by
considering language as a monoid, then representing it in complex matrices with
a compatible translation invariant probability measure. We interpret the
probability measure as arising via the Born rule from a translation invariant
matrix product state.
",1,1,0,1,0,0
2074,Variance Regularizing Adversarial Learning,"  We introduce a novel approach for training adversarial models by replacing
the discriminator score with a bi-modal Gaussian distribution over the
real/fake indicator variables. In order to do this, we train the Gaussian
classifier to match the target bi-modal distribution implicitly through
meta-adversarial training. We hypothesize that this approach ensures a non-zero
gradient to the generator, even in the limit of a perfect classifier. We test
our method against standard benchmark image datasets as well as show the
classifier output distribution is smooth and has overlap between the real and
fake modes.
",1,0,0,1,0,0
14936,On Convergence Property of Implicit Self-paced Objective,"  Self-paced learning (SPL) is a new methodology that simulates the learning
principle of humans/animals to start learning easier aspects of a learning
task, and then gradually take more complex examples into training. This
new-coming learning regime has been empirically substantiated to be effective
in various computer vision and pattern recognition tasks. Recently, it has been
proved that the SPL regime has a close relationship to a implicit self-paced
objective function. While this implicit objective could provide helpful
interpretations to the effectiveness, especially the robustness, insights under
the SPL paradigms, there are still no theoretical results strictly proved to
verify such relationship. To this issue, in this paper, we provide some
convergence results on this implicit objective of SPL. Specifically, we prove
that the learning process of SPL always converges to critical points of this
implicit objective under some mild conditions. This result verifies the
intrinsic relationship between SPL and this implicit objective, and makes the
previous robustness analysis on SPL complete and theoretically rational.
",1,0,0,0,0,0
7827,Risk-Sensitive Cooperative Games for Human-Machine Systems,"  Autonomous systems can substantially enhance a human's efficiency and
effectiveness in complex environments. Machines, however, are often unable to
observe the preferences of the humans that they serve. Despite the fact that
the human's and machine's objectives are aligned, asymmetric information, along
with heterogeneous sensitivities to risk by the human and machine, make their
joint optimization process a game with strategic interactions. We propose a
framework based on risk-sensitive dynamic games; the human seeks to optimize
her risk-sensitive criterion according to her true preferences, while the
machine seeks to adaptively learn the human's preferences and at the same time
provide a good service to the human. We develop a class of performance measures
for the proposed framework based on the concept of regret. We then evaluate
their dependence on the risk-sensitivity and the degree of uncertainty. We
present applications of our framework to self-driving taxis, and robo-financial
advising.
",1,0,0,1,0,0
995,On self-affine sets,"  We survey the dimension theory of self-affine sets for general mathematical
audience. The article is in Finnish.
",0,0,1,0,0,0
9358,A Multi-Wavelength Analysis of Dust and Gas in the SR 24S Transition Disk,"  We present new Atacama Large Millimeter/sub-millimeter Array (ALMA) 1.3 mm
continuum observations of the SR 24S transition disk with an angular resolution
$\lesssim0.18""$ (12 au radius). We perform a multi-wavelength investigation by
combining new data with previous ALMA data at 0.45 mm. The visibilities and
images of the continuum emission at the two wavelengths are well characterized
by a ring-like emission. Visibility modeling finds that the ring-like emission
is narrower at longer wavelengths, in good agreement with models of dust
trapping in pressure bumps, although there are complex residuals that suggest
potentially asymmetric structures. The 0.45 mm emission has a shallower profile
inside the central cavity than the 1.3 mm emission. In addition, we find that
the $^{13}$CO and C$^{18}$O (J=2-1) emission peaks at the center of the
continuum cavity. We do not detect either continuum or gas emission from the
northern companion to this system (SR 24N), which is itself a binary system.
The upper limit for the dust disk mass of SR 24N is $\lesssim
0.12\,M_{\bigoplus}$, which gives a disk mass ratio in dust between the two
components of $M_{\mathrm{dust, SR\,24S}}/M_{\mathrm{dust,
SR\,24N}}\gtrsim840$. The current ALMA observations may imply that either
planets have already formed in the SR 24N disk or that dust growth to mm-sizes
is inhibited there and that only warm gas, as seen by ro-vibrational CO
emission inside the truncation radii of the binary, is present.
",0,1,0,0,0,0
15898,What we really want to find by Sentiment Analysis: The Relationship between Computational Models and Psychological State,"  As the first step to model emotional state of a person, we build sentiment
analysis models with existing deep neural network algorithms and compare the
models with psychological measurements to enlighten the relationship. In the
experiments, we first examined psychological state of 64 participants and asked
them to summarize the story of a book, Chronicle of a Death Foretold (Marquez,
1981). Secondly, we trained models using crawled 365,802 movie review data;
then we evaluated participants' summaries using the pretrained model as a
concept of transfer learning. With the background that emotion affects on
memories, we investigated the relationship between the evaluation score of the
summaries from computational models and the examined psychological
measurements. The result shows that although CNN performed the best among other
deep neural network algorithms (LSTM, GRU), its results are not related to the
psychological state. Rather, GRU shows more explainable results depending on
the psychological state. The contribution of this paper can be summarized as
follows: (1) we enlighten the relationship between computational models and
psychological measurements. (2) we suggest this framework as objective methods
to evaluate the emotion; the real sentiment analysis of a person.
",1,0,0,0,0,0
1051,Pulse rate estimation using imaging photoplethysmography: generic framework and comparison of methods on a publicly available dataset,"  Objective: to establish an algorithmic framework and a benchmark dataset for
comparing methods of pulse rate estimation using imaging photoplethysmography
(iPPG). Approach: first we reveal essential steps of pulse rate estimation from
facial video and review methods applied at each of the steps. Then we
investigate performance of these methods for DEAP dataset
www.eecs.qmul.ac.uk/mmv/datasets/deap/ containing facial videos and reference
contact photoplethysmograms. Main results: best assessment precision is
achieved when pulse rate is estimated using continuous wavelet transform from
iPPG extracted by the POS method (overall mean absolute error below 2 heart
beats per minute). Significance: we provide a generic framework for theoretical
comparison of methods for pulse rate estimation from iPPG and report results
for the most popular methods on a publicly available dataset that can be used
as a benchmark.
",0,1,0,0,0,0
681,Deep Fluids: A Generative Network for Parameterized Fluid Simulations,"  This paper presents a novel generative model to synthesize fluid simulations
from a set of reduced parameters. A convolutional neural network is trained on
a collection of discrete, parameterizable fluid simulation velocity fields. Due
to the capability of deep learning architectures to learn representative
features of the data, our generative model is able to accurately approximate
the training data set, while providing plausible interpolated in-betweens. The
proposed generative model is optimized for fluids by a novel loss function that
guarantees divergence-free velocity fields at all times. In addition, we
demonstrate that we can handle complex parameterizations in reduced spaces, and
advance simulations in time by integrating in the latent space with a second
network. Our method models a wide variety of fluid behaviors, thus enabling
applications such as fast construction of simulations, interpolation of fluids
with different parameters, time re-sampling, latent space simulations, and
compression of fluid simulation data. Reconstructed velocity fields are
generated up to 700x faster than traditional CPU solvers, while achieving
compression rates of over 1300x.
",0,0,0,1,0,0
1284,A note on the violation of Bell's inequality,"  With Bell's inequalities one has a formal expression to show how essentially
all local theories of natural phenomena that are formulated within the
framework of realism may be tested using a simple experimental arrangement. For
the case of entangled pairs of spin-1/2 particles we propose an alternative
measurement setup which is consistent to the necessary assumptions
corresponding to the derivation of the Bell inequalities. We find that the Bell
inequalities are never violated with respect to our suggested measurement
process.
",0,1,0,0,0,0
7472,Structural and electronic properties of germanene on MoS$_2$,"  To date, germanene has only been synthesized on metallic substrates. A
metallic substrate is usually detrimental for the two-dimensional Dirac nature
of germanene because the important electronic states near the Fermi level of
germanene can hybridize with the electronic states of the metallic substrate.
Here we report the successful synthesis of germanene on molybdenum disulfide
(MoS$_2$), a band gap material. Pre-existing defects in the MoS$_2$ surface act
as preferential nucleation sites for the germanene islands. The lattice
constant of the germanene layer (3.8 $\pm$ 0.2 \AA) is about 20\% larger than
the lattice constant of the MoS$_2$ substrate (3.16 \AA). Scanning tunneling
spectroscopy measurements and density functional theory calculations reveal
that there are, besides the linearly dispersing bands at the $K$ points, two
parabolic bands that cross the Fermi level at the $\Gamma$ point.
",0,1,0,0,0,0
9377,Placing your Coins on a Shelf,"  We consider the problem of packing a family of disks ""on a shelf"", that is,
such that each disk touches the $x$-axis from above and such that no two disks
overlap. We prove that the problem of minimizing the distance between the
leftmost point and the rightmost point of any disk is NP-hard. On the positive
side, we show how to approximate this problem within a factor of 4/3 in $O(n
\log n)$ time, and provide an $O(n \log n)$-time exact algorithm for a special
case, in particular when the ratio between the largest and smallest radius is
at most four.
",1,0,1,0,0,0
4885,Cloud-based Deep Learning of Big EEG Data for Epileptic Seizure Prediction,"  Developing a Brain-Computer Interface~(BCI) for seizure prediction can help
epileptic patients have a better quality of life. However, there are many
difficulties and challenges in developing such a system as a real-life support
for patients. Because of the nonstationary nature of EEG signals, normal and
seizure patterns vary across different patients. Thus, finding a group of
manually extracted features for the prediction task is not practical. Moreover,
when using implanted electrodes for brain recording massive amounts of data are
produced. This big data calls for the need for safe storage and high
computational resources for real-time processing. To address these challenges,
a cloud-based BCI system for the analysis of this big EEG data is presented.
First, a dimensionality-reduction technique is developed to increase
classification accuracy as well as to decrease the communication bandwidth and
computation time. Second, following a deep-learning approach, a stacked
autoencoder is trained in two steps for unsupervised feature extraction and
classification. Third, a cloud-computing solution is proposed for real-time
analysis of big EEG data. The results on a benchmark clinical dataset
illustrate the superiority of the proposed patient-specific BCI as an
alternative method and its expected usefulness in real-life support of epilepsy
patients.
",1,0,0,1,0,0
4328,Two-dimensional electron gas at the interface of the ferroelectric-antiferromagnetic heterostructure Ba_0.8Sr_0.2TiO_3/LaMnO_3,"  The temperature dependence of the electrical resistivity of the
heterostructures consisting of single crystalline LaMnO$_3$ samples with
different crystallographic orientations covered by the epitaxial ferroelectric
Ba$_{0.8}$Sr$_{0.2}$TiO$_3$ film has been studied. Results obtained for the
heterostructure have been compared with the electrical resistivity of the
single crystalline LaMnO$_3$ without the film. It was found that for the
samples with the films where the polarization axis is perpendicular to the
crystal surface the electrical resistivity strongly decreases, and at the
temperature below ~160 K undergoes the insulator-metal transition. Ab-initio
calculations were also performed for the structural and electronic properties
of the BaTiO$_3$/LaMnO$_3$ heterostructure. Transition to the 2D electron gas
at the interface is shown.
",0,1,0,0,0,0
14702,Adversarial Examples: Attacks and Defenses for Deep Learning,"  With rapid progress and significant successes in a wide spectrum of
applications, deep learning is being applied in many safety-critical
environments. However, deep neural networks have been recently found vulnerable
to well-designed input samples, called adversarial examples. Adversarial
examples are imperceptible to human but can easily fool deep neural networks in
the testing/deploying stage. The vulnerability to adversarial examples becomes
one of the major risks for applying deep neural networks in safety-critical
environments. Therefore, attacks and defenses on adversarial examples draw
great attention. In this paper, we review recent findings on adversarial
examples for deep neural networks, summarize the methods for generating
adversarial examples, and propose a taxonomy of these methods. Under the
taxonomy, applications for adversarial examples are investigated. We further
elaborate on countermeasures for adversarial examples and explore the
challenges and the potential solutions.
",1,0,0,1,0,0
8804,Absence of magnetic long range order in Ba$_3$ZnRu$_2$O$_9$: A spin-liquid candidate in the $S=3/2$ dimer lattice,"  We have discovered a novel candidate for a spin liquid state in a ruthenium
oxide composed of dimers of $S = $ 3/2 spins of Ru$^{5+}$,Ba$_3$ZnRu$_2$O$_9$.
This compound lacks a long range order down to 37 mK, which is a temperature
5000-times lower than the magnetic interaction scale of around 200 K. Partial
substitution for Zn can continuously vary the magnetic ground state from an
antiferromagnetic order to a spin-gapped state through the liquid state. This
indicates that the spin-liquid state emerges from a delicate balance of inter-
and intra-dimer interactions, and the spin state of the dimer plays a vital
role. This unique feature should realize a new type of quantum magnetism.
",0,1,0,0,0,0
7004,ReBNet: Residual Binarized Neural Network,"  This paper proposes ReBNet, an end-to-end framework for training
reconfigurable binary neural networks on software and developing efficient
accelerators for execution on FPGA. Binary neural networks offer an intriguing
opportunity for deploying large-scale deep learning models on
resource-constrained devices. Binarization reduces the memory footprint and
replaces the power-hungry matrix-multiplication with light-weight XnorPopcount
operations. However, binary networks suffer from a degraded accuracy compared
to their fixed-point counterparts. We show that the state-of-the-art methods
for optimizing binary networks accuracy, significantly increase the
implementation cost and complexity. To compensate for the degraded accuracy
while adhering to the simplicity of binary networks, we devise the first
reconfigurable scheme that can adjust the classification accuracy based on the
application. Our proposition improves the classification accuracy by
representing features with multiple levels of residual binarization. Unlike
previous methods, our approach does not exacerbate the area cost of the
hardware accelerator. Instead, it provides a tradeoff between throughput and
accuracy while the area overhead of multi-level binarization is negligible.
",1,0,0,0,0,0
13666,Minimax Game-Theoretic Approach to Multiscale H-infinity Optimal Filtering,"  Sensing in complex systems requires large-scale information exchange and
on-the-go communications over heterogeneous networks and integrated processing
platforms. Many networked cyber-physical systems exhibit hierarchical
infrastructures of information flows, which naturally leads to a multi-level
tree-like information structure in which each level corresponds to a particular
scale of representation. This work focuses on the multiscale fusion of data
collected at multiple levels of the system. We propose a multiscale state-space
model to represent multi-resolution data over the hierarchical information
system and formulate a multi-stage dynamic zero-sum game to design a
multi-scale $H_{\infty}$ robust filter. We present numerical experiments for
one and two-dimensional signals and provide a comparative analysis of the
minimax filter with the standard Kalman filter to show the improvement in
signal-to-noise ratio (SNR).
",1,0,0,0,0,0
12229,A Hierarchical Max-infinitely Divisible Process for Extreme Areal Precipitation Over Watersheds,"  Understanding the spatial extent of extreme precipitation is necessary for
determining flood risk and adequately designing infrastructure (e.g.,
stormwater pipes) to withstand such hazards. While environmental phenomena
typically exhibit weakening spatial dependence at increasingly extreme levels,
limiting max-stable process models for block maxima have a rigid dependence
structure that does not capture this type of behavior. We propose a flexible
Bayesian model from a broader family of max-infinitely divisible processes that
allows for weakening spatial dependence at increasingly extreme levels, and due
to a hierarchical representation of the likelihood in terms of random effects,
our inference approach scales to large datasets. The proposed model is
constructed using flexible random basis functions that are estimated from the
data, allowing for straightforward inspection of the predominant spatial
patterns of extremes. In addition, the described process possesses
max-stability as a special case, making inference on the tail dependence class
possible. We apply our model to extreme precipitation in eastern North America,
and show that the proposed model adequately captures the extremal behavior of
the data.
",0,0,0,1,0,0
7253,Statistical inference for misspecified ergodic Lévy driven stochastic differential equation models,"  This paper deals with the estimation problem of misspecified ergodic Lévy
driven stochastic differential equation models based on high-frequency samples.
We utilize the widely applicable and tractable Gaussian quasi-likelihood
approach which focuses on (conditional) mean and variance structure. It is
shown that the corresponding Gaussian quasi-likelihood estimators of drift and
scale parameters satisfy tail probability estimates and asymptotic normality at
the same rate as correctly specified case. In this process, extended Poisson
equation for time-homogeneous Feller Markov processes plays an important role
to handle misspecification effect. Our result confirms the practical usefulness
of the Gaussian quasi-likelihood approach for SDE models, more firmly.
",0,0,1,1,0,0
12489,Hedging in fractional Black-Scholes model with transaction costs,"  We consider conditional-mean hedging in a fractional Black-Scholes pricing
model in the presence of proportional transaction costs. We develop an explicit
formula for the conditional-mean hedging portfolio in terms of the recently
discovered explicit conditional law of the fractional Brownian motion.
",0,0,1,1,0,0
5665,Revisiting Simple Neural Networks for Learning Representations of Knowledge Graphs,"  We address the problem of learning vector representations for entities and
relations in Knowledge Graphs (KGs) for Knowledge Base Completion (KBC). This
problem has received significant attention in the past few years and multiple
methods have been proposed. Most of the existing methods in the literature use
a predefined characteristic scoring function for evaluating the correctness of
KG triples. These scoring functions distinguish correct triples (high score)
from incorrect ones (low score). However, their performance vary across
different datasets. In this work, we demonstrate that a simple neural network
based score function can consistently achieve near start-of-the-art performance
on multiple datasets. We also quantitatively demonstrate biases in standard
benchmark datasets, and highlight the need to perform evaluation spanning
various datasets.
",1,0,0,1,0,0
20237,Analysis of Dropout in Online Learning,"  Deep learning is the state-of-the-art in fields such as visual object
recognition and speech recognition. This learning uses a large number of layers
and a huge number of units and connections. Therefore, overfitting is a serious
problem with it, and the dropout which is a kind of regularization tool is
used. However, in online learning, the effect of dropout is not well known.
This paper presents our investigation on the effect of dropout in online
learning. We analyzed the effect of dropout on convergence speed near the
singular point. Our results indicated that dropout is effective in online
learning. Dropout tends to avoid the singular point for convergence speed near
that point.
",1,0,0,1,0,0
3618,Manipulating magnetism by ultrafast control of the exchange interaction,"  In recent years, the optical control of exchange interactions has emerged as
an exciting new direction in the study of the ultrafast optical control of
magnetic order. Here we review recent theoretical works on antiferromagnetic
systems, devoted to i) simulating the ultrafast control of exchange
interactions, ii) modeling the strongly nonequilibrium response of the magnetic
order and iii) the relation with relevant experimental works developed in
parallel. In addition to the excitation of spin precession, we discuss examples
of rapid cooling and the control of ultrafast coherent longitudinal spin
dynamics in response to femtosecond optically induced perturbations of exchange
interactions. These elucidate the potential for exploiting the control of
exchange interactions to find new scenarios for both faster and more
energy-efficient manipulation of magnetism.
",0,1,0,0,0,0
19990,The response of the terrestrial bow shock and magnetopause of the long term decline in solar polar fields,"  The location of the terrestrial magnetopause (MP) and it's subsolar stand-off
distance depends not only on the solar wind dynamic pressure and the
interplanetary magnetic field (IMF), both of which play a crucial role in
determining it's shape, but also on the nature of the processes involved in the
interaction between the solar wind and the magnetosphere. The stand-off
distance of the earth's MP and bow shock (BS) also define the extent of
terrestrial magnetic fields into near-earth space on the sunward side and have
important consequences for space weather. However, asymmetries due to the
direction of the IMF are hard to account for, making it nearly impossible to
favour any specific model over the other in estimating the extent of the MP or
BS. Thus, both numerical and empirical models have been used and compared to
estimate the BS and MP stand-off distances as well as the MP shape, in the
period Jan. 1975-Dec. 2016, covering solar cycles 21-24. The computed MP and BS
stand-off distances have been found to be increasing steadily over the past two
decades, since ~1995, spanning solar cycles 23 and 24. The increasing trend is
consistent with earlier reported studies of a long term and steady decline in
solar polar magnetic fields and solar wind micro-turbulence levels. The present
study, thus, highlights the response of the terrestrial magnetosphere to the
long term global changes in both solar and solar wind activity, through a
detailed study of the extent and shape of the terrestrial MP and BS over the
past four solar cycles, a period spanning the last four decades.
",0,1,0,0,0,0
10623,Depth Creates No Bad Local Minima,"  In deep learning, \textit{depth}, as well as \textit{nonlinearity}, create
non-convex loss surfaces. Then, does depth alone create bad local minima? In
this paper, we prove that without nonlinearity, depth alone does not create bad
local minima, although it induces non-convex loss surface. Using this insight,
we greatly simplify a recently proposed proof to show that all of the local
minima of feedforward deep linear neural networks are global minima. Our
theoretical results generalize previous results with fewer assumptions, and
this analysis provides a method to show similar results beyond square loss in
deep linear models.
",1,0,1,1,0,0
2329,Min-Max Regret Scheduling To Minimize the Total Weight of Late Jobs With Interval Uncertainty,"  We study the single machine scheduling problem with the objective to minimize
the total weight of late jobs. It is assumed that the processing times of jobs
are not exactly known at the time when a complete schedule must be dispatched.
Instead, only interval bounds for these parameters are given. In contrast to
the stochastic optimization approach, we consider the problem of finding a
robust schedule, which minimizes the maximum regret of a solution. Heuristic
algorithm based on mixed-integer linear programming is presented and examined
through computational experiments.
",1,0,0,0,0,0
15959,Computational Experiments on $a^4+b^4+c^4+d^4=(a+b+c+d)^4$,"  Computational approaches to finding non-trivial integer solutions of the
equation in the title are discussed. We summarize previous work and provide
several new solutions.
",0,0,1,0,0,0
16879,On the Number of Bins in Equilibria for Signaling Games,"  We investigate the equilibrium behavior for the decentralized quadratic cheap
talk problem in which an encoder and a decoder, viewed as two decision makers,
have misaligned objective functions. In prior work, we have shown that the
number of bins under any equilibrium has to be at most countable, generalizing
a classical result due to Crawford and Sobel who considered sources with
density supported on $[0,1]$. In this paper, we refine this result in the
context of exponential and Gaussian sources. For exponential sources, a
relation between the upper bound on the number of bins and the misalignment in
the objective functions is derived, the equilibrium costs are compared, and it
is shown that there also exist equilibria with infinitely many bins under
certain parametric assumptions. For Gaussian sources, it is shown that there
exist equilibria with infinitely many bins.
",1,0,0,0,0,0
2689,Turning Internet of Things(IoT) into Internet of Vulnerabilities (IoV) : IoT Botnets,"  Internet of Things (IoT) is the next big evolutionary step in the world of
internet. The main intention behind the IoT is to enable safer living and risk
mitigation on different levels of life. With the advent of IoT botnets, the
view towards IoT devices has changed from enabler of enhanced living into
Internet of vulnerabilities for cyber criminals. IoT botnets has exposed two
different glaring issues, 1) A large number of IoT devices are accessible over
public Internet. 2) Security (if considered at all) is often an afterthought in
the architecture of many wide spread IoT devices. In this article, we briefly
outline the anatomy of the IoT botnets and their basic mode of operations. Some
of the major DDoS incidents using IoT botnets in recent times along with the
corresponding exploited vulnerabilities will be discussed. We also provide
remedies and recommendations to mitigate IoT related cyber risks and briefly
illustrate the importance of cyber insurance in the modern connected world.
",1,0,0,0,0,0
17745,Bayesian Nonparametric Inference for M/G/1 Queueing Systems,"  In this work, nonparametric statistical inference is provided for the
continuous-time M/G/1 queueing model from a Bayesian point of view. The
inference is based on observations of the inter-arrival and service times.
Beside other characteristics of the system, particular interest is in the
waiting time distribution which is not accessible in closed form. Thus, we use
an indirect statistical approach by exploiting the Pollaczek-Khinchine
transform formula for the Laplace transform of the waiting time distribution.
Due to this, an estimator is defined and its frequentist validation in terms of
posterior consistency and posterior normality is studied. It will turn out that
we can hereby make inference for the observables separately and compose the
results subsequently by suitable techniques.
",0,0,1,1,0,0
11619,Representations associated to small nilpotent orbits for complex Spin groups,"  This paper provides a comparison between the $K$-structure of unipotent
representations and regular sections of bundles on nilpotent orbits for complex
groups of type $D$. Precisely, let $ G_ 0 =Spin(2n,\mathbb C)$ be the Spin
complex group viewed as a real group, and $K\cong G_0$ be the complexification
of the maximal compact subgroup of $G_0$. We compute $K$-spectra of the regular
functions on some small nilpotent orbits $\mathcal O$ transforming according to
characters $\psi$ of $C_{ K}(\mathcal O)$ trivial on the connected component of
the identity $C_{ K}(\mathcal O)^0$. We then match them with the ${K}$-types of
the genuine (i.e. representations which do not factor to $SO(2n,\mathbb C)$)
unipotent representations attached to $\mathcal O$.
",0,0,1,0,0,0
14520,The VISTA ZYJHKs Photometric System: Calibration from 2MASS,"  In this paper we describe the routine photometric calibration of data taken
with the VIRCAM instrument on the ESO VISTA telescope. The broadband ZYJHKs
data are directly calibrated from 2MASS point sources visible in every VISTA
image. We present the empirical transformations between the 2MASS and VISTA,
and WFCAM and VISTA, photometric systems for regions of low reddening. We
investigate the long-term performance of VISTA+VIRCAM. An investigation of the
dependence of the photometric calibration on interstellar reddening leads to
these conclusions: (1) For all broadband filters, a linear colour-dependent
correction compensates the gross effects of reddening where $E(B-V)<5.0$. (2)
For $Z$ and $Y$, there is a significantly larger scatter above E(B-V)=5.0, and
insufficient measurements to adequately constrain the relation beyond this
value. (3) The $JHK\!s$ filters can be corrected to a few percent up to
E(B-V)=10.0. We analyse spatial systematics over month-long timescales, both
inter- and intra-detector and show that these are present only at very low
levels in VISTA. We monitor and remove residual detector-to-detector offsets.
We compare the calibration of the main pipeline products: pawprints and tiles.
We show how variable seeing and transparency affect the final calibration
accuracy of VISTA tiles, and discuss a technique, {\it grouting}, for
mitigating these effects. Comparison between repeated reference fields is used
to demonstrate that the VISTA photometry is precise to better than $\simeq2\%$
for the $Y$$J$$H$$Ks$ bands and $3\%$ for the $Z$ bands. Finally we present
empirically determined offsets to transform VISTA magnitudes into a true Vega
system.
",0,1,0,0,0,0
1603,Inconsistency of Template Estimation with the Fr{é}chet mean in Quotient Space,"  We tackle the problem of template estimation when data have been randomly
transformed under an isometric group action in the presence of noise. In order
to estimate the template, one often minimizes the variance when the influence
of the transformations have been removed (computation of the Fr{é}chet mean
in quotient space). The consistency bias is defined as the distance (possibly
zero) between the orbit of the template and the orbit of one element which
minimizes the variance. In this article we establish an asymptotic behavior of
the consistency bias with respect to the noise level. This behavior is linear
with respect to the noise level. As a result the inconsistency is unavoidable
as soon as the noise is large enough. In practice, the template estimation with
a finite sample is often done with an algorithm called max-max. We show the
convergence of this algorithm to an empirical Karcher mean. Finally, our
numerical experiments show that the bias observed in practice cannot be
attributed to the small sample size or to a convergence problem but is indeed
due to the previously studied inconsistency.
",0,0,1,1,0,0
16511,Parametric Polynomial Preserving Recovery on Manifolds,"  This paper investigates gradient recovery schemes for data defined on
discretized manifolds. The proposed method, parametric polynomial preserving
recovery (PPPR), does not require the tangent spaces of the exact manifolds
which have been assumed for some significant gradient recovery methods in the
literature. Another advantage is that superconvergence is guaranteed for PPPR
without the symmetric condition which has been asked in the existing
techniques. There is also numerical evidence that the superconvergence by PPPR
is high curvature stable, which distinguishes itself from the other methods. As
an application, we show that its capability of constructing an asymptotically
exact \textit{a posteriori} error estimator. Several numerical examples on
two-dimensional surfaces are presented to support the theoretical results and
make comparisons with state of the art methods.
",0,0,1,0,0,0
4142,Face Detection and Face Recognition In the Wild Using Off-the-Shelf Freely Available Components,"  This paper presents an easy and efficient face detection and face recognition
approach using free software components from the internet. Face detection and
face recognition problems have wide applications in home and office security.
Therefore this work will helpful for those searching for a free face
off-the-shelf face detection system. Using this system, faces can be detected
in uncontrolled environments. In the detection phase, every individual face is
detected and in the recognition phase the detected faces are compared with the
faces in a given data set and recognized.
",1,0,0,0,0,0
2249,"$W$-entropy, super Perelman Ricci flows and $(K, m)$-Ricci solitons","  In this paper, we prove the characterization of the $(K, \infty)$-super
Perelman Ricci flows by various functional inequalities and gradient estimate
for the heat semigroup generated by the Witten Laplacian on manifolds equipped
with time dependent metrics and potentials. As a byproduct, we derive the
Hamilton type dimension free Harnack inequality on manifolds with $(K,
\infty)$-super Perelman Ricci flows. Based on a new second order differential
inequality on the Boltzmann-Shannon entropy for the heat equation of the Witten
Laplacian, we introduce a new $W$-entropy quantity and prove its monotonicity
for the heat equation of the Witten Laplacian on complete Riemannian manifolds
with the $CD(K, \infty)$-condition and on compact manifolds with $(K,
\infty)$-super Perelman Ricci flows. Our results characterize the $(K,
\infty)$-Ricci solitons and the $(K, \infty)$-Perelman Ricci flows. We also
prove a second order differential entropy inequality on $(K, m)$-super Ricci
flows, which can be used to characterize the $(K, m)$-Ricci solitons and the
$(K, m)$-Ricci flows. Finally, we give a probabilistic interpretation of the
$W$-entropy for the heat equation of the Witten Laplacian on manifolds with the
$CD(K, m)$-condition.
",0,0,1,0,0,0
9724,Discontinuity-Sensitive Optimal Control Learning by Mixture of Experts,"  This paper proposes a discontinuity-sensitive approach to learn the solutions
of parametric optimal control problems with high accuracy. Many tasks, ranging
from model predictive control to reinforcement learning, may be solved by
learning optimal solutions as a function of problem parameters. However,
nonconvexity, discrete homotopy classes, and control switching cause
discontinuity in the parameter-solution mapping, thus making learning difficult
for traditional continuous function approximators. A mixture of experts (MoE)
model composed of a classifier and several regressors is proposed to address
such an issue. The optimal trajectories of different parameters are clustered
such that in each cluster the trajectories are continuous function of problem
parameters. Numerical examples on benchmark problems show that training the
classifier and regressors individually outperforms joint training of MoE. With
suitably chosen clusters, this approach not only achieves lower prediction
error with less training data and fewer model parameters, but also leads to
dramatic improvements in the reliability of trajectory tracking compared to
traditional universal function approximation models (e.g., neural networks).
",1,0,0,0,0,0
3602,Discovery of Shifting Patterns in Sequence Classification,"  In this paper, we investigate the multi-variate sequence classification
problem from a multi-instance learning perspective. Real-world sequential data
commonly show discriminative patterns only at specific time periods. For
instance, we can identify a cropland during its growing season, but it looks
similar to a barren land after harvest or before planting. Besides, even within
the same class, the discriminative patterns can appear in different periods of
sequential data. Due to such property, these discriminative patterns are also
referred to as shifting patterns. The shifting patterns in sequential data
severely degrade the performance of traditional classification methods without
sufficient training data.
We propose a novel sequence classification method by automatically mining
shifting patterns from multi-variate sequence. The method employs a
multi-instance learning approach to detect shifting patterns while also
modeling temporal relationships within each multi-instance bag by an LSTM model
to further improve the classification performance. We extensively evaluate our
method on two real-world applications - cropland mapping and affective state
recognition. The experiments demonstrate the superiority of our proposed method
in sequence classification performance and in detecting discriminative shifting
patterns.
",1,0,0,1,0,0
13886,Joint Inference of User Community and Interest Patterns in Social Interaction Networks,"  Online social media have become an integral part of our social beings.
Analyzing conversations in social media platforms can lead to complex
probabilistic models to understand social interaction networks. In this paper,
we present a modeling approach for characterizing social interaction networks
by jointly inferring user communities and interests based on social media
interactions. We present several pattern inference models: i) Interest pattern
model (IPM) captures population level interaction topics, ii) User interest
pattern model (UIPM) captures user specific interaction topics, and iii)
Community interest pattern model (CIPM) captures both community structures and
user interests. We test our methods on Twitter data collected from Purdue
University community. From our model results, we observe the interaction topics
and communities related to two big events within Purdue University community,
namely Purdue Day of Giving and Senator Bernie Sanders' visit to Purdue
University as part of Indiana Primary Election 2016. Constructing social
interaction networks based on user interactions accounts for the similarity of
users' interactions on various topics of interest and indicates their community
belonging further beyond connectivity. We observed that the
degree-distributions of such networks follow power-law that is indicative of
the existence of fewer nodes in the network with higher levels of interactions,
and many other nodes with less interactions. We also discuss the application of
such networks as a useful tool to effectively disseminate specific information
to the target audience towards planning any large-scale events and demonstrate
how to single out specific nodes in a given community by running network
algorithms.
",1,1,0,0,0,0
9062,Three-Dimensional Numerical Modeling of Shear Stimulation of Naturally Fractured Reservoirs,"  Shear dilation based hydraulic stimulations enable exploitation of geothermal
energy from reservoirs with inadequate initial permeability. While contributing
to enhancing the reservoir's permeability, hydraulic stimulation processes may
lead to undesired seismic activity. Here, we present a three dimensional
numerical model aiming to increase understanding of this mechanism and its
consequences. The fractured reservoir is modeled as a network of explicitly
represented large scale fractures immersed in a permeable rock matrix. The
numerical formulation is constructed by coupling three physical processes:
fluid flow, fracture deformation, and rock matrix deformation. For flow
simulations, the discrete fracture matrix model is used, which allows the fluid
transport from high permeable conductive fractures to the rock matrix and vice
versa. The mechanical behavior of the fractures is modeled using a hyperbolic
model with reversible and irreversible deformations. Linear elasticity is
assumed for the mechanical deformation and stress alteration of the rock
matrix. Fractures are modeled as lower dimensional surfaces embodied in the
domain, subjected to specific governing equations for their deformation along
the tangential and normal directions. Both the fluid flow and momentum balance
equations are approximated by finite volume discretizations. The new numerical
model is demonstrated considering a three dimensional fractured formation with
a network of 20 explicitly represented fractures. The effects of fluid exchange
between fractures and rock matrix on the permeability evolution and the
generated seismicity are examined for test cases resembling realistic reservoir
conditions.
",0,1,0,0,0,0
6186,Segmented Terahertz Electron Accelerator and Manipulator (STEAM),"  Acceleration and manipulation of ultrashort electron bunches are the basis
behind electron and X-ray devices used for ultrafast, atomic-scale imaging and
spectroscopy. Using laser-generated THz drivers enables intrinsic
synchronization as well as dramatic gains in field strengths, field gradients
and component compactness, leading to shorter electron bunches, higher
spatio-temporal resolution and smaller infrastructures. We present a segmented
THz electron accelerator and manipulator (STEAM) with extended interaction
lengths capable of performing multiple high-field operations on the energy and
phase-space of ultrashort bunches with moderate charge. With this single
device, powered by few-microjoule, single-cycle, 0.3 THz pulses, we demonstrate
record THz-device acceleration of >30 keV, streaking with <10 fs resolution,
focusing with >2 kT/m strengths, compression to ~100 fs as well as real-time
switching between these modes of operation. The STEAM device demonstrates the
feasibility of future THz-based compact electron guns, accelerators, ultrafast
electron diffractometers and Free-Electron Lasers with transformative impact.
",0,1,0,0,0,0
4437,"Reversible Sequences of Cardinals, Reversible Equivalence Relations, and Similar Structures","  A relational structure ${\mathbb X}$ is said to be reversible iff every
bijective endomorphism $f:X\rightarrow X$ is an automorphism. We define a
sequence of non-zero cardinals $\langle \kappa_i :i\in I\rangle$ to be
reversible iff each surjection $f :I\rightarrow I$ such that $\kappa_j
=\sum_{i\in f^{-1}[\{ j \}]}\kappa_i$, for all $j\in I $, is a bijection, and
characterize such sequences: either $\langle \kappa_i :i\in I\rangle$ is a
finite-to-one sequence, or $\kappa_i\in {\mathbb N}$, for all $i\in I$, $K:=\{
m\in {\mathbb N} : \kappa_i =m $, for infinitely many $i\in I \}$ is a
non-empty independent set, and $\gcd (K)$ divides at most finitely many
elements of the set $\{ \kappa_i :i\in I \}$. We isolate a class of binary
structures such that a structure from the class is reversible iff the sequence
of cardinalities of its connectivity components is reversible. In particular,
we characterize reversible equivalence relations, reversible posets which are
disjoint unions of cardinals $\leq \omega$, and some similar structures. In
addition, we show that a poset with linearly ordered connectivity components is
reversible, if the corresponding sequence of cardinalities is reversible and,
using this fact, detect a wide class of examples of reversible posets and
topological spaces.
",0,0,1,0,0,0
8599,Homotopy Parametric Simplex Method for Sparse Learning,"  High dimensional sparse learning has imposed a great computational challenge
to large scale data analysis. In this paper, we are interested in a broad class
of sparse learning approaches formulated as linear programs parametrized by a
{\em regularization factor}, and solve them by the parametric simplex method
(PSM). Our parametric simplex method offers significant advantages over other
competing methods: (1) PSM naturally obtains the complete solution path for all
values of the regularization parameter; (2) PSM provides a high precision dual
certificate stopping criterion; (3) PSM yields sparse solutions through very
few iterations, and the solution sparsity significantly reduces the
computational cost per iteration. Particularly, we demonstrate the superiority
of PSM over various sparse learning approaches, including Dantzig selector for
sparse linear regression, LAD-Lasso for sparse robust linear regression, CLIME
for sparse precision matrix estimation, sparse differential network estimation,
and sparse Linear Programming Discriminant (LPD) analysis. We then provide
sufficient conditions under which PSM always outputs sparse solutions such that
its computational performance can be significantly boosted. Thorough numerical
experiments are provided to demonstrate the outstanding performance of the PSM
method.
",1,0,1,1,0,0
3558,GIER: A Danish computer from 1961 with a role in the modern revolution of astronomy,"  A Danish computer, GIER, from 1961 played a vital role in the development of
a new method for astrometric measurement. This method, photon counting
astrometry, ultimately led to two satellites with a significant role in the
modern revolution of astronomy. A GIER was installed at the Hamburg Observatory
in 1964 where it was used to implement the entirely new method for the
measurement of stellar positions by means of a meridian circle, then the
fundamental instrument of astrometry. An expedition to Perth in Western
Australia with the instrument and the computer was a success. This method was
also implemented in space in the first ever astrometric satellite Hipparcos
launched by ESA in 1989. The Hipparcos results published in 1997 revolutionized
astrometry with an impact in all branches of astronomy from the solar system
and stellar structure to cosmic distances and the dynamics of the Milky Way. In
turn, the results paved the way for a successor, the one million times more
powerful Gaia astrometry satellite launched by ESA in 2013. Preparations for a
Gaia successor in twenty years are making progress.
",0,1,0,0,0,0
3885,The Taipan Galaxy Survey: Scientific Goals and Observing Strategy,"  Taipan is a multi-object spectroscopic galaxy survey starting in 2017 that
will cover 2pi steradians over the southern sky, and obtain optical spectra for
about two million galaxies out to z<0.4. Taipan will use the newly-refurbished
1.2m UK Schmidt Telescope at Siding Spring Observatory with the new TAIPAN
instrument, which includes an innovative 'Starbugs' positioning system capable
of rapidly and simultaneously deploying up to 150 spectroscopic fibres (and up
to 300 with a proposed upgrade) over the 6-deg diameter focal plane, and a
purpose-built spectrograph operating from 370 to 870nm with resolving power
R>2000. The main scientific goals of Taipan are: (i) to measure the distance
scale of the Universe (primarily governed by the local expansion rate, H_0) to
1% precision, and the structure growth rate of structure to 5%; (ii) to make
the most extensive map yet constructed of the mass distribution and motions in
the local Universe, using peculiar velocities based on improved Fundamental
Plane distances, which will enable sensitive tests of gravitational physics;
and (iii) to deliver a legacy sample of low-redshift galaxies as a unique
laboratory for studying galaxy evolution as a function of mass and environment.
The final survey, which will be completed within 5 years, will consist of a
complete magnitude-limited sample (i<17) of about 1.2x10^6 galaxies,
supplemented by an extension to higher redshifts and fainter magnitudes
(i<18.1) of a luminous red galaxy sample of about 0.8x10^6 galaxies.
Observations and data processing will be carried out remotely and in a
fully-automated way, using a purpose-built automated 'virtual observer'
software and an automated data reduction pipeline. The Taipan survey is
deliberately designed to maximise its legacy value, by complementing and
enhancing current and planned surveys of the southern sky at wavelengths from
the optical to the radio.
",0,1,0,0,0,0
20355,"On the origin of the shallow and ""replica"" bands in FeSe monolayer superconductors","  We compare electronic structures of single FeSe layer films on SrTiO$_3$
substrate (FeSe/STO) and K$_x$Fe$_{2-y}$Se$_{2}$ superconductors obtained from
extensive LDA and LDA+DMFT calculations with the results of ARPES experiments.
It is demonstrated that correlation effects on Fe-3d states are sufficient in
principle to explain the formation of the shallow electron -- like bands at the
M(X)-point. However, in FeSe/STO these effects alone are apparently
insufficient for the simultaneous elimination of the hole -- like Fermi surface
around the $\Gamma$-point which is not observed in ARPES experiments. Detailed
comparison of ARPES detected and calculated quasiparticle bands shows
reasonable agreement between theory and experiment. Analysis of the bands with
respect to their origin and orbital composition shows, that for FeSe/STO system
the experimentally observed ""replica"" quasiparticle band at the M-point
(usually attributed to forward scattering interactions with optical phonons in
SrTiO$_3$ substrate) can be reasonably understood just as the LDA calculated
Fe-3d$_{xy}$ band, renormalized by electronic correlations. The only
manifestation of the substrate reduces to lifting the degeneracy between
Fe-3d$_{xz}$ and Fe-3d$_{yz}$ bands in the vicinity of M-point. For the case of
K$_x$Fe$_{2-y}$Se$_{2}$ most bands observed in ARPES can also be understood as
correlation renormalized Fe-3d LDA calculated bands, with overall semi --
quantitative agreement with LDA+DMFT calculations.
",0,1,0,0,0,0
11669,Kropina change of a Finsler space with m-th root metric,"  In this paper, we find a condition under which a Finsler space with Kropina
change of mth-root metric is projectively related to a mth-root metric and also
we find a condition under which this Kropina transformed mth-root metric is
locally dually flat. Moreover we find the condition for its Projective
flatness.
",0,0,1,0,0,0
15527,The photon identification loophole in EPRB experiments: computer models with single-wing selection,"  Recent Einstein-Podolsky-Rosen-Bohm experiments [M. Giustina et al. Phys.
Rev. Lett. 115, 250401 (2015); L. K. Shalm et al. Phys. Rev. Lett. 115, 250402
(2015)] that claim to be loophole free are scrutinized and are shown to suffer
a photon identification loophole. The combination of a digital computer and
discrete-event simulation is used to construct a minimal but faithful model of
the most perfected realization of these laboratory experiments. In contrast to
prior simulations, all photon selections are strictly made, as they are in the
actual experiments, at the local station and no other ""post-selection"" is
involved. The simulation results demonstrate that a manifestly non-quantum
model that identifies photons in the same local manner as in these experiments
can produce correlations that are in excellent agreement with those of the
quantum theoretical description of the corresponding thought experiment, in
conflict with Bell's theorem. The failure of Bell's theorem is possible because
of our recognition of the photon identification loophole. Such identification
measurement-procedures are necessarily included in all actual experiments but
are not included in the theory of Bell and his followers.
",0,1,0,0,0,0
3588,A class of multi-resolution approximations for large spatial datasets,"  Gaussian processes are popular and flexible models for spatial, temporal, and
functional data, but they are computationally infeasible for large datasets. We
discuss Gaussian-process approximations that use basis functions at multiple
resolutions to achieve fast inference and that can (approximately) represent
any spatial covariance structure. We consider two special cases of this
multi-resolution-approximation framework, a taper version and a
domain-partitioning (block) version. We describe theoretical properties and
inference procedures, and study the computational complexity of the methods.
Numerical comparisons and an application to satellite data are also provided.
",0,0,0,1,0,0
362,Making Sense of Physics through Stories: High School Students Narratives about Electric Charges and Interactions,"  Educational research has shown that narratives are useful tools that can help
young students make sense of scientific phenomena. Based on previous research,
I argue that narratives can also become tools for high school students to make
sense of concepts such as the electric field. In this paper I examine high
school students visual and oral narratives in which they describe the
interaction among electric charges as if they were characters of a cartoon
series. The study investigates: given the prompt to produce narratives for
electrostatic phenomena during a classroom activity prior to receiving formal
instruction, (1) what ideas of electrostatics do students attend to in their
narratives?; (2) what role do students narratives play in their understanding
of electrostatics? The participants were a group of high school students
engaged in an open-ended classroom activity prior to receiving formal
instruction about electrostatics. During the activity, the group was asked to
draw comic strips for electric charges. In addition to individual work,
students shared their work within small groups as well as with the whole group.
Post activity, six students from a small group were interviewed individually
about their work. In this paper I present two cases in which students produced
narratives to express their ideas about electrostatics in different ways. In
each case, I present student work for the comic strip activity (visual
narratives), their oral descriptions of their work (oral narratives) during the
interview and/or to their peers during class, and the their ideas of the
electric interactions expressed through their narratives.
",0,1,0,0,0,0
17576,Effective difference elimination and Nullstellensatz,"  We prove effective Nullstellensatz and elimination theorems for difference
equations in sequence rings. More precisely, we compute an explicit function of
geometric quantities associated to a system of difference equations (and these
geometric quantities may themselves be bounded by a function of the number of
variables, the order of the equations, and the degrees of the equations) so
that for any system of difference equations in variables $\mathbf{x} = (x_1,
\ldots, x_m)$ and $\mathbf{u} = (u_1, \ldots, u_r)$, if these equations have
any nontrivial consequences in the $\mathbf{x}$ variables, then such a
consequence may be seen algebraically considering transforms up to the order of
our bound. Specializing to the case of $m = 0$, we obtain an effective method
to test whether a given system of difference equations is consistent.
",0,0,1,0,0,0
19397,Clustering Analysis on Locally Asymptotically Self-similar Processes with Known Number of Clusters,"  We study the problems of clustering locally asymptotically self-similar
stochastic processes, when the true number of clusters is priorly known. A new
covariance-based dissimilarity measure is introduced, from which the so-called
approximately asymptotically consistent clustering algorithms are obtained. In
a simulation study, clustering data sampled from multifractional Brownian
motions is performed to illustrate the approximated asymptotic consistency of
the proposed algorithms.
",0,0,0,1,0,0
2281,Statistical Mechanics of Node-perturbation Learning with Noisy Baseline,"  Node-perturbation learning is a type of statistical gradient descent
algorithm that can be applied to problems where the objective function is not
explicitly formulated, including reinforcement learning. It estimates the
gradient of an objective function by using the change in the object function in
response to the perturbation. The value of the objective function for an
unperturbed output is called a baseline. Cho et al. proposed node-perturbation
learning with a noisy baseline. In this paper, we report on building the
statistical mechanics of Cho's model and on deriving coupled differential
equations of order parameters that depict learning dynamics. We also show how
to derive the generalization error by solving the differential equations of
order parameters. On the basis of the results, we show that Cho's results are
also apply in general cases and show some general performances of Cho's model.
",1,0,0,1,0,0
18101,"A new, large-scale map of interstellar reddening derived from HI emission","  We present a new map of interstellar reddening, covering the 39\% of the sky
with low {\rm HI} column densities ($N_{\rm HI} < 4\times10^{20}\,\rm cm^{-2}$
or $E(B-V)\approx 45\rm\, mmag$) at $16\overset{'}{.}1$ resolution, based on
all-sky observations of Galactic HI emission by the HI4PI Survey. In this low
column density regime, we derive a characteristic value of $N_{\rm HI}/E(B-V) =
8.8\times10^{21}\, \rm\, cm^{2}\, mag^{-1}$ for gas with $|v_{\rm LSR}| <
90\,\rm km\, s^{-1}$ and find no significant reddening associated with gas at
higher velocities. We compare our HI-based reddening map with the Schlegel,
Finkbeiner, and Davis (1998, SFD) reddening map and find them consistent to
within a scatter of $\simeq 5\,\rm mmag$. Further, the differences between our
map and the SFD map are in excellent agreement with the low resolution
($4\overset{\circ}{.}5$) corrections to the SFD map derived by Peek and Graves
(2010) based on observed reddening toward passive galaxies. We therefore argue
that our HI-based map provides the most accurate interstellar reddening
estimates in the low column density regime to date. Our reddening map is made
publicly available (this http URL).
",0,1,0,0,0,0
10703,A Data-driven Model for Interaction-aware Pedestrian Motion Prediction in Object Cluttered Environments,"  This paper reports on a data-driven, interaction-aware motion prediction
approach for pedestrians in environments cluttered with static obstacles. When
navigating in such workspaces shared with humans, robots need accurate motion
predictions of the surrounding pedestrians. Human navigation behavior is mostly
influenced by their surrounding pedestrians and by the static obstacles in
their vicinity. In this paper we introduce a new model based on Long-Short Term
Memory (LSTM) neural networks, which is able to learn human motion behavior
from demonstrated data. To the best of our knowledge, this is the first
approach using LSTMs, that incorporates both static obstacles and surrounding
pedestrians for trajectory forecasting. As part of the model, we introduce a
new way of encoding surrounding pedestrians based on a 1d-grid in polar angle
space. We evaluate the benefit of interaction-aware motion prediction and the
added value of incorporating static obstacles on both simulation and real-world
datasets by comparing with state-of-the-art approaches. The results show, that
our new approach outperforms the other approaches while being very
computationally efficient and that taking into account static obstacles for
motion predictions significantly improves the prediction accuracy, especially
in cluttered environments.
",1,0,0,0,0,0
16119,Some remarks on upper bounds for Weierstrass primary factors and their application in spectral theory,"  We study upper bounds on Weierstrass primary factors and discuss their
application in spectral theory. One of the main aims of this note is to draw
attention to works of Blumenthal and Denjoy from 1910, but we also provide some
new results and some numerical computations of our own.
",0,0,1,0,0,0
3657,Dynamic scaling analysis of the long-range RKKY Ising spin glass Dy$_{x}$Y$_{1-x}$Ru$_{2}$Si$_{2}$,"  Dynamic scaling analyses of linear and nonlinear ac susceptibilities in a
model magnet of the long-rang RKKY Ising spin glass (SG)
Dy$_{x}$Y$_{1-x}$Ru$_{2}$Si$_{2}$ were examined. The obtained set of the
critical exponents, $\gamma$ $\sim$ 1, $\beta$ $\sim$ 1, $\delta$ $\sim$ 2, and
$z\nu$ $\sim$ 3.4, indicates the SG phase transition belongs to a different
universality class from either the canonical (Heisenberg) or the short-range
Ising SGs. The analyses also reveal a finite-temperature SG transition with the
same critical exponents under a magnetic field and the phase transition line
$T_{\mbox{g}}(H)$ described by $T_{\mbox{g}}(H)$ $=$
$T_{\mbox{g}}(0)(1-AH^{2/\phi})$ with $\phi$ $\sim$ 2. The crossover exponent
$\phi$ obeys the scaling relation $\phi$ $=$ $\gamma + \beta$ within the margin
of errors. These results strongly suggest the spontaneous
replica-symmetry-breaking (RSB) with a {\it non- or marginal-mean-field
universality class} in the long-range RKKY Ising SG.
",0,1,0,0,0,0
20126,Fast Radio Map Construction and Position Estimation via Direct Mapping for WLAN Indoor Localization System,"  The main limitation that constrains the fast and comprehensive application of
Wireless Local Area Network (WLAN) based indoor localization systems with
Received Signal Strength (RSS) positioning algorithms is the building of the
fingerprinting radio map, which is time-consuming especially when the indoor
environment is large and/or with high frequent changes. Different approaches
have been proposed to reduce workload, including fingerprinting deployment and
update efforts, but the performance degrades greatly when the workload is
reduced below a certain level. In this paper, we propose an indoor localization
scenario that applies metric learning and manifold alignment to realize direct
mapping localization (DML) using a low resolution radio map with single sample
of RSS that reduces the fingerprinting workload by up to 87\%. Compared to
previous work. The proposed two localization approaches, DML and $k$ nearest
neighbors based on reconstructed radio map (reKNN), were shown to achieve less
than 4.3\ m and 3.7\ m mean localization error respectively in a typical office
environment with an area of approximately 170\ m$^2$, while the unsupervised
localization with perturbation algorithm was shown to achieve 4.7\ m mean
localization error with 8 times more workload than the proposed methods. As for
the room level localization application, both DML and reKNN can meet the
requirement with at most 9\ m of localization error which is enough to tell
apart different rooms with over 99\% accuracy.
",1,0,0,1,0,0
6775,Reactive User Behavior and Mobility Models,"  In this paper, we present a set of simulation models to more realistically
mimic the behaviour of users reading messages. We propose a User Behaviour
Model, where a simulated user reacts to a message by a flexible set of possible
reactions (e.g. ignore, read, like, save, etc.) and a mobility-based reaction
(visit a place, run away from danger, etc.). We describe our models and their
implementation in OMNeT++. We strongly believe that these models will
significantly contribute to the state of the art of simulating realistically
opportunistic networks.
",1,0,0,0,0,0
2788,Unveiling Swarm Intelligence with Network Science$-$the Metaphor Explained,"  Self-organization is a natural phenomenon that emerges in systems with a
large number of interacting components. Self-organized systems show robustness,
scalability, and flexibility, which are essential properties when handling
real-world problems. Swarm intelligence seeks to design nature-inspired
algorithms with a high degree of self-organization. Yet, we do not know why
swarm-based algorithms work well and neither we can compare the different
approaches in the literature. The lack of a common framework capable of
characterizing these several swarm-based algorithms, transcending their
particularities, has led to a stream of publications inspired by different
aspects of nature without much regard as to whether they are similar to already
existing approaches. We address this gap by introducing a network-based
framework$-$the interaction network$-$to examine computational swarm-based
systems via the optics of social dynamics. We discuss the social dimension of
several swarm classes and provide a case study of the Particle Swarm
Optimization. The interaction network enables a better understanding of the
plethora of approaches currently available by looking at them from a general
perspective focusing on the structure of the social interactions.
",1,0,0,0,0,0
5200,Low Mach number limit of a pressure correction MAC scheme for compressible barotropic flows,"  We study the incompressible limit of a pressure correction MAC scheme [3] for
the unstationary compressible barotropic Navier-Stokes equations. Provided the
initial data are well-prepared, the solution of the numerical scheme converges,
as the Mach number tends to zero, towards the solution of the classical
pressure correction inf-sup stable MAC scheme for the incompressible
Navier-Stokes equations.
",0,1,1,0,0,0
15338,Rigorous statistical analysis of HTTPS reachability,"  The use of secure connections using HTTPS as the default means, or even the
only means, to connect to web servers is increasing. It is being pushed from
both sides: from the bottom up by client distributions and plugins, and from
the top down by organisations such as Google. However, there are potential
technical hurdles that might lock some clients out of the modern web. This
paper seeks to measure and precisely quantify those hurdles in the wild. More
than three million measurements provide statistically significant evidence of
degradation. We show this through a variety of statistical techniques. Various
factors are shown to influence the problem, ranging from the client's browser,
to the locale from which they connect.
",1,0,0,1,0,0
6953,Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery,"  Obtaining models that capture imaging markers relevant for disease
progression and treatment monitoring is challenging. Models are typically based
on large amounts of data with annotated examples of known markers aiming at
automating detection. High annotation effort and the limitation to a vocabulary
of known markers limit the power of such approaches. Here, we perform
unsupervised learning to identify anomalies in imaging data as candidates for
markers. We propose AnoGAN, a deep convolutional generative adversarial network
to learn a manifold of normal anatomical variability, accompanying a novel
anomaly scoring scheme based on the mapping from image space to a latent space.
Applied to new data, the model labels anomalies, and scores image patches
indicating their fit into the learned distribution. Results on optical
coherence tomography images of the retina demonstrate that the approach
correctly identifies anomalous images, such as images containing retinal fluid
or hyperreflective foci.
",1,0,0,0,0,0
11750,Ultra-light and strong: the massless harmonic oscillator and its singular path integral,"  In classical mechanics, a light particle bound by a strong elastic force just
oscillates at high frequency in the region allowed by its initial position and
velocity. In quantum mechanics, instead, the ground state of the particle
becomes completely de-localized in the limit $m \to 0$. The harmonic oscillator
thus ceases to be a useful microscopic physical model in the limit $m \to 0$,
but its Feynman path integral has interesting singularities which make it a
prototype of other systems exhibiting a ""quantum runaway"" from the classical
configurations near the minimum of the action. The probability density of the
coherent runaway modes can be obtained as the solution of a Fokker-Planck
equation associated to the condition $S=S_{min}$. This technique can be applied
also to other systems, notably to a dimensional reduction of the
Einstein-Hilbert action.
",0,1,0,0,0,0
8317,P-wave superfluidity of atomic lattice fermions,"  We discuss the emergence of p-wave superfluidity of identical atomic fermions
in a two-dimensional optical lattice. The optical lattice potential manifests
itself in an interplay between an increase in the density of states on the
Fermi surface and the modification of the fermion-fermion interaction
(scattering) amplitude. The density of states is enhanced due to an increase of
the effective mass of atoms. In deep lattices the scattering amplitude is
strongly reduced compared to free space due to a small overlap of wavefunctions
of fermion sitting in the neighboring lattice sites, which suppresses the
p-wave superfluidity. However, for moderate lattice depths the enhancement of
the density of states can compensate the decrease of the scattering amplitude.
Moreover, the lattice setup significantly reduces inelastic collisional losses,
which allows one to get closer to a p-wave Feshbach resonance. This opens
possibilities to obtain the topological $p_x+ip_y$ superfluid phase, especially
in the recently proposed subwavelength lattices. We demonstrate this for the
two-dimensional version of the Kronig-Penney model allowing a transparent
physical analysis.
",0,1,0,0,0,0
7618,Finite-Size Effects in Non-Neutral Two-Dimensional Coulomb Fluids,"  Thermodynamic potential of a neutral two-dimensional (2D) Cou\-lomb fluid,
confined to a large domain with a smooth boundary, exhibits at any (inverse)
temperature $\beta$ a logarithmic finite-size correction term whose universal
prefactor depends only on the Euler number of the domain and the conformal
anomaly number $c=-1$. A minimal free boson conformal field theory, which is
equivalent to the 2D symmetric two-component plasma of elementary $\pm e$
charges at coupling constant $\Gamma=\beta e^2$, was studied in the past. It
was shown that creating a non-neutrality by spreading out a charge $Q e$ at
infinity modifies the anomaly number to $c(Q,\Gamma) = - 1 + 3\Gamma Q^2$.
Here, we study the effect of non-neutrality on the finite-size expansion of the
free energy for another Coulomb fluid, namely the 2D one-component plasma
(jellium) composed of identical pointlike $e$-charges in a homogeneous
background surface charge density. For the disk geometry of the confining
domain we find that the non-neutrality induces the same change of the anomaly
number in the finite-size expansion. We derive this result first at the
free-fermion coupling $\Gamma\equiv\beta e^2=2$ and then, by using a mapping of
the 2D one-component plasma onto an anticommuting field theory formulated on a
chain, for an arbitrary coupling constant.
",0,1,0,0,0,0
12798,Analysis and Applications of Delay Differential Equations in Biology and Medicine,"  The main purpose of this paper is to provide a summary of the fundamental
methods for analyzing delay differential equations arising in biology and
medicine. These methods are employed to illustrate the effects of time delay on
the behavior of solutions, which include destabilization of steady states,
periodic and oscillatory solutions, bifurcations, and stability switches. The
biological interpretations of delay effects are briefly discussed.
",0,0,1,0,0,0
20301,The interplay between Steinberg algebras and partial skew rings,"  We study the interplay between Steinberg algebras and partial skew rings: For
a partial action of a group in a Hausdorff, locally compact, totally
disconnected topological space, we realize the associated partial skew group
ring as a Steinberg algebra (over the transformation groupoid attached to the
partial action). We then apply this realization to characterize diagonal
preserving isomorphisms of partial skew group rings, over commutative algebras,
in terms of continuous orbit equivalence of the associated partial actions.
Finally, we show that any Steinberg algebra, associated to a Hausdorff ample
groupoid, can be seen as a partial skew inverse semigroup ring.
",0,0,1,0,0,0
7971,An Empirical Bayes Approach to Regularization Using Previously Published Models,"  This manuscript proposes a novel empirical Bayes technique for regularizing
regression coefficients in predictive models. When predictions from a
previously published model are available, this empirical Bayes method provides
a natural mathematical framework for shrinking coefficients toward the
estimates implied by the body of existing research rather than the shrinkage
toward zero provided by traditional L1 and L2 penalization schemes. The method
is applied to two different prediction problems. The first involves the
construction of a model for predicting whether a single nucleotide polymorphism
(SNP) of the KCNQ1 gene will result in dysfunction of the corresponding voltage
gated ion channel. The second involves the prediction of preoperative serum
creatinine change in patients undergoing cardiac surgery.
",0,0,0,1,0,0
11670,Direct Visualization of 2D Topological Insulator in Single-layer 1T'-WTe2,"  We grow nearly freestanding single-layer 1T'-WTe2 on graphitized 6H-SiC(0001)
by using molecular beam epitaxy (MBE), and characterize its electronic
structure with scanning tunneling microscopy / spectroscopy (STM/STS). We
demonstrate the existence of topological edge states at the periphery of
single-layer WTe2 islands. Surprisingly, we also find a band gap in the bulk
and the semiconducting behaviors of the single-layer WTe2 at low temperature,
which is likely resulted from an incommensurate charge density wave (CDW)
transition. The realization of two-dimensional topological insulators (2D TIs)
in single-layer transition metal dichalcogenide (TMD) thus provides a promising
platform for further exploration of the 2D TIs' physics and related
applications.
",0,1,0,0,0,0
1726,On the presentation of Hecke-Hopf algebras for non-simply-laced type,"  Hecke-Hopf algebras were defined by A. Berenstein and D. Kazhdan. We give an
explicit presentation of an Hecke-Hopf algebra when the parameter $m_{ij},$
associated to any two distinct vertices $i$ and $j$ in the presentation of a
Coxeter group, equals $4,$ $5$ or $6$. As an application, we give a proof of a
conjecture of Berenstein and Kazhdan when the Coxeter group is crystallographic
and non-simply-laced. As another application, we show that another conjecture
of Berenstein and Kazhdan holds when $m_{ij},$ associated to any two distinct
vertices $i$ and $j,$ equals $4$ and that the conjecture does not hold when
some $m_{ij}$ equals $6$ by giving a counterexample to it.
",0,0,1,0,0,0
19082,"Integrable systems, symmetries and quantization","  These notes correspond to a mini-course given at the Poisson 2016 conference
in Geneva. Starting from classical integrable systems in the sense of
Liouville, we explore the notion of non-degenerate singularity and expose
recent research in connection with semi-toric systems. The quantum and
semiclassical counterpart will also be presented, in the viewpoint of the
inverse question: from the quantum mechanical spectrum, can you recover the
classical system?
",0,1,1,0,0,0
10657,A Characterization Theorem for a Modal Description Logic,"  Modal description logics feature modalities that capture dependence of
knowledge on parameters such as time, place, or the information state of
agents. E.g., the logic S5-ALC combines the standard description logic ALC with
an S5-modality that can be understood as an epistemic operator or as
representing (undirected) change. This logic embeds into a corresponding modal
first-order logic S5-FOL. We prove a modal characterization theorem for this
embedding, in analogy to results by van Benthem and Rosen relating ALC to
standard first-order logic: We show that S5-ALC with only local roles is, both
over finite and over unrestricted models, precisely the bisimulation invariant
fragment of S5-FOL, thus giving an exact description of the expressive power of
S5-ALC with only local roles.
",1,0,1,0,0,0
7124,Look No Further: Adapting the Localization Sensory Window to the Temporal Characteristics of the Environment,"  Many localization algorithms use a spatiotemporal window of sensory
information in order to recognize spatial locations, and the length of this
window is often a sensitive parameter that must be tuned to the specifics of
the application. This letter presents a general method for environment-driven
variation of the length of the spatiotemporal window based on searching for the
most significant localization hypothesis, to use as much context as is
appropriate but not more. We evaluate this approach on benchmark datasets using
visual and Wi-Fi sensor modalities and a variety of sensory comparison
front-ends under in-order and out-of-order traversals of the environment. Our
results show that the system greatly reduces the maximum distance traveled
without localization compared to a fixed-length approach while achieving
competitive localization accuracy, and our proposed method achieves this
performance without deployment-time tuning.
",1,0,0,0,0,0
16997,Expansion of percolation critical points for Hamming graphs,"  The Hamming graph $H(d,n)$ is the Cartesian product of $d$ complete graphs on
$n$ vertices. Let $m=d(n-1)$ be the degree and $V = n^d$ be the number of
vertices of $H(d,n)$. Let $p_c^{(d)}$ be the critical point for bond
percolation on $H(d,n)$. We show that, for $d \in \mathbb N$ fixed and $n \to
\infty$,
\begin{equation*}
p_c^{(d)}= \dfrac{1}{m} + \dfrac{2d^2-1}{2(d-1)^2}\dfrac{1}{m^2}
+ O(m^{-3}) + O(m^{-1}V^{-1/3}),
\end{equation*} which extends the asymptotics found in
\cite{BorChaHofSlaSpe05b} by one order. The term $O(m^{-1}V^{-1/3})$ is the
width of the critical window. For $d=4,5,6$ we have $m^{-3} =
O(m^{-1}V^{-1/3})$, and so the above formula represents the full asymptotic
expansion of $p_c^{(d)}$. In \cite{FedHofHolHul16a} \st{we show that} this
formula is a crucial ingredient in the study of critical bond percolation on
$H(d,n)$ for $d=2,3,4$. The proof uses a lace expansion for the upper bound and
a novel comparison with a branching random walk for the lower bound. The proof
of the lower bound also yields a refined asymptotics for the susceptibility of
a subcritical Erdős-Rényi random graph.
",0,0,1,0,0,0
7534,CELLO-3D: Estimating the Covariance of ICP in the Real World,"  The fusion of Iterative Closest Point (ICP) reg- istrations in existing state
estimation frameworks relies on an accurate estimation of their uncertainty. In
this paper, we study the estimation of this uncertainty in the form of a
covariance. First, we scrutinize the limitations of existing closed-form
covariance estimation algorithms over 3D datasets. Then, we set out to estimate
the covariance of ICP registrations through a data-driven approach, with over 5
100 000 registrations on 1020 pairs from real 3D point clouds. We assess our
solution upon a wide spectrum of environments, ranging from structured to
unstructured and indoor to outdoor. The capacity of our algorithm to predict
covariances is accurately assessed, as well as the usefulness of these
estimations for uncertainty estimation over trajectories. The proposed method
estimates covariances better than existing closed-form solutions, and makes
predictions that are consistent with observed trajectories.
",1,0,0,0,0,0
1282,"Contiguous Relations, Laplace's Methods and Continued Fractions for 3F2(1)","  Using contiguous relations we construct an infinite number of continued
fraction expansions for ratios of generalized hypergeometric series 3F2(1). We
establish exact error term estimates for their approximants and prove their
rapid convergences. To do so we develop a discrete version of Laplace's method
for hypergeometric series in addition to the use of ordinary (continuous)
Laplace's method for Euler's hypergeometric integrals.
",0,0,1,0,0,0
738,Existence results for primitive elements in cubic and quartic extensions of a finite field,"  With $\Fq$ the finite field of $q$ elements, we investigate the following
question. If $\gamma$ generates $\Fqn$ over $\Fq$ and $\beta$ is a non-zero
element of $\Fqn$, is there always an $a \in \Fq$ such that $\beta(\gamma + a)$
is a primitive element? We resolve this case when $n=3$, thereby proving a
conjecture by Cohen. We also improve substantially on what is known when $n=4$.
",0,0,1,0,0,0
20087,Koszul duality via suspending Lefschetz fibrations,"  Let $M$ be a Liouville 6-manifold which is the smooth fiber of a Lefschetz
fibration on $\mathbb{C}^4$ constructed by suspending a Lefschetz fibration on
$\mathbb{C}^3$. We prove that for many examples including stabilizations of
Milnor fibers of hypersurface cusp singularities, the compact Fukaya category
$\mathcal{F}(M)$ and the wrapped Fukaya category $\mathcal{W}(M)$ are related
through $A_\infty$-Koszul duality, by identifying them with cyclic and
Calabi-Yau completions of the same quiver algebra. This implies the
split-generation of the compact Fukaya category $\mathcal{F}(M)$ by vanishing
cycles. Moreover, new examples of Liouville manifolds which admit
quasi-dilations in the sense of Seidel-Solomon are obtained.
",0,0,1,0,0,0
367,Phase correction for ALMA - Investigating water vapour radiometer scaling:The long-baseline science verification data case study,"  The Atacama Large millimetre/submillimetre Array (ALMA) makes use of water
vapour radiometers (WVR), which monitor the atmospheric water vapour line at
183 GHz along the line of sight above each antenna to correct for phase delays
introduced by the wet component of the troposphere. The application of WVR
derived phase corrections improve the image quality and facilitate successful
observations in weather conditions that were classically marginal or poor. We
present work to indicate that a scaling factor applied to the WVR solutions can
act to further improve the phase stability and image quality of ALMA data. We
find reduced phase noise statistics for 62 out of 75 datasets from the
long-baseline science verification campaign after a WVR scaling factor is
applied. The improvement of phase noise translates to an expected coherence
improvement in 39 datasets. When imaging the bandpass source, we find 33 of the
39 datasets show an improvement in the signal-to-noise ratio (S/N) between a
few to ~30 percent. There are 23 datasets where the S/N of the science image is
improved: 6 by <1%, 11 between 1 and 5%, and 6 above 5%. The higher frequencies
studied (band 6 and band 7) are those most improved, specifically datasets with
low precipitable water vapour (PWV), <1mm, where the dominance of the wet
component is reduced. Although these improvements are not profound, phase
stability improvements via the WVR scaling factor come into play for the higher
frequency (>450 GHz) and long-baseline (>5km) observations. These inherently
have poorer phase stability and are taken in low PWV (<1mm) conditions for
which we find the scaling to be most effective. A promising explanation for the
scaling factor is the mixing of dry and wet air components, although other
origins are discussed. We have produced a python code to allow ALMA users to
undertake WVR scaling tests and make improvements to their data.
",0,1,0,0,0,0
1508,Sparse Inverse Covariance Estimation for Chordal Structures,"  In this paper, we consider the Graphical Lasso (GL), a popular optimization
problem for learning the sparse representations of high-dimensional datasets,
which is well-known to be computationally expensive for large-scale problems.
Recently, we have shown that the sparsity pattern of the optimal solution of GL
is equivalent to the one obtained from simply thresholding the sample
covariance matrix, for sparse graphs under different conditions. We have also
derived a closed-form solution that is optimal when the thresholded sample
covariance matrix has an acyclic structure. As a major generalization of the
previous result, in this paper we derive a closed-form solution for the GL for
graphs with chordal structures. We show that the GL and thresholding
equivalence conditions can significantly be simplified and are expected to hold
for high-dimensional problems if the thresholded sample covariance matrix has a
chordal structure. We then show that the GL and thresholding equivalence is
enough to reduce the GL to a maximum determinant matrix completion problem and
drive a recursive closed-form solution for the GL when the thresholded sample
covariance matrix has a chordal structure. For large-scale problems with up to
450 million variables, the proposed method can solve the GL problem in less
than 2 minutes, while the state-of-the-art methods converge in more than 2
hours.
",0,0,0,1,0,0
14341,Dehn functions of subgroups of right-angled Artin groups,"  We show that for each positive integer $k$ there exist right-angled Artin
groups containing free-by-cyclic subgroups whose monodromy automorphisms grow
as $n^k$. As a consequence we produce examples of right-angled Artin groups
containing finitely presented subgroups whose Dehn functions grow as $n^{k+2}$.
",0,0,1,0,0,0
2924,A Theory of Solvability for Lossless Power Flow Equations -- Part I: Fixed-Point Power Flow,"  This two-part paper details a theory of solvability for the power flow
equations in lossless power networks. In Part I, we derive a new formulation of
the lossless power flow equations, which we term the fixed-point power flow.
The model is stated for both meshed and radial networks, and is parameterized
by several graph-theoretic matrices -- the power network stiffness matrices --
which quantify the internal coupling strength of the network. The model leads
immediately to an explicit approximation of the high-voltage power flow
solution. For standard test cases, we find that iterates of the fixed-point
power flow converge rapidly to the high-voltage power flow solution, with the
approximate solution yielding accurate predictions near base case loading. In
Part II, we leverage the fixed-point power flow to study power flow
solvability, and for radial networks we derive conditions guaranteeing the
existence and uniqueness of a high-voltage power flow solution. These
conditions (i) imply exponential convergence of the fixed-point power flow
iteration, and (ii) properly generalize the textbook two-bus system results.
",0,0,1,0,0,0
10044,Deep Relaxation: partial differential equations for optimizing deep neural networks,"  In this paper we establish a connection between non-convex optimization
methods for training deep neural networks and nonlinear partial differential
equations (PDEs). Relaxation techniques arising in statistical physics which
have already been used successfully in this context are reinterpreted as
solutions of a viscous Hamilton-Jacobi PDE. Using a stochastic control
interpretation allows we prove that the modified algorithm performs better in
expectation that stochastic gradient descent. Well-known PDE regularity results
allow us to analyze the geometry of the relaxed energy landscape, confirming
empirical evidence. The PDE is derived from a stochastic homogenization
problem, which arises in the implementation of the algorithm. The algorithms
scale well in practice and can effectively tackle the high dimensionality of
modern neural networks.
",1,0,1,0,0,0
17461,Localization Algorithm with Circular Representation in 2D and its Similarity to Mammalian Brains,"  Extended Kalman filter (EKF) does not guarantee consistent mean and
covariance under linearization, even though it is the main framework for
robotic localization. While Lie group improves the modeling of the state space
in localization, the EKF on Lie group still relies on the arbitrary Gaussian
assumption in face of nonlinear models. We instead use von Mises filter for
orientation estimation together with the conventional Kalman filter for
position estimation, and thus we are able to characterize the first two moments
of the state estimates. Since the proposed algorithm holds a solid
probabilistic basis, it is fundamentally relieved from the inconsistency
problem. Furthermore, we extend the localization algorithm to fully circular
representation even for position, which is similar to grid patterns found in
mammalian brains and in recurrent neural networks. The applicability of the
proposed algorithms is substantiated not only by strong mathematical foundation
but also by the comparison against other common localization methods.
",1,0,0,0,1,0
11672,Mobile big data analysis with machine learning,"  This paper investigates to identify the requirement and the development of
machine learning-based mobile big data analysis through discussing the insights
of challenges in the mobile big data (MBD). Furthermore, it reviews the
state-of-the-art applications of data analysis in the area of MBD. Firstly, we
introduce the development of MBD. Secondly, the frequently adopted methods of
data analysis are reviewed. Three typical applications of MBD analysis, namely
wireless channel modeling, human online and offline behavior analysis, and
speech recognition in the internet of vehicles, are introduced respectively.
Finally, we summarize the main challenges and future development directions of
mobile big data analysis.
",0,0,0,1,0,0
5045,On some mellin transforms for the Riemann zeta function in the critical strip,"  We offer two new Mellin transform evaluations for the Riemann zeta function
in the region $0<\Re(s)<1.$ Some discussion is offered in the way of evaluating
some further Fourier integrals involving the Riemann xi function.
",0,0,1,0,0,0
17173,Active matter invasion of a viscous fluid: unstable sheets and a no-flow theorem,"  We investigate the dynamics of a dilute suspension of hydrodynamically
interacting motile or immotile stress-generating swimmers or particles as they
invade a surrounding viscous fluid. Colonies of aligned pusher particles are
shown to elongate in the direction of particle orientation and undergo a
cascade of transverse concentration instabilities, governed at small times by
an equation which also describes the Saffman-Taylor instability in a Hele-Shaw
cell, or Rayleigh-Taylor instability in two-dimensional flow through a porous
medium. Thin sheets of aligned pusher particles are always unstable, while
sheets of aligned puller particles can either be stable (immotile particles),
or unstable (motile particles) with a growth rate which is non-monotonic in the
force dipole strength. We also prove a surprising ""no-flow theorem"": a
distribution initially isotropic in orientation loses isotropy immediately but
in such a way that results in no fluid flow everywhere and for all time.
",0,0,0,0,1,0
7784,ScaleSimulator: A Fast and Cycle-Accurate Parallel Simulator for Architectural Exploration,"  Design of next generation computer systems should be supported by simulation
infrastructure that must achieve a few contradictory goals such as fast
execution time, high accuracy, and enough flexibility to allow comparison
between large numbers of possible design points. Most existing architecture
level simulators are designed to be flexible and to execute the code in
parallel for greater efficiency, but at the cost of scarified accuracy. This
paper presents the ScaleSimulator simulation environment, which is based on a
new design methodology whose goal is to achieve near cycle accuracy while still
being flexible enough to simulate many different future system architectures
and efficient enough to run meaningful workloads. We achieve these goals by
making the parallelism a first-class citizen in our methodology. Thus, this
paper focuses mainly on the ScaleSimulator design points that enable better
parallel execution while maintaining the scalability and cycle accuracy of a
simulated architecture. The paper indicates that the new proposed
ScaleSimulator tool can (1) efficiently parallelize the execution of a
cycle-accurate architecture simulator, (2) efficiently simulate complex
architectures (e.g., out-of-order CPU pipeline, cache coherency protocol, and
network) and massive parallel systems, and (3) use meaningful workloads, such
as full simulation of OLTP benchmarks, to examine future architectural choices.
",1,0,0,0,0,0
20396,"First principles study of structural, magnetic and electronic properties of CrAs","  We report ab initio density functional calculations of the structural and
magnetic properties, and the electronic structure of CrAs. To simulate the
observed pressure-driven experimental results, we perform our analysis for
different volumes of the unit cell, showing that the structural, magnetic and
electronic properties strongly depend on the size of the cell. We find that the
calculated quantities are in good agreement with the experimental data, and we
review our results in terms of the observed superconductivity.
",0,1,0,0,0,0
7919,System Level Framework for Assessing the Accuracy of Neonatal EEG Acquisition,"  Significant research has been conducted in recent years to design low-cost
alternatives to the current EEG monitoring systems used in healthcare
facilities. Testing such systems on a vulnerable population such as newborns is
complicated due to ethical and regulatory considerations that slow down the
technical development. This paper presents and validates a method for
quantifying the accuracy of neonatal EEG acquisition systems and electrode
technologies via clinical data simulations that do not require neonatal
participants. The proposed method uses an extensive neonatal EEG database to
simulate analogue signals, which are subsequently passed through electrical
models of the skin-electrode interface, which are developed using wet and dry
EEG electrode designs. The signal losses in the system are quantified at each
stage of the acquisition process for electrode and acquisition board losses.
SNR, correlation and noise values were calculated. The results verify that
low-cost EEG acquisition systems are capable of obtaining clinical grade EEG.
Although dry electrodes result in a significant increase in the skin-electrode
impedance, accurate EEG recordings are still achievable.
",0,0,0,1,0,0
3100,VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning,"  Deep generative models provide powerful tools for distributions over
complicated manifolds, such as those of natural images. But many of these
methods, including generative adversarial networks (GANs), can be difficult to
train, in part because they are prone to mode collapse, which means that they
characterize only a few modes of the true distribution. To address this, we
introduce VEEGAN, which features a reconstructor network, reversing the action
of the generator by mapping from data to noise. Our training objective retains
the original asymptotic consistency guarantee of GANs, and can be interpreted
as a novel autoencoder loss over the noise. In sharp contrast to a traditional
autoencoder over data points, VEEGAN does not require specifying a loss
function over the data, but rather only over the representations, which are
standard normal by assumption. On an extensive set of synthetic and real world
image datasets, VEEGAN indeed resists mode collapsing to a far greater extent
than other recent GAN variants, and produces more realistic samples.
",0,0,0,1,0,0
5388,Generalised Discount Functions applied to a Monte-Carlo AImu Implementation,"  In recent years, work has been done to develop the theory of General
Reinforcement Learning (GRL). However, there are few examples demonstrating
these results in a concrete way. In particular, there are no examples
demonstrating the known results regarding gener- alised discounting. We have
added to the GRL simulation platform AIXIjs the functionality to assign an
agent arbitrary discount functions, and an environment which can be used to
determine the effect of discounting on an agent's policy. Using this, we
investigate how geometric, hyperbolic and power discounting affect an informed
agent in a simple MDP. We experimentally reproduce a number of theoretical
results, and discuss some related subtleties. It was found that the agent's
behaviour followed what is expected theoretically, assuming appropriate
parameters were chosen for the Monte-Carlo Tree Search (MCTS) planning
algorithm.
",1,0,0,0,0,0
791,Amortized Inference Regularization,"  The variational autoencoder (VAE) is a popular model for density estimation
and representation learning. Canonically, the variational principle suggests to
prefer an expressive inference model so that the variational approximation is
accurate. However, it is often overlooked that an overly-expressive inference
model can be detrimental to the test set performance of both the amortized
posterior approximator and, more importantly, the generative density estimator.
In this paper, we leverage the fact that VAEs rely on amortized inference and
propose techniques for amortized inference regularization (AIR) that control
the smoothness of the inference model. We demonstrate that, by applying AIR, it
is possible to improve VAE generalization on both inference and generative
performance. Our paper challenges the belief that amortized inference is simply
a mechanism for approximating maximum likelihood training and illustrates that
regularization of the amortization family provides a new direction for
understanding and improving generalization in VAEs.
",0,0,0,1,0,0
1874,Equitable neighbour-sum-distinguishing edge and total colourings,"  With any (not necessarily proper) edge $k$-colouring
$\gamma:E(G)\longrightarrow\{1,\dots,k\}$ of a graph $G$,one can associate a
vertex colouring $\sigma\_{\gamma}$ given by $\sigma\_{\gamma}(v)=\sum\_{e\ni
v}\gamma(e)$.A neighbour-sum-distinguishing edge $k$-colouring is an edge
colouring whose associated vertex colouring is proper.The
neighbour-sum-distinguishing index of a graph $G$ is then the smallest $k$ for
which $G$ admitsa neighbour-sum-distinguishing edge $k$-colouring.These notions
naturally extends to total colourings of graphs that assign colours to both
vertices and edges.We study in this paper equitable
neighbour-sum-distinguishing edge colourings andtotal colourings, that is
colourings $\gamma$ for whichthe number of elements in any two colour classes
of $\gamma$ differ by at most one.We determine the equitable
neighbour-sum-distinguishing indexof complete graphs, complete bipartite graphs
and forests,and the equitable neighbour-sum-distinguishing total chromatic
numberof complete graphs and bipartite graphs.
",1,0,1,0,0,0
9984,A cost effective and reliable environment monitoring system for HPC applications,"  We present a slow control system to gather all relevant environment
information necessary to effectively and reliably run an HPC (High Performance
Computing) system at a high value over price ratio. The scalable and reliable
overall concept is presented as well as a newly developed hardware device for
sensor read out. This device incorporates a Raspberry Pi, an Arduino and PoE
(Power over Ethernet) functionality in a compact form factor. The system is in
use at the 2 PFLOPS cluster of the Johannes Gutenberg-University and
Helmholtz-Institute in Mainz.
",1,0,0,0,0,0
2735,"Navigate, Understand, Communicate: How Developers Locate Performance Bugs","  Background: Performance bugs can lead to severe issues regarding computation
efficiency, power consumption, and user experience. Locating these bugs is a
difficult task because developers have to judge for every costly operation
whether runtime is consumed necessarily or unnecessarily. Objective: We wanted
to investigate how developers, when locating performance bugs, navigate through
the code, understand the program, and communicate the detected issues. Method:
We performed a qualitative user study observing twelve developers trying to fix
documented performance bugs in two open source projects. The developers worked
with a profiling and analysis tool that visually depicts runtime information in
a list representation and embedded into the source code view. Results: We
identified typical navigation strategies developers used for pinpointing the
bug, for instance, following method calls based on runtime consumption. The
integration of visualization and code helped developers to understand the bug.
Sketches visualizing data structures and algorithms turned out to be valuable
for externalizing and communicating the comprehension process for complex bugs.
Conclusion: Fixing a performance bug is a code comprehension and navigation
problem. Flexible navigation features based on executed methods and a close
integration of source code and performance information support the process.
",1,0,0,0,0,0
13324,Information transmission on hybrid networks,"  Many real-world communication networks often have hybrid nature with both
fixed nodes and moving modes, such as the mobile phone networks mainly composed
of fixed base stations and mobile phones. In this paper, we discuss the
information transmission process on the hybrid networks with both fixed and
mobile nodes. The fixed nodes (base stations) are connected as a spatial
lattice on the plane forming the information-carrying backbone, while the
mobile nodes (users), which are the sources and destinations of information
packets, connect to their current nearest fixed nodes respectively to deliver
and receive information packets. We observe the phase transition of traffic
load in the hybrid network when the packet generation rate goes from below and
then above a critical value, which measures the network capacity of packets
delivery. We obtain the optimal speed of moving nodes leading to the maximum
network capacity. We further improve the network capacity by rewiring the fixed
nodes and by considering the current load of fixed nodes during packets
transmission. Our purpose is to optimize the network capacity of hybrid
networks from the perspective of network science, and provide some insights for
the construction of future communication infrastructures.
",1,1,0,0,0,0
6577,What Would a Graph Look Like in This Layout? A Machine Learning Approach to Large Graph Visualization,"  Using different methods for laying out a graph can lead to very different
visual appearances, with which the viewer perceives different information.
Selecting a ""good"" layout method is thus important for visualizing a graph. The
selection can be highly subjective and dependent on the given task. A common
approach to selecting a good layout is to use aesthetic criteria and visual
inspection. However, fully calculating various layouts and their associated
aesthetic metrics is computationally expensive. In this paper, we present a
machine learning approach to large graph visualization based on computing the
topological similarity of graphs using graph kernels. For a given graph, our
approach can show what the graph would look like in different layouts and
estimate their corresponding aesthetic metrics. An important contribution of
our work is the development of a new framework to design graph kernels. Our
experimental study shows that our estimation calculation is considerably faster
than computing the actual layouts and their aesthetic metrics. Also, our graph
kernels outperform the state-of-the-art ones in both time and accuracy. In
addition, we conducted a user study to demonstrate that the topological
similarity computed with our graph kernel matches perceptual similarity
assessed by human users.
",1,0,0,1,0,0
17211,Polarization of the Vaccination Debate on Facebook,"  Vaccine hesitancy has been recognized as a major global health threat. Having
access to any type of information in social media has been suggested as a
potential powerful influence factor to hesitancy. Recent studies in other
fields than vaccination show that access to a wide amount of content through
the Internet without intermediaries resolved into major segregation of the
users in polarized groups. Users select the information adhering to theirs
system of beliefs and tend to ignore dissenting information. In this paper we
assess whether there is polarization in Social Media use in the field of
vaccination. We perform a thorough quantitative analysis on Facebook analyzing
2.6M users interacting with 298.018 posts over a time span of seven years and 5
months. We used community detection algorithms to automatically detect the
emergent communities from the users activity and to quantify the cohesiveness
over time of the communities. Our findings show that content consumption about
vaccines is dominated by the echo-chamber effect and that polarization
increased over years. Communities emerge from the users consumption habits,
i.e. the majority of users only consumes information in favor or against
vaccines, not both. The existence of echo-chambers may explain why social-media
campaigns providing accurate information may have limited reach, may be
effective only in sub-groups and might even foment further polarization of
opinions. The introduction of dissenting information into a sub-group is
disregarded and can have a backfire effect, further reinforcing the existing
opinions within the sub-group.
",1,0,0,0,0,0
7322,Algebraic multiscale method for flow in heterogeneous porous media with embedded discrete fractures (F-AMS),"  This paper introduces an Algebraic MultiScale method for simulation of flow
in heterogeneous porous media with embedded discrete Fractures (F-AMS). First,
multiscale coarse grids are independently constructed for both porous matrix
and fracture networks. Then, a map between coarse- and fine-scale is obtained
by algebraically computing basis functions with local support. In order to
extend the localization assumption to the fractured media, four types of basis
functions are investigated: (1) Decoupled-AMS, in which the two media are
completely decoupled, (2) Frac-AMS and (3) Rock-AMS, which take into account
only one-way transmissibilities, and (4) Coupled-AMS, in which the matrix and
fracture interpolators are fully coupled. In order to ensure scalability, the
F-AMS framework permits full flexibility in terms of the resolution of the
fracture coarse grids. Numerical results are presented for two- and
three-dimensional heterogeneous test cases. During these experiments, the
performance of F-AMS, paired with ILU(0) as second-stage smoother in a
convergent iterative procedure, is studied by monitoring CPU times and
convergence rates. Finally, in order to investigate the scalability of the
method, an extensive benchmark study is conducted, where a commercial algebraic
multigrid solver is used as reference. The results show that, given an
appropriate coarsening strategy, F-AMS is insensitive to severe fracture and
matrix conductivity contrasts, as well as the length of the fracture networks.
Its unique feature is that a fine-scale mass conservative flux field can be
reconstructed after any iteration, providing efficient approximate solutions in
time-dependent simulations.
",1,1,0,0,0,0
2111,Concentration of $1$-Lipschitz functions on manifolds with boundary with Dirichlet boundary condition,"  In this paper, we consider a concentration of measure problem on Riemannian
manifolds with boundary. We study concentration phenomena of non-negative
$1$-Lipschitz functions with Dirichlet boundary condition around zero, which is
called boundary concentration phenomena. We first examine relation between
boundary concentration phenomena and large spectral gap phenomena of Dirichlet
eigenvalues of Laplacian. We will obtain analogue of the Gromov-V. D. Milman
theorem and the Funano-Shioya theorem for closed manifolds. Furthermore, to
capture boundary concentration phenomena, we introduce a new invariant called
the observable inscribed radius. We will formulate comparison theorems for such
invariant under a lower Ricci curvature bound, and a lower mean curvature bound
for the boundary. Based on such comparison theorems, we investigate various
boundary concentration phenomena of sequences of manifolds with boundary.
",0,0,1,0,0,0
4719,Training Deep AutoEncoders for Collaborative Filtering,"  This paper proposes a novel model for the rating prediction task in
recommender systems which significantly outperforms previous state-of-the art
models on a time-split Netflix data set. Our model is based on deep autoencoder
with 6 layers and is trained end-to-end without any layer-wise pre-training. We
empirically demonstrate that: a) deep autoencoder models generalize much better
than the shallow ones, b) non-linear activation functions with negative parts
are crucial for training deep models, and c) heavy use of regularization
techniques such as dropout is necessary to prevent over-fiting. We also propose
a new training algorithm based on iterative output re-feeding to overcome
natural sparseness of collaborate filtering. The new algorithm significantly
speeds up training and improves model performance. Our code is available at
this https URL
",1,0,0,1,0,0
16046,Prior Variances and Depth Un-Biased Estimators in EEG Focal Source Imaging,"  In electroencephalography (EEG) source imaging, the inverse source estimates
are depth biased in such a way that their maxima are often close to the
sensors. This depth bias can be quantified by inspecting the statistics (mean
and co-variance) of these estimates. In this paper, we find weighting factors
within a Bayesian framework for the used L1/L2 sparsity prior that the
resulting maximum a posterior (MAP) estimates do not favor any particular
source location. Due to the lack of an analytical expression for the MAP
estimate when this sparsity prior is used, we solve the weights indirectly.
First, we calculate the Gaussian prior variances that lead to depth un-biased
maximum a posterior (MAP) estimates. Subsequently, we approximate the
corresponding weight factors in the sparsity prior based on the solved Gaussian
prior variances. Finally, we reconstruct focal source configurations using the
sparsity prior with the proposed weights and two other commonly used choices of
weights that can be found in literature.
",0,1,0,0,0,0
9636,Comprehensive evaluation of statistical speech waveform synthesis,"  Statistical TTS systems that directly predict the speech waveform have
recently reported improvements in synthesis quality. This investigation
evaluates Amazon's statistical speech waveform synthesis (SSWS) system. An
in-depth evaluation of SSWS is conducted across a number of domains to better
understand the consistency in quality. The results of this evaluation are
validated by repeating the procedure on a separate group of testers. Finally,
an analysis of the nature of speech errors of SSWS compared to hybrid unit
selection synthesis is conducted to identify the strengths and weaknesses of
SSWS. Having a deeper insight into SSWS allows us to better define the focus of
future work to improve this new technology.
",1,0,0,0,0,0
2694,Stochastic Chemical Reaction Networks for Robustly Approximating Arbitrary Probability Distributions,"  We show that discrete distributions on the $d$-dimensional non-negative
integer lattice can be approximated arbitrarily well via the marginals of
stationary distributions for various classes of stochastic chemical reaction
networks. We begin by providing a class of detailed balanced networks and prove
that they can approximate any discrete distribution to any desired accuracy.
However, these detailed balanced constructions rely on the ability to
initialize a system precisely, and are therefore susceptible to perturbations
in the initial conditions. We therefore provide another construction based on
the ability to approximate point mass distributions and prove that this
construction is capable of approximating arbitrary discrete distributions for
any choice of initial condition. In particular, the developed models are
ergodic, so their limit distributions are robust to a finite number of
perturbations over time in the counts of molecules.
",0,0,0,0,1,0
18415,Estimators for a Class of Bivariate Measures of Concordance for Copulas,"  In the present paper we propose and study estimators for a wide class of
bivariate measures of concordance for copulas. These measures of concordance
are generated by a copula and generalize Spearman's rho and Gini's gamma. In
the case of Spearman's rho and Gini's gamma the estimators turn out to be the
usual sample versions of these measures of concordance.
",0,0,1,1,0,0
8494,Sensitivity analysis using perturbed-law based indices for quantiles and application to an industrial case,"  In this paper, we present perturbed law-based sensitivity indices and how to
adapt them for quantile-oriented sensitivity analysis. We exhibit a simple way
to compute these indices in practice using an importance sampling estimator for
quantiles. Some useful asymptotic results about this estimator are also
provided. Finally, we apply this method to the study of a numerical model which
simulates the behaviour of a component in a hydraulic system in case of severe
transient solicitations. The sensitivity analysis is used to assess the impact
of epistemic uncertainties about some physical parameters on the output of the
model.
",0,0,1,1,0,0
16304,Arbitrage-Free Interpolation in Models of Market Observable Interest Rates,"  Models which postulate lognormal dynamics for interest rates which are
compounded according to market conventions, such as forward LIBOR or forward
swap rates, can be constructed initially in a discrete tenor framework.
Interpolating interest rates between maturities in the discrete tenor structure
is equivalent to extending the model to continuous tenor. The present paper
sets forth an alternative way of performing this extension; one which preserves
the Markovian properties of the discrete tenor models and guarantees the
positivity of all interpolated rates.
",0,0,0,0,0,1
576,Discriminant circle bundles over local models of Strebel graphs and Boutroux curves,"  We study special circle bundles over two elementary moduli spaces of
meromorphic quadratic differentials with real periods denoted by $\mathcal
Q_0^{\mathbb R}(-7)$ and $\mathcal Q^{\mathbb R}_0([-3]^2)$. The space
$\mathcal Q_0^{\mathbb R}(-7)$ is the moduli space of meromorphic quadratic
differentials on the Riemann sphere with one pole of order 7 with real periods;
it appears naturally in the study of a neighbourhood of the Witten's cycle
$W_1$ in the combinatorial model based on Jenkins-Strebel quadratic
differentials of $\mathcal M_{g,n}$. The space $\mathcal Q^{\mathbb
R}_0([-3]^2)$ is the moduli space of meromorphic quadratic differentials on the
Riemann sphere with two poles of order at most 3 with real periods; it appears
in description of a neighbourhood of Kontsevich's boundary $W_{-1,-1}$ of the
combinatorial model. The application of the formalism of the Bergman
tau-function to the combinatorial model (with the goal of computing
analytically Poincare dual cycles to certain combinations of tautological
classes) requires the study of special sections of circle bundles over
$\mathcal Q_0^{\mathbb R}(-7)$ and $\mathcal Q^{\mathbb R}_0([-3]^2)$; in the
case of the space $\mathcal Q_0^{\mathbb R}(-7)$ a section of this circle
bundle is given by the argument of the modular discriminant. We study the
spaces $\mathcal Q_0^{\mathbb R}(-7)$ and $\mathcal Q^{\mathbb R}_0([-3]^2)$,
also called the spaces of Boutroux curves, in detail, together with
corresponding circle bundles.
",0,1,1,0,0,0
8491,Metropolis-Hastings Algorithms for Estimating Betweenness Centrality in Large Networks,"  Betweenness centrality is an important index widely used in different domains
such as social networks, traffic networks and the world wide web. However, even
for mid-size networks that have only a few hundreds thousands vertices, it is
computationally expensive to compute exact betweenness scores. Therefore in
recent years, several approximate algorithms have been developed. In this
paper, first given a network $G$ and a vertex $r \in V(G)$, we propose a
Metropolis-Hastings MCMC algorithm that samples from the space $V(G)$ and
estimates betweenness score of $r$. The stationary distribution of our MCMC
sampler is the optimal sampling proposed for betweenness centrality estimation.
We show that our MCMC sampler provides an $(\epsilon,\delta)$-approximation,
where the number of required samples depends on the position of $r$ in $G$ and
in many cases, it is a constant. Then, given a network $G$ and a set $R \subset
V(G)$, we present a Metropolis-Hastings MCMC sampler that samples from the
joint space $R$ and $V(G)$ and estimates relative betweenness scores of the
vertices in $R$. We show that for any pair $r_i, r_j \in R$, the ratio of the
expected values of the estimated relative betweenness scores of $r_i$ and $r_j$
respect to each other is equal to the ratio of their betweenness scores. We
also show that our joint-space MCMC sampler provides an
$(\epsilon,\delta)$-approximation of the relative betweenness score of $r_i$
respect to $r_j$, where the number of required samples depends on the position
of $r_j$ in $G$ and in many cases, it is a constant.
",1,0,0,0,0,0
18566,Toward Finding Latent Cities with Non-Negative Matrix Factorization,"  In the last decade, digital footprints have been used to cluster population
activity into functional areas of cities.
However, a key aspect has been overlooked: we experience our cities not only
by performing activities at specific destinations, but also by moving from one
place to another.
In this paper, we propose to analyze and cluster the city based on how people
move through it. Particularly, we introduce Mobilicities, automatically
generated travel patterns inferred from mobile phone network data using NMF, a
matrix factorization model.
We evaluate our method in a large city and we find that mobilicities reveal
latent but at the same time interpretable mobility structures of the city. Our
results provide evidence on how clustering and visualization of aggregated
phone logs could be used in planning systems to interactively analyze city
structure and population activity.
",1,0,0,0,0,0
8502,Distributed model predictive control for continuous-time nonlinear systems based on suboptimal ADMM,"  The paper presents a distributed model predictive control (DMPC) scheme for
continuous-time nonlinear systems based on the alternating direction method of
multipliers (ADMM). A stopping criterion in the ADMM algorithm limits the
iterations and therefore the required communication effort during the
distributed MPC solution at the expense of a suboptimal solution. Stability
results are presented for the suboptimal DMPC scheme under two different ADMM
convergence assumptions. In particular, it is shown that the required
iterations in each ADMM step are bounded, which is also confirmed in simulation
studies.
",0,0,1,0,0,0
16518,The Carnegie-Chicago Hubble Program: Discovery of the Most Distant Ultra-faint Dwarf Galaxy in the Local Universe,"  Ultra-faint dwarf galaxies (UFDs) are the faintest known galaxies and due to
their incredibly low surface brightness, it is difficult to find them beyond
the Local Group. We report a serendipitous discovery of an UFD, Fornax UFD1, in
the outskirts of NGC 1316, a giant galaxy in the Fornax cluster. The new galaxy
is located at a projected radius of 55 kpc in the south-east of NGC 1316. This
UFD is found as a small group of resolved stars in the Hubble Space Telescope
images of a halo field of NGC 1316, obtained as part of the Carnegie-Chicago
Hubble Program. Resolved stars in this galaxy are consistent with being mostly
metal-poor red giant branch (RGB) stars. Applying the tip of the RGB method to
the mean magnitude of the two brightest RGB stars, we estimate the distance to
this galaxy, 19.0 +- 1.3 Mpc. Fornax UFD1 is probably a member of the Fornax
cluster. The color-magnitude diagram of these stars is matched by a 12 Gyr
isochrone with low metallicity ([Fe/H] ~ -2.4). Total magnitude and effective
radius of Fornax UFD1 are Mv ~ -7.6 +- 0.2 mag and r_eff = 146 +- 9 pc, which
are similar to those of Virgo UFD1 that was discovered recently in the
intracluster field of Virgo by Jang & Lee (2014).Fornax UFD1 is the most
distant known UFD that is confirmed by resolved stars. This indicates that UFDs
are ubiquitous and that more UFDs remain to be discovered in the Fornax
cluster.
",0,1,0,0,0,0
19974,Aperture synthesis imaging of the carbon AGB star R Sculptoris: Detection of a complex structure and a dominating spot on the stellar disk,"  We present near-infrared interferometry of the carbon-rich asymptotic giant
branch (AGB) star R Sculptoris.
The visibility data indicate a broadly circular resolved stellar disk with a
complex substructure. The observed AMBER squared visibility values show drops
at the positions of CO and CN bands, indicating that these lines form in
extended layers above the photosphere. The AMBER visibility values are best fit
by a model without a wind. The PIONIER data are consistent with the same model.
We obtain a Rosseland angular diameter of 8.9+-0.3 mas, corresponding to a
Rosseland radius of 355+-55 Rsun, an effective temperature of 2640+-80 K, and a
luminosity of log L/Lsun=3.74+-0.18. These parameters match evolutionary tracks
of initial mass 1.5+-0.5 Msun and current mass 1.3+-0.7 Msun. The reconstructed
PIONIER images exhibit a complex structure within the stellar disk including a
dominant bright spot located at the western part of the stellar disk. The spot
has an H-band peak intensity of 40% to 60% above the average intensity of the
limb-darkening-corrected stellar disk. The contrast between the minimum and
maximum intensity on the stellar disk is about 1:2.5.
Our observations are broadly consistent with predictions by dynamic
atmosphere and wind models, although models with wind appear to have a
circumstellar envelope that is too extended compared to our observations. The
detected complex structure within the stellar disk is most likely caused by
giant convection cells, resulting in large-scale shock fronts, and their
effects on clumpy molecule and dust formation seen against the photosphere at
distances of 2-3 stellar radii.
",0,1,0,0,0,0
12120,On a generalization of Lie($k$): a CataLAnKe theorem,"  We define a generalization of the free Lie algebra based on an $n$-ary
commutator and call it the free LAnKe. We show that the action of the symmetric
group $S_{2n-1}$ on the multilinear component with $2n-1$ generators is given
by the representation $S^{2^{n-1}1}$, whose dimension is the $n$th Catalan
number. An application involving Specht modules of staircase shape is
presented. We also introduce a conjecture that extends the relation between the
Whitehouse representation and Lie($k$).
",0,0,1,0,0,0
8497,Giant Planets Can Act As Stabilizing Agents on Debris Disks,"  We have explored the evolution of a cold debris disk under the gravitational
influence of dwarf planet sized objects (DPs), both in the presence and absence
of an interior giant planet. Through detailed long-term numerical simulations,
we demonstrate that, when the giant planet is not present, DPs can stir the
eccentricities and inclinations of disk particles, in linear proportion to the
total mass of the DPs; on the other hand, when the giant planet is included in
the simulations, the stirring is approximately proportional to the mass
squared. This creates two regimes: below a disk mass threshold (defined by the
total mass of DPs), the giant planet acts as a stabilizing agent of the orbits
of cometary nucleii, diminishing the effect of the scatterers; above the
threshold, the giant contributes to the dispersion of the particles.
",0,1,0,0,0,0
5724,"Optimal Output Regulation for Square, Over-Actuated and Under-Actuated Linear Systems","  This paper considers two different problems in trajectory tracking control
for linear systems. First, if the control is not unique which is most input
energy efficient. Second, if exact tracking is infeasible which control
performs most accurately. These are typical challenges for over-actuated
systems and for under-actuated systems, respectively. We formulate both goals
as optimal output regulation problems. Then we contribute two new sets of
regulator equations to output regulation theory that provide the desired
solutions. A thorough study indicates solvability and uniqueness under weak
assumptions. E.g., we can always determine the solution of the classical
regulator equations that is most input energy efficient. This is of great value
if there are infinitely many solutions. We derive our results by a linear
quadratic tracking approach and establish a useful link to output regulation
theory.
",1,0,0,0,0,0
6145,Driver Action Prediction Using Deep (Bidirectional) Recurrent Neural Network,"  Advanced driver assistance systems (ADAS) can be significantly improved with
effective driver action prediction (DAP). Predicting driver actions early and
accurately can help mitigate the effects of potentially unsafe driving
behaviors and avoid possible accidents. In this paper, we formulate driver
action prediction as a timeseries anomaly prediction problem. While the anomaly
(driver actions of interest) detection might be trivial in this context,
finding patterns that consistently precede an anomaly requires searching for or
extracting features across multi-modal sensory inputs. We present such a driver
action prediction system, including a real-time data acquisition, processing
and learning framework for predicting future or impending driver action. The
proposed system incorporates camera-based knowledge of the driving environment
and the driver themselves, in addition to traditional vehicle dynamics. It then
uses a deep bidirectional recurrent neural network (DBRNN) to learn the
correlation between sensory inputs and impending driver behavior achieving
accurate and high horizon action prediction. The proposed system performs
better than other existing systems on driver action prediction tasks and can
accurately predict key driver actions including acceleration, braking, lane
change and turning at durations of 5sec before the action is executed by the
driver.
",1,0,0,1,0,0
8174,Existence of global weak solutions to the kinetic Peterlin model,"  We consider a class of kinetic models for polymeric fluids motivated by the
Peterlin dumbbell theories for dilute polymer solutions with a nonlinear spring
law for an infinitely extensible spring. The polymer molecules are suspended in
an incompressible viscous Newtonian fluid confined to a bounded domain in two
or three space dimensions. The unsteady motion of the solvent is described by
the incompressible Navier-Stokes equations with the elastic extra stress tensor
appearing as a forcing term in the momentum equation. The elastic stress tensor
is defined by the Kramers expression through the probability density function
that satisfies the corresponding Fokker-Planck equation. In this case, a
coefficient depending on the average length of polymer molecules appears in the
latter equation. Following the recent work of Barrett and Süli we prove the
existence of global-in-time weak solutions to the kinetic Peterlin model in two
space dimensions.
",0,0,1,0,0,0
10012,Nonlinear stability for the Maxwell--Born--Infeld system on a Schwarzschild background,"  In this paper we prove small data global existence for solutions to the
Maxwell--Born--Infeld (MBI) system on a fixed Schwarzschild background. This
system has appeared in the context of string theory and can be seen as a
nonlinear model problem for the stability of the background metric itself, due
to its tensorial and quasilinear nature. The MBI system models nonlinear
electromagnetism and does not display birefringence. The key element in our
proof lies in the observation that there exists a first-order differential
transformation which brings solutions of the spin $\pm 1$ Teukolsky equations,
satisfied by the extreme components of the field, into solutions of a ""good""
equation (the Fackerell--Ipser Equation). This strategy was established in [F.
Pasqualotto, The spin $\pm 1$ Teukolsky equations and the Maxwell system on
Schwarzschild, Preprint 2016, arXiv:1612.07244] for the linear Maxwell field on
Schwarzschild. We show that analogous Fackerell--Ipser equations hold for the
MBI system on a fixed Schwarzschild background, which are however nonlinearly
coupled. To essentially decouple these right hand sides, we setup a bootstrap
argument. We use the $r^p$ method of Dafermos and Rodnianski in [M. Dafermos
and I. Rodnianski, A new physical-space approach to decay for the wave equation
with applications to black hole spacetimes, in XVIth International Congress on
Mathematical Physics, Pavel Exner ed., Prague 2009 pp. 421-433, 2009,
arXiv:0910.4957] in order to deduce decay of some null components, and we infer
decay for the remaining quantities by integrating the MBI system as transport
equations.
",0,0,1,0,0,0
11473,Efficient Bayesian inference for multivariate factor stochastic volatility models with leverage,"  This paper discusses the efficient Bayesian estimation of a multivariate
factor stochastic volatility (Factor MSV) model with leverage. We propose a
novel approach to construct the sampling schemes that converges to the
posterior distribution of the latent volatilities and the parameters of
interest of the Factor MSV model based on recent advances in Particle Markov
chain Monte Carlo (PMCMC). As opposed to the approach of Chib et al. (2006} and
Omori et al. (2007}, our approach does not require approximating the joint
distribution of outcome and volatility innovations by a mixture of bivariate
normal distributions. To sample the free elements of the loading matrix we
employ the interweaving method used in Kastner et al. (2017} in the Particle
Metropolis within Gibbs (PMwG) step. The proposed method is illustrated
empirically using a simulated dataset and a sample of daily US stock returns.
",0,0,0,1,0,0
6350,Guessing Attacks on Distributed-Storage Systems,"  The secrecy of a distributed-storage system for passwords is studied. The
encoder, Alice, observes a length-n password and describes it using two hints,
which she stores in different locations. The legitimate receiver, Bob, observes
both hints. In one scenario the requirement is that the expected number of
guesses it takes Bob to guess the password approach one as n tends to infinity,
and in the other that the expected size of the shortest list that Bob must form
to guarantee that it contain the password approach one. The eavesdropper, Eve,
sees only one of the hints. Assuming that Alice cannot control which hints Eve
observes, the largest normalized (by n) exponent that can be guaranteed for the
expected number of guesses it takes Eve to guess the password is characterized
for each scenario. Key to the proof are new results on Arikan's guessing and
Bunte and Lapidoth's task-encoding problem; in particular, the paper
establishes a close relation between the two problems. A rate-distortion
version of the model is also discussed, as is a generalization that allows for
Alice to produce {\delta} (not necessarily two) hints, for Bob to observe {\nu}
(not necessarily two) of the hints, and for Eve to observe {\eta} (not
necessarily one) of the hints. The generalized model is robust against {\delta}
- {\nu} disk failures.
",1,0,1,0,0,0
1272,Characterizing the impact of model error in hydrogeologic time series recovery inverse problems,"  Hydrogeologic models are commonly over-smoothed relative to reality, owing to
the difficulty of obtaining accurate high-resolution information about the
subsurface. When used in an inversion context, such models may introduce
systematic biases which cannot be encapsulated by an unbiased ""observation
noise"" term of the type assumed by standard regularization theory and typical
Bayesian formulations. Despite its importance, model error is difficult to
encapsulate systematically and is often neglected. Here, model error is
considered for a hydrogeologically important class of inverse problems that
includes interpretation of hydraulic transients and contaminant source history
inference: reconstruction of a time series that has been convolved against a
transfer function (i.e., impulse response) that is only approximately known.
Using established harmonic theory along with two results established here
regarding triangular Toeplitz matrices, upper and lower error bounds are
derived for the effect of systematic model error on time series recovery for
both well-determined and over-determined inverse problems. A Monte Carlo study
of a realistic hydraulic reconstruction problem is presented, and the lower
error bound is seen informative about expected behavior. A possible diagnostic
criterion for blind transfer function characterization is also uncovered.
",0,0,1,0,0,0
20656,Selective inference for effect modification via the lasso,"  Effect modification occurs when the effect of the treatment on an outcome
varies according to the level of other covariates and often has important
implications in decision making. When there are tens or hundreds of covariates,
it becomes necessary to use the observed data to select a simpler model for
effect modification and then make valid statistical inference. We propose a two
stage procedure to solve this problem. First, we use Robinson's transformation
to decouple the nuisance parameters from the treatment effect of interest and
use machine learning algorithms to estimate the nuisance parameters. Next,
after plugging in the estimates of the nuisance parameters, we use the Lasso to
choose a low-complexity model for effect modification. Compared to a full model
consisting of all the covariates, the selected model is much more
interpretable. Compared to the univariate subgroup analyses, the selected model
greatly reduces the number of false discoveries. We show that the conditional
selective inference for the selected model is asymptotically valid given the
rate assumptions in classical semiparametric regression. Extensive simulation
studies are conducted to verify the asymptotic results and an epidemiological
application is used to demonstrate the method.
",0,0,1,1,0,0
5668,Variance-Reduced Stochastic Learning under Random Reshuffling,"  Several useful variance-reduced stochastic gradient algorithms, such as SVRG,
SAGA, Finito, and SAG, have been proposed to minimize empirical risks with
linear convergence properties to the exact minimizer. The existing convergence
results assume uniform data sampling with replacement. However, it has been
observed in related works that random reshuffling can deliver superior
performance over uniform sampling and, yet, no formal proofs or guarantees of
exact convergence exist for variance-reduced algorithms under random
reshuffling. This paper makes two contributions. First, it resolves this open
issue and provides the first theoretical guarantee of linear convergence under
random reshuffling for SAGA; the argument is also adaptable to other
variance-reduced algorithms. Second, under random reshuffling, the paper
proposes a new amortized variance-reduced gradient (AVRG) algorithm with
constant storage requirements compared to SAGA and with balanced gradient
computations compared to SVRG. AVRG is also shown analytically to converge
linearly.
",1,0,0,1,0,0
7661,"Converting of algebraic Diophantine equations to a diagonal form with the help of an integer non-orthogonal transformation, maintaining the asymptotic behavior of the number of its integer solutions","  The author showed that any homogeneous algebraic Diophantine equation of the
second order can be converted to a diagonal form using an integer
non-orthogonal transformation maintaining asymptotic behavior of the number of
its integer solutions. In this paper, we consider the transformation to the
diagonal form of a wider class of algebraic second-order Diophantine equations,
and also we consider the conditions for converting higher order algebraic
Diophantine equations to this form. The author found an asymptotic estimate for
the number of integer solutions of the diagonal Thue equation of odd degree
with an amount of variables greater than two, and also he got and asymptotic
estimates of the number of integer solutions of other types of diagonal
algebraic Diophantine equations.
",0,0,1,0,0,0
8841,Clarifying Trust in Social Internet of Things,"  A social approach can be exploited for the Internet of Things (IoT) to manage
a large number of connected objects. These objects operate as autonomous agents
to request and provide information and services to users. Establishing
trustworthy relationships among the objects greatly improves the effectiveness
of node interaction in the social IoT and helps nodes overcome perceptions of
uncertainty and risk. However, there are limitations in the existing trust
models. In this paper, a comprehensive model of trust is proposed that is
tailored to the social IoT. The model includes ingredients such as trustor,
trustee, goal, trustworthiness evaluation, decision, action, result, and
context. Building on this trust model, we clarify the concept of trust in the
social IoT in five aspects such as (1) mutuality of trustor and trustee, (2)
inferential transfer of trust, (3) transitivity of trust, (4) trustworthiness
update, and (5) trustworthiness affected by dynamic environment. With network
connectivities that are from real-world social networks, a series of
simulations are conducted to evaluate the performance of the social IoT
operated with the proposed trust model. An experimental IoT network is used to
further validate the proposed trust model.
",1,0,0,0,0,0
6955,Non-Gaussian Stochastic Volatility Model with Jumps via Gibbs Sampler,"  In this work, we propose a model for estimating volatility from financial
time series, extending the non-Gaussian family of space-state models with exact
marginal likelihood proposed by Gamerman, Santos and Franco (2013). On the
literature there are models focused on estimating financial assets risk,
however, most of them rely on MCMC methods based on Metropolis algorithms,
since full conditional posterior distributions are not known. We present an
alternative model capable of estimating the volatility, in an automatic way,
since all full conditional posterior distributions are known, and it is
possible to obtain an exact sample of parameters via Gibbs Sampler. The
incorporation of jumps in returns allows the model to capture speculative
movements of the data, so that their influence does not propagate to
volatility. We evaluate the performance of the algorithm using synthetic and
real data time series.
Keywords: Financial time series, Stochastic volatility, Gibbs Sampler,
Dynamic linear models.
",0,0,0,0,0,1
17964,High-order schemes for the Euler equations in cylindrical/spherical coordinates,"  We consider implementations of high-order finite difference Weighted
Essentially Non-Oscillatory (WENO) schemes for the Euler equations in
cylindrical and spherical coordinate systems with radial dependence only. The
main concern of this work lies in ensuring both high-order accuracy and
conservation. Three different spatial discretizations are assessed: one that is
shown to be high-order accurate but not conservative, one conservative but not
high-order accurate, and a new approach that is both high-order accurate and
conservative. For cylindrical and spherical coordinates, we present convergence
results for the advection equation and the Euler equations with an acoustics
problem; we then use the Sod shock tube and the Sedov point-blast problems in
cylindrical coordinates to verify our analysis and implementations.
",0,1,1,0,0,0
16115,Using Optimal Ratio Mask as Training Target for Supervised Speech Separation,"  Supervised speech separation uses supervised learning algorithms to learn a
mapping from an input noisy signal to an output target. With the fast
development of deep learning, supervised separation has become the most
important direction in speech separation area in recent years. For the
supervised algorithm, training target has a significant impact on the
performance. Ideal ratio mask is a commonly used training target, which can
improve the speech intelligibility and quality of the separated speech.
However, it does not take into account the correlation between noise and clean
speech. In this paper, we use the optimal ratio mask as the training target of
the deep neural network (DNN) for speech separation. The experiments are
carried out under various noise environments and signal to noise ratio (SNR)
conditions. The results show that the optimal ratio mask outperforms other
training targets in general.
",1,0,0,0,0,0
7303,A Highly Efficient Polarization-Independent Metamaterial-Based RF Energy-Harvesting Rectenna for Low-Power Applications,"  A highly-efficient multi-resonant RF energy-harvesting rectenna based on a
metamaterial perfect absorber featuring closely-spaced polarization-independent
absorption modes is presented. Its effective area is larger than its physical
area, and so efficiencies of 230% and 130% are measured at power densities of
10 uW/cm2 and 1 uW/cm2 respectively, for a linear absorption mode at 0.75 GHz.
The rectenna exhibits a broad polarization-independent region between 1.4 GHz
and 1.7 GHz with maximum efficiencies of 167% and 36% for those same power
densities. Additionally, by adjustment of the distance between the rectenna and
a reflecting ground plane, the absorption frequency can be adjusted to a
limited extent within the polarization-independent region. Lastly, the rectenna
should be capable of delivering 100 uW of power to a device located within 50 m
of a cell-phone tower under ideal conditions.
",0,1,0,0,0,0
18919,"Nonlocal Neural Networks, Nonlocal Diffusion and Nonlocal Modeling","  Nonlocal neural networks have been proposed and shown to be effective in
several computer vision tasks, where the nonlocal operations can directly
capture long-range dependencies in the feature space. In this paper, we study
the nature of diffusion and damping effect of nonlocal networks by doing
spectrum analysis on the weight matrices of the well-trained networks, and then
propose a new formulation of the nonlocal block. The new block not only learns
the nonlocal interactions but also has stable dynamics, thus allowing deeper
nonlocal structures. Moreover, we interpret our formulation from the general
nonlocal modeling perspective, where we make connections between the proposed
nonlocal network and other nonlocal models, such as nonlocal diffusion process
and Markov jump process.
",0,0,0,1,0,0
20165,Competition between Chaotic and Non-Chaotic Phases in a Quadratically Coupled Sachdev-Ye-Kitaev Model,"  The Sachdev-Ye-Kitaev (SYK) model is a concrete solvable model to study
non-Fermi liquid properties, holographic duality and maximally chaotic
behavior. In this work, we consider a generalization of the SYK model that
contains two SYK models with different number of Majorana modes coupled by
quadratic terms. This model is also solvable, and the solution shows a
zero-temperature quantum phase transition between two non-Fermi liquid chaotic
phases. This phase transition is driven by tuning the ratio of two mode
numbers, and a Fermi liquid non-chaotic phase sits at the critical point with
equal mode number. At finite temperature, the Fermi liquid phase expands to a
finite regime. More intriguingly, a different non-Fermi liquid phase emerges at
finite temperature. We characterize the phase diagram in term of the spectral
function, the Lyapunov exponent and the entropy. Our results illustrate a
concrete example of quantum phase transition and critical regime between two
non-Fermi liquid phases.
",0,1,0,0,0,0
12375,High-performance nanoscale topological energy transduction,"  The realization of high-performance, small-footprint, on-chip inductors
remains a challenge in radio-frequency and power microelectronics, where they
perform vital energy transduction in filters and power converters. Modern
planar inductors consist of metallic spirals that consume significant chip
area, resulting in low inductance densities. We present a novel method for
magnetic energy transduction that utilizes ferromagnetic islands (FIs) on the
surface of a 3D time-reversal-invariant topological insulator (TI) to produce
paradigmatically different inductors. Depending on the chemical potential, the
FIs induce either an anomalous or quantum anomalous Hall effect in the
topological surface states. These Hall effects direct current around the FIs,
concentrating magnetic flux and producing a highly inductive device. Using a
novel self-consistent simulation that couples AC non-equilibrium Green
functions to fully electrodynamic solutions of Maxwell's equations, we
demonstrate excellent inductance densities up to terahertz frequencies, thus
harnessing the unique properties of topological materials for practical device
applications.
",0,1,0,0,0,0
18150,Soft Pneumatic Gelatin Actuator for Edible Robotics,"  We present a fully edible pneumatic actuator based on gelatin-glycerol
composite. The actuator is monolithic, fabricated via a molding process, and
measures 90 mm in length, 20 mm in width, and 17 mm in thickness. Thanks to the
composite mechanical characteristics similar to those of silicone elastomers,
the actuator exhibits a bending angle of 170.3 ° and a blocked force of
0.34 N at the applied pressure of 25 kPa. These values are comparable to
elastomer based pneumatic actuators. As a validation example, two actuators are
integrated to form a gripper capable of handling various objects, highlighting
the high performance and applicability of the edible actuator. These edible
actuators, combined with other recent edible materials and electronics, could
lay the foundation for a new type of edible robots.
",1,0,0,0,0,0
11963,Deep laser cooling in optical trap: two-level quantum model,"  We study laser cooling of $^{24}$Mg atoms in dipole optical trap with pumping
field resonant to narrow $(3s3s)\,^1S_0 \rightarrow \, (3s3p)\,^{3}P_1$
($\lambda = 457$ nm) optical transition. For description of laser cooling of
atoms in the optical trap with taking into account quantum recoil effects we
consider two quantum models. The first one is based on direct numerical
solution of quantum kinetic equation for atom density matrix and the second one
is simplified model based on decomposition of atom density matrix over
vibration states in the dipole trap. We search pumping field intensity and
detuning for minimum cooling energy and fast laser cooling.
",0,1,0,0,0,0
6114,On Reduced Input-Output Dynamic Mode Decomposition,"  The identification of reduced-order models from high-dimensional data is a
challenging task, and even more so if the identified system should not only be
suitable for a certain data set, but generally approximate the input-output
behavior of the data source. In this work, we consider the input-output dynamic
mode decomposition method for system identification. We compare excitation
approaches for the data-driven identification process and describe an
optimization-based stabilization strategy for the identified systems.
",1,0,0,0,0,0
6254,Rescaling and other forms of unsupervised preprocessing introduce bias into cross-validation,"  Cross-validation of predictive models is the de-facto standard for model
selection and evaluation. In proper use, it provides an unbiased estimate of a
model's predictive performance. However, data sets often undergo a preliminary
data-dependent transformation, such as feature rescaling or dimensionality
reduction, prior to cross-validation. It is widely believed that such a
preprocessing stage, if done in an unsupervised manner that does not consider
the class labels or response values, has no effect on the validity of
cross-validation. In this paper, we show that this belief is not true.
Preliminary preprocessing can introduce either a positive or negative bias into
the estimates of model performance. Thus, it may lead to sub-optimal choices of
model parameters and invalid inference. In light of this, the scientific
community should re-examine the use of preliminary preprocessing prior to
cross-validation across the various application domains. By default, all data
transformations, including unsupervised preprocessing stages, should be learned
only from the training samples, and then merely applied to the validation and
testing samples.
",1,0,0,1,0,0
20765,An explicit analysis of the entropic penalty in linear programming,"  Solving linear programs by using entropic penalization has recently attracted
new interest in the optimization community, since this strategy forms the basis
for the fastest-known algorithms for the optimal transport problem, with many
applications in modern large-scale machine learning. Crucial to these
applications has been an analysis of how quickly solutions to the penalized
program approach true optima to the original linear program. More than 20 years
ago, Cominetti and San Martín showed that this convergence is exponentially
fast; however, their proof is asymptotic and does not give any indication of
how accurately the entropic program approximates the original program for any
particular choice of the penalization parameter. We close this long-standing
gap in the literature regarding entropic penalization by giving a new proof of
the exponential convergence, valid for any linear program. Our proof is
non-asymptotic, yields explicit constants, and has the virtue of being
extremely simple. We provide matching lower bounds and show that the entropic
approach does not lead to a near-linear time approximation scheme for the
linear assignment problem.
",0,0,0,1,0,0
6066,Compact Cardinals and Eight Values in Cichoń's Diagram,"  Assuming three strongly compact cardinals, it is consistent that \[ \aleph_1
< \mathrm{add}(\mathrm{null}) < \mathrm{cov}(\mathrm{null}) < \mathfrak{b} <
\mathfrak{d} < \mathrm{non}(\mathrm{null}) < \mathrm{cof}(\mathrm{null}) <
2^{\aleph_0}.\] Under the same assumption, it is consistent that \[ \aleph_1 <
\mathrm{add}(\mathrm{null}) < \mathrm{cov}(\mathrm{null}) <
\mathrm{non}(\mathrm{meager}) < \mathrm{cov}(\mathrm{meager}) <
\mathrm{non}(\mathrm{null}) < \mathrm{cof}(\mathrm{null}) < 2^{\aleph_0}.\]
",0,0,1,0,0,0
16836,Geometric mean of probability measures and geodesics of Fisher information metric,"  The space of all probability measures having positive density function on a
connected compact smooth manifold $M$, denoted by $\mathcal{P}(M)$, carries the
Fisher information metric $G$. We define the geometric mean of probability
measures by the aid of which we investigate information geometry of
$\mathcal{P}(M)$, equipped with $G$. We show that a geodesic segment joining
arbitrary probability measures $\mu_1$ and $\mu_2$ is expressed by using the
normalized geometric mean of its endpoints. As an application, we show that any
two points of $\mathcal{P}(M)$ can be joined by a geodesic. Moreover, we prove
that the function $\ell$ defined by $\ell(\mu_1, \mu_2):=2\arccos\int_M
\sqrt{p_1\,p_2}\,d\lambda$, $\mu_i=p_i\,\lambda$, $i=1,2$ gives the distance
function on $\mathcal{P}(M)$. It is shown that geodesics are all minimal.
",0,0,1,0,0,0
5030,Coherence and its Role in Excitation Energy Transfer in Fenna-Mathews-Olson Complex,"  We show that the coherence between different bacteriochlorophyll-a (BChla)
sites in the Fenna-Mathews-Olson complex is an essential ingredient for
excitation energy transfer between various sites. The coherence delocalizes the
excitation energy, which results in the redistribution of excitation among all
the BChla sites in the steady state. We further show that the system remains
partially coherent at the steady state. In our numerical simulation of the
non-Markovian density matrix equation, we consider both the inhomogeneity of
the protein environment and the effect of active vibronic modes.
",0,1,0,0,0,0
2749,Bit Fusion: Bit-Level Dynamically Composable Architecture for Accelerating Deep Neural Networks,"  Fully realizing the potential of acceleration for Deep Neural Networks (DNNs)
requires understanding and leveraging algorithmic properties. This paper builds
upon the algorithmic insight that bitwidth of operations in DNNs can be reduced
without compromising their classification accuracy. However, to prevent
accuracy loss, the bitwidth varies significantly across DNNs and it may even be
adjusted for each layer. Thus, a fixed-bitwidth accelerator would either offer
limited benefits to accommodate the worst-case bitwidth requirements, or lead
to a degradation in final accuracy. To alleviate these deficiencies, this work
introduces dynamic bit-level fusion/decomposition as a new dimension in the
design of DNN accelerators. We explore this dimension by designing Bit Fusion,
a bit-flexible accelerator, that constitutes an array of bit-level processing
elements that dynamically fuse to match the bitwidth of individual DNN layers.
This flexibility in the architecture enables minimizing the computation and the
communication at the finest granularity possible with no loss in accuracy. We
evaluate the benefits of BitFusion using eight real-world feed-forward and
recurrent DNNs. The proposed microarchitecture is implemented in Verilog and
synthesized in 45 nm technology. Using the synthesis results and cycle accurate
simulation, we compare the benefits of Bit Fusion to two state-of-the-art DNN
accelerators, Eyeriss and Stripes. In the same area, frequency, and process
technology, BitFusion offers 3.9x speedup and 5.1x energy savings over Eyeriss.
Compared to Stripes, BitFusion provides 2.6x speedup and 3.9x energy reduction
at 45 nm node when BitFusion area and frequency are set to those of Stripes.
Scaling to GPU technology node of 16 nm, BitFusion almost matches the
performance of a 250-Watt Titan Xp, which uses 8-bit vector instructions, while
BitFusion merely consumes 895 milliwatts of power.
",1,0,0,0,0,0
10367,A Relaxed Kačanov Iteration for the $p$-Poisson Problem,"  In this paper, we introduce an iterative linearization scheme that allows to
approximate the weak solution of the $p$-Poisson problem
\begin{align*}
-\operatorname{div}(|\nabla u|^{p-2}\nabla u) &= f\quad\text{in }\Omega,
u&= 0\quad\text{on}\partial\Omega
\end{align*} for $1 < p \leq 2$. The algorithm can be interpreted as a
relaxed Kačanov iteration. We prove that the algorithm converges at least
with an algebraic rate.
",0,0,1,0,0,0
14544,Ensemble Methods for Personalized E-Commerce Search Challenge at CIKM Cup 2016,"  Personalized search has been a hot research topic for many years and has been
widely used in e-commerce. This paper describes our solution to tackle the
challenge of personalized e-commerce search at CIKM Cup 2016. The goal of this
competition is to predict search relevance and re-rank the result items in SERP
according to the personalized search, browsing and purchasing preferences.
Based on a detailed analysis of the provided data, we extract three different
types of features, i.e., statistic features, query-item features and session
features. Different models are used on these features, including logistic
regression, gradient boosted decision trees, rank svm and a novel deep match
model. With the blending of multiple models, a stacking ensemble model is built
to integrate the output of individual models and produce a more accurate
prediction result. Based on these efforts, our solution won the champion of the
competition on all the evaluation metrics.
",1,0,0,0,0,0
4985,Strong deformations of DNA: Effect on the persistence length,"  Extreme deformations of the DNA double helix attracted a lot of attention
during the past decades. Particularly, the determination of the persistence
length of DNA with extreme local disruptions, or kinks, has become a crucial
problem in the studies of many important biological processes. In this paper we
review an approach to calculate the persistence length of the double helix by
taking into account the formation of kinks of arbitrary configuration. The
reviewed approach improves the Kratky--Porod model to determine the type and
nature of kinks that occur in the double helix, by measuring a reduction of the
persistence length of the kinkable DNA.
",0,0,0,0,1,0
11716,A Framework for Accurate Drought Forecasting System Using Semantics-Based Data Integration Middleware,"  Technological advancement in Wireless Sensor Networks (WSN) has made it
become an invaluable component of a reliable environmental monitoring system;
they form the digital skin' through which to 'sense' and collect the context of
the surroundings and provides information on the process leading to complex
events such as drought. However, these environmental properties are measured by
various heterogeneous sensors of different modalities in distributed locations
making up the WSN, using different abstruse terms and vocabulary in most cases
to denote the same observed property, causing data heterogeneity. Adding
semantics and understanding the relationships that exist between the observed
properties, and augmenting it with local indigenous knowledge is necessary for
an accurate drought forecasting system. In this paper, we propose the framework
for the semantic representation of sensor data and integration with indigenous
knowledge on drought using a middleware for an efficient drought forecasting
system.
",1,0,0,0,0,0
4213,Coherent structures and spectral energy transfer in turbulent plasma: a space-filter approach,"  Plasma turbulence at scales of the order of the ion inertial length is
mediated by several mechanisms, including linear wave damping, magnetic
reconnection, formation and dissipation of thin current sheets, stochastic
heating. It is now understood that the presence of localized coherent
structures enhances the dissipation channels and the kinetic features of the
plasma. However, no formal way of quantifying the relationship between
scale-to-scale energy transfer and the presence of spatial structures has so
far been presented. In this letter we quantify such relationship analyzing the
results of a two-dimensional high-resolution Hall-MHD simulation. In
particular, we employ the technique of space-filtering to derive a spectral
energy flux term which defines, in any point of the computational domain, the
signed flux of spectral energy across a given wavenumber. The characterization
of coherent structures is performed by means of a traditional two-dimensional
wavelet transformation. By studying the correlation between the spectral energy
flux and the wavelet amplitude, we demonstrate the strong relationship between
scale-to-scale transfer and coherent structures. Furthermore, by conditioning
one quantity with respect to the other, we are able for the first time to
quantify the inhomogeneity of the turbulence cascade induced by topological
structures in the magnetic field. Taking into account the low filling-factor of
coherent structures (i.e. they cover a small portion of space), it emerges that
80% of the spectral energy transfer (both in the direct and inverse cascade
directions) is localized in about 50% of space, and 50% of the energy transfer
is localized in only 25% of space.
",0,1,0,0,0,0
4396,Experimental Evaluation of Book Drawing Algorithms,"  A $k$-page book drawing of a graph $G=(V,E)$ consists of a linear ordering of
its vertices along a spine and an assignment of each edge to one of the $k$
pages, which are half-planes bounded by the spine. In a book drawing, two edges
cross if and only if they are assigned to the same page and their vertices
alternate along the spine. Crossing minimization in a $k$-page book drawing is
NP-hard, yet book drawings have multiple applications in visualization and
beyond. Therefore several heuristic book drawing algorithms exist, but there is
no broader comparative study on their relative performance. In this paper, we
propose a comprehensive benchmark set of challenging graph classes for book
drawing algorithms and provide an extensive experimental study of the
performance of existing book drawing algorithms.
",1,0,0,0,0,0
14156,Projected Primal-Dual Gradient Flow of Augmented Lagrangian with Application to Distributed Maximization of the Algebraic Connectivity of a Network,"  In this paper, a projected primal-dual gradient flow of augmented Lagrangian
is presented to solve convex optimization problems that are not necessarily
strictly convex. The optimization variables are restricted by a convex set with
computable projection operation on its tangent cone as well as equality
constraints. As a supplement of the analysis in
\cite{niederlander2016distributed}, we show that the projected dynamical system
converges to one of the saddle points and hence finding an optimal solution.
Moreover, the problem of distributedly maximizing the algebraic connectivity of
an undirected network by optimizing the port gains of each nodes (base
stations) is considered. The original semi-definite programming (SDP) problem
is relaxed into a nonlinear programming (NP) problem that will be solved by the
aforementioned projected dynamical system. Numerical examples show the
convergence of the aforementioned algorithm to one of the optimal solutions.
The effect of the relaxation is illustrated empirically with numerical
examples. A methodology is presented so that the number of iterations needed to
reach the equilibrium is suppressed. Complexity per iteration of the algorithm
is illustrated with numerical examples.
",0,0,1,0,0,0
10359,Deep Rewiring: Training very sparse deep networks,"  Neuromorphic hardware tends to pose limits on the connectivity of deep
networks that one can run on them. But also generic hardware and software
implementations of deep learning run more efficiently for sparse networks.
Several methods exist for pruning connections of a neural network after it was
trained without connectivity constraints. We present an algorithm, DEEP R, that
enables us to train directly a sparsely connected neural network. DEEP R
automatically rewires the network during supervised training so that
connections are there where they are most needed for the task, while its total
number is all the time strictly bounded. We demonstrate that DEEP R can be used
to train very sparse feedforward and recurrent neural networks on standard
benchmark tasks with just a minor loss in performance. DEEP R is based on a
rigorous theoretical foundation that views rewiring as stochastic sampling of
network configurations from a posterior.
",1,0,0,1,0,0
14063,A Connectome Based Hexagonal Lattice Convolutional Network Model of the Drosophila Visual System,"  What can we learn from a connectome? We constructed a simplified model of the
first two stages of the fly visual system, the lamina and medulla. The
resulting hexagonal lattice convolutional network was trained using
backpropagation through time to perform object tracking in natural scene
videos. Networks initialized with weights from connectome reconstructions
automatically discovered well-known orientation and direction selectivity
properties in T4 neurons and their inputs, while networks initialized at random
did not. Our work is the first demonstration, that knowledge of the connectome
can enable in silico predictions of the functional properties of individual
neurons in a circuit, leading to an understanding of circuit function from
structure alone.
",0,0,0,0,1,0
19145,Eigenvalue Decay Implies Polynomial-Time Learnability for Neural Networks,"  We consider the problem of learning function classes computed by neural
networks with various activations (e.g. ReLU or Sigmoid), a task believed to be
computationally intractable in the worst-case. A major open problem is to
understand the minimal assumptions under which these classes admit provably
efficient algorithms. In this work we show that a natural distributional
assumption corresponding to {\em eigenvalue decay} of the Gram matrix yields
polynomial-time algorithms in the non-realizable setting for expressive classes
of networks (e.g. feed-forward networks of ReLUs). We make no assumptions on
the structure of the network or the labels. Given sufficiently-strong
polynomial eigenvalue decay, we obtain {\em fully}-polynomial time algorithms
in {\em all} the relevant parameters with respect to square-loss. Milder decay
assumptions also lead to improved algorithms. This is the first purely
distributional assumption that leads to polynomial-time algorithms for networks
of ReLUs, even with one hidden layer. Further, unlike prior distributional
assumptions (e.g., the marginal distribution is Gaussian), eigenvalue decay has
been observed in practice on common data sets.
",1,0,0,0,0,0
1484,Intuitionistic Layered Graph Logic: Semantics and Proof Theory,"  Models of complex systems are widely used in the physical and social
sciences, and the concept of layering, typically building upon graph-theoretic
structure, is a common feature. We describe an intuitionistic substructural
logic called ILGL that gives an account of layering. The logic is a bunched
system, combining the usual intuitionistic connectives, together with a
non-commutative, non-associative conjunction (used to capture layering) and its
associated implications. We give soundness and completeness theorems for a
labelled tableaux system with respect to a Kripke semantics on graphs. We then
give an equivalent relational semantics, itself proven equivalent to an
algebraic semantics via a representation theorem. We utilise this result in two
ways. First, we prove decidability of the logic by showing the finite
embeddability property holds for the algebraic semantics. Second, we prove a
Stone-type duality theorem for the logic. By introducing the notions of ILGL
hyperdoctrine and indexed layered frame we are able to extend this result to a
predicate version of the logic and prove soundness and completeness theorems
for an extension of the layered graph semantics . We indicate the utility of
predicate ILGL with a resource-labelled bigraph model.
",1,0,0,0,0,0
14688,"A generative model for sparse, evolving digraphs","  Generating graphs that are similar to real ones is an open problem, while the
similarity notion is quite elusive and hard to formalize. In this paper, we
focus on sparse digraphs and propose SDG, an algorithm that aims at generating
graphs similar to real ones. Since real graphs are evolving and this evolution
is important to study in order to understand the underlying dynamical system,
we tackle the problem of generating series of graphs. We propose SEDGE, an
algorithm meant to generate series of graphs similar to a real series. SEDGE is
an extension of SDG. We consider graphs that are representations of software
programs and show experimentally that our approach outperforms other existing
approaches. Experiments show the performance of both algorithms.
",1,0,0,0,0,0
12083,Light curves of hydrogen-poor Superluminous Supernovae from the Palomar Transient Factory,"  We investigate the light-curve properties of a sample of 26 spectroscopically
confirmed hydrogen-poor superluminous supernovae (SLSNe-I) in the Palomar
Transient Factory (PTF) survey. These events are brighter than SNe Ib/c and SNe
Ic-BL, on average, by about 4 and 2~mag, respectively. The peak absolute
magnitudes of SLSNe-I in rest-frame $g$ band span $-22\lesssim M_g
\lesssim-20$~mag, and these peaks are not powered by radioactive $^{56}$Ni,
unless strong asymmetries are at play. The rise timescales are longer for SLSNe
than for normal SNe Ib/c, by roughly 10 days, for events with similar decay
times. Thus, SLSNe-I can be considered as a separate population based on
photometric properties. After peak, SLSNe-I decay with a wide range of slopes,
with no obvious gap between rapidly declining and slowly declining events. The
latter events show more irregularities (bumps) in the light curves at all
times. At late times, the SLSN-I light curves slow down and cluster around the
$^{56}$Co radioactive decay rate. Powering the late-time light curves with
radioactive decay would require between 1 and 10${\rm M}_\odot$ of Ni masses.
Alternatively, a simple magnetar model can reasonably fit the majority of
SLSNe-I light curves, with four exceptions, and can mimic the radioactive decay
of $^{56}$Co, up to $\sim400$ days from explosion. The resulting spin values do
not correlate with the host-galaxy metallicities. Finally, the analysis of our
sample cannot strengthen the case for using SLSNe-I for cosmology.
",0,1,0,0,0,0
5063,"The Guiding Influence of Stanley Mandelstam, from S-Matrix Theory to String Theory","  The guiding influence of some of Stanley Mandelstam's key contributions to
the development of theoretical high energy physics is discussed, from the
motivation for the study of the analytic properties of the scattering matrix
through to dual resonance models and their evolution into string theory.
",0,1,0,0,0,0
7677,A statistical test for the Zipf's law by deviations from the Heaps' law,"  We explore a probabilistic model of an artistic text: words of the text are
chosen independently of each other in accordance with a discrete probability
distribution on an infinite dictionary. The words are enumerated 1, 2,
$\ldots$, and the probability of appearing the $i$'th word is asymptotically a
power function. Bahadur proved that in this case the number of different words
depends on the length of the text is asymptotically a power function, too. On
the other hand, in the applied statistics community, there exist statements
supported by empirical observations, the Zipf's and the Heaps' laws. We
highlight the links between Bahadur results and Zipf's/Heaps' laws, and
introduce and analyse a corresponding statistical test.
",0,0,1,1,0,0
16208,SideEye: A Generative Neural Network Based Simulator of Human Peripheral Vision,"  Foveal vision makes up less than 1% of the visual field. The other 99% is
peripheral vision. Precisely what human beings see in the periphery is both
obvious and mysterious in that we see it with our own eyes but can't visualize
what we see, except in controlled lab experiments. Degradation of information
in the periphery is far more complex than what might be mimicked with a radial
blur. Rather, behaviorally-validated models hypothesize that peripheral vision
measures a large number of local texture statistics in pooling regions that
overlap and grow with eccentricity. In this work, we develop a new method for
peripheral vision simulation by training a generative neural network on a
behaviorally-validated full-field synthesis model. By achieving a 21,000 fold
reduction in running time, our approach is the first to combine realism and
speed of peripheral vision simulation to a degree that provides a whole new way
to approach visual design: through peripheral visualization.
",1,0,0,0,0,0
16411,Evidence for OH or H2O on the surface of 433 Eros and 1036 Ganymed,"  Water and hydroxyl, once thought to be found only in the primitive airless
bodies that formed beyond roughly 2.5-3 AU, have recently been detected on the
Moon and Vesta, which both have surfaces dominated by evolved, non-primitive
compositions. In both these cases, the water/OH is thought to be exogenic,
either brought in via impacts with comets or hydrated asteroids or created via
solar wind interactions with silicates in the regolith or both. Such exogenic
processes should also be occurring on other airless body surfaces. To test this
hypothesis, we used the NASA Infrared Telescope Facility (IRTF) to measure
reflectance spectra (2.0 to 4.1 {\mu}m) of two large near-Earth asteroids
(NEAs) with compositions generally interpreted as anhydrous: 433 Eros and 1036
Ganymed. OH is detected on both of these bodies in the form of absorption
features near 3 {\mu}m. The spectra contain a component of thermal emission at
longer wavelengths, from which we estimate thermal of 167+/- 98 J m-2s-1/2K-1
for Eros (consistent with previous estimates) and 214+/- 80 J m-2s-1/2K-1 for
Ganymed, the first reported measurement of thermal inertia for this object.
These observations demonstrate that processes responsible for water/OH creation
on large airless bodies also act on much smaller bodies.
",0,1,0,0,0,0
9266,Fractional Derivatives of Convex Lyapunov Functions and Control Problems in Fractional Order Systems,"  The paper is devoted to the development of control procedures with a guide
for conflict-controlled dynamical systems described by ordinary fractional
differential equations with the Caputo derivative of an order $\alpha \in (0,
1).$ For the case when the guide is in a certain sense a copy of the system, a
mutual aiming procedure between the initial system and the guide is elaborated.
The proof of proximity between motions of the systems is based on the estimate
of the fractional derivative of the superposition of a convex Lyapunov function
and a function represented by the fractional integral of an essentially bounded
measurable function. This estimate can be considered as a generalization of the
known estimates of such type. An example is considered which illustrates the
workability of the proposed control procedures.
",0,0,1,0,0,0
6352,Iterative Collaborative Filtering for Sparse Matrix Estimation,"  The sparse matrix estimation problem consists of estimating the distribution
of an $n\times n$ matrix $Y$, from a sparsely observed single instance of this
matrix where the entries of $Y$ are independent random variables. This captures
a wide array of problems; special instances include matrix completion in the
context of recommendation systems, graphon estimation, and community detection
in (mixed membership) stochastic block models. Inspired by classical
collaborative filtering for recommendation systems, we propose a novel
iterative, collaborative filtering-style algorithm for matrix estimation in
this generic setting. We show that the mean squared error (MSE) of our
estimator converges to $0$ at the rate of $O(d^2 (pn)^{-2/5})$ as long as
$\omega(d^5 n)$ random entries from a total of $n^2$ entries of $Y$ are
observed (uniformly sampled), $\mathbb{E}[Y]$ has rank $d$, and the entries of
$Y$ have bounded support. The maximum squared error across all entries
converges to $0$ with high probability as long as we observe a little more,
$\Omega(d^5 n \ln^2(n))$ entries. Our results are the best known sample
complexity results in this generality.
",0,0,1,1,0,0
422,Rethinking Information Sharing for Actionable Threat Intelligence,"  In the past decade, the information security and threat landscape has grown
significantly making it difficult for a single defender to defend against all
attacks at the same time. This called for introduc- ing information sharing, a
paradigm in which threat indicators are shared in a community of trust to
facilitate defenses. Standards for representation, exchange, and consumption of
indicators are pro- posed in the literature, although various issues are
undermined. In this paper, we rethink information sharing for actionable
intelli- gence, by highlighting various issues that deserve further explo-
ration. We argue that information sharing can benefit from well- defined use
models, threat models, well-understood risk by mea- surement and robust
scoring, well-understood and preserved pri- vacy and quality of indicators and
robust mechanism to avoid free riding behavior of selfish agent. We call for
using the differential nature of data and community structures for optimizing
sharing.
",1,0,0,0,0,0
13884,GIANT: Globally Improved Approximate Newton Method for Distributed Optimization,"  For distributed computing environment, we consider the empirical risk
minimization problem and propose a distributed and communication-efficient
Newton-type optimization method. At every iteration, each worker locally finds
an Approximate NewTon (ANT) direction, which is sent to the main driver. The
main driver, then, averages all the ANT directions received from workers to
form a {\it Globally Improved ANT} (GIANT) direction. GIANT is highly
communication efficient and naturally exploits the trade-offs between local
computations and global communications in that more local computations result
in fewer overall rounds of communications. Theoretically, we show that GIANT
enjoys an improved convergence rate as compared with first-order methods and
existing distributed Newton-type methods. Further, and in sharp contrast with
many existing distributed Newton-type methods, as well as popular first-order
methods, a highly advantageous practical feature of GIANT is that it only
involves one tuning parameter. We conduct large-scale experiments on a computer
cluster and, empirically, demonstrate the superior performance of GIANT.
",1,0,0,1,0,0
18199,Efficiency Analysis of ASP Encodings for Sequential Pattern Mining Tasks,"  This article presents the use of Answer Set Programming (ASP) to mine
sequential patterns. ASP is a high-level declarative logic programming paradigm
for high level encoding combinatorial and optimization problem solving as well
as knowledge representation and reasoning. Thus, ASP is a good candidate for
implementing pattern mining with background knowledge, which has been a data
mining issue for a long time. We propose encodings of the classical sequential
pattern mining tasks within two representations of embeddings (fill-gaps vs
skip-gaps) and for various kinds of patterns: frequent, constrained and
condensed. We compare the computational performance of these encodings with
each other to get a good insight into the efficiency of ASP encodings. The
results show that the fill-gaps strategy is better on real problems due to
lower memory consumption. Finally, compared to a constraint programming
approach (CPSM), another declarative programming paradigm, our proposal showed
comparable performance.
",1,0,0,1,0,0
7742,Theoretical properties of quasi-stationary Monte Carlo methods,"  This paper gives foundational results for the application of
quasi-stationarity to Monte Carlo inference problems. We prove natural
sufficient conditions for the quasi-limiting distribution of a killed diffusion
to coincide with a target density of interest. We also quantify the rate of
convergence to quasi-stationarity by relating the killed diffusion to an
appropriate Langevin diffusion. As an example, we consider in detail a killed
Ornstein--Uhlenbeck process with Gaussian quasi-stationary distribution.
",0,0,1,1,0,0
20296,How to Beat Science and Influence People: Policy Makers and Propaganda in Epistemic Networks,"  In their recent book Merchants of Doubt [New York:Bloomsbury 2010], Naomi
Oreskes and Erik Conway describe the ""tobacco strategy"", which was used by the
tobacco industry to influence policy makers regarding the health risks of
tobacco products. The strategy involved two parts, consisting of (1) promoting
and sharing independent research supporting the industry's preferred position
and (2) funding additional research, but selectively publishing the results. We
introduce a model of the Tobacco Strategy, and use it to argue that both prongs
of the strategy can be extremely effective--even when policy makers rationally
update on all evidence available to them. As we elaborate, this model helps
illustrate the conditions under which the Tobacco Strategy is particularly
successful. In addition, we show how journalists engaged in ""fair"" reporting
can inadvertently mimic the effects of industry on public belief.
",1,0,0,0,0,0
19541,Direct simulation of liquid-gas-solid flow with a free surface lattice Boltzmann method,"  Direct numerical simulation of liquid-gas-solid flows is uncommon due to the
considerable computational cost. As the grid spacing is determined by the
smallest involved length scale, large grid sizes become necessary -- in
particular if the bubble-particle aspect ratio is on the order of 10 or larger.
Hence, it arises the question of both feasibility and reasonability. In this
paper, we present a fully parallel, scalable method for direct numerical
simulation of bubble-particle interaction at a size ratio of 1-2 orders of
magnitude that makes simulations feasible on currently available
super-computing resources. With the presented approach, simulations of bubbles
in suspension columns consisting of more than $100\,000$ fully resolved
particles become possible. Furthermore, we demonstrate the significance of
particle-resolved simulations by comparison to previous unresolved solutions.
The results indicate that fully-resolved direct numerical simulation is indeed
necessary to predict the flow structure of bubble-particle interaction problems
correctly.
",0,1,0,0,0,0
1058,"Smart ""Predict, then Optimize""","  Many real-world analytics problems involve two significant challenges:
prediction and optimization. Due to the typically complex nature of each
challenge, the standard paradigm is to predict, then optimize. By and large,
machine learning tools are intended to minimize prediction error and do not
account for how the predictions will be used in a downstream optimization
problem. In contrast, we propose a new and very general framework, called Smart
""Predict, then Optimize"" (SPO), which directly leverages the optimization
problem structure, i.e., its objective and constraints, for designing
successful analytics tools. A key component of our framework is the SPO loss
function, which measures the quality of a prediction by comparing the objective
values of the solutions generated using the predicted and observed parameters,
respectively. Training a model with respect to the SPO loss is computationally
challenging, and therefore we also develop a surrogate loss function, called
the SPO+ loss, which upper bounds the SPO loss, has desirable convexity
properties, and is statistically consistent under mild conditions. We also
propose a stochastic gradient descent algorithm which allows for situations in
which the number of training samples is large, model regularization is desired,
and/or the optimization problem of interest is nonlinear or integer. Finally,
we perform computational experiments to empirically verify the success of our
SPO framework in comparison to the standard predict-then-optimize approach.
",1,0,0,1,0,0
848,NeuroNER: an easy-to-use program for named-entity recognition based on neural networks,"  Named-entity recognition (NER) aims at identifying entities of interest in a
text. Artificial neural networks (ANNs) have recently been shown to outperform
existing NER systems. However, ANNs remain challenging to use for non-expert
users. In this paper, we present NeuroNER, an easy-to-use named-entity
recognition tool based on ANNs. Users can annotate entities using a graphical
web-based user interface (BRAT): the annotations are then used to train an ANN,
which in turn predict entities' locations and categories in new texts. NeuroNER
makes this annotation-training-prediction flow smooth and accessible to anyone.
",1,0,0,1,0,0
457,"Mutual Information, Relative Entropy and Estimation Error in Semi-martingale Channels","  Fundamental relations between information and estimation have been
established in the literature for the continuous-time Gaussian and Poisson
channels, in a long line of work starting from the classical representation
theorems by Duncan and Kabanov respectively. In this work, we demonstrate that
such relations hold for a much larger family of continuous-time channels. We
introduce the family of semi-martingale channels where the channel output is a
semi-martingale stochastic process, and the channel input modulates the
characteristics of the semi-martingale. For these channels, which includes as a
special case the continuous time Gaussian and Poisson models, we establish new
representations relating the mutual information between the channel input and
output to an optimal causal filtering loss, thereby unifying and considerably
extending results from the Gaussian and Poisson settings. Extensions to the
setting of mismatched estimation are also presented where the relative entropy
between the laws governing the output of the channel under two different input
distributions is equal to the cumulative difference between the estimation loss
incurred by using the mismatched and optimal causal filters respectively. The
main tool underlying these results is the Doob--Meyer decomposition of a class
of likelihood ratio sub-martingales. The results in this work can be viewed as
the continuous-time analogues of recent generalizations for relations between
information and estimation for discrete-time Lévy channels.
",1,0,0,0,0,0
6980,On (in)stabilities of perturbations in mimetic models with higher derivatives,"  Usually when applying the mimetic model to the early universe, higher
derivative terms are needed to promote the mimetic field to be dynamical.
However such models suffer from the ghost and/or the gradient instabilities and
simple extensions cannot cure this pathology. We point out in this paper that
it is possible to overcome this difficulty by considering the direct couplings
of the higher derivatives of the mimetic field to the curvature of the
spacetime.
",0,1,0,0,0,0
5709,Nonlinear Mapping Convergence and Application to Social Networks,"  This paper discusses discrete-time maps of the form $x(k + 1) = F(x(k))$,
focussing on equilibrium points of such maps. Under some circumstances,
Lefschetz fixed-point theory can be used to establish the existence of a single
locally attractive equilibrium (which is sometimes globally attractive) when a
general property of local attractivity is known for any equilibrium. Problems
in social networks often involve such discrete-time systems, and we make an
application to one such problem.
",1,0,1,0,0,0
8536,Means of infinite sets I,"  We open a new field on how one can define means on infinite sets. We
investigate many different ways on how such means can be constructed. One
method is based on sequences of ideals, other deals with accumulation points,
one uses isolated points, other deals with average using integral, other with
limit of average on surroundings and one deals with evenly distributed samples.
We study various properties of such means and their relations to each other.
",0,0,1,0,0,0
1825,Optimal Oil Production and Taxation in Presence of Global Disruptions,"  This paper studies the optimal extraction policy of an oil field as well as
the efficient taxation of the revenues generated. Taking into account the fact
that the oil price in worldwide commodity markets fluctuates randomly following
global and seasonal macroeconomic parameters, we model the evolution of the oil
price as a mean reverting regime-switching jump diffusion process. Given that
oil producing countries rely on oil sale revenues as well as taxes levied on
oil companies for a good portion of the revenue side of their budgets, we
formulate this problem as a differential game where the two players are the
mining company whose aim is to maximize the revenues generated from its
extracting activities and the government agency in charge of regulating and
taxing natural resources. We prove the existence of a Nash equilibrium and the
convergence of an approximating scheme for the value functions. Furthermore,
optimal extraction and fiscal policies that should be applied when the
equilibrium is reached are derived.A numerical example is presented to
illustrate these results.
",0,0,1,0,0,0
15476,Finiteness of étale fundamental groups by reduction modulo $p$,"  We introduce a spreading out technique to deduce finiteness results for
étale fundamental groups of complex varieties by characteristic $p$ methods,
and apply this to recover a finiteness result proven recently for local
fundamental groups in characteristic $0$ using birational geometry.
",0,0,1,0,0,0
8206,Incremental Sharpe and other performance ratios,"  We present a new methodology of computing incremental contribution for
performance ratios for portfolio like Sharpe, Treynor, Calmar or Sterling
ratios. Using Euler's homogeneous function theorem, we are able to decompose
these performance ratios as a linear combination of individual modified
performance ratios. This allows understanding the drivers of these performance
ratios as well as deriving a condition for a new asset to provide incremental
performance for the portfolio. We provide various numerical examples of this
performance ratio decomposition.
",0,0,0,0,0,1
1084,"ZebraLancer: Crowdsource Knowledge atop Open Blockchain, Privately and Anonymously","  We design and implement the first private and anonymous decentralized
crowdsourcing system ZebraLancer. It realizes the fair exchange (i.e. security
against malicious workers and dishonest requesters) without using any
third-party arbiter. More importantly, it overcomes two fundamental challenges
of decentralization, i.e. data leakage and identity breach.
First, our outsource-then-prove methodology resolves the critical tension
between blockchain transparency and data confidentiality without sacrificing
the fairness of exchange. ZebraLancer ensures: a requester will not pay more
than what data deserve, according to a policy announced when her task is
published through the blockchain; each worker indeed gets a payment based on
the policy, if submits data to the blockchain; the above properties are
realized not only without a central arbiter, but also without leaking the data
to blockchain network.
Furthermore, the blockchain transparency might allow one to infer private
information of workers/requesters through their participation history.
ZebraLancer solves the problem by allowing anonymous participations without
surrendering user accountability. Specifically, workers cannot misuse anonymity
to submit multiple times to reap rewards, and an anonymous requester cannot
maliciously submit colluded answers to herself to repudiate payments. The idea
behind is a subtle linkability: if one authenticates twice in a task, everybody
can tell, or else staying anonymous. To realize such delicate linkability, we
put forth a novel cryptographic notion, the common-prefix-linkable anonymous
authentication.
Finally, we implement our protocol for a common image annotation task and
deploy it in a test net of Ethereum. The experiment results show the
applicability of our protocol and highlight subtleties of tailoring the
protocol to be compatible with the existing real-world open blockchain.
",1,0,0,0,0,0
17359,MinimalRNN: Toward More Interpretable and Trainable Recurrent Neural Networks,"  We introduce MinimalRNN, a new recurrent neural network architecture that
achieves comparable performance as the popular gated RNNs with a simplified
structure. It employs minimal updates within RNN, which not only leads to
efficient learning and testing but more importantly better interpretability and
trainability. We demonstrate that by endorsing the more restrictive update
rule, MinimalRNN learns disentangled RNN states. We further examine the
learning dynamics of different RNN structures using input-output Jacobians, and
show that MinimalRNN is able to capture longer range dependencies than existing
RNN architectures.
",1,0,0,1,0,0
12039,Efficient and Scalable View Generation from a Single Image using Fully Convolutional Networks,"  Single-image-based view generation (SIVG) is important for producing 3D
stereoscopic content. Here, handling different spatial resolutions as input and
optimizing both reconstruction accuracy and processing speed is desirable.
Latest approaches are based on convolutional neural network (CNN), and they
generate promising results. However, their use of fully connected layers as
well as pre-trained VGG forces a compromise between reconstruction accuracy and
processing speed. In addition, this approach is limited to the use of a
specific spatial resolution. To remedy these problems, we propose exploiting
fully convolutional networks (FCN) for SIVG. We present two FCN architectures
for SIVG. The first one is based on combination of an FCN and a view-rendering
network called DeepView$_{ren}$. The second one consists of decoupled networks
for luminance and chrominance signals, denoted by DeepView$_{dec}$. To train
our solutions we present a large dataset of 2M stereoscopic images. Results
show that both of our architectures improve accuracy and speed over the state
of the art. DeepView$_{ren}$ generates competitive accuracy to the state of the
art, however, with the fastest processing speed of all. That is x5 times faster
speed and x24 times lower memory consumption compared to the state of the art.
DeepView$_{dec}$ has much higher accuracy, but with x2.5 times faster speed and
x12 times lower memory consumption. We evaluated our approach with both
objective and subjective studies.
",1,0,0,0,0,0
5376,Anisotropic twicing for single particle reconstruction using autocorrelation analysis,"  The missing phase problem in X-ray crystallography is commonly solved using
the technique of molecular replacement, which borrows phases from a previously
solved homologous structure, and appends them to the measured Fourier
magnitudes of the diffraction patterns of the unknown structure. More recently,
molecular replacement has been proposed for solving the missing orthogonal
matrices problem arising in Kam's autocorrelation analysis for single particle
reconstruction using X-ray free electron lasers and cryo-EM. In classical
molecular replacement, it is common to estimate the magnitudes of the unknown
structure as twice the measured magnitudes minus the magnitudes of the
homologous structure, a procedure known as `twicing'. Mathematically, this is
equivalent to finding an unbiased estimator for a complex-valued scalar. We
generalize this scheme for the case of estimating real or complex valued
matrices arising in single particle autocorrelation analysis. We name this
approach ""Anisotropic Twicing"" because unlike the scalar case, the unbiased
estimator is not obtained by a simple magnitude isotropic correction. We
compare the performance of the least squares, twicing and anisotropic twicing
estimators on synthetic and experimental datasets. We demonstrate 3D homology
modeling in cryo-EM directly from experimental data without iterative
refinement or class averaging, for the first time.
",1,0,0,1,0,0
13110,Edge-Based Recognition of Novel Objects for Robotic Grasping,"  In this paper, we investigate the problem of grasping novel objects in
unstructured environments. To address this problem, consideration of the object
geometry, reachability and force closure analysis are required. We propose a
framework for grasping unknown objects by localizing contact regions on the
contours formed by a set of depth edges in a single view 2D depth image.
According to the edge geometric features obtained from analyzing the data of
the depth map, the contact regions are determined. Finally,We validate the
performance of the approach by applying it to the scenes with both single and
multiple objects, using Baxter manipulator.
",1,0,0,0,0,0
4768,Mining Target Attribute Subspace and Set of Target Communities in Large Attributed Networks,"  Community detection provides invaluable help for various applications, such
as marketing and product recommendation. Traditional community detection
methods designed for plain networks may not be able to detect communities with
homogeneous attributes inside on attributed networks with attribute
information. Most of recent attribute community detection methods may fail to
capture the requirements of a specific application and not be able to mine the
set of required communities for a specific application. In this paper, we aim
to detect the set of target communities in the target subspace which has some
focus attributes with large importance weights satisfying the requirements of a
specific application. In order to improve the university of the problem, we
address the problem in an extreme case where only two sample nodes in any
potential target community are provided. A Target Subspace and Communities
Mining (TSCM) method is proposed. In TSCM, a sample information extension
method is designed to extend the two sample nodes to a set of exemplar nodes
from which the target subspace is inferred. Then the set of target communities
are located and mined based on the target subspace. Experiments on synthetic
datasets demonstrate the effectiveness and efficiency of our method and
applications on real-world datasets show its application values.
",1,1,0,0,0,0
934,GraphCombEx: A Software Tool for Exploration of Combinatorial Optimisation Properties of Large Graphs,"  We present a prototype of a software tool for exploration of multiple
combinatorial optimisation problems in large real-world and synthetic complex
networks. Our tool, called GraphCombEx (an acronym of Graph Combinatorial
Explorer), provides a unified framework for scalable computation and
presentation of high-quality suboptimal solutions and bounds for a number of
widely studied combinatorial optimisation problems. Efficient representation
and applicability to large-scale graphs and complex networks are particularly
considered in its design. The problems currently supported include maximum
clique, graph colouring, maximum independent set, minimum vertex clique
covering, minimum dominating set, as well as the longest simple cycle problem.
Suboptimal solutions and intervals for optimal objective values are estimated
using scalable heuristics. The tool is designed with extensibility in mind,
with the view of further problems and both new fast and high-performance
heuristics to be added in the future. GraphCombEx has already been successfully
used as a support tool in a number of recent research studies using
combinatorial optimisation to analyse complex networks, indicating its promise
as a research software tool.
",1,0,0,0,0,0
18447,On the Performance of Network Parallel Training in Artificial Neural Networks,"  Artificial Neural Networks (ANNs) have received increasing attention in
recent years with applications that span a wide range of disciplines including
vital domains such as medicine, network security and autonomous transportation.
However, neural network architectures are becoming increasingly complex and
with an increasing need to obtain real-time results from such models, it has
become pivotal to use parallelization as a mechanism for speeding up network
training and deployment. In this work we propose an implementation of Network
Parallel Training through Cannon's Algorithm for matrix multiplication. We show
that increasing the number of processes speeds up training until the point
where process communication costs become prohibitive; this point varies by
network complexity. We also show through empirical efficiency calculations that
the speedup obtained is superlinear.
",1,0,0,1,0,0
16359,Compact Multi-Class Boosted Trees,"  Gradient boosted decision trees are a popular machine learning technique, in
part because of their ability to give good accuracy with small models. We
describe two extensions to the standard tree boosting algorithm designed to
increase this advantage. The first improvement extends the boosting formalism
from scalar-valued trees to vector-valued trees. This allows individual trees
to be used as multiclass classifiers, rather than requiring one tree per class,
and drastically reduces the model size required for multiclass problems. We
also show that some other popular vector-valued gradient boosted trees
modifications fit into this formulation and can be easily obtained in our
implementation. The second extension, layer-by-layer boosting, takes smaller
steps in function space, which is empirically shown to lead to a faster
convergence and to a more compact ensemble. We have added both improvements to
the open-source TensorFlow Boosted trees (TFBT) package, and we demonstrate
their efficacy on a variety of multiclass datasets. We expect these extensions
will be of particular interest to boosted tree applications that require small
models, such as embedded devices, applications requiring fast inference, or
applications desiring more interpretable models.
",1,0,0,1,0,0
904,Adaptive Quantization for Deep Neural Network,"  In recent years Deep Neural Networks (DNNs) have been rapidly developed in
various applications, together with increasingly complex architectures. The
performance gain of these DNNs generally comes with high computational costs
and large memory consumption, which may not be affordable for mobile platforms.
Deep model quantization can be used for reducing the computation and memory
costs of DNNs, and deploying complex DNNs on mobile equipment. In this work, we
propose an optimization framework for deep model quantization. First, we
propose a measurement to estimate the effect of parameter quantization errors
in individual layers on the overall model prediction accuracy. Then, we propose
an optimization process based on this measurement for finding optimal
quantization bit-width for each layer. This is the first work that
theoretically analyse the relationship between parameter quantization errors of
individual layers and model accuracy. Our new quantization algorithm
outperforms previous quantization optimization methods, and achieves 20-40%
higher compression rate compared to equal bit-width quantization at the same
model prediction accuracy.
",1,0,0,1,0,0
