ID,TITLE,ABSTRACT,Computer Science,Physics,Mathematics,Statistics,Quantitative Biology,Quantitative Finance
20258,Dynamic Layer Normalization for Adaptive Neural Acoustic Modeling in Speech Recognition,"  Layer normalization is a recently introduced technique for normalizing the
activities of neurons in deep neural networks to improve the training speed and
stability. In this paper, we introduce a new layer normalization technique
called Dynamic Layer Normalization (DLN) for adaptive neural acoustic modeling
in speech recognition. By dynamically generating the scaling and shifting
parameters in layer normalization, DLN adapts neural acoustic models to the
acoustic variability arising from various factors such as speakers, channel
noises, and environments. Unlike other adaptive acoustic models, our proposed
approach does not require additional adaptation data or speaker information
such as i-vectors. Moreover, the model size is fixed as it dynamically
generates adaptation parameters. We apply our proposed DLN to deep
bidirectional LSTM acoustic models and evaluate them on two benchmark datasets
for large vocabulary ASR experiments: WSJ and TED-LIUM release 2. The
experimental results show that our DLN improves neural acoustic models in terms
of transcription accuracy by dynamically adapting to various speakers and
environments.
",1,0,0,0,0,0
483,Susceptibility Propagation by Using Diagonal Consistency,"  A susceptibility propagation that is constructed by combining a belief
propagation and a linear response method is used for approximate computation
for Markov random fields. Herein, we formulate a new, improved susceptibility
propagation by using the concept of a diagonal matching method that is based on
mean-field approaches to inverse Ising problems. The proposed susceptibility
propagation is robust for various network structures, and it is reduced to the
ordinary susceptibility propagation and to the adaptive
Thouless-Anderson-Palmer equation in special cases.
",0,0,1,1,0,0
4190,The Robot Routing Problem for Collecting Aggregate Stochastic Rewards,"  We propose a new model for formalizing reward collection problems on graphs
with dynamically generated rewards which may appear and disappear based on a
stochastic model. The *robot routing problem* is modeled as a graph whose nodes
are stochastic processes generating potential rewards over discrete time. The
rewards are generated according to the stochastic process, but at each step, an
existing reward disappears with a given probability. The edges in the graph
encode the (unit-distance) paths between the rewards' locations. On visiting a
node, the robot collects the accumulated reward at the node at that time, but
traveling between the nodes takes time. The optimization question asks to
compute an optimal (or epsilon-optimal) path that maximizes the expected
collected rewards.
We consider the finite and infinite-horizon robot routing problems. For
finite-horizon, the goal is to maximize the total expected reward, while for
infinite horizon we consider limit-average objectives. We study the
computational and strategy complexity of these problems, establish NP-lower
bounds and show that optimal strategies require memory in general. We also
provide an algorithm for computing epsilon-optimal infinite paths for arbitrary
epsilon > 0.
",1,0,1,0,0,0
9839,"Probability, Statistics and Planet Earth, I: Geotemporal covariances","  The study of covariances (or positive definite functions) on the sphere (the
Earth, in our motivation) goes back to Bochner and Schoenberg (1940--42) and to
the first author (1969, 1973), among others. Extending to the geotemporal case
(sphere cross line, for position and time) was for a long time an obstacle to
geostatistical modelling. The characterisation question here was raised by the
authors and Mijatović in 2016, and answered by Berg and Porcu in 2017.
Extensions to multiple products (of spheres and lines) follows similarly
(Guella, Menegatto and Peron, 2016). We survey results of this type, and
related applications e.g. in numerical weather prediction.
",0,1,0,1,0,0
16592,Counting the number of metastable states in the modularity landscape: Algorithmic detectability limit of greedy algorithms in community detection,"  Modularity maximization using greedy algorithms continues to be a popular
approach toward community detection in graphs, even after various better
forming algorithms have been proposed. Apart from its clear mechanism and ease
of implementation, this approach is persistently popular because, presumably,
its risk of algorithmic failure is not well understood. This Rapid
Communication provides insight into this issue by estimating the algorithmic
performance limit of modularity maximization. This is achieved by counting the
number of metastable states under a local update rule. Our results offer a
quantitative insight into the level of sparsity at which a greedy algorithm
typically fails.
",1,0,0,0,0,0
3753,"Sublogarithmic Distributed Algorithms for Lovász Local lemma, and the Complexity Hierarchy","  Locally Checkable Labeling (LCL) problems include essentially all the classic
problems of $\mathsf{LOCAL}$ distributed algorithms. In a recent enlightening
revelation, Chang and Pettie [arXiv 1704.06297] showed that any LCL (on bounded
degree graphs) that has an $o(\log n)$-round randomized algorithm can be solved
in $T_{LLL}(n)$ rounds, which is the randomized complexity of solving (a
relaxed variant of) the Lovász Local Lemma (LLL) on bounded degree $n$-node
graphs. Currently, the best known upper bound on $T_{LLL}(n)$ is $O(\log n)$,
by Chung, Pettie, and Su [PODC'14], while the best known lower bound is
$\Omega(\log\log n)$, by Brandt et al. [STOC'16]. Chang and Pettie conjectured
that there should be an $O(\log\log n)$-round algorithm.
Making the first step of progress towards this conjecture, and providing a
significant improvement on the algorithm of Chung et al. [PODC'14], we prove
that $T_{LLL}(n)= 2^{O(\sqrt{\log\log n})}$. Thus, any $o(\log n)$-round
randomized distributed algorithm for any LCL problem on bounded degree graphs
can be automatically sped up to run in $2^{O(\sqrt{\log\log n})}$ rounds.
Using this improvement and a number of other ideas, we also improve the
complexity of a number of graph coloring problems (in arbitrary degree graphs)
from the $O(\log n)$-round results of Chung, Pettie and Su [PODC'14] to
$2^{O(\sqrt{\log\log n})}$. These problems include defective coloring, frugal
coloring, and list vertex-coloring.
",1,0,0,0,0,0
12921,Holography and Koszul duality: the example of the $M2$ brane,"  Si Li and author suggested in that, in some cases, the AdS/CFT correspondence
can be formulated in terms of the algebraic operation of Koszul duality. In
this paper this suggestion is checked explicitly for $M2$ branes in an
$\Omega$-background. The algebra of supersymmetric operators on a stack of $K$
$M2$ branes is shown to be Koszul dual, in large $K$, to the algebra of
supersymmetric operators of $11$-dimensional supergravity in an
$\Omega$-background (using the formulation of supergravity in an
$\Omega$-background presented in arXiv:1610.04144).
The twisted form of supergravity that is used here can be quantized to all
orders in perturbation theory. We find that the Koszul duality result holds to
all orders in perturbation theory, in both the gravitational theory and the
theory on the $M2$. (However, there is a certain non-linear identification of
the coupling constants on each side which I was unable to determine
explicitly).
It is also shown that the algebra of operators on $K$ $M2$ branes, as $K \to
\infty$, is a quantum double-loop algebra (a two-variable analog of the
Yangian). This algebra is also the Koszul dual of the algebra of operators on
the gravitational theory. An explicit presentation for this algebra is
presented, and it is shown that this algebra is the unique quantization of its
classical limit. Some conjectural applications to enumerative geometry of
Calabi-Yau threefolds are also presented.
",0,0,1,0,0,0
16945,Maximal polynomial modulations of singular integrals,"  Let $K$ be a standard Hölder continuous Calderón--Zygmund kernel on
$\mathbb{R}^{\mathbf{d}}$ whose truncations define $L^2$ bounded operators. We
show that the maximal operator obtained by modulating $K$ by polynomial phases
of a fixed degree is bounded on $L^p(\mathbb{R}^{\mathbf{d}})$ for $1 < p <
\infty$. This extends Sjölin's multidimensional Carleson theorem and Lie's
polynomial Carleson theorem.
",0,0,1,0,0,0
3247,PULSEDYN - A dynamical simulation tool for studying strongly nonlinear chains,"  We introduce PULSEDYN, a particle dynamics program in $C++$, to solve
many-body nonlinear systems in one dimension. PULSEDYN is designed to make
computing accessible to non-specialists in the field of nonlinear dynamics of
many-body systems and to ensure transparency and easy benchmarking of numerical
results for an integrable model (Toda chain) and three non-integrable models
(Fermi-Pasta-Ulam-Tsingou, Morse and Lennard-Jones). To achieve the latter, we
have made our code open source and free to distribute. We examine (i) soliton
propagation and two-soliton collision in the Toda system, (ii) the recurrence
phenomenon in the Fermi-Pasta-Ulam-Tsingou system and the decay of a single
localized nonlinear excitation in the same system through quasi-equilibrium to
an equipartitioned state, and SW propagation in chains with (iii) Morse and
(iv) Lennard-Jones potentials. We recover well known results from theory and
other numerical results in the literature. We have obtained these results by
setting up a parameter file interface which allows the code to be used as a
black box. Therefore, we anticipate that the code would prove useful to
students and non-specialists. At the same time, PULSEDYN provides
scientifically accurate simulations thus making the study of rich dynamical
processes broadly accessible.
",0,1,0,0,0,0
19373,Variational Bayesian Inference For A Scale Mixture Of Normal Distributions Handling Missing Data,"  In this paper, a scale mixture of Normal distributions model is developed for
classification and clustering of data having outliers and missing values. The
classification method, based on a mixture model, focuses on the introduction of
latent variables that gives us the possibility to handle sensitivity of model
to outliers and to allow a less restrictive modelling of missing data.
Inference is processed through a Variational Bayesian Approximation and a
Bayesian treatment is adopted for model learning, supervised classification and
clustering.
",0,0,0,1,0,0
11812,Decomposition theorems for asymptotic property C and property A,"  We combine aspects of the notions of finite decomposition complexity and
asymptotic property C into a notion that we call finite APC-decomposition
complexity. Any space with finite decomposition complexity has finite
APC-decomposition complexity and any space with asymptotic property C has
finite APC-decomposition complexity. Moreover, finite APC-decomposition
complexity implies property A for metric spaces. We also show that finite
APC-decomposition complexity is preserved by direct products of groups and
spaces, amalgamated products of groups, and group extensions, among other
constructions.
",0,0,1,0,0,0
3201,A Bootstrap Lasso + Partial Ridge Method to Construct Confidence Intervals for Parameters in High-dimensional Sparse Linear Models,"  For high-dimensional sparse linear models, how to construct confidence
intervals for coefficients remains a difficult question. The main reason is the
complicated limiting distributions of common estimators such as the Lasso.
Several confidence interval construction methods have been developed, and
Bootstrap Lasso+OLS is notable for its simple technicality, good
interpretability, and comparable performance with other more complicated
methods. However, Bootstrap Lasso+OLS depends on the beta-min assumption, a
theoretic criterion that is often violated in practice. In this paper, we
introduce a new method called Bootstrap Lasso+Partial Ridge (LPR) to relax this
assumption. LPR is a two-stage estimator: first using Lasso to select features
and subsequently using Partial Ridge to refit the coefficients. Simulation
results show that Bootstrap LPR outperforms Bootstrap Lasso+OLS when there
exist small but non-zero coefficients, a common situation violating the
beta-min assumption. For such coefficients, compared to Bootstrap Lasso+OLS,
confidence intervals constructed by Bootstrap LPR have on average 50% larger
coverage probabilities. Bootstrap LPR also has on average 35% shorter
confidence interval lengths than the de-sparsified Lasso methods, regardless of
whether linear models are misspecified. Additionally, we provide theoretical
guarantees of Bootstrap LPR under appropriate conditions and implement it in
the R package ""HDCI.""
",0,0,0,1,0,0
18454,"Julian Ernst Besag, 26 March 1945 -- 6 August 2010, a biographical memoir","  Julian Besag was an outstanding statistical scientist, distinguished for his
pioneering work on the statistical theory and analysis of spatial processes,
especially conditional lattice systems. His work has been seminal in
statistical developments over the last several decades ranging from image
analysis to Markov chain Monte Carlo methods. He clarified the role of
auto-logistic and auto-normal models as instances of Markov random fields and
paved the way for their use in diverse applications. Later work included
investigations into the efficacy of nearest neighbour models to accommodate
spatial dependence in the analysis of data from agricultural field trials,
image restoration from noisy data, and texture generation using lattice models.
",0,0,0,1,0,0
6391,ECO-AMLP: A Decision Support System using an Enhanced Class Outlier with Automatic Multilayer Perceptron for Diabetes Prediction,"  With advanced data analytical techniques, efforts for more accurate decision
support systems for disease prediction are on rise. Surveys by World Health
Organization (WHO) indicate a great increase in number of diabetic patients and
related deaths each year. Early diagnosis of diabetes is a major concern among
researchers and practitioners. The paper presents an application of
\textit{Automatic Multilayer Perceptron }which\textit{ }is combined with an
outlier detection method \textit{Enhanced Class Outlier Detection using
distance based algorithm }to create a prediction framework named as Enhanced
Class Outlier with Automatic Multi layer Perceptron (ECO-AMLP). A series of
experiments are performed on publicly available Pima Indian Diabetes Dataset to
compare ECO-AMLP with other individual classifiers as well as ensemble based
methods. The outlier technique used in our framework gave better results as
compared to other pre-processing and classification techniques. Finally, the
results are compared with other state-of-the-art methods reported in literature
for diabetes prediction on PIDD and achieved accuracy of 88.7\% bests all other
reported studies.
",1,0,0,0,0,0
4154,Hierarchical Summarization of Metric Changes,"  We study changes in metrics that are defined on a cartesian product of trees.
Such metrics occur naturally in many practical applications, where a global
metric (such as revenue) can be broken down along several hierarchical
dimensions (such as location, gender, etc).
Given a change in such a metric, our goal is to identify a small set of
non-overlapping data segments that account for the change. An organization
interested in improving the metric can then focus their attention on these data
segments.
Our key contribution is an algorithm that mimics the operation of a
hierarchical organization of analysts. The algorithm has been successfully
applied, for example within Google Adwords to help advertisers triage the
performance of their advertising campaigns.
We show that the algorithm is optimal for two dimensions, and has an
approximation ratio $\log^{d-2}(n+1)$ for $d \geq 3$ dimensions, where $n$ is
the number of input data segments. For the Adwords application, we can show
that our algorithm is in fact a $2$-approximation.
Mathematically, we identify a certain data pattern called a \emph{conflict}
that both guides the design of the algorithm, and plays a central role in the
hardness results. We use these conflicts to both derive a lower bound of
$1.144^{d-2}$ (again $d\geq3$) for our algorithm, and to show that the problem
is NP-hard, justifying the focus on approximation.
",1,0,0,0,0,0
16428,Are there needles in a moving haystack? Adaptive sensing for detection of dynamically evolving signals,"  In this paper we investigate the problem of detecting dynamically evolving
signals. We model the signal as an $n$ dimensional vector that is either zero
or has $s$ non-zero components. At each time step $t\in \mathbb{N}$ the
non-zero components change their location independently with probability $p$.
The statistical problem is to decide whether the signal is a zero vector or in
fact it has non-zero components. This decision is based on $m$ noisy
observations of individual signal components collected at times $t=1,\ldots,m$.
We consider two different sensing paradigms, namely adaptive and non-adaptive
sensing. For non-adaptive sensing the choice of components to measure has to be
decided before the data collection process started, while for adaptive sensing
one can adjust the sensing process based on observations collected earlier. We
characterize the difficulty of this detection problem in both sensing paradigms
in terms of the aforementioned parameters, with special interest to the speed
of change of the active components. In addition we provide an adaptive sensing
algorithm for this problem and contrast its performance to that of non-adaptive
detection algorithms.
",0,0,1,1,0,0
17237,Multivariant Assertion-based Guidance in Abstract Interpretation,"  Approximations during program analysis are a necessary evil, as they ensure
essential properties, such as soundness and termination of the analysis, but
they also imply not always producing useful results. Automatic techniques have
been studied to prevent precision loss, typically at the expense of larger
resource consumption. In both cases (i.e., when analysis produces inaccurate
results and when resource consumption is too high), it is necessary to have
some means for users to provide information to guide analysis and thus improve
precision and/or performance. We present techniques for supporting within an
abstract interpretation framework a rich set of assertions that can deal with
multivariance/context-sensitivity, and can handle different run-time semantics
for those assertions that cannot be discharged at compile time. We show how the
proposed approach can be applied to both improving precision and accelerating
analysis. We also provide some formal results on the effects of such assertions
on the analysis results.
",1,0,0,0,0,0
10232,Frobenius elements in Galois representations with SL_n image,"  Suppose we have a elliptic curve over a number field whose mod $l$
representation has image isomorphic to $SL_2(\mathbb{F}_l)$. We present a
method to determine Frobenius elements of the associated Galois group which
incorporates the linear structure available. We are able to distinguish
$SL_n(\mathbb{F}_l)$-conjugacy from $GL_n(\mathbb{F}_l)$-conjugacy; this can be
thought of as being analogous to a result which distinguishes $A_n$-conjugacy
from $S_n$-conjugacy when the Galois group is considered as a permutation
group.
",0,0,1,0,0,0
11234,Quantum gravity corrections to the thermodynamics and phase transition of Schwarzschild black hole,"  In this work, we derive a new kind of rainbow functions, which has
generalized uncertainty principle parameter. Then, we investigate modified
thermodynamic quantities and phase transition of rainbow Schwarzschild black
hole by employing this new kind of rainbow functions. Our results demonstrate
that the effect of rainbow gravity and generalized uncertainty principle have a
great effect on the picture of Hawking radiation. It prevents black holes from
total evaporation and causes the remnant. In addition, after analyzing the the
modified local thermodynamic quantities, we find that effect of rainbow gravity
and generalized uncertainty principle lead to one first-order phase transition,
two second-order phase transitions, and two Hawking-Page-type phase transitions
in the thermodynamic system of rainbow Schwarzschild black hole.
",0,1,0,0,0,0
1312,Estimating a network from multiple noisy realizations,"  Complex interactions between entities are often represented as edges in a
network. In practice, the network is often constructed from noisy measurements
and inevitably contains some errors. In this paper we consider the problem of
estimating a network from multiple noisy observations where edges of the
original network are recorded with both false positives and false negatives.
This problem is motivated by neuroimaging applications where brain networks of
a group of patients with a particular brain condition could be viewed as noisy
versions of an unobserved true network corresponding to the disease. The key to
optimally leveraging these multiple observations is to take advantage of
network structure, and here we focus on the case where the true network
contains communities. Communities are common in real networks in general and in
particular are believed to be presented in brain networks. Under a community
structure assumption on the truth, we derive an efficient method to estimate
the noise levels and the original network, with theoretical guarantees on the
convergence of our estimates. We show on synthetic networks that the
performance of our method is close to an oracle method using the true parameter
values, and apply our method to fMRI brain data, demonstrating that it
constructs stable and plausible estimates of the population network.
",0,0,1,1,0,0
5352,Coexistence of quantum and classical flows in quantum turbulence in the $T=0$ limit,"  Tangles of quantized vortex line of initial density ${\cal L}(0) \sim 6\times
10^3$\,cm$^{-2}$ and variable amplitude of fluctuations of flow velocity $U(0)$
at the largest length scale were generated in superfluid $^4$He at $T=0.17$\,K,
and their free decay ${\cal L}(t)$ was measured. If $U(0)$ is small, the excess
random component of vortex line length firstly decays as ${\cal L} \propto
t^{-1}$ until it becomes comparable with the structured component responsible
for the classical velocity field, and the decay changes to ${\cal L} \propto
t^{-3/2}$. The latter regime always ultimately prevails, provided the classical
description of $U$ holds. A quantitative model of coexisting cascades of
quantum and classical energies describes all regimes of the decay.
",0,1,0,0,0,0
16405,A Knowledge-Based Analysis of the Blockchain Protocol,"  At the heart of the Bitcoin is a blockchain protocol, a protocol for
achieving consensus on a public ledger that records bitcoin transactions. To
the extent that a blockchain protocol is used for applications such as contract
signing and making certain transactions (such as house sales) public, we need
to understand what guarantees the protocol gives us in terms of agents'
knowledge. Here, we provide a complete characterization of agent's knowledge
when running a blockchain protocol using a variant of common knowledge that
takes into account the fact that agents can enter and leave the system, it is
not known which agents are in fact following the protocol (some agents may want
to deviate if they can gain by doing so), and the fact that the guarantees
provided by blockchain protocols are probabilistic. We then consider some
scenarios involving contracts and show that this level of knowledge suffices
for some scenarios, but not others.
",1,0,0,0,0,0
12360,Herding behavior in cryptocurrency markets,"  There are no solid arguments to sustain that digital currencies are the
future of online payments or the disruptive technology that some of its former
participants declared when used to face critiques. This paper aims to solve the
cryptocurrency puzzle from a behavioral finance perspective by finding the
parallelism between biases present in financial markets that could be applied
to cryptomarkets. Moreover, it is suggested that cryptocurrencies' prices are
driven by herding, hence this study test herding behavior under asymmetric and
symmetric conditions and the existence of different herding regimes by
employing the Markov-Switching approach.
",0,0,0,0,0,1
9133,Halo nonlinear reconstruction,"  We apply the nonlinear reconstruction method to simulated halo fields. For
halo number density $2.77\times 10^{-2}$ $(h^{-1} {\rm Mpc})^{-3}$ at $z=0$,
corresponding to the SDSS main sample density, we find the scale where the
noise saturates the linear signal is improved to $k\gtrsim0.36\ h {\rm
Mpc}^{-1}$, a factor of $2.29$ improvement in scale, or $12$ in number of
linear modes. The improvement is less for higher redshift or lower halo
density. We expect this to substantially improve the BAO accuracy of dense, low
redshift surveys, including the SDSS main sample, 6dFGS and 21cm intensity
mapping initiatives.
",0,1,0,0,0,0
6101,Gradient descent GAN optimization is locally stable,"  Despite the growing prominence of generative adversarial networks (GANs),
optimization in GANs is still a poorly understood topic. In this paper, we
analyze the ""gradient descent"" form of GAN optimization i.e., the natural
setting where we simultaneously take small gradient steps in both generator and
discriminator parameters. We show that even though GAN optimization does not
correspond to a convex-concave game (even for simple parameterizations), under
proper conditions, equilibrium points of this optimization procedure are still
\emph{locally asymptotically stable} for the traditional GAN formulation. On
the other hand, we show that the recently proposed Wasserstein GAN can have
non-convergent limit cycles near equilibrium. Motivated by this stability
analysis, we propose an additional regularization term for gradient descent GAN
updates, which \emph{is} able to guarantee local stability for both the WGAN
and the traditional GAN, and also shows practical promise in speeding up
convergence and addressing mode collapse.
",1,0,1,1,0,0
9572,Hyperelliptic Jacobians and isogenies,"  Motivated by results of Mestre and Voisin, in this note we mainly consider
abelian varieties isogenous to hyperelliptic Jacobians
In the first part we prove that a very general hyperelliptic Jacobian of
genus $g\ge 4$ is not isogenous to a non-hyperelliptic Jacobian. As a
consequence we obtain that the Intermediate Jacobian of a very general cubic
threefold is not isogenous to a Jacobian. Another corollary tells that the
Jacobian of a very general $d$-gonal curve of genus $g \ge 4$ is not isogenous
to a different Jacobian.
In the second part we consider a closed subvariety $\mathcal Y \subset
\mathcal A_g$ of the moduli space of principally polarized varieties of
dimension $g\ge 3$. We show that if a very general element of $\mathcal Y$ is
dominated by a hyperelliptic Jacobian, then $\dim \mathcal Y\ge 2g$. In
particular, if the general element in $\mathcal Y$ is simple, its Kummer
variety does not contain rational curves. Finally we show that a closed
subvariety $\mathcal Y\subset \mathcal M_g$ of dimension $2g-1$ such that the
Jacobian of a very general element of $\mathcal Y$ is dominated by a
hyperelliptic Jacobian is contained either in the hyperelliptic or in the
trigonal locus.
",0,0,1,0,0,0
2256,Collective Effects in Nanolasers Explained by Generalized Rate Equations,"  We study the stationary photon output and statistics of small lasers. Our
closed-form expressions clarify the contribution of collective effects due to
the interaction between quantum emitters. We generalize laser rate equations
and explain photon trapping: a decrease of the photon number output below the
lasing threshold, derive an expression for the stationary cavity mode
autocorrelation function $g_2$, which implies that collective effects may
strongly influence the photon statistics. We identify conditions for coherent,
thermal and superthermal radiation, the latter being a unique fingerprint for
collective emission in lasers. These generic analytical results agree with
recent experiments, complement numerical results, and provide insight into and
design rules for nanolasers.
",0,1,0,0,0,0
7584,The Ramsey theory of the universal homogeneous triangle-free graph,"  The universal homogeneous triangle-free graph, constructed by Henson and
denoted $\mathcal{H}_3$, is the triangle-free analogue of the Rado graph. While
the Ramsey theory of the Rado graph has been completely established, beginning
with Erdős-Hajnal-Posá and culminating in work of Sauer and
Laflamme-Sauer-Vuksanovic, the Ramsey theory of $\mathcal{H}_3$ had only
progressed to bounds for vertex colorings (Komjáth-Rödl) and edge
colorings (Sauer). This was due to a lack of broadscale techniques.
We solve this problem in general: For each finite triangle-free graph $G$,
there is a finite number $T(G)$ such that for any coloring of all copies of $G$
in $\mathcal{H}_3$ into finitely many colors, there is a subgraph of
$\mathcal{H}_3$ which is again universal homogeneous triangle-free in which the
coloring takes no more than $T(G)$ colors. This is the first such result for a
homogeneous structure omitting copies of some non-trivial finite structure. The
proof entails developments of new broadscale techniques, including a flexible
method for constructing trees which code $\mathcal{H}_3$ and the development of
their Ramsey theory.
",0,0,1,0,0,0
4642,End-to-End Sound Source Separation Conditioned On Instrument Labels,"  Can we perform an end-to-end sound source separation (SSS) with a variable
number of sources using a deep learning model? This paper presents an extension
of the Wave-U-Net model which allows end-to-end monaural source separation with
a non-fixed number of sources. Furthermore, we propose multiplicative
conditioning with instrument labels at the bottleneck of the Wave-U-Net and
show its effect on the separation results. This approach can be further
extended to other types of conditioning such as audio-visual SSS and
score-informed SSS.
",1,0,0,0,0,0
8035,"Whole planet coupling between climate, mantle, and core: Implications for the evolution of rocky planets","  Earth's climate, mantle, and core interact over geologic timescales. Climate
influences whether plate tectonics can take place on a planet, with cool
climates being favorable for plate tectonics because they enhance stresses in
the lithosphere, suppress plate boundary annealing, and promote hydration and
weakening of the lithosphere. Plate tectonics plays a vital role in the
long-term carbon cycle, which helps to maintain a temperate climate. Plate
tectonics provides long-term cooling of the core, which is vital for generating
a magnetic field, and the magnetic field is capable of shielding atmospheric
volatiles from the solar wind. Coupling between climate, mantle, and core can
potentially explain the divergent evolution of Earth and Venus. As Venus lies
too close to the sun for liquid water to exist, there is no long-term carbon
cycle and thus an extremely hot climate. Therefore plate tectonics cannot
operate and a long-lived core dynamo cannot be sustained due to insufficient
core cooling. On planets within the habitable zone where liquid water is
possible, a wide range of evolutionary scenarios can take place depending on
initial atmospheric composition, bulk volatile content, or the timing of when
plate tectonics initiates, among other factors. Many of these evolutionary
trajectories would render the planet uninhabitable. However, there is still
significant uncertainty over the nature of the coupling between climate,
mantle, and core. Future work is needed to constrain potential evolutionary
scenarios and the likelihood of an Earth-like evolution.
",0,1,0,0,0,0
7639,Covering and tiling hypergraphs with tight cycles,"  Given $3 \leq k \leq s$, we say that a $k$-uniform hypergraph $C^k_s$ is a
tight cycle on $s$ vertices if there is a cyclic ordering of the vertices of
$C^k_s$ such that every $k$ consecutive vertices under this ordering form an
edge. We prove that if $k \ge 3$ and $s \ge 2k^2$, then every $k$-uniform
hypergraph on $n$ vertices with minimum codegree at least $(1/2 + o(1))n$ has
the property that every vertex is covered by a copy of $C^k_s$. Our result is
asymptotically best possible for infinitely many pairs of $s$ and $k$, e.g.
when $s$ and $k$ are coprime.
A perfect $C^k_s$-tiling is a spanning collection of vertex-disjoint copies
of $C^k_s$. When $s$ is divisible by $k$, the problem of determining the
minimum codegree that guarantees a perfect $C^k_s$-tiling was solved by a
result of Mycroft. We prove that if $k \ge 3$ and $s \ge 5k^2$ is not divisible
by $k$ and $s$ divides $n$, then every $k$-uniform hypergraph on $n$ vertices
with minimum codegree at least $(1/2 + 1/(2s) + o(1))n$ has a perfect
$C^k_s$-tiling. Again our result is asymptotically best possible for infinitely
many pairs of $s$ and $k$, e.g. when $s$ and $k$ are coprime with $k$ even.
",0,0,1,0,0,0
13292,What drives galactic magnetism?,"  We aim to use statistical analysis of a large number of various galaxies to
probe, model, and understand relations between different galaxy properties and
magnetic fields. We have compiled a sample of 55 galaxies including low-mass
dwarf and Magellanic-types, normal spirals and several massive starbursts, and
applied principal component analysis (PCA) and regression methods to assess the
impact of various galaxy properties on the observed magnetic fields. According
to PCA the global galaxy parameters (like HI, H2, and dynamical mass, star
formation rate (SFR), near-infrared luminosity, size, and rotational velocity)
are all mutually correlated and can be reduced to a single principal component.
Further PCA performed for global and intensive (not size related) properties of
galaxies (such as gas density, and surface density of the star formation rate,
SSFR), indicates that magnetic field strength B is connected mainly to the
intensive parameters, while the global parameters have only weak relationships
with B. We find that the tightest relationship of B is with SSFR, which is
described by a power-law with an index of 0.33+-0.03. The observed weaker
associations of B with galaxy dynamical mass and the rotational velocity we
interpret as indirect ones, resulting from the observed connection of the
global SFR with the available total H2 mass in galaxies. Using our sample we
constructed a diagram of B across the Hubble sequence which reveals that high
values of B are not restricted by the Hubble type. However, weaker fields
appear exclusively in later Hubble types and B as low as about 5muG is not seen
among typical spirals. The processes of generation of magnetic field in the
dwarf and Magellanic-type galaxies are similar to those in the massive spirals
and starbursts and are mainly coupled to local star-formation activity
involving the small-scale dynamo mechanism.
",0,1,0,0,0,0
10280,On spectral properties of Neuman-Poincare operator and plasmonic resonances in 3D elastostatics,"  We consider plasmon resonances and cloaking for the elastostatic system in
$\mathbb{R}^3$ via the spectral theory of Neumann-Poincaré operator. We first
derive the full spectral properties of the Neumann-Poincaré operator for the
3D elastostatic system in the spherical geometry. The spectral result is of
significant interest for its own sake, and serves as a highly nontrivial
extension of the corresponding 2D study in [8]. The derivation of the spectral
result in 3D involves much more complicated and subtle calculations and
arguments than that for the 2D case. Then we consider a 3D plasmonic structure
in elastostatics which takes a general core-shell-matrix form with the
metamaterial located in the shell. Using the obtained spectral result, we
provide an accurate characterisation of the anomalous localised resonance and
cloaking associated to such a plasmonic structure.
",0,0,1,0,0,0
11606,Learning Data Manifolds with a Cutting Plane Method,"  We consider the problem of classifying data manifolds where each manifold
represents invariances that are parameterized by continuous degrees of freedom.
Conventional data augmentation methods rely upon sampling large numbers of
training examples from these manifolds; instead, we propose an iterative
algorithm called M_{CP} based upon a cutting-plane approach that efficiently
solves a quadratic semi-infinite programming problem to find the maximum margin
solution. We provide a proof of convergence as well as a polynomial bound on
the number of iterations required for a desired tolerance in the objective
function. The efficiency and performance of M_{CP} are demonstrated in
high-dimensional simulations and on image manifolds generated from the ImageNet
dataset. Our results indicate that M_{CP} is able to rapidly learn good
classifiers and shows superior generalization performance compared with
conventional maximum margin methods using data augmentation methods.
",1,0,0,1,0,0
11011,"Ultra-Fast Relaxation, Decoherence and Localization of Photoexcited States in $π$-Conjugated Polymers: A TEBD Study","  The exciton relaxation dynamics of photoexcited electronic states in
poly($p$-phenylenevinylene) (PPV) are theoretically investigated within a
coarse-grained model, in which both the exciton and nuclear degrees of freedom
are treated quantum mechanically. The Frenkel-Holstein Hamiltonian is used to
describe the strong exciton-phonon coupling present in the system, while
external damping of the internal nuclear degrees of freedom are accounted for
by a Lindblad master equation. Numerically, the dynamics are computed using the
time evolving block decimation (TEBD) and quantum jump trajectory techniques.
The values of the model parameters physically relevant to polymer systems
naturally lead to a separation of time scales, with the ultra-fast dynamics
corresponding to energy transfer from the exciton to the internal phonon modes
(i.e., the C-C bond oscillations), while the longer time dynamics correspond to
damping of these phonon modes by the external dissipation. Associated with
these time scales, we investigate the following processes that are indicative
of the system relaxing onto the emissive chromophores of the polymer: 1)
Exciton-polaron formation occurs on an ultra-fast time scale, with the
associated exciton-phonon correlations present within half a vibrational time
period of the C-C bond oscillations. 2) Exciton decoherence is driven by the
decay in the vibrational overlaps associated with exciton-polaron formation,
occurring on the same time scale. 3) Exciton density localization is driven by
the external dissipation, arising from `wavefunction collapse' occurring as a
result of the system-environment interactions. Finally, we show how
fluorescence anisotropy measurements can be used to investigate the exciton
decoherence process during the relaxation dynamics.
",0,1,0,0,0,0
14873,Identification of a complete YPT1 Rab GTPase sequence from the fungal pathogen Colletotrichum incanum,"  Colletotrichum represent a genus of fungal species primarily known as plant
pathogens with severe economic impacts in temperate, subtropical and tropical
climates Consensus taxonomy and classification systems for Colletotrichum
species have been undergoing revision as high resolution genomic data becomes
available. Here we propose an alternative annotation that provides a complete
sequence for a Colletotrichum YPT1 gene homolog using the whole genome shotgun
sequence of Colletotrichum incanum isolated from soybean crops in Illinois,
USA.
",0,0,0,0,1,0
14466,Introducing symplectic billiards,"  In this article we introduce a simple dynamical system called symplectic
billiards. As opposed to usual/Birkhoff billiards, where length is the
generating function, for symplectic billiards symplectic area is the generating
function. We explore basic properties and exhibit several similarities, but
also differences of symplectic billiards to Birkhoff billiards.
",0,0,1,0,0,0
15536,"A Multi-Scale Analysis of 27,000 Urban Street Networks: Every US City, Town, Urbanized Area, and Zillow Neighborhood","  OpenStreetMap offers a valuable source of worldwide geospatial data useful to
urban researchers. This study uses the OSMnx software to automatically download
and analyze 27,000 US street networks from OpenStreetMap at metropolitan,
municipal, and neighborhood scales - namely, every US city and town, census
urbanized area, and Zillow-defined neighborhood. It presents empirical findings
on US urban form and street network characteristics, emphasizing measures
relevant to graph theory, transportation, urban design, and morphology such as
structure, connectedness, density, centrality, and resilience. In the past,
street network data acquisition and processing have been challenging and ad
hoc. This study illustrates the use of OSMnx and OpenStreetMap to consistently
conduct street network analysis with extremely large sample sizes, with clearly
defined network definitions and extents for reproducibility, and using
nonplanar, directed graphs. These street networks and measures data have been
shared in a public repository for other researchers to use.
",1,1,0,0,0,0
11660,Two-walks degree assortativity in graphs and networks,"  Degree ssortativity is the tendency for nodes of high degree (resp.low
degree) in a graph to be connected to high degree nodes (resp. to low degree
ones). It is sually quantified by the Pearson correlation coefficient of the
degree-degree correlation. Here we extend this concept to account for the
effect of second neighbours to a given node in a graph. That is, we consider
the two-walks degree of a node as the sum of all the degrees of its adjacent
nodes. The two-walks degree assortativity of a graph is then the Pearson
correlation coefficient of the two-walks degree-degree correlation. We found
here analytical expression for this two-walks degree assortativity index as a
function of contributing subgraphs. We then study all the 261,000 connected
graphs with 9 nodes and observe the existence of assortative-assortative and
disassortative-disassortative graphs according to degree and two-walks degree,
respectively. More surprinsingly, we observe a class of graphs which are degree
disassortative and two-walks degree assortative. We explain the existence of
some of these graphs due to the presence of certain topological features, such
as a node of low-degree connected to high-degree ones. More importantly, we
study a series of 49 real-world networks, where we observe the existence of the
disassortative-assortative class in several of them. In particular, all
biological networks studied here were in this class. We also conclude that no
graphs/networks are possible with assortative-disassortative structure.
",1,1,0,0,0,0
16974,A Polynomial-Time Algorithm for Solving the Minimal Observability Problem in Conjunctive Boolean Networks,"  Many complex systems in biology, physics, and engineering include a large
number of state-variables, and measuring the full state of the system is often
impossible. Typically, a set of sensors is used to measure part of the
state-variables. A system is called observable if these measurements allow to
reconstruct the entire state of the system. When the system is not observable,
an important and practical problem is how to add a \emph{minimal} number of
sensors so that the system becomes observable. This minimal observability
problem is practically useful and theoretically interesting, as it pinpoints
the most informative nodes in the system. We consider the minimal observability
problem for an important special class of Boolean networks, called conjunctive
Boolean networks (CBNs). Using a graph-theoretic approach, we provide a
necessary and sufficient condition for observability of a CBN with $n$
state-variables, and an efficient~$O(n^2)$-time algorithm for solving the
minimal observability problem. We demonstrate the usefulness of these results
by studying the properties of a class of random CBNs.
",1,0,1,0,0,0
2863,Resource Allocation for Containing Epidemics from Temporal Network Data,"  We study the problem of containing epidemic spreading processes in temporal
networks. We specifically focus on the problem of finding a resource allocation
to suppress epidemic infection, provided that an empirical time-series data of
connectivities between nodes is available. Although this problem is of
practical relevance, it has not been clear how an empirical time-series data
can inform our strategy of resource allocations, due to the computational
complexity of the problem. In this direction, we present a computationally
efficient framework for finding a resource allocation that satisfies a given
budget constraint and achieves a given control performance. The framework is
based on convex programming and, moreover, allows the performance measure to be
described by a wide class of functionals called posynomials with nonnegative
exponents. We illustrate our theoretical results using a data of temporal
interaction networks within a primary school.
",1,0,0,0,0,0
5208,The Odyssey Approach for Optimizing Federated SPARQL Queries,"  Answering queries over a federation of SPARQL endpoints requires combining
data from more than one data source. Optimizing queries in such scenarios is
particularly challenging not only because of (i) the large variety of possible
query execution plans that correctly answer the query but also because (ii)
there is only limited access to statistics about schema and instance data of
remote sources. To overcome these challenges, most federated query engines rely
on heuristics to reduce the space of possible query execution plans or on
dynamic programming strategies to produce optimal plans. Nevertheless, these
plans may still exhibit a high number of intermediate results or high execution
times because of heuristics and inaccurate cost estimations. In this paper, we
present Odyssey, an approach that uses statistics that allow for a more
accurate cost estimation for federated queries and therefore enables Odyssey to
produce better query execution plans. Our experimental results show that
Odyssey produces query execution plans that are better in terms of data
transfer and execution time than state-of-the-art optimizers. Our experiments
using the FedBench benchmark show execution time gains of at least 25 times on
average.
",1,0,0,0,0,0
15281,Brain EEG Time Series Selection: A Novel Graph-Based Approach for Classification,"  Brain Electroencephalography (EEG) classification is widely applied to
analyze cerebral diseases in recent years. Unfortunately, invalid/noisy EEGs
degrade the diagnosis performance and most previously developed methods ignore
the necessity of EEG selection for classification. To this end, this paper
proposes a novel maximum weight clique-based EEG selection approach, named
mwcEEGs, to map EEG selection to searching maximum similarity-weighted cliques
from an improved Fréchet distance-weighted undirected EEG graph
simultaneously considering edge weights and vertex weights. Our mwcEEGs
improves the classification performance by selecting intra-clique pairwise
similar and inter-clique discriminative EEGs with similarity threshold
$\delta$. Experimental results demonstrate the algorithm effectiveness compared
with the state-of-the-art time series selection algorithms on real-world EEG
datasets.
",0,0,0,1,1,0
1643,A functional perspective on emergent supersymmetry,"  We investigate the emergence of ${\cal N}=1$ supersymmetry in the long-range
behavior of three-dimensional parity-symmetric Yukawa systems. We discuss a
renormalization approach that manifestly preserves supersymmetry whenever such
symmetry is realized, and use it to prove that supersymmetry-breaking operators
are irrelevant, thus proving that such operators are suppressed in the
infrared. All our findings are illustrated with the aid of the
$\epsilon$-expansion and a functional variant of perturbation theory, but we
provide numerical estimates of critical exponents that are based on the
non-perturbative functional renormalization group.
",0,1,0,0,0,0
2703,A unifying framework for the modelling and analysis of STR DNA samples arising in forensic casework,"  This paper presents a new framework for analysing forensic DNA samples using
probabilistic genotyping. Specifically it presents a mathematical framework for
specifying and combining the steps in producing forensic casework
electropherograms of short tandem repeat loci from DNA samples. It is
applicable to both high and low template DNA samples, that is, samples
containing either high or low amounts DNA. A specific model is developed within
the framework, by way of particular modelling assumptions and approximations,
and its interpretive power presented on examples using simulated data and data
from a publicly available dataset. The framework relies heavily on the use of
univariate and multivariate probability generating functions. It is shown that
these provide a succinct and elegant mathematical scaffolding to model the key
steps in the process. A significant development in this paper is that of new
numerical methods for accurately and efficiently evaluating the probability
distribution of amplicons arising from the polymerase chain reaction process,
which is modelled as a discrete multi-type branching process. Source code in
the scripting languages Python, R and Julia is provided for illustration of
these methods. These new developments will be of general interest to persons
working outside the province of forensic DNA interpretation that this paper
focuses on.
",0,0,0,1,1,0
7730,Overlapping community detection using superior seed set selection in social networks,"  Community discovery in the social network is one of the tremendously
expanding areas which earn interest among researchers for the past one decade.
There are many already existing algorithms. However, new seed-based algorithms
establish an emerging drift in this area. The basic idea behind these
strategies is to identify exceptional nodes in the given network, called seeds,
around which communities can be located. This paper proposes a blended strategy
for locating suitable superior seed set by applying various centrality measures
and using them to find overlapping communities. The examination of the
algorithm has been performed regarding the goodness of the identified
communities with the help of intra-cluster density and inter-cluster density.
Finally, the runtime of the proposed algorithm has been compared with the
existing community detection algorithms showing remarkable improvement.
",1,0,0,0,0,0
1969,A Las Vegas algorithm to solve the elliptic curve discrete logarithm problem,"  In this paper, we describe a new Las Vegas algorithm to solve the elliptic
curve discrete logarithm problem. The algorithm depends on a property of the
group of rational points of an elliptic curve and is thus not a generic
algorithm. The algorithm that we describe has some similarities with the most
powerful index-calculus algorithm for the discrete logarithm problem over a
finite field.
",1,0,1,0,0,0
13208,The inseparability of sampling and time and its influence on attempts to unify the molecular and fossil records,"  The two major approaches to studying macroevolution in deep time are the
fossil record and reconstructed relationships among extant taxa from molecular
data. Results based on one approach sometimes conflict with those based on the
other, with inconsistencies often attributed to inherent flaws of one (or the
other) data source. What is unquestionable is that both the molecular and
fossil records are limited reflections of the same evolutionary history, and
any contradiction between them represents a failure of our existing models to
explain the patterns we observe. Fortunately, the different limitations of each
record provide an opportunity to test or calibrate the other, and new
methodological developments leverage both records simultaneously. However, we
must reckon with the distinct relationships between sampling and time in the
fossil record and molecular phylogenies. These differences impact our
recognition of baselines, and the analytical incorporation of age estimate
uncertainty. These differences in perspective also influence how different
practitioners view the past and evolutionary time itself, bearing important
implications for the generality of methodological advancements, and differences
in the philosophical approach to macroevolutionary theory across fields.
",0,0,0,0,1,0
4035,Approximate Ranking from Pairwise Comparisons,"  A common problem in machine learning is to rank a set of n items based on
pairwise comparisons. Here ranking refers to partitioning the items into sets
of pre-specified sizes according to their scores, which includes identification
of the top-k items as the most prominent special case. The score of a given
item is defined as the probability that it beats a randomly chosen other item.
Finding an exact ranking typically requires a prohibitively large number of
comparisons, but in practice, approximate rankings are often adequate.
Accordingly, we study the problem of finding approximate rankings from pairwise
comparisons. We analyze an active ranking algorithm that counts the number of
comparisons won, and decides whether to stop or which pair of items to compare
next, based on confidence intervals computed from the data collected in
previous steps. We show that this algorithm succeeds in recovering approximate
rankings using a number of comparisons that is close to optimal up to
logarithmic factors. We also present numerical results, showing that in
practice, approximation can drastically reduce the number of comparisons
required to estimate a ranking.
",0,0,0,1,0,0
13588,Photoinduced filling of near nodal gap in Bi$_2$Sr$_2$CaCu$_2$O$_{8+δ}$,"  We report time and angle resolved spectroscopic measurements in optimally
doped Bi$_2$Sr$_2$CaCu$_2$O$_{8+\delta}$. The spectral function is monitored as
a function of temperature, photoexcitation density and delay time from the pump
pulse. According to our data, the superconducting gap becomes slightly stiffer
when moving off the nodal direction. The nodal quasiparticles develop a faster
dynamics when pumping the superconductor with a fluence that is large enough to
induce the total collapse of the gap. We discuss the observed relaxation in
terms of a dynamical reformation of Cooper pairs.
",0,1,0,0,0,0
12506,Exact time-dependent exchange-correlation potential in electron scattering processes,"  We identify peak and valley structures in the exact exchange-correlation
potential of time-dependent density functional theory that are crucial for
time-resolved electron scattering in a model one-dimensional system. These
structures are completely missed by adiabatic approximations which consequently
significantly underestimate the scattering probability. A recently-proposed
non-adiabatic approximation is shown to correctly capture the approach of the
electron to the target when the initial Kohn-Sham state is chosen judiciously,
and is more accurate than standard adiabatic functionals, but it ultimately
fails to accurately capture reflection. These results may explain the
underestimate of scattering probabilities in some recent studies on molecules
and surfaces.
",0,1,0,0,0,0
12138,STWalk: Learning Trajectory Representations in Temporal Graphs,"  Analyzing the temporal behavior of nodes in time-varying graphs is useful for
many applications such as targeted advertising, community evolution and outlier
detection. In this paper, we present a novel approach, STWalk, for learning
trajectory representations of nodes in temporal graphs. The proposed framework
makes use of structural properties of graphs at current and previous time-steps
to learn effective node trajectory representations. STWalk performs random
walks on a graph at a given time step (called space-walk) as well as on graphs
from past time-steps (called time-walk) to capture the spatio-temporal behavior
of nodes. We propose two variants of STWalk to learn trajectory
representations. In one algorithm, we perform space-walk and time-walk as part
of a single step. In the other variant, we perform space-walk and time-walk
separately and combine the learned representations to get the final trajectory
embedding. Extensive experiments on three real-world temporal graph datasets
validate the effectiveness of the learned representations when compared to
three baseline methods. We also show the goodness of the learned trajectory
embeddings for change point detection, as well as demonstrate that arithmetic
operations on these trajectory representations yield interesting and
interpretable results.
",1,0,0,1,0,0
6600,The symplectic approach of gauged linear $σ$-model,"  Witten's Gauged Linear $\sigma$-Model (GLSM) unifies the Gromov-Witten theory
and the Landau-Ginzburg theory, and provides a global perspective on mirror
symmetry. In this article, we summarize a mathematically rigorous construction
of the GLSM in the geometric phase using methods from symplectic geometry.
",0,0,1,0,0,0
20218,Stability interchanges in a curved Sitnikov problem,"  We consider a curved Sitnikov problem, in which an infinitesimal particle
moves on a circle under the gravitational influence of two equal masses in
Keplerian motion within a plane perpendicular to that circle. There are two
equilibrium points, whose stability we are studying. We show that one of the
equilibrium points undergoes stability interchanges as the semi-major axis of
the Keplerian ellipses approaches the diameter of that circle. To derive this
result, we first formulate and prove a general theorem on stability
interchanges, and then we apply it to our model. The motivation for our model
resides with the $n$-body problem in spaces of constant curvature.
",0,1,1,0,0,0
5736,Controllability to Equilibria of the 1-D Fokker-Planck Equation with Zero-Flux Boundary Condition,"  We consider the problem of controlling the spatiotemporal probability
distribution of a robotic swarm that evolves according to a reflected diffusion
process, using the space- and time-dependent drift vector field parameter as
the control variable. In contrast to previous work on control of the
Fokker-Planck equation, a zero-flux boundary condition is imposed on the
partial differential equation that governs the swarm probability distribution,
and only bounded vector fields are considered to be admissible as control
parameters. Under these constraints, we show that any initial probability
distribution can be transported to a target probability distribution under
certain assumptions on the regularity of the target distribution. In
particular, we show that if the target distribution is (essentially) bounded,
has bounded first-order and second-order partial derivatives, and is bounded
from below by a strictly positive constant, then this distribution can be
reached exactly using a drift vector field that is bounded in space and time.
Our proof is constructive and based on classical linear semigroup theoretic
concepts.
",1,0,1,0,0,0
3911,Some Remarks about the Complexity of Epidemics Management,"  Recent outbreaks of Ebola, H1N1 and other infectious diseases have shown that
the assumptions underlying the established theory of epidemics management are
too idealistic. For an improvement of procedures and organizations involved in
fighting epidemics, extended models of epidemics management are required. The
necessary extensions consist in a representation of the management loop and the
potential frictions influencing the loop. The effects of the non-deterministic
frictions can be taken into account by including the measures of robustness and
risk in the assessment of management options. Thus, besides of the increased
structural complexity resulting from the model extensions, the computational
complexity of the task of epidemics management - interpreted as an optimization
problem - is increased as well. This is a serious obstacle for analyzing the
model and may require an additional pre-processing enabling a simplification of
the analysis process. The paper closes with an outlook discussing some
forthcoming problems.
",0,1,0,0,0,0
19605,Unbiased inference for discretely observed hidden Markov model diffusions,"  We develop an importance sampling (IS) type estimator for Bayesian joint
inference on the model parameters and latent states of a class of hidden Markov
models. The hidden state dynamics is a diffusion process and noisy observations
are obtained at discrete points in time. We suppose that the diffusion dynamics
can not be simulated exactly and hence one must time-discretise the diffusion.
Our approach is based on particle marginal Metropolis--Hastings, particle
filters, and multilevel Monte Carlo. The resulting IS type estimator leads to
inference without a bias from the time-discretisation. We give convergence
results and recommend allocations for algorithm inputs. In contrast to existing
unbiased methods requiring strong conditions on the diffusion and tailored
solutions, our method relies on standard Euler approximations of the diffusion.
Our method is parallelisable, and can be computationally efficient. The
user-friendly approach is illustrated with two examples.
",0,0,0,1,0,0
11934,Multidimensional upwind hydrodynamics on unstructured meshes using Graphics Processing Units I. Two-dimensional uniform meshes,"  We present a new method for numerical hydrodynamics which uses a
multidimensional generalisation of the Roe solver and operates on an
unstructured triangular mesh. The main advantage over traditional methods based
on Riemann solvers, which commonly use one-dimensional flux estimates as
building blocks for a multidimensional integration, is its inherently
multidimensional nature, and as a consequence its ability to recognise
multidimensional stationary states that are not hydrostatic. A second novelty
is the focus on Graphics Processing Units (GPUs). By tailoring the algorithms
specifically to GPUs we are able to get speedups of 100-250 compared to a
desktop machine. We compare the multidimensional upwind scheme to a
traditional, dimensionally split implementation of the Roe solver on several
test problems, and we find that the new method significantly outperforms the
Roe solver in almost all cases. This comes with increased computational costs
per time step, which makes the new method approximately a factor of 2 slower
than a dimensionally split scheme acting on a structured grid.
",0,1,0,0,0,0
7470,New results on sum-product type growth over fields,"  We prove a range of new sum-product type growth estimates over a general
field $\mathbb{F}$, in particular the special case $\mathbb{F}=\mathbb{F}_p$.
They are unified by the theme of ""breaking the $3/2$ threshold"", epitomising
the previous state of the art. These estimates stem from specially suited
applications of incidence bounds over $\mathbb{F}$, which apply to higher
moments of representation functions.
We establish the estimate $|R[A]| \gtrsim |A|^{8/5}$ for cardinality of the
set $R[A]$ of distinct cross-ratios defined by triples of elements of a
(sufficiently small if $\mathbb{F}$ has positive characteristic, similarly for
the rest of the estimates) set $A\subset \mathbb{F}$, pinned at infinity. The
cross-ratio naturally arises in various sum-product type questions of
projective nature and is the unifying concept underlying most of our results.
It enables one to take advantage of its symmetry properties as an onset of
growth of, for instance, products of difference sets. The geometric nature of
the cross-ratio enables us to break the version of the above threshold for the
minimum number of distinct triangle areas $Ouu'$, defined by points $u,u'$ of a
non-collinear point set $P\subset \mathbb{F}^2$.
Another instance of breaking the threshold is showing that if $A$ is
sufficiently small and has additive doubling constant $M$, then $|AA|\gtrsim
M^{-2}|A|^{14/9}$. This result has a second moment version, which allows for
new upper bounds for the number of collinear point triples in the set $A\times
A\subset \mathbb{F}^2$, the quantity often arising in applications of geometric
incidence estimates.
",0,0,1,0,0,0
9130,In-silico Feedback Control of a MIMO Synthetic Toggle Switch via Pulse-Width Modulation,"  The synthetic toggle switch, first proposed by Gardner & Collins [1] is a
MIMO control system that can be controlled by varying the concentrations of two
inducer molecules, aTc and IPTG, to achieve a desired level of expression of
the two genes it comprises. It has been shown [2] that this can be accomplished
through an open-loop external control strategy where the two inputs are
selected as mutually exclusive periodic pulse waves of appropriate amplitude
and duty-cycle. In this paper, we use a recently derived average model of the
genetic toggle switch subject to these inputs to synthesize new feedback
control approaches that adjust the inputs duty-cycle in real-time via two
different possible strategies, a model based hybrid PI-PWM approach and a
so-called Zero-Average dynamics (ZAD) controller. The controllers are validated
in-silico via both deterministic and stochastic simulations (SSA) illustrating
the advantages and limitations of each strategy
",1,0,0,0,0,0
2394,Combining Neural Networks and Tree Search for Task and Motion Planning in Challenging Environments,"  We consider task and motion planning in complex dynamic environments for
problems expressed in terms of a set of Linear Temporal Logic (LTL)
constraints, and a reward function. We propose a methodology based on
reinforcement learning that employs deep neural networks to learn low-level
control policies as well as task-level option policies. A major challenge in
this setting, both for neural network approaches and classical planning, is the
need to explore future worlds of a complex and interactive environment. To this
end, we integrate Monte Carlo Tree Search with hierarchical neural net control
policies trained on expressive LTL specifications. This paper investigates the
ability of neural networks to learn both LTL constraints and control policies
in order to generate task plans in complex environments. We demonstrate our
approach in a simulated autonomous driving setting, where a vehicle must drive
down a road in traffic, avoid collisions, and navigate an intersection, all
while obeying given rules of the road.
",1,0,0,0,0,0
19100,Recursive simplex stars,"  This paper proposes a new method which builds a simplex based approximation
of a $d-1$-dimensional manifold $M$ separating a $d$-dimensional compact set
into two parts, and an efficient algorithm classifying points according to this
approximation. In a first variant, the approximation is made of simplices that
are defined in the cubes of a regular grid covering the compact set, from
boundary points that approximate the intersection between $M$ and the edges of
the cubes. All the simplices defined in a cube share the barycentre of the
boundary points located in the cube and include simplices similarly defined in
cube facets, and so on recursively. In a second variant, the Kuhn triangulation
is used to break the cubes into simplices and the approximation is defined in
these simplices from the boundary points computed on their edges, with the same
principle. Both the approximation in cubes and in simplices define a separating
surface on the whole grid and classifying a point on one side or the other of
this surface requires only a small number (at most $d$) of simple tests. Under
some conditions on the definition of the boundary points and on the reach of
$M$, for both variants the Hausdorff distance between $M$ and its approximation
decreases like $\mathcal{O}(d n_G^{-2})$, where $n_G$ is the number of points
on each axis of the grid. The approximation in cubes requires computing less
boundary points than the approximation in simplices but the latter is always a
manifold and is more accurate for a given value of $n_G$. The paper reports
tests of the method when varying $n_G$ and the dimensionality of the space (up
to 9).
",1,0,0,0,0,0
19659,Contributors profile modelization in crowdsourcing platforms,"  The crowdsourcing consists in the externalisation of tasks to a crowd of
people remunerated to execute this ones. The crowd, usually diversified, can
include users without qualification and/or motivation for the tasks. In this
paper we will introduce a new method of user expertise modelization in the
crowdsourcing platforms based on the theory of belief functions in order to
identify serious and qualificated users.
",1,0,0,0,0,0
15475,Relativistic verifiable delegation of quantum computation,"  The importance of being able to verify quantum computation delegated to
remote servers increases with recent development of quantum technologies. In
some of the proposed protocols for this task, a client delegates her quantum
computation to non-communicating servers. The fact that the servers do not
communicate is not physically justified and it is essential for the proof of
security of such protocols. For the best of our knowledge, we present in this
work the first verifiable delegation scheme where a classical client delegates
her quantum computation to two entangled servers that are allowed to
communicate, but respecting the plausible assumption that information cannot be
propagated faster than speed of light. We achieve this result by proposing the
first one-round two-prover game for the Local Hamiltonian problem where provers
only need polynomial time quantum computation and access to copies of the
groundstate of the Hamiltonian.
",1,0,0,0,0,0
16824,Evidence for mixed rationalities in preference formation,"  Understanding the mechanisms underlying the formation of cultural traits,
such as preferences, opinions and beliefs is an open challenge. Trait formation
is intimately connected to cultural dynamics, which has been the focus of a
variety of quantitative models. Recently, some studies have emphasized the
importance of connecting those models to snapshots of cultural dynamics that
are empirically accessible. By analyzing data obtained from different sources,
it has been suggested that culture has properties that are universally present,
and that empirical cultural states differ systematically from randomized
counterparts. Hence, a question about the mechanism responsible for the
observed patterns naturally arises. This study proposes a stochastic structural
model for generating cultural states that retain those robust, empirical
properties. One ingredient of the model, already used in previous work, assumes
that every individual's set of traits is partly dictated by one of several,
universal ""rationalities"", informally postulated by several social science
theories. The second, new ingredient taken from the same theories assumes that,
apart from a dominant rationality, each individual also has a certain exposure
to the other rationalities. It is shown that both ingredients are required for
reproducing the empirical regularities. This key result suggests that the
effects of cultural dynamics in the real world can be described as an interplay
of multiple, mixing rationalities, and thus provides indirect evidence for the
class of social science theories postulating such mixing. The model should be
seen as a static, effective description of culture, while a dynamical, more
fundamental description is left for future research.
",1,1,0,0,0,0
4442,Factor Analysis for Spectral Estimation,"  Power spectrum estimation is an important tool in many applications, such as
the whitening of noise. The popular multitaper method enjoys significant
success, but fails for short signals with few samples. We propose a statistical
model where a signal is given by a random linear combination of fixed, yet
unknown, stochastic sources. Given multiple such signals, we estimate the
subspace spanned by the power spectra of these fixed sources. Projecting
individual power spectrum estimates onto this subspace increases estimation
accuracy. We provide accuracy guarantees for this method and demonstrate it on
simulated and experimental data from cryo-electron microscopy.
",0,0,1,1,0,0
20304,From sudden quench to adiabatic dynamics in the attractive Hubbard model,"  We study the crossover between the sudden quench limit and the adiabatic
dynamics of superconducting states in the attractive Hubbard model. We focus on
the dynamics induced by the change of the attractive interaction during a
finite ramp time which is varied in order to track the evolution of the
dynamical phase diagram from the sudden quench to the equilibrium limit. Two
different dynamical regimes are realized for quenches towards weak and strong
coupling interactions. At weak coupling the dynamics depends only on the energy
injected into the system, whereas a dynamics retaining memory of the initial
state takes place at strong coupling. We show that this is related to a sharp
transition between a weak and a strong coupling quench dynamical regime, which
defines the boundaries beyond which a dynamics independent from the initial
state is recovered. Comparing the dynamics in the superconducting and
non-superconducting phases we argue that this is due to the lack of an
adiabatic connection to the equilibrium ground state for non-equilibrium
superconducting states in the strong coupling quench regime.
",0,1,0,0,0,0
2091,Thermophysical characteristics of the large main-belt asteroid (349) Dembowska,"  (349) Dembowska, a large, bright main-belt asteroid, has a fast rotation and
oblique spin axis. It may have experienced partial melting and differentiation.
We constrain Dembowska's thermophysical properties, e.g., thermal inertia,
roughness fraction, geometric albedo and effective diameter within 3$\sigma$
uncertainty of $\Gamma=20^{+12}_{-7}\rm~Jm^{-2}s^{-0.5}K^{-1}$, $f_{\rm
r}=0.25^{+0.60}_{-0.25}$, $p_{\rm v}=0.309^{+0.026}_{-0.038}$, and $D_{\rm
eff}=155.8^{+7.5}_{-6.2}\rm~km$, by utilizing the Advanced Thermophysical Model
(ATPM) to analyse four sets of thermal infrared data obtained by IRAS, AKARI,
WISE and Subaru/COMICS at different epochs. In addition, by modeling the
thermal lightcurve observed by WISE, we obtain the rotational phases of each
dataset. These rotationally resolved data do not reveal significant variations
of thermal inertia and roughness across the surface, indicating the surface of
Dembowska should be covered by a dusty regolith layer with few rocks or
boulders. Besides, the low thermal inertia of Dembowska show no significant
difference with other asteroids larger than 100 km, indicating the dynamical
lives of these large asteroids are long enough to make the surface to have
sufficiently low thermal inertia. Furthermore, based on the derived surface
thermophysical properties, as well as the known orbital and rotational
parameters, we can simulate Dembowska's surface and subsurface temperature
throughout its orbital period. The surface temperature varies from $\sim40$ K
to $\sim220$ K, showing significant seasonal variation, whereas the subsurface
temperature achieves equilibrium temperature about $120\sim160$ K below
$30\sim50$ cm depth.
",0,1,0,0,0,0
5793,"Portable, high-performance containers for HPC","  Building and deploying software on high-end computing systems is a
challenging task. High performance applications have to reliably run across
multiple platforms and environments, and make use of site-specific resources
while resolving complicated software-stack dependencies. Containers are a type
of lightweight virtualization technology that attempt to solve this problem by
packaging applications and their environments into standard units of software
that are: portable, easy to build and deploy, have a small footprint, and low
runtime overhead. In this work we present an extension to the container runtime
of Shifter that provides containerized applications with a mechanism to access
GPU accelerators and specialized networking from the host system, effectively
enabling performance portability of containers across HPC resources. The
presented extension makes possible to rapidly deploy high-performance software
on supercomputers from containerized applications that have been developed,
built, and tested in non-HPC commodity hardware, e.g. the laptop or workstation
of a researcher.
",1,0,0,0,0,0
17250,Giant paramagnetism induced valley polarization of electrons in charge-tunable monolayer MoSe2,"  For applications exploiting the valley pseudospin degree of freedom in
transition metal dichalcogenide monolayers, efficient preparation of electrons
or holes in a single valley is essential. Here, we show that a magnetic field
of 7 Tesla leads to a near-complete valley polarization of electrons in MoSe2
monolayer with a density 1.6x10^{12} cm^{-2}; in the absence of exchange
interactions favoring single-valley occupancy, a similar degree of valley
polarization would have required a pseudospin g-factor exceeding 40. To
investigate the magnetic response, we use polarization resolved
photoluminescence as well as resonant reflection measurements. In the latter,
we observe gate voltage dependent transfer of oscillator strength from the
exciton to the attractive-Fermi-polaron: stark differences in the spectrum of
the two light helicities provide a confirmation of valley polarization. Our
findings suggest an interaction induced giant paramagnetic response of MoSe2,
which paves the way for valleytronics applications.
",0,1,0,0,0,0
4521,End-to-End Learning of Geometry and Context for Deep Stereo Regression,"  We propose a novel deep learning architecture for regressing disparity from a
rectified pair of stereo images. We leverage knowledge of the problem's
geometry to form a cost volume using deep feature representations. We learn to
incorporate contextual information using 3-D convolutions over this volume.
Disparity values are regressed from the cost volume using a proposed
differentiable soft argmin operation, which allows us to train our method
end-to-end to sub-pixel accuracy without any additional post-processing or
regularization. We evaluate our method on the Scene Flow and KITTI datasets and
on KITTI we set a new state-of-the-art benchmark, while being significantly
faster than competing approaches.
",1,0,0,0,0,0
14060,DLBI: Deep learning guided Bayesian inference for structure reconstruction of super-resolution fluorescence microscopy,"  Super-resolution fluorescence microscopy, with a resolution beyond the
diffraction limit of light, has become an indispensable tool to directly
visualize biological structures in living cells at a nanometer-scale
resolution. Despite advances in high-density super-resolution fluorescent
techniques, existing methods still have bottlenecks, including extremely long
execution time, artificial thinning and thickening of structures, and lack of
ability to capture latent structures. Here we propose a novel deep learning
guided Bayesian inference approach, DLBI, for the time-series analysis of
high-density fluorescent images. Our method combines the strength of deep
learning and statistical inference, where deep learning captures the underlying
distribution of the fluorophores that are consistent with the observed
time-series fluorescent images by exploring local features and correlation
along time-axis, and statistical inference further refines the ultrastructure
extracted by deep learning and endues physical meaning to the final image.
Comprehensive experimental results on both real and simulated datasets
demonstrate that our method provides more accurate and realistic local patch
and large-field reconstruction than the state-of-the-art method, the 3B
analysis, while our method is more than two orders of magnitude faster. The
main program is available at this https URL
",0,0,0,1,0,0
14186,Giant Thermal Conductivity Enhancement in Multilayer MoS2 under Highly Compressive Strain,"  Multilayer MoS2 possesses highly anisotropic thermal conductivities along
in-plane and cross-plane directions that could hamper heat dissipation in
electronics. With about 9% cross-plane compressive strain created by
hydrostatic pressure in a diamond anvil cell, we observed about 12 times
increase in the cross-plane thermal conductivity of multilayer MoS2. Our
experimental and theoretical studies reveal that this drastic change arises
from the greatly strengthened interlayer interaction and heavily modified
phonon dispersions along cross-plane direction, with negligible contribution
from electronic thermal conductivity, despite its enhancement of 4 orders of
magnitude. The anisotropic thermal conductivity in the multilayer MoS2 at
ambient environment becomes almost isotropic under highly compressive strain,
effectively transitioning from 2D to 3D heat dissipation. This strain tuning
approach also makes possible parallel tuning of structural, thermal and
electrical properties, and can be extended to the whole family of 2D Van der
Waals solids, down to two layer systems.
",0,1,0,0,0,0
15064,Observational Learning by Reinforcement Learning,"  Observational learning is a type of learning that occurs as a function of
observing, retaining and possibly replicating or imitating the behaviour of
another agent. It is a core mechanism appearing in various instances of social
learning and has been found to be employed in several intelligent species,
including humans. In this paper, we investigate to what extent the explicit
modelling of other agents is necessary to achieve observational learning
through machine learning. Especially, we argue that observational learning can
emerge from pure Reinforcement Learning (RL), potentially coupled with memory.
Through simple scenarios, we demonstrate that an RL agent can leverage the
information provided by the observations of an other agent performing a task in
a shared environment. The other agent is only observed through the effect of
its actions on the environment and never explicitly modeled. Two key aspects
are borrowed from observational learning: i) the observer behaviour needs to
change as a result of viewing a 'teacher' (another agent) and ii) the observer
needs to be motivated somehow to engage in making use of the other agent's
behaviour. The later is naturally modeled by RL, by correlating the learning
agent's reward with the teacher agent's behaviour.
",1,0,0,1,0,0
2484,Fixing an error in Caponnetto and de Vito (2007),"  The seminal paper of Caponnetto and de Vito (2007) provides minimax-optimal
rates for kernel ridge regression in a very general setting. Its proof,
however, contains an error in its bound on the effective dimensionality. In
this note, we explain the mistake, provide a correct bound, and show that the
main theorem remains true.
",0,0,1,1,0,0
19800,Astronomical random numbers for quantum foundations experiments,"  Photons from distant astronomical sources can be used as a classical source
of randomness to improve fundamental tests of quantum nonlocality,
wave-particle duality, and local realism through Bell's inequality and
delayed-choice quantum eraser tests inspired by Wheeler's cosmic-scale
Mach-Zehnder interferometer gedankenexperiment. Such sources of random numbers
may also be useful for information-theoretic applications such as key
distribution for quantum cryptography. Building on the design of an
""astronomical random-number generator"" developed for the recent ""cosmic Bell""
experiment [Handsteiner et al., Phys. Rev. Lett. 118, 060401 (2017)], in this
paper we report on the design and characterization of a device that, with
20-nanosecond latency, outputs a bit based on whether the wavelength of an
incoming photon is greater than or less than 700 nm. Using the one-meter
telescope at the Jet Propulsion Laboratory (JPL) Table Mountain Observatory, we
generated random bits from astronomical photons in both color channels from 50
stars of varying color and magnitude, and from 12 quasars with redshifts up to
$z = 3.9$. With stars, we achieved bit rates of $\sim 1 \times 10^6$ Hz /
m$^2$, limited by saturation for our single-photon detectors, and with quasars
of magnitudes between 12.9 and 16, we achieved rates between $\sim 10^2$ and $2
\times 10^3$ Hz /m$^2$. For bright quasars, the resulting bitstreams exhibit
sufficiently low amounts of statistical predictability as quantified by the
mutual information. In addition, a sufficiently high fraction of bits generated
are of true astronomical origin in order to address both the locality and
freedom-of-choice loopholes when used to set the measurement settings in a test
of the Bell-CHSH inequality.
",0,1,0,0,0,0
650,On fibering compact manifold over the circle,"  In this paper, we show that any compact manifold that carries a
SL(n;R)-foliation is fibered on the circle S^1.
",0,0,1,0,0,0
9774,On the Parallel Parameterized Complexity of the Graph Isomorphism Problem,"  In this paper, we study the parallel and the space complexity of the graph
isomorphism problem (\GI{}) for several parameterizations. Let
$\mathcal{H}=\{H_1,H_2,\cdots,H_l\}$ be a finite set of graphs where
$|V(H_i)|\leq d$ for all $i$ and for some constant $d$. Let $\mathcal{G}$ be an
$\mathcal{H}$-free graph class i.e., none of the graphs $G\in \mathcal{G}$
contain any $H \in \mathcal{H}$ as an induced subgraph. We show that \GI{}
parameterized by vertex deletion distance to $\mathcal{G}$ is in a
parameterized version of $\AC^1$, denoted $\PL$-$\AC^1$, provided the colored
graph isomorphism problem for graphs in $\mathcal{G}$ is in $\AC^1$. From this,
we deduce that \GI{} parameterized by the vertex deletion distance to cographs
is in $\PL$-$\AC^1$.
The parallel parameterized complexity of \GI{} parameterized by the size of a
feedback vertex set remains an open problem. Towards this direction we show
that the graph isomorphism problem is in $\PL$-$\TC^0$ when parameterized by
vertex cover or by twin-cover.
Let $\mathcal{G}'$ be a graph class such that recognizing graphs from
$\mathcal{G}'$ and the colored version of \GI{} for $\mathcal{G}'$ is in
logspace ($\L$). We show that \GI{} for bounded vertex deletion distance to
$\mathcal{G}'$ is in $\L$. From this, we obtain logspace algorithms for \GI{}
for graphs with bounded vertex deletion distance to interval graphs and graphs
with bounded vertex deletion distance to cographs.
",1,0,0,0,0,0
11133,Toward Common Components for Open Workflow Systems,"  The role of scalable high-performance workflows and flexible workflow
management systems that can support multiple simulations will continue to
increase in importance. For example, with the end of Dennard scaling, there is
a need to substitute a single long running simulation with multiple repeats of
shorter simulations, or concurrent replicas. Further, many scientific problems
involve ensembles of simulations in order to solve a higher-level problem or
produce statistically meaningful results. However most supercomputing software
development and performance enhancements have focused on optimizing single-
simulation performance. On the other hand, there is a strong inconsistency in
the definition and practice of workflows and workflow management systems. This
inconsistency often centers around the difference between several different
types of workflows, including modeling and simulation, grid, uncertainty
quantification, and purely conceptual workflows. This work explores this
phenomenon by examining the different types of workflows and workflow
management systems, reviewing the perspective of a large supercomputing
facility, examining the common features and problems of workflow management
systems, and finally presenting a proposed solution based on the concept of
common building blocks. The implications of the continuing proliferation of
workflow management systems and the lack of interoperability between these
systems are discussed from a practical perspective. In doing so, we have begun
an investigation of the design and implementation of open workflow systems for
supercomputers based upon common components.
",1,0,0,0,0,0
17284,Generator Reversal,"  We consider the problem of training generative models with deep neural
networks as generators, i.e. to map latent codes to data points. Whereas the
dominant paradigm combines simple priors over codes with complex deterministic
models, we propose instead to use more flexible code distributions. These
distributions are estimated non-parametrically by reversing the generator map
during training. The benefits include: more powerful generative models, better
modeling of latent structure and explicit control of the degree of
generalization.
",1,0,0,1,0,0
4795,"Light emission by accelerated electric, toroidal and anapole dipolar sources","  Emission of electromagnetic radiation by accelerated particles with electric,
toroidal and anapole dipole moments is analyzed. It is shown that ellipticity
of the emitted light can be used to differentiate between electric and toroidal
dipole sources, and that anapoles, elementary neutral non-radiating
configurations, which consist of electric and toroidal dipoles, can emit light
under uniform acceleration. The existence of non-radiating configurations in
electrodynamics implies that it is impossible to fully determine the internal
makeup of the emitter given only the distribution of the emitted light. Here we
demonstrate that there is a loop-hole in this `inverse source problem'. Our
results imply that there may be a whole range of new phenomena to be discovered
by studying the electromagnetic response of matter under acceleration.
",0,1,0,0,0,0
12117,Paris-Lille-3D: a large and high-quality ground truth urban point cloud dataset for automatic segmentation and classification,"  This paper introduces a new Urban Point Cloud Dataset for Automatic
Segmentation and Classification acquired by Mobile Laser Scanning (MLS). We
describe how the dataset is obtained from acquisition to post-processing and
labeling. This dataset can be used to learn classification algorithm, however,
given that a great attention has been paid to the split between the different
objects, this dataset can also be used to learn the segmentation. The dataset
consists of around 2km of MLS point cloud acquired in two cities. The number of
points and range of classes make us consider that it can be used to train
Deep-Learning methods. Besides we show some results of automatic segmentation
and classification. The dataset is available at:
this http URL
",1,0,0,1,0,0
12388,Evidence for a Dusty Dark Dwarf Galaxy in the Quadruple Lens MG0414+0534,"  We report the $4 \, \sigma$ detection of a faint object with a flux of ~ 0.3
mJy, in the vicinity of the quadruply lensed QSO MG0414+0534 using the Atacama
Large Millimeter/submillimeter array (ALMA) Band 7. The object is most probably
a dusty dark dwarf galaxy, which has not been detected in either the optical,
near-infrared (NIR) or radio (cm) bands. An anomaly in the flux ratio of the
lensed images observed in Band 7 and the mid-infrared (MIR) band and the
reddening of the QSO light color can be simultaneously explained if we consider
the object as a lensing substructure with an ellipticity ~ 0.7 at a redshift of
$0.5 \lesssim z \lesssim 1$. Using the best-fit lens models with three lenses,
we find that the dark matter plus baryon mass associated with the object is
$\sim 10^9\, M_{\odot}$, the dust mass is $\sim 10^7\,M_{\odot}$ and the linear
size is $\gtrsim 5\,$kpc. Thus our findings suggest that the object is a dusty
dark dwarf galaxy. A substantial portion of faint submillimeter galaxies (SMGs)
in the universe may be attributed to such dark objects.
",0,1,0,0,0,0
1475,On the quantum differentiation of smooth real-valued functions,"  Calculating the value of $C^{k\in\{1,\infty\}}$ class of smoothness
real-valued function's derivative in point of $\mathbb{R}^+$ in radius of
convergence of its Taylor polynomial (or series), applying an analog of
Newton's binomial theorem and $q$-difference operator. $(P,q)$-power difference
introduced in section 5. Additionally, by means of Newton's interpolation
formula, the discrete analog of Taylor series, interpolation using
$q$-difference and $p,q$-power difference is shown.
",0,0,1,0,0,0
20173,Circularly polarized vacuum field in three-dimensional chiral photonic crystals probed by quantum dot emission,"  The quantum nature of light-matter interactions in a circularly polarized
vacuum field was probed by spontaneous emission from quantum dots in
three-dimensional chiral photonic crystals. Due to the circularly polarized
eigenmodes along the helical axis in the GaAs-based mirror-asymmetric
structures we studied, we observed highly circularly polarized emission from
the quantum dots. Both spectroscopic and time-resolved measurements confirmed
that the obtained circularly polarized light was influenced by a large
difference in the photonic density of states between the orthogonal components
of the circular polarization in the vacuum field.
",0,1,0,0,0,0
16805,"Equilibria, information and frustration in heterogeneous network games with conflicting preferences","  Interactions between people are the basis on which the structure of our
society arises as a complex system and, at the same time, are the starting
point of any physical description of it. In the last few years, much
theoretical research has addressed this issue by combining the physics of
complex networks with a description of interactions in terms of evolutionary
game theory. We here take this research a step further by introducing a most
salient societal factor such as the individuals' preferences, a characteristic
that is key to understand much of the social phenomenology these days. We
consider a heterogeneous, agent-based model in which agents interact
strategically with their neighbors but their preferences and payoffs for the
possible actions differ. We study how such a heterogeneous network behaves
under evolutionary dynamics and different strategic interactions, namely
coordination games and best shot games. With this model we study the emergence
of the equilibria predicted analytically in random graphs under best response
dynamics, and we extend this test to unexplored contexts like proportional
imitation and scale free networks. We show that some theoretically predicted
equilibria do not arise in simulations with incomplete Information, and we
demonstrate the importance of the graph topology and the payoff function
parameters for some games. Finally, we discuss our results with available
experimental evidence on coordination games, showing that our model agrees
better with the experiment that standard economic theories, and draw hints as
to how to maximize social efficiency in situations of conflicting preferences.
",1,1,0,0,0,0
18644,Underground tests of quantum mechanics. Whispers in the cosmic silence?,"  By performing X-rays measurements in the ""cosmic silence"" of the underground
laboratory of Gran Sasso, LNGS-INFN, we test a basic principle of quantum
mechanics: the Pauli Exclusion Principle (PEP), for electrons. We present the
achieved results of the VIP experiment and the ongoing VIP2 measurement aiming
to gain two orders of magnitude improvement in testing PEP. We also use a
similar experimental technique to search for radiation (X and gamma) predicted
by continuous spontaneous localization models, which aim to solve the
""measurement problem"".
",0,1,0,0,0,0
14790,Boundary Algebraic Bethe Ansatz for a nineteen vertex model with $U_{q}[\mathrm{osp}(2|2)^{(2)}] symmetry$,"  The boundary algebraic Bethe Ansatz for a supersymmetric nineteen
vertex-model constructed from a three-dimensional representation of the twisted
quantum affine Lie superalgebra $U_{q}[\mathrm{osp}(2|2)^{(2)}]$ is presented.
The eigenvalues and eigenvectors of Sklyanin's transfer matrix, with diagonal
reflection $K$-matrices, are calculated and the corresponding Bethe Ansatz
equations are obtained.
",0,1,0,0,0,0
1329,Search for Interstellar LiH in the Milky Way,"  We report the results of a sensitive search for the 443.952902 GHz $J=1-0$
transition of the LiH molecule toward two interstellar clouds in the Milky Way,
W49N and Sgr B2 (Main), that has been carried out using the Atacama Pathfinder
Experiment (APEX) telescope. The results obtained toward W49N place an upper
limit of $1.9 \times 10^{-11}\, (3\sigma)$ on the LiH abundance, $N({\rm
LiH})/N({\rm H}_2)$, in a foreground, diffuse molecular cloud along the
sight-line to W49N, corresponding to 0.5% of the solar system lithium
abundance. Those obtained toward Sgr B2 (Main) place an abundance limit $N({\rm
LiH})/N({\rm H}_2) < 3.6 \times 10^{-13} \,(3\sigma)$ in the dense gas within
the Sgr B2 cloud itself. These limits are considerably smaller that those
implied by the tentative detection of LiH reported previously for the $z=0.685$
absorber toward B0218+357.
",0,1,0,0,0,0
7540,A DIRT-T Approach to Unsupervised Domain Adaptation,"  Domain adaptation refers to the problem of leveraging labeled data in a
source domain to learn an accurate model in a target domain where labels are
scarce or unavailable. A recent approach for finding a common representation of
the two domains is via domain adversarial training (Ganin & Lempitsky, 2015),
which attempts to induce a feature extractor that matches the source and target
feature distributions in some feature space. However, domain adversarial
training faces two critical limitations: 1) if the feature extraction function
has high-capacity, then feature distribution matching is a weak constraint, 2)
in non-conservative domain adaptation (where no single classifier can perform
well in both the source and target domains), training the model to do well on
the source domain hurts performance on the target domain. In this paper, we
address these issues through the lens of the cluster assumption, i.e., decision
boundaries should not cross high-density data regions. We propose two novel and
related models: 1) the Virtual Adversarial Domain Adaptation (VADA) model,
which combines domain adversarial training with a penalty term that punishes
the violation the cluster assumption; 2) the Decision-boundary Iterative
Refinement Training with a Teacher (DIRT-T) model, which takes the VADA model
as initialization and employs natural gradient steps to further minimize the
cluster assumption violation. Extensive empirical results demonstrate that the
combination of these two models significantly improve the state-of-the-art
performance on the digit, traffic sign, and Wi-Fi recognition domain adaptation
benchmarks.
",0,0,0,1,0,0
8422,Generalization of two Bonnet's Theorems to the relative Differential Geometry of the 3-dimensional Euclidean space,"  This paper is devoted to the 3-dimensional relative differential geometry of
surfaces. In the Euclidean space $\R{E} ^3 $ we consider a surface $\varPhi
%\colon \vect{x} = \vect{x}(u^1,u^2) $ with position vector field $\vect{x}$,
which is relatively normalized by a relative normalization $\vect{y}% (u^1,u^2)
$. A surface $\varPhi^*% \colon \vect{x}^* = \vect{x}^*(u^1,u^2) $ with
position vector field $\vect{x}^* = \vect{x} + \mu \, \vect{y}$, where $\mu$ is
a real constant, is called a relatively parallel surface to $\varPhi$. Then
$\vect{y}$ is also a relative normalization of $\varPhi^*$. The aim of this
paper is to formulate and prove the relative analogues of two well known
theorems of O.~Bonnet which concern the parallel surfaces (see~\cite{oB1853}).
",0,0,1,0,0,0
9967,"Fast, Accurate and Lightweight Super-Resolution with Neural Architecture Search","  Deep convolution neural networks demonstrate impressive results in the
super-resolution domain. A series of studies concentrate on improving peak
signal noise ratio (PSNR) by using much deeper layers, which are not friendly
to constrained resources. Pursuing a trade-off between the restoration capacity
and the simplicity of models is still non-trivial. Recent contributions are
struggling to manually maximize this balance, while our work achieves the same
goal automatically with neural architecture search. Specifically, we handle
super-resolution with a multi-objective approach. We also propose an elastic
search tactic at both micro and macro level, based on a hybrid controller that
profits from evolutionary computation and reinforcement learning. Quantitative
experiments help us to draw a conclusion that our generated models dominate
most of the state-of-the-art methods with respect to the individual FLOPS.
",1,0,0,0,0,0
1505,Revisiting Imidazolium Based Ionic Liquids: Effect of the Conformation Bias of the [NTf$_{2}$] Anion Studied By Molecular Dynamics Simulations,"  We study ionic liquids composed 1-alkyl-3-methylimidazolium cations and
bis(trifluoromethyl-sulfonyl)imide anions ([C$_n$MIm][NTf$_2$]) with varying
chain-length $n\!=\!2, 4, 6, 8$ by using molecular dynamics simulations. We
show that a reparametrization of the dihedral potentials as well as charges of
the [NTf$_2$] anion leads to an improvment of the force field model introduced
by Köddermann {\em et al.} [ChemPhysChem, \textbf{8}, 2464 (2007)] (KPL-force
field). A crucial advantage of the new parameter set is that the minimum energy
conformations of the anion ({\em trans} and {\em gauche}), as deduced from {\em
ab initio} calculations and {\sc Raman} experiments, are now both well
represented by our model. In addition, the results for [C$_n$MIm][NTf$_2$] show
that this modification leads to an even better agreement between experiment and
molecular dynamics simulation as demonstrated for densities, diffusion
coefficients, vaporization enthalpies, reorientational correlation times, and
viscosities. Even though we focused on a better representation of the anion
conformation, also the alkyl chain-length dependence of the cation behaves
closer to the experiment. We strongly encourage to use the new NGKPL force
field for the [NTf$_2$] anion instead of the earlier KPL parameter set for
computer simulations aiming to describe the thermodynamics, dynamics and also
structure of imidazolium based ionic liquids.
",0,1,0,0,0,0
15096,Small-space encoding LCE data structure with constant-time queries,"  The \emph{longest common extension} (\emph{LCE}) problem is to preprocess a
given string $w$ of length $n$ so that the length of the longest common prefix
between suffixes of $w$ that start at any two given positions is answered
quickly. In this paper, we present a data structure of $O(z \tau^2 +
\frac{n}{\tau})$ words of space which answers LCE queries in $O(1)$ time and
can be built in $O(n \log \sigma)$ time, where $1 \leq \tau \leq \sqrt{n}$ is a
parameter, $z$ is the size of the Lempel-Ziv 77 factorization of $w$ and
$\sigma$ is the alphabet size. This is an \emph{encoding} data structure, i.e.,
it does not access the input string $w$ when answering queries and thus $w$ can
be deleted after preprocessing. On top of this main result, we obtain further
results using (variants of) our LCE data structure, which include the
following:
- For highly repetitive strings where the $z\tau^2$ term is dominated by
$\frac{n}{\tau}$, we obtain a \emph{constant-time and sub-linear space} LCE
query data structure.
- Even when the input string is not well compressible via Lempel-Ziv 77
factorization, we still can obtain a \emph{constant-time and sub-linear space}
LCE data structure for suitable $\tau$ and for $\sigma \leq 2^{o(\log n)}$.
- The time-space trade-off lower bounds for the LCE problem by Bille et al.
[J. Discrete Algorithms, 25:42-50, 2014] and by Kosolobov [CoRR,
abs/1611.02891, 2016] can be ""surpassed"" in some cases with our LCE data
structure.
",1,0,0,0,0,0
9810,Mean Actor Critic,"  We propose a new algorithm, Mean Actor-Critic (MAC), for discrete-action
continuous-state reinforcement learning. MAC is a policy gradient algorithm
that uses the agent's explicit representation of all action values to estimate
the gradient of the policy, rather than using only the actions that were
actually executed. We prove that this approach reduces variance in the policy
gradient estimate relative to traditional actor-critic methods. We show
empirical results on two control domains and on six Atari games, where MAC is
competitive with state-of-the-art policy search algorithms.
",1,0,0,1,0,0
2291,Settling the query complexity of non-adaptive junta testing,"  We prove that any non-adaptive algorithm that tests whether an unknown
Boolean function $f: \{0, 1\}^n\to \{0, 1\}$ is a $k$-junta or $\epsilon$-far
from every $k$-junta must make $\widetilde{\Omega}(k^{3/2} / \epsilon)$ many
queries for a wide range of parameters $k$ and $\epsilon$. Our result
dramatically improves previous lower bounds from [BGSMdW13, STW15], and is
essentially optimal given Blais's non-adaptive junta tester from [Blais08],
which makes $\widetilde{O}(k^{3/2})/\epsilon$ queries. Combined with the
adaptive tester of [Blais09] which makes $O(k\log k + k /\epsilon)$ queries,
our result shows that adaptivity enables polynomial savings in query complexity
for junta testing.
",1,0,0,0,0,0
1289,Implementation of a Distributed Coherent Quantum Observer,"  This paper considers the problem of implementing a previously proposed
distributed direct coupling quantum observer for a closed linear quantum
system. By modifying the form of the previously proposed observer, the paper
proposes a possible experimental implementation of the observer plant system
using a non-degenerate parametric amplifier and a chain of optical cavities
which are coupled together via optical interconnections. It is shown that the
distributed observer converges to a consensus in a time averaged sense in which
an output of each element of the observer estimates the specified output of the
quantum plant.
",1,0,1,0,0,0
17685,On locally compact semitopological $0$-bisimple inverse $ω$-semigroups,"  We describe the structure of Hausdorff locally compact semitopological
$0$-bisimple inverse $\omega$-semigroups with compact maximal subgroups. In
particular, we show that a Hausdorff locally compact semitopological
$0$-bisimple inverse $\omega$-semigroup with a compact maximal subgroup is
either compact or topologically isomorphic to the topological sum of its
$\mathscr{H}$-classes. We describe the structure of Hausdorff locally compact
semitopological $0$-bisimple inverse $\omega$-semigroups with a monothetic
maximal subgroups. In particular we prove the dichotomy for $T_1$ locally
compact semitopological Reilly semigroup
$\left(\textbf{B}(\mathbb{Z}_{+},\theta)^0,\tau\right)$ with adjoined zero and
with a non-annihilating homomorphism $\theta\colon \mathbb{Z}_{+}\to
\mathbb{Z}_{+}$: $\left(\textbf{B}(\mathbb{Z}_{+},\theta)^0,\tau\right)$ is
either compact or discrete. At the end we discuss on the remainder under the
closure of the discrete Reilly semigroup $\textbf{B}(\mathbb{Z}_{+},\theta)^0$
in a semitopological semigroup.
",0,0,1,0,0,0
15871,Dust Growth and Magnetic Fields: from Cores to Disks (even down to Planets),"  The recent rapid progress in observations of circumstellar disks and
extrasolar planets has reinforced the importance of understanding an intimate
coupling between star and planet formation. Under such a circumstance, it may
be invaluable to attempt to specify when and how planet formation begins in
star-forming regions and to identify what physical processes/quantities are the
most significant to make a link between star and planet formation. To this end,
we have recently developed a couple of projects. These include an observational
project about dust growth in Class 0 YSOs and a theoretical modeling project of
the HL Tauri disk. For the first project, we utilize the archive data of radio
interferometric observations, and examine whether dust growth, a first step of
planet formation, occurs in Class 0 YSOs. We find that while our observational
results can be reproduced by the presence of large ($\sim$ mm) dust grains for
some of YSOs under the single-component modified blackbody formalism, an
interpretation of no dust growth would be possible when a more detailed model
is used. For the second project, we consider an origin of the disk
configuration around HL Tauri, focusing on magnetic fields. We find that
magnetically induced disk winds may play an important role in the HL Tauri
disk. The combination of these attempts may enable us to move towards a
comprehensive understanding of how star and planet formation are intimately
coupled with each other.
",0,1,0,0,0,0
13286,Dual Discriminator Generative Adversarial Nets,"  We propose in this paper a novel approach to tackle the problem of mode
collapse encountered in generative adversarial network (GAN). Our idea is
intuitive but proven to be very effective, especially in addressing some key
limitations of GAN. In essence, it combines the Kullback-Leibler (KL) and
reverse KL divergences into a unified objective function, thus it exploits the
complementary statistical properties from these divergences to effectively
diversify the estimated density in capturing multi-modes. We term our method
dual discriminator generative adversarial nets (D2GAN) which, unlike GAN, has
two discriminators; and together with a generator, it also has the analogy of a
minimax game, wherein a discriminator rewards high scores for samples from data
distribution whilst another discriminator, conversely, favoring data from the
generator, and the generator produces data to fool both two discriminators. We
develop theoretical analysis to show that, given the maximal discriminators,
optimizing the generator of D2GAN reduces to minimizing both KL and reverse KL
divergences between data distribution and the distribution induced from the
data generated by the generator, hence effectively avoiding the mode collapsing
problem. We conduct extensive experiments on synthetic and real-world
large-scale datasets (MNIST, CIFAR-10, STL-10, ImageNet), where we have made
our best effort to compare our D2GAN with the latest state-of-the-art GAN's
variants in comprehensive qualitative and quantitative evaluations. The
experimental results demonstrate the competitive and superior performance of
our approach in generating good quality and diverse samples over baselines, and
the capability of our method to scale up to ImageNet database.
",1,0,0,1,0,0
11571,"Geometric Ergodicity of the MUCOGARCH(1,1) process","  For the multivariate COGARCH(1,1) volatility process we show sufficient
conditions for the existence of a unique stationary distribution, for the
geometric ergodicity and for the finiteness of moments of the stationary
distribution. One of the conditions demands a sufficiently fast exponential
decay of the MUCOGARCH(1,1) volatility process. Furthermore, we show easily
applicable sufficient conditions for the needed irreducibility of the
volatility process living in the cone of positive semidefinite matrices, if the
driving Lévy process is a compound Poisson process.
",0,0,1,0,0,0
2306,Comparative Study of Virtual Machines and Containers for DevOps Developers,"  In this work, we plan to develop a system to compare virtual machines with
container technology. We would devise ways to measure the administrator effort
of containers vs. Virtual Machines (VMs). Metrics that will be tested against
include human efforts required, ease of migration, resource utilization and
ease of use using containers and virtual machines.
",1,0,0,0,0,0
1339,Equal confidence weighted expectation value estimates,"  In this article the issues are discussed with the Bayesian approach,
least-square fits, and most-likely fits. Trying to counter these issues, a
method, based on weighted confidence, is proposed for estimating probabilities
and other observables. This method sums over different model parameter
combinations but does not require the need for making assumptions on priors or
underlying probability functions. Moreover, by construction the results are
invariant under reparametrization of the model parameters. In one case the
result appears similar as in Bayesian statistics but in general there is no
agreement. The binomial distribution is also studied which turns out to be
useful for making predictions on production processes without the need to make
further assumptions. In the last part, the case of a simple linear fit (a
multi-variate example) is studied using the standard approaches and the
confidence weighted approach.
",0,0,1,1,0,0
19683,Characterization of a Deuterium-Deuterium Plasma Fusion Neutron Generator,"  We characterize the neutron output of a deuterium-deuterium plasma fusion
neutron generator, model 35-DD-W-S, manufactured by NSD/Gradel-Fusion. The
measured energy spectrum is found to be dominated by neutron peaks at 2.2 MeV
and 2.7 MeV. A detailed GEANT4 simulation accurately reproduces the measured
energy spectrum and confirms our understanding of the fusion process in this
generator. Additionally, a contribution of 14.1 MeV neutrons from
deuterium-tritium fusion is found at a level of~$3.5\%$, from tritium produced
in previous deuterium-deuterium reactions. We have measured both the absolute
neutron flux as well as its relative variation on the operational parameters of
the generator. We find the flux to be proportional to voltage $V^{3.32 \pm
0.14}$ and current $I^{0.97 \pm 0.01}$. Further, we have measured the angular
dependence of the neutron emission with respect to the polar angle. We conclude
that it is well described by isotropic production of neutrons within the
cathode field cage.
",0,1,0,0,0,0
6815,Towards quantitative methods to assess network generative models,"  Assessing generative models is not an easy task. Generative models should
synthesize graphs which are not replicates of real networks but show
topological features similar to real graphs. We introduce an approach for
assessing graph generative models using graph classifiers. The inability of an
established graph classifier for distinguishing real and synthesized graphs
could be considered as a performance measurement for graph generators.
",1,0,0,0,0,0
20871,Common fixed points via $λ$-sequences in $G$-metric spaces,"  In this article, we use $\lambda$-sequences to derive common fixed points for
a family of self-mappings defined on a complete $G$-metric space. We imitate
some existing techniques in our proofs and show that the tools emlyed can be
used at a larger scale. These results generalise well known results in the
literature.
",0,0,1,0,0,0
20886,On the well-posedness of SPDEs with singular drift in divergence form,"  We prove existence and uniqueness of strong solutions for a class of
second-order stochastic PDEs with multiplicative Wiener noise and drift of the
form $\operatorname{div} \gamma(\nabla \cdot)$, where $\gamma$ is a maximal
monotone graph in $\mathbb{R}^n \times \mathbb{R}^n$ obtained as the
subdifferential of a convex function satisfying very mild assumptions on its
behavior at infinity. The well-posedness result complements the corresponding
one in our recent work arXiv:1612.08260 where, under the additional assumption
that $\gamma$ is single-valued, a solution with better integrability and
regularity properties is constructed. The proof given here, however, is
self-contained.
",0,0,1,0,0,0
14994,Statistical study of auroral omega bands,"  The presence of very few statistical studies on auroral omega bands motivated
us to test-use a semi-automatic method for identifying large-scale undulations
of the diffuse aurora boundary and to investigate their occurrence. Five
identical all-sky cameras with overlapping fields of view provided data for 438
auroral omega-like structures over Fennoscandian Lapland from 1996 to 2007. The
results from this set of omega band events agree remarkably well with previous
observations of omega band occurrence in magnetic local time (MLT), lifetime,
location between the region 1 and 2 field-aligned currents, as well as current
density estimates. The average peak emission height of omega forms corresponds
to the estimated precipitation energies of a few keV, which experienced no
significant change during the events. Analysis of both local and global
magnetic indices demonstrates that omega bands are observed during substorm
expansion and recovery phases that are more intense than average substorm
expansion and recovery phases in the same region. The omega occurrence with
respect to the substorm expansion and recovery phases is in a very good
agreement with an earlier observed distribution of fast earthward flows in the
plasma sheet during expansion and recovery phases. These findings support the
theory that omegas are produced by fast earthward flows and auroral streamers,
despite the rarity of good conjugate observations.
",0,1,0,0,0,0
6978,Spontaneous symmetry breaking as a triangular relation between pairs of Goldstone bosons and the degenerate vacuum: Interactions of D-branes,"  We formulate the Nambu-Goldstone theorem as a triangular relation between
pairs of Goldstone bosons with the degenerate vacuum. The vacuum degeneracy is
then a natural consequence of this relation. Inside the scenario of String
Theory, we then find that there is a correspondence between the way how the
$D$-branes interact and the properties of the Goldstone bosons.
",0,1,0,0,0,0
16793,Re-Evaluating the Netflix Prize - Human Uncertainty and its Impact on Reliability,"  In this paper, we examine the statistical soundness of comparative
assessments within the field of recommender systems in terms of reliability and
human uncertainty. From a controlled experiment, we get the insight that users
provide different ratings on same items when repeatedly asked. This volatility
of user ratings justifies the assumption of using probability densities instead
of single rating scores. As a consequence, the well-known accuracy metrics
(e.g. MAE, MSE, RMSE) yield a density themselves that emerges from convolution
of all rating densities. When two different systems produce different RMSE
distributions with significant intersection, then there exists a probability of
error for each possible ranking. As an application, we examine possible ranking
errors of the Netflix Prize. We are able to show that all top rankings are more
or less subject to high probabilities of error and that some rankings may be
deemed to be caused by mere chance rather than system quality.
",1,0,0,0,0,0
5934,An Introduction to Animal Movement Modeling with Hidden Markov Models using Stan for Bayesian Inference,"  Hidden Markov models (HMMs) are popular time series model in many fields
including ecology, economics and genetics. HMMs can be defined over discrete or
continuous time, though here we only cover the former. In the field of movement
ecology in particular, HMMs have become a popular tool for the analysis of
movement data because of their ability to connect observed movement data to an
underlying latent process, generally interpreted as the animal's unobserved
behavior. Further, we model the tendency to persist in a given behavior over
time. Notation presented here will generally follow the format of Zucchini et
al. (2016) and cover HMMs applied in an unsupervised case to animal movement
data, specifically positional data. We provide Stan code to analyze movement
data of the wild haggis as presented first in Michelot et al. (2016).
",0,0,0,1,1,0
14944,Geometry and Arithmetic of Crystallographic Sphere Packings,"  We introduce the notion of a ""crystallographic sphere packing,"" defined to be
one whose limit set is that of a geometrically finite hyperbolic reflection
group in one higher dimension. We exhibit for the first time an infinite family
of conformally-inequivalent such with all radii being reciprocals of integers.
We then prove a result in the opposite direction: the ""superintegral"" ones
exist only in finitely many ""commensurability classes,"" all in dimensions below
30.
",0,0,1,0,0,0
19189,Analytical solution of the integral equation for partial wave Coulomb t-matrices at excited-state energy,"  Starting from the integral representation of the three-dimensional Coulomb
transition matrix elaborated by us formerly with the use of specific symmetry
of the interaction in a four-dimensional Euclidean space introduced by Fock,
the possibility of the analytical solving of the integral equation for the
partial wave transition matrices at the excited bound state energy has been
studied. New analytical expressions for the partial s-, p- and d-wave Coulomb
t-matrices for like-charged particles and the expression for the partial d-wave
t-matrix for unlike-charged particles at the energy of the first excited bound
state have been derived.
",0,1,0,0,0,0
19496,Tuning Goodness-of-Fit Tests,"  As modern precision cosmological measurements continue to show agreement with
the broad features of the standard $\Lambda$-Cold Dark Matter ($\Lambda$CDM)
cosmological model, we are increasingly motivated to look for small departures
from the standard model's predictions which might not be detected with standard
approaches. While searches for extensions and modifications of $\Lambda$CDM
have to date turned up no convincing evidence of beyond-the-standard-model
cosmology, the list of models compared against $\Lambda$CDM is by no means
complete and is often governed by readily-coded modifications to standard
Boltzmann codes. Also, standard goodness-of-fit methods such as a naive
$\chi^2$ test fail to put strong pressure on the null $\Lambda$CDM hypothesis,
since modern datasets have orders of magnitudes more degrees of freedom than
$\Lambda$CDM. Here we present a method of tuning goodness-of-fit tests to
detect potential sub-dominant extra-$\Lambda$CDM signals present in the data
through compressing observations in a way that maximizes extra-$\Lambda$CDM
signal variation over noise and $\Lambda$CDM variation. This method, based on a
Karhunen-Loève transformation of the data, is tuned to be maximally
sensitive to particular types of variations characteristic of the tuning model;
but, unlike direct model comparison, the test is also sensitive to features
that only partially mimic the tuning model. As an example of its use, we apply
this method in the context of a nonstandard primordial power spectrum compared
against the $2015$ $Planck$ CMB temperature and polarization power spectrum. We
find weak evidence of extra-$\Lambda$CDM physics, conceivably due to known
systematics in the 2015 Planck polarization release.
",0,1,0,0,0,0
2196,Do triangle-free planar graphs have exponentially many 3-colorings?,"  Thomassen conjectured that triangle-free planar graphs have an exponential
number of $3$-colorings. We show this conjecture to be equivalent to the
following statement: there exists a positive real $\alpha$ such that whenever
$G$ is a planar graph and $A$ is a subset of its edges whose deletion makes $G$
triangle-free, there exists a subset $A'$ of $A$ of size at least $\alpha|A|$
such that $G-(A\setminus A')$ is $3$-colorable. This equivalence allows us to
study restricted situations, where we can prove the statement to be true.
",0,0,1,0,0,0
4931,Informed Asymptotically Optimal Anytime Search,"  Path planning in robotics often requires finding high-quality solutions to
continuously valued and/or high-dimensional problems. These problems are
challenging and most planning algorithms instead solve simplified
approximations. Popular approximations include graphs and random samples, as
respectively used by informed graph-based searches and anytime sampling-based
planners. Informed graph-based searches, such as A*, traditionally use
heuristics to search a priori graphs in order of potential solution quality.
This makes their search efficient but leaves their performance dependent on the
chosen approximation. If its resolution is too low then they may not find a
(suitable) solution but if it is too high then they may take a prohibitively
long time to do so. Anytime sampling-based planners, such as RRT*,
traditionally use random sampling to approximate the problem domain
incrementally. This allows them to increase resolution until a suitable
solution is found but makes their search dependent on the order of
approximation. Arbitrary sequences of random samples expand the approximation
in every direction and fill the problem domain but may be prohibitively
inefficient at containing a solution. This paper unifies and extends these two
approaches to develop Batch Informed Trees (BIT*), an informed, anytime
sampling-based planner. BIT* solves continuous path planning problems
efficiently by using sampling and heuristics to alternately approximate and
search the problem domain. Its search is ordered by potential solution quality,
as in A*, and its approximation improves indefinitely with additional
computational time, as in RRT*. It is shown analytically to be almost-surely
asymptotically optimal and experimentally to outperform existing sampling-based
planners, especially on high-dimensional planning problems.
",1,0,0,0,0,0
13151,Ensemble of Part Detectors for Simultaneous Classification and Localization,"  Part-based representation has been proven to be effective for a variety of
visual applications. However, automatic discovery of discriminative parts
without object/part-level annotations is challenging. This paper proposes a
discriminative mid-level representation paradigm based on the responses of a
collection of part detectors, which only requires the image-level labels.
Towards this goal, we first develop a detector-based spectral clustering method
to mine the representative and discriminative mid-level patterns for detector
initialization. The advantage of the proposed pattern mining technology is that
the distance metric based on detectors only focuses on discriminative details,
and a set of such grouped detectors offer an effective way for consistent
pattern mining. Relying on the discovered patterns, we further formulate the
detector learning process as a confidence-loss sparse Multiple Instance
Learning (cls-MIL) task, which considers the diversity of the positive samples,
while avoid drifting away the well localized ones by assigning a confidence
value to each positive sample. The responses of the learned detectors can form
an effective mid-level image representation for both image classification and
object localization. Experiments conducted on benchmark datasets demonstrate
the superiority of our method over existing approaches.
",1,0,0,0,0,0
17035,Character sums for elliptic curve densities,"  If $E$ is an elliptic curve over $\mathbb{Q}$, then it follows from work of
Serre and Hooley that, under the assumption of the Generalized Riemann
Hypothesis, the density of primes $p$ such that the group of
$\mathbb{F}_p$-rational points of the reduced curve $\tilde{E}(\mathbb{F}_p)$
is cyclic can be written as an infinite product $\prod \delta_\ell$ of local
factors $\delta_\ell$ reflecting the degree of the $\ell$-torsion fields,
multiplied by a factor that corrects for the entanglements between the various
torsion fields. We show that this correction factor can be interpreted as a
character sum, and the resulting description allows us to easily determine
non-vanishing criteria for it. We apply this method in a variety of other
settings. Among these, we consider the aforementioned problem with the
additional condition that the primes $p$ lie in a given arithmetic progression.
We also study the conjectural constants appearing in Koblitz's conjecture, a
conjecture which relates to the density of primes $p$ for which the cardinality
of the group of $\mathbb{F}_p$-points of $E$ is prime.
",0,0,1,0,0,0
11584,Insulator to Metal Transition in WO$_3$ Induced by Electrolyte Gating,"  Tungsten oxide and its associated bronzes (compounds of tungsten oxide and an
alkali metal) are well known for their interesting optical and electrical
characteristics. We have modified the transport properties of thin WO$_3$ films
by electrolyte gating using both ionic liquids and polymer electrolytes. We are
able to tune the resistivity of the gated film by more than five orders of
magnitude, and a clear insulator-to-metal transition is observed. To clarify
the doping mechanism, we have performed a series of incisive operando
experiments, ruling out both a purely electronic effect (charge accumulation
near the interface) and oxygen-related mechanisms. We propose instead that
hydrogen intercalation is responsible for doping WO$_3$ into a highly
conductive ground state and provide evidence that it can be described as a
dense polaronic gas.
",0,1,0,0,0,0
1078,From a normal insulator to a topological insulator in plumbene,"  Plumbene, similar to silicene, has a buckled honeycomb structure with a large
band gap ($\sim 400$ meV). All previous studies have shown that it is a normal
insulator. Here, we perform first-principles calculations and employ a
sixteen-band tight-binding model with nearest-neighbor and
next-nearest-neighbor hopping terms to investigate electronic structures and
topological properties of the plumbene monolayer. We find that it can become a
topological insulator with a large bulk gap ($\sim 200$ meV) through electron
doping, and the nontrivial state is very robust with respect to external
strain. Plumbene can be an ideal candidate for realizing the quantum spin Hall
effect at room temperature. By investigating effects of external electric and
magnetic fields on electronic structures and transport properties of plumbene,
we present two rich phase diagrams with and without electron doping, and
propose a theoretical design for a four-state spin-valley filter.
",0,1,0,0,0,0
13022,A Driver-in-the Loop Fuel Economic Control Strategy for Connected Vehicles in Urban Roads,"  In this paper, we focus on developing driver-in-the loop fuel economic
control strategy for multiple connected vehicles. The control strategy is
considered to work in a driver assistance framework where the controller gives
command to a driver to follow while considering the ability of the driver in
following control commands. Our proposed method uses vehicle-to-vehicle (V2V)
communication, exploits traffic lights' Signal Phase and Timing (SPAT)
information, models driver error injection with Markov chain, and employs
scenario tree based stochastic model predictive control to improve vehicle fuel
economy and traffic mobility. The proposed strategy is decentralized in nature
as every vehicle evaluates its own strategy using only local information.
Simulation results show the effect of consideration of driver error injection
when synthesizing fuel economic controllers in a driver assistance fashion.
",1,0,0,0,0,0
20852,Introducing Geometric Algebra to Geometric Computing Software Developers: A Computational Thinking Approach,"  Designing software systems for Geometric Computing applications can be a
challenging task. Software engineers typically use software abstractions to
hide and manage the high complexity of such systems. Without the presence of a
unifying algebraic system to describe geometric models, the use of software
abstractions alone can result in many design and maintenance problems.
Geometric Algebra (GA) can be a universal abstract algebraic language for
software engineering geometric computing applications. Few sources, however,
provide enough information about GA-based software implementations targeting
the software engineering community. In particular, successfully introducing GA
to software engineers requires quite different approaches from introducing GA
to mathematicians or physicists. This article provides a high-level
introduction to the abstract concepts and algebraic representations behind the
elegant GA mathematical structure. The article focuses on the conceptual and
representational abstraction levels behind GA mathematics with sufficient
references for more details. In addition, the article strongly recommends
applying the methods of Computational Thinking in both introducing GA to
software engineers, and in using GA as a mathematical language for developing
Geometric Computing software systems.
",1,0,0,0,0,0
8604,The evolution of gravitons in accelerating cosmologies: the case of extended gravity,"  We discuss the production and evolution of cosmological gravitons showing how
the cosmological background affects their dynamics. Besides, the detection of
cosmological gravitons could constitute an extremely important signature to
discriminate among different cosmological models. Here we consider the cases of
scalar-tensor gravity and $f(R)$ gravity where it is demonstrated the
amplification of graviton amplitude changes if compared with General
Relativity. Possible observational constraints are discussed.
",0,1,0,0,0,0
8488,GdRh$_2$Si$_2$: An exemplary tetragonal system for antiferromagnetic order with weak in-plane anisotropy,"  The anisotropy of magnetic properties commonly is introduced in textbooks
using the case of an antiferromagnetic system with Ising type anisotropy. This
model presents huge anisotropic magnetization and a pronounced metamagnetic
transition and is well-known and well-documented both, in experiments and
theory. In contrast, the case of an antiferromagnetic $X$-$Y$ system with weak
in-plane anisotropy is only poorly documented. We studied the anisotropic
magnetization of the compound GdRh$_2$Si$_2$ and found that it is a perfect
model system for such a weak-anisotropy setting because the Gd$^{3+}$ ions in
GdRh$_2$Si$_2$ have a pure spin moment of S=7/2 which orders in a simple AFM
structure with ${\bf Q} = (001)$. We observed experimentally in $M(B)$ a
continuous spin-flop transition and domain effects for field applied along the
$[100]$- and the $[110]$-direction, respectively. We applied a mean field model
for the free energy to describe our data and combine it with an Ising chain
model to account for domain effects. Our calculations reproduce the
experimental data very well. In addition, we performed magnetic X-ray
scattering and X-ray magnetic circular dichroism measurements, which confirm
the AFM propagation vector to be ${\bf Q} = (001)$ and indicate the absence of
polarization on the rhodium atoms.
",0,1,0,0,0,0
19520,Off-diagonal asymptotic properties of Bergman kernels associated to analytic Kähler potentials,"  We prove a new off-diagonal asymptotic of the Bergman kernels associated to
tensor powers of a positive line bundle on a compact Kähler manifold. We show
that if the Kähler potential is real analytic, then the Bergman kernel
accepts a complete asymptotic expansion in a neighborhood of the diagonal of
shrinking size $k^{-\frac14}$. These improve the earlier results in the subject
for smooth potentials, where an expansion exists in a $k^{-\frac12}$
neighborhood of the diagonal. We obtain our results by finding upper bounds of
the form $C^m m!^{2}$ for the Bergman coefficients $b_m(x, \bar y)$, which is
an interesting problem on its own. We find such upper bounds using the method
of Berman-Berndtsson-Sjöstrand. We also show that sharpening these upper
bounds would improve the rate of shrinking neighborhoods of the diagonal $x=y$
in our results. In the special case of metrics with local constant holomorphic
sectional curvatures, we obtain off-diagonal asymptotic in a fixed (as $k \to
\infty$) neighborhood of the diagonal, which recovers a result of Berman [Ber]
(see Remark 3.5 of [Ber] for higher dimensions). In this case, we also find an
explicit formula for the Bergman kernel mod $O(e^{-k \delta} )$.
",0,0,1,0,0,0
9712,Hashing over Predicted Future Frames for Informed Exploration of Deep Reinforcement Learning,"  In deep reinforcement learning (RL) tasks, an efficient exploration mechanism
should be able to encourage an agent to take actions that lead to less frequent
states which may yield higher accumulative future return. However, both knowing
about the future and evaluating the frequentness of states are non-trivial
tasks, especially for deep RL domains, where a state is represented by
high-dimensional image frames. In this paper, we propose a novel informed
exploration framework for deep RL, where we build the capability for an RL
agent to predict over the future transitions and evaluate the frequentness for
the predicted future frames in a meaningful manner. To this end, we train a
deep prediction model to predict future frames given a state-action pair, and a
convolutional autoencoder model to hash over the seen frames. In addition, to
utilize the counts derived from the seen frames to evaluate the frequentness
for the predicted frames, we tackle the challenge of matching the predicted
future frames and their corresponding seen frames at the latent feature level.
In this way, we derive a reliable metric for evaluating the novelty of the
future direction pointed by each action, and hence inform the agent to explore
the least frequent one.
",1,0,0,1,0,0
3690,Chemical evolution of 244Pu in the solar vicinity and its implication for the properties of r-process production,"  Meteoritic abundances of r-process elements are analyzed to deduce the
history of chemical enrichment by r-process from the beginning of disk
formation to the present time in the solar vicinity, by combining the abundance
information from short-lived radioactive nuclei such as 244Pu with that from
stable r-process nuclei such as Eu. These two types of nuclei can be associated
with one r-process event and cumulation of events till formation of the solar
system, respectively. With help of the observed local star formation history,
we deduce the chemical evolution of 244Pu and obtain three main results: (i)
the last r-process event occurred 130-140 Myr before formation of the solar
system, (ii) the present-day low 244Pu abundance as measured in deep sea
reservoirs results from the low recent star formation rate compared to ~4.5 - 5
Gyr ago, and (iii) there were ~15 r-process events in the solar vicinity from
formation of the Galaxy to the time of solar system formation and ~30 r-process
events to the present time. Then, adopting a reasonable hypothesis that a
neutron star merger is the r-process production site, we find that the ejected
r-process elements are extensively spread out and mixed with interstellar
matter with a mass of ~3.5 million solar masses, which is about 100 times
larger than that for supernova ejecta. In addition, the event frequency of
r-process production is estimated to be one per about 1400 core-collapse
supernovae, which is identical to the frequency of neutron star mergers
estimated from the analysis of stellar abundances.
",0,1,0,0,0,0
13604,SlimNets: An Exploration of Deep Model Compression and Acceleration,"  Deep neural networks have achieved increasingly accurate results on a wide
variety of complex tasks. However, much of this improvement is due to the
growing use and availability of computational resources (e.g use of GPUs, more
layers, more parameters, etc). Most state-of-the-art deep networks, despite
performing well, over-parameterize approximate functions and take a significant
amount of time to train. With increased focus on deploying deep neural networks
on resource constrained devices like smart phones, there has been a push to
evaluate why these models are so resource hungry and how they can be made more
efficient. This work evaluates and compares three distinct methods for deep
model compression and acceleration: weight pruning, low rank factorization, and
knowledge distillation. Comparisons on VGG nets trained on CIFAR10 show that
each of the models on their own are effective, but that the true power lies in
combining them. We show that by combining pruning and knowledge distillation
methods we can create a compressed network 85 times smaller than the original,
all while retaining 96% of the original model's accuracy.
",0,0,0,1,0,0
10202,Contextual Parameter Generation for Universal Neural Machine Translation,"  We propose a simple modification to existing neural machine translation (NMT)
models that enables using a single universal model to translate between
multiple languages while allowing for language specific parameterization, and
that can also be used for domain adaptation. Our approach requires no changes
to the model architecture of a standard NMT system, but instead introduces a
new component, the contextual parameter generator (CPG), that generates the
parameters of the system (e.g., weights in a neural network). This parameter
generator accepts source and target language embeddings as input, and generates
the parameters for the encoder and the decoder, respectively. The rest of the
model remains unchanged and is shared across all languages. We show how this
simple modification enables the system to use monolingual data for training and
also perform zero-shot translation. We further show it is able to surpass
state-of-the-art performance for both the IWSLT-15 and IWSLT-17 datasets and
that the learned language embeddings are able to uncover interesting
relationships between languages.
",0,0,0,1,0,0
17861,Simulating the interaction between a falling super-quadric object and a soap film,"  The interaction that occurs between a light solid object and a horizontal
soap film of a bamboo foam contained in a cylindrical tube is simulated in 3D.
We vary the shape of the falling object from a sphere to a cube by changing a
single shape parameter as well as varying the initial orientation and position
of the object. We investigate in detail how the soap film deforms in all these
cases, and determine the network and pressure forces that a foam exerts on a
falling object, due to surface tension and bubble pressure respectively. We
show that a cubic particle in a particular orientation experiences the largest
drag force, and that this orientation is also the most likely outcome of
dropping a cube from an arbitrary orientation through a bamboo foam.
",0,1,0,0,0,0
9326,Hydrodynamic stability in the presence of a stochastic forcing:a case study in convection,"  We investigate the stability of a statistically stationary conductive state
for Rayleigh-Bénard convection between stress-free plates that arises due to
a bulk stochastic internal heating. This setup may be seen as a generalization
to a stochastic setting of the seminal 1916 study of Lord Rayleigh. Our results
indicate that stochastic forcing at small magnitude has a stabilizing effect,
while strong stochastic forcing has a destabilizing effect. The methodology put
forth in this article, which combines rigorous analysis with careful
computation, also provides an approach to hydrodynamic stability for a variety
of systems subject to a large scale stochastic forcing.
",0,1,1,0,0,0
3504,Simplicial Closure and higher-order link prediction,"  Networks provide a powerful formalism for modeling complex systems by using a
model of pairwise interactions. But much of the structure within these systems
involves interactions that take place among more than two nodes at once; for
example, communication within a group rather than person-to person,
collaboration among a team rather than a pair of coauthors, or biological
interaction between a set of molecules rather than just two. Such higher-order
interactions are ubiquitous, but their empirical study has received limited
attention, and little is known about possible organizational principles of such
structures. Here we study the temporal evolution of 19 datasets with explicit
accounting for higher-order interactions. We show that there is a rich variety
of structure in our datasets but datasets from the same system types have
consistent patterns of higher-order structure. Furthermore, we find that tie
strength and edge density are competing positive indicators of higher-order
organization, and these trends are consistent across interactions involving
differing numbers of nodes. To systematically further the study of theories for
such higher-order structures, we propose higher-order link prediction as a
benchmark problem to assess models and algorithms that predict higher-order
structure. We find a fundamental differences from traditional pairwise link
prediction, with a greater role for local rather than long-range information in
predicting the appearance of new interactions.
",1,0,0,1,0,0
20720,Multi-scale bilinear restriction estimates for general phases,"  We prove (adjoint) bilinear restriction estimates for general phases at
different scales in the full non-endpoint mixed norm range, and give bounds
with a sharp and explicit dependence on the phases. These estimates have
applications to high-low frequency interactions for solutions to partial
differential equations, as well as to the linear restriction problem for
surfaces with degenerate curvature. As a consequence, we obtain new bilinear
restriction estimates for elliptic phases and wave/Klein-Gordon interactions in
the full bilinear range, and give a refined Strichartz inequality for the
Klein-Gordon equation. In addition, we extend these bilinear estimates to hold
in adapted function spaces by using a transference type principle which holds
for vector valued waves.
",0,0,1,0,0,0
8172,Sequential Multiple Testing,"  We study an online multiple testing problem where the hypotheses arrive
sequentially in a stream. The test statistics are independent and assumed to
have the same distribution under their respective null hypotheses. We
investigate two procedures LORD and LOND, proposed by (Javanmard and Montanari,
2015), which are proved to control the FDR in an online manner. In some
(static) model, we show that LORD is optimal in some asymptotic sense, in
particular as powerful as the (static) Benjamini-Hochberg procedure to first
asymptotic order. We also quantify the performance of LOND. Some numerical
experiments complement our theory.
",0,0,1,1,0,0
7682,Gradient-based Training of Slow Feature Analysis by Differentiable Approximate Whitening,"  This paper proposes Power Slow Feature Analysis, a gradient-based method to
extract temporally-slow features from a high-dimensional input stream that
varies on a faster time-scale, and a variant of Slow Feature Analysis (SFA).
While displaying performance comparable to hierarchical extensions to the SFA
algorithm, such as Hierarchical Slow Feature Analysis, for a small number of
output-features, our algorithm allows end-to-end training of arbitrary
differentiable approximators (e.g., deep neural networks). We provide
experimental evidence that PowerSFA is able to extract meaningful and
informative low-dimensional features in the case of a) synthetic
low-dimensional data, b) visual data, and also for c) a general dataset for
which symmetric non-temporal relations between points can be defined.
",0,0,0,1,0,0
17336,Stabilization of prethermal Floquet steady states in a periodically driven dissipative Bose-Hubbard model,"  We discuss the effect of dissipation on heating which occurs in periodically
driven quantum many body systems. We especially focus on a periodically driven
Bose-Hubbard model coupled to an energy and particle reservoir. Without
dissipation, this model is known to undergo parametric instabilities which can
be considered as an initial stage of heating. By taking the weak on-site
interaction limit as well as the weak system-reservoir coupling limit, we find
that parametric instabilities are suppressed if the dissipation is stronger
than the on-site interaction strength and stable steady states appear. Our
results demonstrate that periodically-driven systems can emit energy, which is
absorbed from external drivings, to the reservoir so that they can avoid
heating.
",0,1,0,0,0,0
2547,Centralities of Nodes and Influences of Layers in Large Multiplex Networks,"  We formulate and propose an algorithm (MultiRank) for the ranking of nodes
and layers in large multiplex networks. MultiRank takes into account the full
multiplex network structure of the data and exploits the dual nature of the
network in terms of nodes and layers. The proposed centrality of the layers
(influences) and the centrality of the nodes are determined by a coupled set of
equations. The basic idea consists in assigning more centrality to nodes that
receive links from highly influential layers and from already central nodes.
The layers are more influential if highly central nodes are active in them. The
algorithm applies to directed/undirected as well as to weighted/unweighted
multiplex networks. We discuss the application of MultiRank to three major
examples of multiplex network datasets: the European Air Transportation
Multiplex Network, the Pierre Auger Multiplex Collaboration Network and the FAO
Multiplex Trade Network.
",1,1,0,0,0,0
11184,Classification in biological networks with hypergraphlet kernels,"  Biological and cellular systems are often modeled as graphs in which vertices
represent objects of interest (genes, proteins, drugs) and edges represent
relational ties among these objects (binds-to, interacts-with, regulates). This
approach has been highly successful owing to the theory, methodology and
software that support analysis and learning on graphs. Graphs, however, often
suffer from information loss when modeling physical systems due to their
inability to accurately represent multiobject relationships. Hypergraphs, a
generalization of graphs, provide a framework to mitigate information loss and
unify disparate graph-based methodologies. In this paper, we present a
hypergraph-based approach for modeling physical systems and formulate vertex
classification, edge classification and link prediction problems on
(hyper)graphs as instances of vertex classification on (extended, dual)
hypergraphs in a semi-supervised setting. We introduce a novel kernel method on
vertex- and edge-labeled (colored) hypergraphs for analysis and learning. The
method is based on exact and inexact (via hypergraph edit distances)
enumeration of small simple hypergraphs, referred to as hypergraphlets, rooted
at a vertex of interest. We extensively evaluate this method and show its
potential use in a positive-unlabeled setting to estimate the number of missing
and false positive links in protein-protein interaction networks.
",1,0,0,1,0,0
9927,Assortative Mixing Equilibria in Social Network Games,"  It is known that individuals in social networks tend to exhibit homophily
(a.k.a. assortative mixing) in their social ties, which implies that they
prefer bonding with others of their own kind. But what are the reasons for this
phenomenon? Is it that such relations are more convenient and easier to
maintain? Or are there also some more tangible benefits to be gained from this
collective behaviour?
The current work takes a game-theoretic perspective on this phenomenon, and
studies the conditions under which different assortative mixing strategies lead
to equilibrium in an evolving social network. We focus on a biased preferential
attachment model where the strategy of each group (e.g., political or social
minority) determines the level of bias of its members toward other group
members and non-members. Our first result is that if the utility function that
the group attempts to maximize is the degree centrality of the group,
interpreted as the sum of degrees of the group members in the network, then the
only strategy achieving Nash equilibrium is a perfect homophily, which implies
that cooperation with other groups is harmful to this utility function. A
second, and perhaps more surprising, result is that if a reward for inter-group
cooperation is added to the utility function (e.g., externally enforced by an
authority as a regulation), then there are only two possible equilibria,
namely, perfect homophily or perfect heterophily, and it is possible to
characterize their feasibility spaces. Interestingly, these results hold
regardless of the minority-majority ratio in the population.
We believe that these results, as well as the game-theoretic perspective
presented herein, may contribute to a better understanding of the forces that
shape the groups and communities of our society.
",1,1,0,0,0,0
17214,Extraction of Schottky barrier height insensitive to temperature via forward currentvoltage- temperature measurements,"  The thermal stability of most electronic and photo-electronic devices
strongly depends on the relationship between Schottky Barrier Height (SBH) and
temperature. In this paper, the possible of thermionic current depicted via
correct and reliability relationship between forward current and voltage is
consequently discussed, the intrinsic SBH insensitive to temperature can be
calculated by modification on Richardson- Dushman`s formula suggested in this
paper. The results of application on four hetero-junctions prove that the
method proposed is credible in this paper, this suggests that the I/V/T method
is a feasible alternative to characterize these heterojunctions.
",0,1,0,0,0,0
13432,Multi-Relevance Transfer Learning,"  Transfer learning aims to faciliate learning tasks in a label-scarce target
domain by leveraging knowledge from a related source domain with plenty of
labeled data. Often times we may have multiple domains with little or no
labeled data as targets waiting to be solved. Most existing efforts tackle
target domains separately by modeling the `source-target' pairs without
exploring the relatedness between them, which would cause loss of crucial
information, thus failing to achieve optimal capability of knowledge transfer.
In this paper, we propose a novel and effective approach called Multi-Relevance
Transfer Learning (MRTL) for this purpose, which can simultaneously transfer
different knowledge from the source and exploits the shared common latent
factors between target domains. Specifically, we formulate the problem as an
optimization task based on a collective nonnegative matrix tri-factorization
framework. The proposed approach achieves both source-target transfer and
target-target leveraging by sharing multiple decomposed latent subspaces.
Further, an alternative minimization learning algorithm is developed with
convergence guarantee. Empirical study validates the performance and
effectiveness of MRTL compared to the state-of-the-art methods.
",1,0,0,1,0,0
8973,ProtoDash: Fast Interpretable Prototype Selection,"  In this paper we propose an efficient algorithm ProtoDash for selecting
prototypical examples from complex datasets. Our generalizes the learn to
criticize (L2C) work by Kim et al. (2016) to not only select prototypes for a
given sparsity level $m$ but also to associate non-negative (for
interpretability) weights with each of them indicative of the importance of
each prototype. This extension provides a single coherent framework under which
both prototypes and criticisms can be found. Furthermore, our framework works
for any symmetric positive definite kernel thus addressing one of the key open
questions laid out in Kim et al. (2016). Our additional requirement of learning
non-negative weights no longer maintains submodularity of the objective as in
the previous work, however, we show that the problem is weakly submodular and
derive approximation guarantees for our fast ProtoDash algorithm. We
demonstrate the efficacy of our method on diverse domains such as retail, digit
recognition (MNIST) and on publicly available 40 health questionnaires obtained
from the Center for Disease Control (CDC) website maintained by the US Dept. of
Health. We validate the results quantitatively as well as qualitatively based
on expert feedback and recently published scientific studies on public health,
thus showcasing the power of our method in providing actionability (for
retail), utility (for MNIST) and insight (on CDC datasets), which presumably
are the hallmark of an effective interpretable method.
",1,0,0,1,0,0
3844,The localization transition in SU(3) gauge theory,"  We study the Anderson-like localization transition in the spectrum of the
Dirac operator of quenched QCD. Above the deconfining transition we determine
the temperature dependence of the mobility edge separating localized and
delocalized eigenmodes in the spectrum. We show that the temperature where the
mobility edge vanishes and localized modes disappear from the spectrum,
coincides with the critical temperature of the deconfining transition. We also
identify topological charge related close to zero modes in the Dirac spectrum
and show that they account for only a small fraction of localized modes, a
fraction that is rapidly falling as the temperature increases.
",0,1,0,0,0,0
12795,The border support rank of two-by-two matrix multiplication is seven,"  We show that the border support rank of the tensor corresponding to
two-by-two matrix multiplication is seven over the complex numbers. We do this
by constructing two polynomials that vanish on all complex tensors with format
four-by-four-by-four and border rank at most six, but that do not vanish
simultaneously on any tensor with the same support as the two-by-two matrix
multiplication tensor. This extends the work of Hauenstein, Ikenmeyer, and
Landsberg. We also give two proofs that the support rank of the two-by-two
matrix multiplication tensor is seven over any field: one proof using a result
of De Groote saying that the decomposition of this tensor is unique up to
sandwiching, and another proof via the substitution method. These results
answer a question asked by Cohn and Umans. Studying the border support rank of
the matrix multiplication tensor is relevant for the design of matrix
multiplication algorithms, because upper bounds on the border support rank of
the matrix multiplication tensor lead to upper bounds on the computational
complexity of matrix multiplication, via a construction of Cohn and Umans.
Moreover, support rank has applications in quantum communication complexity.
",1,0,1,0,0,0
17431,The local geometry of testing in ellipses: Tight control via localized Kolmogorov widths,"  We study the local geometry of testing a mean vector within a
high-dimensional ellipse against a compound alternative. Given samples of a
Gaussian random vector, the goal is to distinguish whether the mean is equal to
a known vector within an ellipse, or equal to some other unknown vector in the
ellipse. Such ellipse testing problems lie at the heart of several
applications, including non-parametric goodness-of-fit testing, signal
detection in cognitive radio, and regression function testing in reproducing
kernel Hilbert spaces. While past work on such problems has focused on the
difficulty in a global sense, we study difficulty in a way that is localized to
each vector within the ellipse. Our main result is to give sharp upper and
lower bounds on the localized minimax testing radius in terms of an explicit
formula involving the Kolmogorov width of the ellipse intersected with a
Euclidean ball. When applied to particular examples, our general theorems yield
interesting rates that were not known before: as a particular case, for testing
in Sobolev ellipses of smoothness $\alpha$, we demonstrate rates that vary from
$(\sigma^2)^{\frac{4 \alpha}{4 \alpha + 1}}$, corresponding to the classical
global rate, to the faster rate $(\sigma^2)^{\frac{8
\alpha}{8 \alpha + 1}}$, achievable for vectors at favorable locations within
the ellipse. We also show that the optimal test for this problem is achieved by
a linear projection test that is based on an explicit lower-dimensional
projection of the observation vector.
",0,0,1,1,0,0
6660,A remark on oscillatory integrals associated with fewnomials,"  We prove that the $L^2$ bound of an oscillatory integral associated with a
polynomial depends only on the number of monomials that this polynomial
consists of.
",0,0,1,0,0,0
16841,Lattice Model for Production of Gas,"  We define a lattice model for rock, absorbers, and gas that makes it possible
to examine the flow of gas to a complicated absorbing boundary over long
periods of time. The motivation is to deduce the geometry of the boundary from
the time history of gas absorption. We find a solution to this model using
Green's function techniques, and apply the solution to three absorbing networks
of increasing complexity.
",0,1,0,0,0,0
937,Toward Controlled Generation of Text,"  Generic generation and manipulation of text is challenging and has limited
success compared to recent deep generative modeling in visual domain. This
paper aims at generating plausible natural language sentences, whose attributes
are dynamically controlled by learning disentangled latent representations with
designated semantics. We propose a new neural generative model which combines
variational auto-encoders and holistic attribute discriminators for effective
imposition of semantic structures. With differentiable approximation to
discrete text samples, explicit constraints on independent attribute controls,
and efficient collaborative learning of generator and discriminators, our model
learns highly interpretable representations from even only word annotations,
and produces realistic sentences with desired attributes. Quantitative
evaluation validates the accuracy of sentence and attribute generation.
",1,0,0,1,0,0
18141,Variation of field enhancement factor near the emitter tip,"  The field enhancement factor at the emitter tip and its variation in a close
neighbourhood determines the emitter current in a Fowler-Nordheim like
formulation. For an axially symmetric emitter with a smooth tip, it is shown
that the variation can be accounted by a $\cos{\tilde{\theta}}$ factor in
appropriately defined normalized co-ordinates. This is shown analytically for a
hemi-ellipsoidal emitter and confirmed numerically for other emitter shapes
with locally quadratic tips.
",0,1,0,0,0,0
6410,Adversarial Variational Bayes Methods for Tweedie Compound Poisson Mixed Models,"  The Tweedie Compound Poisson-Gamma model is routinely used for modeling
non-negative continuous data with a discrete probability mass at zero. Mixed
models with random effects account for the covariance structure related to the
grouping hierarchy in the data. An important application of Tweedie mixed
models is pricing the insurance policies, e.g. car insurance. However, the
intractable likelihood function, the unknown variance function, and the
hierarchical structure of mixed effects have presented considerable challenges
for drawing inferences on Tweedie. In this study, we tackle the Bayesian
Tweedie mixed-effects models via variational inference approaches. In
particular, we empower the posterior approximation by implicit models trained
in an adversarial setting. To reduce the variance of gradients, we
reparameterize random effects, and integrate out one local latent variable of
Tweedie. We also employ a flexible hyper prior to ensure the richness of the
approximation. Our method is evaluated on both simulated and real-world data.
Results show that the proposed method has smaller estimation bias on the random
effects compared to traditional inference methods including MCMC; it also
achieves a state-of-the-art predictive performance, meanwhile offering a richer
estimation of the variance function.
",0,0,0,1,0,0
12573,Asymptotic theory of multiple-set linear canonical analysis,"  This paper deals with asymptotics for multiple-set linear canonical analysis
(MSLCA). A definition of this analysis, that adapts the classical one to the
context of Euclidean random variables, is given and properties of the related
canonical coefficients are derived. Then, estimators of the MSLCA's elements,
based on empirical covariance operators, are proposed and asymptotics for these
estimators are obtained. More precisely, we prove their consistency and we
obtain asymptotic normality for the estimator of the operator that gives MSLCA,
and also for the estimator of the vector of canonical coefficients. These
results are then used to obtain a test for mutual non-correlation between the
involved Euclidean random variables.
",0,0,1,1,0,0
134,Representing numbers as the sum of squares and powers in the ring $\mathbb{Z}_n$,"  We examine the representation of numbers as the sum of two squares in
$\mathbb{Z}_n$ for a general positive integer $n$. Using this information we
make some comments about the density of positive integers which can be
represented as the sum of two squares and powers of $2$ in $\mathbb{N}$.
",0,0,1,0,0,0
5099,State observation and sensor selection for nonlinear networks,"  A large variety of dynamical systems, such as chemical and biomolecular
systems, can be seen as networks of nonlinear entities. Prediction, control,
and identification of such nonlinear networks require knowledge of the state of
the system. However, network states are usually unknown, and only a fraction of
the state variables are directly measurable. The observability problem concerns
reconstructing the network state from this limited information. Here, we
propose a general optimization-based approach for observing the states of
nonlinear networks and for optimally selecting the observed variables. Our
results reveal several fundamental limitations in network observability, such
as the trade-off between the fraction of observed variables and the observation
length on one side, and the estimation error on the other side. We also show
that owing to the crucial role played by the dynamics, purely graph- theoretic
observability approaches cannot provide conclusions about one's practical
ability to estimate the states. We demonstrate the effectiveness of our methods
by finding the key components in biological and combustion reaction networks
from which we determine the full system state. Our results can lead to the
design of novel sensing principles that can greatly advance prediction and
control of the dynamics of such networks.
",1,0,1,0,0,0
5720,Diagnosing added value of convection-permitting regional models using precipitation event identification and tracking,"  Dynamical downscaling with high-resolution regional climate models may offer
the possibility of realistically reproducing precipitation and weather events
in climate simulations. As resolutions fall to order kilometers, the use of
explicit rather than parametrized convection may offer even greater fidelity.
However, these increased model resolutions both allow and require increasingly
complex diagnostics for evaluating model fidelity. In this study we use a suite
of dynamically downscaled simulations of the summertime U.S. (WRF driven by
NCEP) with systematic variations in parameters and treatment of convection as a
test case for evaluation of model precipitation. In particular, we use a novel
rainstorm identification and tracking algorithm that allocates essentially all
rainfall to individual precipitation events (Chang et al. 2016). This approach
allows multiple insights, including that, at least in these runs, model wet
bias is driven by excessive areal extent of precipitating events. Biases are
time-dependent, producing excessive diurnal cycle amplitude. We show that this
effect is produced not by new production of events but by excessive enlargement
of long-lived precipitation events during daytime, and that in the domain
average, precipitation biases appear best represented as additive offsets. Of
all model configurations evaluated, convection-permitting simulations most
consistently reduced biases in precipitation event characteristics.
",0,1,0,1,0,0
16375,Network Structure of Two-Dimensional Decaying Isotropic Turbulence,"  The present paper reports on our effort to characterize vortical interactions
in complex fluid flows through the use of network analysis. In particular, we
examine the vortex interactions in two-dimensional decaying isotropic
turbulence and find that the vortical interaction network can be characterized
by a weighted scale-free network. It is found that the turbulent flow network
retains its scale-free behavior until the characteristic value of circulation
reaches a critical value. Furthermore, we show that the two-dimensional
turbulence network is resilient against random perturbations but can be greatly
influenced when forcing is focused towards the vortical structures that are
categorized as network hubs. These findings can serve as a network-analytic
foundation to examine complex geophysical and thin-film flows and take
advantage of the rapidly growing field of network theory, which complements
ongoing turbulence research based on vortex dynamics, hydrodynamic stability,
and statistics. While additional work is essential to extend the mathematical
tools from network analysis to extract deeper physical insights of turbulence,
an understanding of turbulence based on the interaction-based network-theoretic
framework presents a promising alternative in turbulence modeling and control
efforts.
",0,1,0,0,0,0
5283,Toeplitz Order,"  A new approach to problems of the Uncertainty Principle in Harmonic Analysis,
based on the use of Toeplitz operators, has brought progress to some of the
classical problems in the area. The goal of this paper is to develop and
systematize the function theoretic component of the Toeplitz approach by
introducing a partial order on the set of inner functions induced by the action
of Toeplitz operators. We study connections of the new order with some of the
classical problems and known results. We discuss remaining problems and
possible directions for further research.
",0,0,1,0,0,0
9367,"Diversity, Topology, and the Risk of Node Re-identification in Labeled Social Graphs","  Real network datasets provide significant benefits for understanding
phenomena such as information diffusion or network evolution. Yet the privacy
risks raised from sharing real graph datasets, even when stripped of user
identity information, are significant. When nodes have associated attributes,
the privacy risks increase. In this paper we quantitatively study the impact of
binary node attributes on node privacy by employing machine-learning-based
re-identification attacks and exploring the interplay between graph topology
and attribute placement. Our experiments show that the population's diversity
on the binary attribute consistently degrades anonymity.
",1,0,0,0,0,0
14112,Notes on rate equations in nonlinear continuum mechanics,"  The paper gives an introduction to rate equations in nonlinear continuum
mechanics which should obey specific transformation rules. Emphasis is placed
on the geometrical nature of the operations involved in order to clarify the
different concepts. The paper is particularly concerned with common classes of
constitutive equations based on corotational stress rates and their proper
implementation in time for solving initial boundary value problems. Hypoelastic
simple shear is considered as an example application for the derived theory and
algorithms.
",1,1,0,0,0,0
5810,Sports stars: analyzing the performance of astronomers at visualization-based discovery,"  In this data-rich era of astronomy, there is a growing reliance on automated
techniques to discover new knowledge. The role of the astronomer may change
from being a discoverer to being a confirmer. But what do astronomers actually
look at when they distinguish between ""sources"" and ""noise?"" What are the
differences between novice and expert astronomers when it comes to visual-based
discovery? Can we identify elite talent or coach astronomers to maximize their
potential for discovery? By looking to the field of sports performance
analysis, we consider an established, domain-wide approach, where the expertise
of the viewer (i.e. a member of the coaching team) plays a crucial role in
identifying and determining the subtle features of gameplay that provide a
winning advantage. As an initial case study, we investigate whether the
SportsCode performance analysis software can be used to understand and document
how an experienced HI astronomer makes discoveries in spectral data cubes. We
find that the process of timeline-based coding can be applied to spectral cube
data by mapping spectral channels to frames within a movie. SportsCode provides
a range of easy to use methods for annotation, including feature-based codes
and labels, text annotations associated with codes, and image-based drawing.
The outputs, including instance movies that are uniquely associated with coded
events, provide the basis for a training program or team-based analysis that
could be used in unison with discipline specific analysis software. In this
coordinated approach to visualization and analysis, SportsCode can act as a
visual notebook, recording the insight and decisions in partnership with
established analysis methods. Alternatively, in situ annotation and coding of
features would be a valuable addition to existing and future visualisation and
analysis packages.
",0,1,0,0,0,0
1273,Suszko's Problem: Mixed Consequence and Compositionality,"  Suszko's problem is the problem of finding the minimal number of truth values
needed to semantically characterize a syntactic consequence relation. Suszko
proved that every Tarskian consequence relation can be characterized using only
two truth values. Malinowski showed that this number can equal three if some of
Tarski's structural constraints are relaxed. By so doing, Malinowski introduced
a case of so-called mixed consequence, allowing the notion of a designated
value to vary between the premises and the conclusions of an argument. In this
paper we give a more systematic perspective on Suszko's problem and on mixed
consequence. First, we prove general representation theorems relating
structural properties of a consequence relation to their semantic
interpretation, uncovering the semantic counterpart of substitution-invariance,
and establishing that (intersective) mixed consequence is fundamentally the
semantic counterpart of the structural property of monotonicity. We use those
to derive maximum-rank results proved recently in a different setting by French
and Ripley, as well as by Blasio, Marcos and Wansing, for logics with various
structural properties (reflexivity, transitivity, none, or both). We strengthen
these results into exact rank results for non-permeable logics (roughly, those
which distinguish the role of premises and conclusions). We discuss the
underlying notion of rank, and the associated reduction proposed independently
by Scott and Suszko. As emphasized by Suszko, that reduction fails to preserve
compositionality in general, meaning that the resulting semantics is no longer
truth-functional. We propose a modification of that notion of reduction,
allowing us to prove that over compact logics with what we call regular
connectives, rank results are maintained even if we request the preservation of
truth-functionality and additional semantic properties.
",1,0,1,0,0,0
4439,Nonparametric Variational Auto-encoders for Hierarchical Representation Learning,"  The recently developed variational autoencoders (VAEs) have proved to be an
effective confluence of the rich representational power of neural networks with
Bayesian methods. However, most work on VAEs use a rather simple prior over the
latent variables such as standard normal distribution, thereby restricting its
applications to relatively simple phenomena. In this work, we propose
hierarchical nonparametric variational autoencoders, which combines
tree-structured Bayesian nonparametric priors with VAEs, to enable infinite
flexibility of the latent representation space. Both the neural parameters and
Bayesian priors are learned jointly using tailored variational inference. The
resulting model induces a hierarchical structure of latent semantic concepts
underlying the data corpus, and infers accurate representations of data
instances. We apply our model in video representation learning. Our method is
able to discover highly interpretable activity hierarchies, and obtain improved
clustering accuracy and generalization capacity based on the learned rich
representations.
",1,0,0,1,0,0
3063,Transfer results for Frobenius extensions,"  We study Frobenius extensions which are free-filtered by a totally ordered,
finitely generated abelian group, and their free-graded counterparts. First we
show that the Frobenius property passes up from a free-graded extension to a
free-filtered extension, then also from a free-filtered extension to the
extension of their Rees algebras. Our main theorem states that, under some
natural hypotheses, a free-filtered extension of algebras is Frobenius if and
only if the associated graded extension is Frobenius. In the final section we
apply this theorem to provide new examples and non-examples of Frobenius
extensions.
",0,0,1,0,0,0
8871,Analysis of Extremely Obese Individuals Using Deep Learning Stacked Autoencoders and Genome-Wide Genetic Data,"  The aetiology of polygenic obesity is multifactorial, which indicates that
life-style and environmental factors may influence multiples genes to aggravate
this disorder. Several low-risk single nucleotide polymorphisms (SNPs) have
been associated with BMI. However, identified loci only explain a small
proportion of the variation ob-served for this phenotype. The linear nature of
genome wide association studies (GWAS) used to identify associations between
genetic variants and the phenotype have had limited success in explaining the
heritability variation of BMI and shown low predictive capacity in
classification studies. GWAS ignores the epistatic interactions that less
significant variants have on the phenotypic outcome. In this paper we utilise a
novel deep learning-based methodology to reduce the high dimensional space in
GWAS and find epistatic interactions between SNPs for classification purposes.
SNPs were filtered based on the effects associations have with BMI. Since
Bonferroni adjustment for multiple testing is highly conservative, an important
proportion of SNPs involved in SNP-SNP interactions are ignored. Therefore,
only SNPs with p-values < 1x10-2 were considered for subsequent epistasis
analysis using stacked auto encoders (SAE). This allows the nonlinearity
present in SNP-SNP interactions to be discovered through progressively smaller
hidden layer units and to initialise a multi-layer feedforward artificial
neural network (ANN) classifier. The classifier is fine-tuned to classify
extremely obese and non-obese individuals. The best results were obtained with
2000 compressed units (SE=0.949153, SP=0.933014, Gini=0.949936,
Lo-gloss=0.1956, AUC=0.97497 and MSE=0.054057). Using 50 compressed units it
was possible to achieve (SE=0.785311, SP=0.799043, Gini=0.703566,
Logloss=0.476864, AUC=0.85178 and MSE=0.156315).
",0,0,0,0,1,0
19563,Transition of multi-diffusive states in a biased periodic potential,"  We study a frequency-dependent damping model of hyper-diffusion within the
generalized Langevin equation. The model allows for the colored noise defined
by its spectral density, assumed to be proportional to $\omega^{\delta-1}$ at
low frequencies with $0<\delta<1$ (sub-Ohmic damping) or $1<\delta<2$
(super-Ohmic damping), where the frequency-dependent damping is deduced from
the noise by means of the fluctuation-dissipation theorem. It is shown that for
super-Ohmic damping and certain parameters, the diffusive process of the
particle in a titled periodic potential undergos sequentially four
time-regimes: thermalization, hyper-diffusion, collapse and asymptotical
restoration. For analysing transition phenomenon of multi-diffusive states, we
demonstrate that the first exist time of the particle escaping from the locked
state into the running state abides by an exponential distribution. The concept
of equivalent velocity trap is introduced in the present model, moreover,
reformation of ballistic diffusive system is also considered as a marginal
situation, however there does not exhibit the collapsed state of diffusion.
",0,1,0,0,0,0
4345,A robotic vision system to measure tree traits,"  The autonomous measurement of tree traits, such as branching structure,
branch diameters, branch lengths, and branch angles, is required for tasks such
as robotic pruning of trees as well as structural phenotyping. We propose a
robotic vision system called the Robotic System for Tree Shape Estimation
(RoTSE) to determine tree traits in field settings. The process is composed of
the following stages: image acquisition with a mobile robot unit, segmentation,
reconstruction, curve skeletonization, conversion to a graph representation,
and then computation of traits. Quantitative and qualitative results on apple
trees are shown in terms of accuracy, computation time, and robustness.
Compared to ground truth measurements, the RoTSE produced the following
estimates: branch diameter (mean-squared error $0.99$ mm), branch length
(mean-squared error $45.64$ mm), and branch angle (mean-squared error $10.36$
degrees). The average run time was 8.47 minutes when the voxel resolution was
$3$ mm$^3$.
",1,0,0,0,0,0
4450,Symmetric structure for the endomorphism algebra of projective-injective module in parabolic category,"  We show that for any singular dominant integral weight $\lambda$ of a complex
semisimple Lie algebra $\mathfrak{g}$, the endomorphism algebra $B$ of any
projective-injective module of the parabolic BGG category
$\mathcal{O}_\lambda^{\mathfrak{p}}$ is a symmetric algebra (as conjectured by
Khovanov) extending the results of Mazorchuk and Stroppel for the regular
dominant integral weight. Moreover, the endomorphism algebra $B$ is equipped
with a homogeneous (non-degenerate) symmetrizing form. In the appendix, there
is a short proof due to K. Coulembier and V. Mazorchuk showing that the
endomorphism algebra $B_\lambda^{\mathfrak{p}}$ of the basic
projective-injective module of $\mathcal{O}_\lambda^{\mathfrak{p}}$ is a
symmetric algebra.
",0,0,1,0,0,0
9150,Hierarchical Attention-Based Recurrent Highway Networks for Time Series Prediction,"  Time series prediction has been studied in a variety of domains. However, it
is still challenging to predict future series given historical observations and
past exogenous data. Existing methods either fail to consider the interactions
among different components of exogenous variables which may affect the
prediction accuracy, or cannot model the correlations between exogenous data
and target data. Besides, the inherent temporal dynamics of exogenous data are
also related to the target series prediction, and thus should be considered as
well. To address these issues, we propose an end-to-end deep learning model,
i.e., Hierarchical attention-based Recurrent Highway Network (HRHN), which
incorporates spatio-temporal feature extraction of exogenous variables and
temporal dynamics modeling of target variables into a single framework.
Moreover, by introducing the hierarchical attention mechanism, HRHN can
adaptively select the relevant exogenous features in different semantic levels.
We carry out comprehensive empirical evaluations with various methods over
several datasets, and show that HRHN outperforms the state of the arts in time
series prediction, especially in capturing sudden changes and sudden
oscillations of time series.
",0,0,0,1,0,0
1494,Improving the phase response of an atom interferometer by means of temporal pulse shaping,"  We study theoretically and experimentally the influence of temporally shaping
the light pulses in an atom interferometer, with a focus on the phase response
of the interferometer. We show that smooth light pulse shapes allow rejecting
high frequency phase fluctuations (above the Rabi frequency) and thus relax the
requirements on the phase noise or frequency noise of the interrogation lasers
driving the interferometer. The light pulse shape is also shown to modify the
scale factor of the interferometer, which has to be taken into account in the
evaluation of its accuracy budget. We discuss the trade-offs to operate when
choosing a particular pulse shape, by taking into account phase noise
rejection, velocity selectivity, and applicability to large momentum transfer
atom interferometry.
",0,1,0,0,0,0
11452,Fast sampling of parameterised Gaussian random fields,"  Gaussian random fields are popular models for spatially varying
uncertainties, arising for instance in geotechnical engineering, hydrology or
image processing. A Gaussian random field is fully characterised by its mean
function and covariance operator. In more complex models these can also be
partially unknown. In this case we need to handle a family of Gaussian random
fields indexed with hyperparameters. Sampling for a fixed configuration of
hyperparameters is already very expensive due to the nonlocal nature of many
classical covariance operators. Sampling from multiple configurations increases
the total computational cost severely. In this report we employ parameterised
Karhunen-Loève expansions for sampling. To reduce the cost we construct a
reduced basis surrogate built from snapshots of Karhunen-Loève eigenvectors.
In particular, we consider Matérn-type covariance operators with unknown
correlation length and standard deviation. We suggest a linearisation of the
covariance function and describe the associated online-offline decomposition.
In numerical experiments we investigate the approximation error of the reduced
eigenpairs. As an application we consider forward uncertainty propagation and
Bayesian inversion with an elliptic partial differential equation where the
logarithm of the diffusion coefficient is a parameterised Gaussian random
field. In the Bayesian inverse problem we employ Markov chain Monte Carlo on
the reduced space to generate samples from the posterior measure. All numerical
experiments are conducted in 2D physical space, with non-separable covariance
operators, and finite element grids with $\sim 10^4$ degrees of freedom.
",1,0,0,1,0,0
1535,Driving Interactive Graph Exploration Using 0-Dimensional Persistent Homology Features,"  Graphs are commonly used to encode relationships among entities, yet, their
abstractness makes them incredibly difficult to analyze. Node-link diagrams are
a popular method for drawing graphs. Classical techniques for the node-link
diagrams include various layout methods that rely on derived information to
position points, which often lack interactive exploration functionalities; and
force-directed layouts, which ignore global structures of the graph. This paper
addresses the graph drawing challenge by leveraging topological features of a
graph as derived information for interactive graph drawing. We first discuss
extracting topological features from a graph using persistent homology. We then
introduce an interactive persistence barcodes to study the substructures of a
force-directed graph layout; in particular, we add contracting and repulsing
forces guided by the 0-dimensional persistent homology features. Finally, we
demonstrate the utility of our approach across three datasets.
",1,0,0,0,0,0
1714,Wasserstein Introspective Neural Networks,"  We present Wasserstein introspective neural networks (WINN) that are both a
generator and a discriminator within a single model. WINN provides a
significant improvement over the recent introspective neural networks (INN)
method by enhancing INN's generative modeling capability. WINN has three
interesting properties: (1) A mathematical connection between the formulation
of the INN algorithm and that of Wasserstein generative adversarial networks
(WGAN) is made. (2) The explicit adoption of the Wasserstein distance into INN
results in a large enhancement to INN, achieving compelling results even with a
single classifier --- e.g., providing nearly a 20 times reduction in model size
over INN for unsupervised generative modeling. (3) When applied to supervised
classification, WINN also gives rise to improved robustness against adversarial
examples in terms of the error reduction. In the experiments, we report
encouraging results on unsupervised learning problems including texture, face,
and object modeling, as well as a supervised classification task against
adversarial attacks.
",1,0,0,0,0,0
13237,Quantum Origami: Transversal Gates for Quantum Computation and Measurement of Topological Order,"  In topology, a torus remains invariant under certain non-trivial
transformations known as modular transformations. In the context of
topologically ordered quantum states of matter, these transformations encode
the braiding statistics and fusion rules of emergent anyonic excitations and
thus serve as a diagnostic of topological order. Moreover, modular
transformations of higher genus surfaces, e.g. a torus with multiple handles,
can enhance the computational power of a topological state, in many cases
providing a universal fault-tolerant set of gates for quantum computation.
However, due to the intrusive nature of modular transformations, which
abstractly involve global operations and manifold surgery, physical
implementations of them in local systems have remained elusive. Here, we show
that by folding manifolds, modular transformations can be applied in a single
shot by independent local unitaries, providing a novel class of transversal
logic gates for fault-tolerant quantum computation. Specifically, we
demonstrate that multi-layer topological states with appropriate boundary
conditions and twist defects allow modular transformations to be effectively
implemented by a finite sequence of local SWAP gates between the layers. We
further provide methods to directly measure the modular matrices, and thus the
fractional statistics of anyonic excitations, providing a novel way to directly
measure topological order.
",0,1,0,0,0,0
2326,A Competitive Algorithm for Online Multi-Robot Exploration of a Translating Plume,"  In this paper, we study the problem of exploring a translating plume with a
team of aerial robots. The shape and the size of the plume are unknown to the
robots. The objective is to find a tour for each robot such that they
collectively explore the plume. Specifically, the tours must be such that each
point in the plume must be visible from the field-of-view of some robot along
its tour. We propose a recursive Depth-First Search (DFS)-based algorithm that
yields a constant competitive ratio for the exploration problem. The
competitive ratio is
$\frac{2(S_r+S_p)(R+\lfloor\log{R}\rfloor)}{(S_r-S_p)(1+\lfloor\log{R}\rfloor)}$
where $R$ is the number of robots, and $S_r$ and $S_p$ are the robot speed and
the plume speed, respectively. We also consider a more realistic scenario where
the plume shape is not restricted to grid cells but an arbitrary shape. We show
our algorithm has
$\frac{2(S_r+S_p)(18R+\lfloor\log{R}\rfloor)}{(S_r-S_p)(1+\lfloor\log{R}\rfloor)}$
competitive ratio under the fat condition. We empirically verify our algorithm
using simulations.
",1,0,0,0,0,0
19577,First functionality tests of a 64 x 64 pixel DSSC sensor module connected to the complete ladder readout,"  The European X-ray Free Electron Laser (XFEL.EU) will provide every 0.1 s a
train of 2700 spatially coherent ultrashort X-ray pulses at 4.5 MHz repetition
rate. The Small Quantum Systems (SQS) instrument and the Spectroscopy and
Coherent Scattering instrument (SCS) operate with soft X-rays between 0.5 keV -
6keV. The DEPFET Sensor with Signal Compression (DSSC) detector is being
developed to meet the requirements set by these two XFEL.EU instruments. The
DSSC imager is a 1 mega-pixel camera able to store up to 800 single-pulse
images per train. The so-called ladder is the basic unit of the DSSC detector.
It is the single unit out of sixteen identical-units composing the
DSSC-megapixel camera, containing all representative electronic components of
the full-size system and allows testing the full electronic chain. Each DSSC
ladder has a focal plane sensor with 128 x 512 pixels. The read-out ASIC
provides full-parallel readout of the sensor pixels. Every read-out channel
contains an amplifier and an analog filter, an up-to 9 bit ADC and the digital
memory. The ASIC amplifier have a double front-end to allow one to use either
DEPFET sensors or Mini-SDD sensors. In the first case, the signal compression
is a characteristic intrinsic of the sensor; in the second case, the
compression is implemented at the first amplification stage. The goal of signal
compression is to meet the requirement of single-photon detection capability
and wide dynamic range. We present the first results of measurements obtained
using a 64 x 64 pixel DEPFET sensor attached to the full final electronic and
data-acquisition chain.
",0,1,0,0,0,0
2251,Extended Trust-Region Problems with One or Two Balls: Exact Copositive and Lagrangian Relaxations,"  We establish a geometric condition guaranteeing exact copositive relaxation
for the nonconvex quadratic optimization problem under two quadratic and
several linear constraints, and present sufficient conditions for global
optimality in terms of generalized Karush-Kuhn-Tucker multipliers. The
copositive relaxation is tighter than the usual Lagrangian relaxation. We
illustrate this by providing a whole class of quadratic optimization problems
that enjoys exactness of copositive relaxation while the usual Lagrangian
duality gap is infinite. Finally, we also provide verifiable conditions under
which both the usual Lagrangian relaxation and the copositive relaxation are
exact for an extended CDT (two-ball trust-region) problem. Importantly, the
sufficient conditions can be verified by solving linear optimization problems.
",0,0,1,0,0,0
5513,On the Global Continuity of the Roots of Families of Monic Polynomials (in Russian),"  We raise a question on the existence of continuous roots of families of monic
polynomials (by the root of a family of polynomials we mean a function of the
coefficients of polynomials of a given family that maps each tuple of
coefficients to a root of the polynomial with these coefficients). We prove
that the family of monic second-degree polynomials with complex coefficients
and the families of monic fourth-degree and fifth-degree polynomials with real
coefficients have no continuous root. We also prove that the family of monic
second-degree polynomials with real coefficients has continuous roots and we
describe the set of all such roots.
",0,0,1,0,0,0
6007,Operationalizing Conflict and Cooperation between Automated Software Agents in Wikipedia: A Replication and Expansion of 'Even Good Bots Fight',"  This paper replicates, extends, and refutes conclusions made in a study
published in PLoS ONE (""Even Good Bots Fight""), which claimed to identify
substantial levels of conflict between automated software agents (or bots) in
Wikipedia using purely quantitative methods. By applying an integrative
mixed-methods approach drawing on trace ethnography, we place these alleged
cases of bot-bot conflict into context and arrive at a better understanding of
these interactions. We found that overwhelmingly, the interactions previously
characterized as problematic instances of conflict are typically better
characterized as routine, productive, even collaborative work. These results
challenge past work and show the importance of qualitative/quantitative
collaboration. In our paper, we present quantitative metrics and qualitative
heuristics for operationalizing bot-bot conflict. We give thick descriptions of
kinds of events that present as bot-bot reverts, helping distinguish conflict
from non-conflict. We computationally classify these kinds of events through
patterns in edit summaries. By interpreting found/trace data in the
socio-technical contexts in which people give that data meaning, we gain more
from quantitative measurements, drawing deeper understandings about the
governance of algorithmic systems in Wikipedia. We have also released our data
collection, processing, and analysis pipeline, to facilitate computational
reproducibility of our findings and to help other researchers interested in
conducting similar mixed-method scholarship in other platforms and contexts.
",1,0,0,0,0,0
14421,Disunited Nations? A Multiplex Network Approach to Detecting Preference Affinity Blocs using Texts and Votes,"  This paper contributes to an emerging literature that models votes and text
in tandem to better understand polarization of expressed preferences. It
introduces a new approach to estimate preference polarization in
multidimensional settings, such as international relations, based on
developments in the natural language processing and network science literatures
-- namely word embeddings, which retain valuable syntactical qualities of human
language, and community detection in multilayer networks, which locates densely
connected actors across multiple, complex networks. We find that the employment
of these tools in tandem helps to better estimate states' foreign policy
preferences expressed in UN votes and speeches beyond that permitted by votes
alone. The utility of these located affinity blocs is demonstrated through an
application to conflict onset in International Relations, though these tools
will be of interest to all scholars faced with the measurement of preferences
and polarization in multidimensional settings.
",1,0,0,0,0,0
18502,Communicating Correlated Sources Over an Interference Channel,"  A new coding technique, based on \textit{fixed block-length} codes, is
proposed for the problem of communicating a pair of correlated sources over a
$2-$user interference channel. Its performance is analyzed to derive a new set
of sufficient conditions. The latter is proven to be strictly less binding than
the current known best, which is due to Liu and Chen [Dec, 2011]. Our findings
are inspired by Dueck's example [March, 1981].
",1,0,0,0,0,0
20961,A New Tracking Algorithm for Multiple Colloidal Particles Close to Contact,"  In this paper, we propose a new algorithm based on radial symmetry center
method to track colloidal particles close to contact, where the optical images
of the particles start to overlap in digital video microscopy. This overlapping
effect is important to observe the pair interaction potential in colloidal
studies and it appears as additional interaction in the measurement of the
interaction with conventional tracking analysis. The proposed algorithm in this
work is simple, fast and applicable for not only two particles but also three
and more particles without any modification. The algorithm uses gradient
vectors of the particle intensity distribution, which allows us to use a part
of the symmetric intensity distribution in the calculation of the actual
particle position. In this study, simulations are performed to see the
performance of the proposed algorithm for two and three particles, where the
simulation images are generated by using fitted curve to experimental particle
image for different sized particles. As a result, the algorithm yields the
maximum error smaller than 2 nm for 5.53 {\mu}m silica particles in contact
condition.
",0,1,0,0,0,0
18332,Deep Learning for Real-time Gravitational Wave Detection and Parameter Estimation with LIGO Data,"  The recent Nobel-prize-winning detections of gravitational waves from merging
black holes and the subsequent detection of the collision of two neutron stars
in coincidence with electromagnetic observations have inaugurated a new era of
multimessenger astrophysics. To enhance the scope of this emergent science, we
proposed the use of deep convolutional neural networks for the detection and
characterization of gravitational wave signals in real-time. This method, Deep
Filtering, was initially demonstrated using simulated LIGO noise. In this
article, we present the extension of Deep Filtering using real data from the
first observing run of LIGO, for both detection and parameter estimation of
gravitational waves from binary black hole mergers with continuous data streams
from multiple LIGO detectors. We show for the first time that machine learning
can detect and estimate the true parameters of a real GW event observed by
LIGO. Our comparisons show that Deep Filtering is far more computationally
efficient than matched-filtering, while retaining similar sensitivity and lower
errors, allowing real-time processing of weak time-series signals in
non-stationary non-Gaussian noise, with minimal resources, and also enables the
detection of new classes of gravitational wave sources that may go unnoticed
with existing detection algorithms. This approach is uniquely suited to enable
coincident detection campaigns of gravitational waves and their multimessenger
counterparts in real-time.
",1,1,0,0,0,0
12308,"Linear, Second order and Unconditionally Energy Stable schemes for a phase-field moving contact line Model","  In this paper, we consider the numerical approximations for solving a
hydrodynamics coupled phase field model consisting of incompressible
Navier-Stokes equations with generalized Navier boundary conditions, and the
Cahn-Hilliard equation with dynamic moving contact line boundary conditions.
The main challenging issue for solving this model numerically is the time
marching problem, i.e., how to develop suitable higher order temporal schemes
while preserving the unconditional energy stability at the discrete level. We
solve this issue by developing two linear, second-order schemes based on the
""Invariant Energy Quadratization"" method for the nonlinear terms in the bulk
and on the boundary, the projection method for the Navier-Stokes equations, and
a subtle implicit-explicit treatment for the stress and convective terms.
Rigorous proofs of the well-posedness of the linear system and the
unconditional energy stabilities are provided. A spectral-Galerkin spatial
discretization is implemented and various numerical results are presented to
verify the second order accuracy and the efficiency of the proposed schemes.
",0,0,1,0,0,0
10581,"Convolutional Analysis Operator Learning: Acceleration, Convergence, Application, and Neural Networks","  Convolutional operator learning is increasingly gaining attention in many
signal processing and computer vision applications. Learning kernels has mostly
relied on so-called local approaches that extract and store many overlapping
patches across training signals. Due to memory demands, local approaches have
limitations when learning kernels from large datasets -- particularly with
multi-layered structures, e.g., convolutional neural network (CNN) -- and/or
applying the learned kernels to high-dimensional signal recovery problems. The
so-called global approach has been studied within the ""synthesis"" signal model,
e.g., convolutional dictionary learning, overcoming the memory problems by
careful algorithmic designs. This paper proposes a new convolutional analysis
operator learning (CAOL) framework in the global approach, and develops a new
convergent Block Proximal Gradient method using a Majorizer (BPG-M) to solve
the corresponding block multi-nonconvex problems. To learn diverse filters
within the CAOL framework, this paper introduces an orthogonality constraint
that enforces a tight-frame (TF) filter condition, and a regularizer that
promotes diversity between filters. Numerical experiments show that, for tight
majorizers, BPG-M significantly accelerates the CAOL convergence rate compared
to the state-of-the-art method, BPG. Numerical experiments for sparse-view
computational tomography show that CAOL using TF filters significantly improves
reconstruction quality compared to a conventional edge-preserving regularizer.
Finally, this paper shows that CAOL can be useful to mathematically model a
CNN, and the corresponding updates obtained via BPG-M coincide with core
modules of the CNN.
",0,0,0,1,0,0
14091,Optimal group testing designs for estimating prevalence with uncertain testing errors,"  We construct optimal designs for group testing experiments where the goal is
to estimate the prevalence of a trait by using a test with uncertain
sensitivity and specificity. Using optimal design theory for approximate
designs, we show that the most efficient design for simultaneously estimating
the prevalence, sensitivity and specificity requires three different group
sizes with equal frequencies. However, if estimating prevalence as accurately
as possible is the only focus, the optimal strategy is to have three group
sizes with unequal frequencies. On the basis of a chlamydia study in the
U.S.A., we compare performances of competing designs and provide insights into
how the unknown sensitivity and specificity of the test affect the performance
of the prevalence estimator. We demonstrate that the locally D- and Ds-optimal
designs proposed have high efficiencies even when the prespecified values of
the parameters are moderately misspecified.
",0,0,1,1,0,0
7726,A Topological proof that $O_2$ is $2$-MCFL,"  We give a new proof of Salvati's theorem that the group language $O_2$ is $2$
multiple context free. Unlike Salvati's proof, our arguments do not use any
idea specific to two-dimensions. This raises the possibility that the argument
might generalize to $O_n$.
",1,0,1,0,0,0
10610,Faster Betweenness Centrality Updates in Evolving Networks,"  Finding central nodes is a fundamental problem in network analysis.
Betweenness centrality is a well-known measure which quantifies the importance
of a node based on the fraction of shortest paths going though it. Due to the
dynamic nature of many today's networks, algorithms that quickly update
centrality scores have become a necessity. For betweenness, several dynamic
algorithms have been proposed over the years, targeting different update types
(incremental- and decremental-only, fully-dynamic). In this paper we introduce
a new dynamic algorithm for updating betweenness centrality after an edge
insertion or an edge weight decrease. Our method is a combination of two
independent contributions: a faster algorithm for updating pairwise distances
as well as number of shortest paths, and a faster algorithm for updating
dependencies. Whereas the worst-case running time of our algorithm is the same
as recomputation, our techniques considerably reduce the number of operations
performed by existing dynamic betweenness algorithms.
",1,0,0,0,0,0
10485,Early stopping for statistical inverse problems via truncated SVD estimation,"  We consider truncated SVD (or spectral cut-off, projection) estimators for a
prototypical statistical inverse problem in dimension $D$. Since calculating
the singular value decomposition (SVD) only for the largest singular values is
much less costly than the full SVD, our aim is to select a data-driven
truncation level $\widehat m\in\{1,\ldots,D\}$ only based on the knowledge of
the first $\widehat m$ singular values and vectors. We analyse in detail
whether sequential {\it early stopping} rules of this type can preserve
statistical optimality. Information-constrained lower bounds and matching upper
bounds for a residual based stopping rule are provided, which give a clear
picture in which situation optimal sequential adaptation is feasible. Finally,
a hybrid two-step approach is proposed which allows for classical oracle
inequalities while considerably reducing numerical complexity.
",0,0,1,1,0,0
10432,Influence of material parameters on the performance of niobium based superconducting RF cavities,"  A detailed thermal analysis of a Niobium (Nb) based superconducting radio
frequency (SRF) cavity in a liquid helium bath is presented, taking into
account the temperature and magnetic field dependence of the surface resistance
and thermal conductivity in the superconducting state of the starting Nb
material (for SRF cavity fabrication) with different impurity levels. The drop
in SRF cavity quality factor (Q_0) in the high acceleration gradient regime
(before ultimate breakdown of the SRF cavity) is studied in details. It is
argued that the high field Q_0-drop in SRF cavity is considerably influenced by
the intrinsic material parameters such as electrical conductivity, and thermal
diffusivity. The detail analysis also shows that the current specification on
the purity of niobium material for SRF cavity fabrication is somewhat over
specified. Niobium material with a relatively low purity can very well serve
the purpose for the accelerators dedicated for spallation neutron source (SNS)
or accelerator driven sub-critical system (ADSS) applications, where the
required accelerating gradient is typically up to 20 MV/m,. This information
will have important implication towards the cost reduction of superconducting
technology based particle accelerators for various applications.
",0,1,0,0,0,0
11151,Kinetic cascade in solar-wind turbulence: 3D3V hybrid-kinetic simulations with electron inertia,"  Understanding the nature of the turbulent fluctuations below the ion
gyroradius in solar-wind turbulence is a great challenge. Recent studies have
been mostly in favor of kinetic Alfvén wave (KAW) type of fluctuations, but
other kinds of fluctuations with characteristics typical of magnetosonic,
whistler and ion Bernstein modes, could also play a role depending on the
plasma parameters. Here we investigate the properties of the sub-proton-scale
cascade with high-resolution hybrid-kinetic simulations of freely-decaying
turbulence in 3D3V phase space, including electron inertia effects. Two proton
plasma beta are explored: the ""intermediate"" $\beta_p=1$ and ""low""
$\beta_p=0.2$ regimes, both typically observed in solar wind and corona. The
magnetic energy spectum exhibits $k_\perp^{-8/3}$ and $k_\|^{-7/2}$ power laws
at $\beta_p=1$, while they are slightly steeper at $\beta_p=0.2$. Nevertheless,
both regimes develop a spectral anisotropy consistent with $k_\|\sim
k_\perp^{2/3}$ at $k_\perp\rho_p>1$, and pronounced small-scale intermittency.
In this context, we find that the kinetic-scale cascade is dominated by
KAW-like fluctuations at $\beta_p=1$, whereas the low-$\beta$ case presents a
more complex scenario suggesting the simultaneous presence of different types
of fluctuations. In both regimes, however, a non-negligible role of ion
Bernstein type of fluctuations at the smallest scales seems to emerge.
",0,1,0,0,0,0
14670,Non-iterative Label Propagation in Optimal Leading Forest,"  Graph based semi-supervised learning (GSSL) has intuitive representation and
can be improved by exploiting the matrix calculation. However, it has to
perform iterative optimization to achieve a preset objective, which usually
leads to low efficiency. Another inconvenience lying in GSSL is that when new
data come, the graph construction and the optimization have to be conducted all
over again. We propose a sound assumption, arguing that: the neighboring data
points are not in peer-to-peer relation, but in a partial-ordered relation
induced by the local density and distance between the data; and the label of a
center can be regarded as the contribution of its followers. Starting from the
assumption, we develop a highly efficient non-iterative label propagation
algorithm based on a novel data structure named as optimal leading forest
(LaPOLeaF). The major weaknesses of the traditional GSSL are addressed by this
study. We further scale LaPOLeaF to accommodate big data by utilizing block
distance matrix technique, parallel computing, and Locality-Sensitive Hashing
(LSH). Experiments on large datasets have shown the promising results of the
proposed methods.
",1,0,0,0,0,0
18007,One-Dimensional Symmetry Protected Topological Phases and their Transitions,"  We present a unified perspective on symmetry protected topological (SPT)
phases in one dimension and address the open question of what characterizes
their phase transitions. In the first part of this work we use symmetry as a
guide to map various well-known fermionic and spin SPTs to a Kitaev chain with
coupling of range $\alpha \in \mathbb Z$. This unified picture uncovers new
properties of old models --such as how the cluster state is the fixed point
limit of the Affleck-Kennedy-Lieb-Tasaki state in disguise-- and elucidates the
connection between fermionic and bosonic phases --with the Hubbard chain
interpolating between four Kitaev chains and a spin chain in the Haldane phase.
In the second part, we study the topological phase transitions between these
models in the presence of interactions. This leads us to conjecture that the
critical point between any SPT with $d$-dimensional edge modes and the trivial
phase has a central charge $c \geq \log_2 d$. We analytically verify this for
many known transitions. This agrees with the intuitive notion that the phase
transition is described by a delocalized edge mode, and that the central charge
of a conformal field theory is a measure of the gapless degrees of freedom.
",0,1,0,0,0,0
16344,Bayesian model checking: A comparison of tests,"  Two procedures for checking Bayesian models are compared using a simple test
problem based on the local Hubble expansion. Over four orders of magnitude,
p-values derived from a global goodness-of-fit criterion for posterior
probability density functions (Lucy 2017) agree closely with posterior
predictive p-values. The former can therefore serve as an effective proxy for
the difficult-to-calculate posterior predictive p-values.
",0,1,0,1,0,0
11328,Intrinsic Analysis of the Sample Fréchet Mean and Sample Mean of Complex Wishart Matrices,"  We consider two types of averaging of complex covariance matrices, a sample
mean (average) and the sample Fréchet mean. We analyse the performance of
these quantities as estimators for the true covariance matrix via `intrinsic'
versions of bias and mean square error, a methodology which takes account of
geometric structure. We derive simple expressions for the intrinsic bias in
both cases, and the simple average is seen to be preferable. The same is true
for the asymptotic Riemannian risk, and for the Riemannian risk itself in the
scalar case. Combined with a similar preference for the simple average using
non-intrinsic analysis, we conclude that the simple average is preferred
overall to the sample Fréchet mean in this context.
",0,0,1,1,0,0
20293,Measuring the Eccentricity of Items,"  The long-tail phenomenon tells us that there are many items in the tail.
However, not all tail items are the same. Each item acquires different kinds of
users. Some items are loved by the general public, while some items are
consumed by eccentric fans. In this paper, we propose a novel metric, item
eccentricity, to incorporate this difference between consumers of the items.
Eccentric items are defined as items that are consumed by eccentric users. We
used this metric to analyze two real-world datasets of music and movies and
observed the characteristics of items in terms of eccentricity. The results
showed that our defined eccentricity of an item does not change much over time,
and classified eccentric and noneccentric items present significantly distinct
characteristics. The proposed metric effectively separates the eccentric and
noneccentric items mixed in the tail, which could not be done with the previous
measures, which only consider the popularity of items.
",1,0,0,0,0,0
17904,Concept Drift Detection and Adaptation with Hierarchical Hypothesis Testing,"  A fundamental issue for statistical classification models in a streaming
environment is that the joint distribution between predictor and response
variables changes over time (a phenomenon also known as concept drifts), such
that their classification performance deteriorates dramatically. In this paper,
we first present a hierarchical hypothesis testing (HHT) framework that can
detect and also adapt to various concept drift types (e.g., recurrent or
irregular, gradual or abrupt), even in the presence of imbalanced data labels.
A novel concept drift detector, namely Hierarchical Linear Four Rates (HLFR),
is implemented under the HHT framework thereafter. By substituting a
widely-acknowledged retraining scheme with an adaptive training strategy, we
further demonstrate that the concept drift adaptation capability of HLFR can be
significantly boosted. The theoretical analysis on the Type-I and Type-II
errors of HLFR is also performed. Experiments on both simulated and real-world
datasets illustrate that our methods outperform state-of-the-art methods in
terms of detection precision, detection delay as well as the adaptability
across different concept drift types.
",1,0,0,1,0,0
1347,Automated Assistants to Identify and Prompt Action on Visual News Bias,"  Bias is a common problem in today's media, appearing frequently in text and
in visual imagery. Users on social media websites such as Twitter need better
methods for identifying bias. Additionally, activists --those who are motivated
to effect change related to some topic, need better methods to identify and
counteract bias that is contrary to their mission. With both of these use cases
in mind, in this paper we propose a novel tool called UnbiasedCrowd that
supports identification of, and action on bias in visual news media. In
particular, it addresses the following key challenges (1) identification of
bias; (2) aggregation and presentation of evidence to users; (3) enabling
activists to inform the public of bias and take action by engaging people in
conversation with bots. We describe a preliminary study on the Twitter platform
that explores the impressions that activists had of our tool, and how people
reacted and engaged with online bots that exposed visual bias. We conclude by
discussing design and implication of our findings for creating future systems
to identify and counteract the effects of news bias.
",1,0,0,0,0,0
5438,Optimal segmentation of directed graph and the minimum number of feedback arcs,"  The minimum feedback arc set problem asks to delete a minimum number of arcs
(directed edges) from a digraph (directed graph) to make it free of any
directed cycles. In this work we approach this fundamental cycle-constrained
optimization problem by considering a generalized task of dividing the digraph
into D layers of equal size. We solve the D-segmentation problem by the
replica-symmetric mean field theory and belief-propagation heuristic
algorithms. The minimum feedback arc density of a given random digraph ensemble
is then obtained by extrapolating the theoretical results to the limit of large
D. A divide-and-conquer algorithm (nested-BPR) is devised to solve the minimum
feedback arc set problem with very good performance and high efficiency.
",1,1,0,0,0,0
1457,Fast Global Convergence via Landscape of Empirical Loss,"  While optimizing convex objective (loss) functions has been a powerhouse for
machine learning for at least two decades, non-convex loss functions have
attracted fast growing interests recently, due to many desirable properties
such as superior robustness and classification accuracy, compared with their
convex counterparts. The main obstacle for non-convex estimators is that it is
in general intractable to find the optimal solution. In this paper, we study
the computational issues for some non-convex M-estimators. In particular, we
show that the stochastic variance reduction methods converge to the global
optimal with linear rate, by exploiting the statistical property of the
population loss. En route, we improve the convergence analysis for the batch
gradient method in \cite{mei2016landscape}.
",0,0,0,1,0,0
19159,Detailed experimental and numerical analysis of a cylindrical cup deep drawing: pros and cons of using solid-shell elements,"  The Swift test was originally proposed as a formability test to reproduce the
conditions observed in deep drawing operations. This test consists on forming a
cylindrical cup from a circular blank, using a flat bottom cylindrical punch
and has been extensively studied using both analytical and numerical methods.
This test can also be combined with the Demeri test, which consists in cutting
a ring from the wall of a cylindrical cup, in order to open it afterwards to
measure the springback. This combination allows their use as benchmark test, in
order to improve the knowledge concerning the numerical simulation models,
through the comparison between experimental and numerical results. The focus of
this study is the experimental and numerical analyses of the Swift cup test,
followed by the Demeri test, performed with an AA5754-O alloy at room
temperature. In this context, a detailed analysis of the punch force evolution,
the thickness evolution along the cup wall, the earing profile, the strain
paths and their evolution and the ring opening is performed. The numerical
simulation is performed using the finite element code ABAQUS, with solid and
solid-shell elements, in order to compare the computational efficiency of these
type of elements. The results show that the solid-shell element is more
cost-effective than the solid, presenting global accurate predictions, excepted
for the thinning zones. Both the von Mises and the Hill48 yield criteria
predict the strain distributions in the final cup quite accurately. However,
improved knowledge concerning the stress states is still required, because the
Hill48 criterion showed difficulties in the correct prediction of the
springback, whatever the type of finite element adopted.
",0,1,0,0,0,0
9193,code2seq: Generating Sequences from Structured Representations of Code,"  The ability to generate natural language sequences from source code snippets
has a variety of applications such as code summarization, documentation, and
retrieval. Sequence-to-sequence (seq2seq) models, adopted from neural machine
translation (NMT), have achieved state-of-the-art performance on these tasks by
treating source code as a sequence of tokens. We present ${\rm {\scriptsize
CODE2SEQ}}$: an alternative approach that leverages the syntactic structure of
programming languages to better encode source code. Our model represents a code
snippet as the set of compositional paths in its abstract syntax tree (AST) and
uses attention to select the relevant paths while decoding. We demonstrate the
effectiveness of our approach for two tasks, two programming languages, and
four datasets of up to $16$M examples. Our model significantly outperforms
previous models that were specifically designed for programming languages, as
well as state-of-the-art NMT models. An interactive online demo of our model is
available at this http URL. Our code, data and trained models are
available at this http URL.
",1,0,0,1,0,0
13794,Proceedings of the 2017 AdKDD & TargetAd Workshop,"  Proceedings of the 2017 AdKDD and TargetAd Workshop held in conjunction with
the 23rd ACM SIGKDD Conference on Knowledge Discovery and Data Mining Halifax,
Nova Scotia, Canada.
",1,0,0,0,0,0
17717,Atomistic simulations of dislocation/precipitation interactions in Mg-Al alloys and implications for precipitation hardening,"  Atomistic simulations were carried out to analyze the interaction between $<
a>$ basal dislocations and precipitates in Mg-Al alloys and the associated
strengthening mechanisms.
",0,1,0,0,0,0
2886,Low Resolution Face Recognition Using a Two-Branch Deep Convolutional Neural Network Architecture,"  We propose a novel couple mappings method for low resolution face recognition
using deep convolutional neural networks (DCNNs). The proposed architecture
consists of two branches of DCNNs to map the high and low resolution face
images into a common space with nonlinear transformations. The branch
corresponding to transformation of high resolution images consists of 14 layers
and the other branch which maps the low resolution face images to the common
space includes a 5-layer super-resolution network connected to a 14-layer
network. The distance between the features of corresponding high and low
resolution images are backpropagated to train the networks. Our proposed method
is evaluated on FERET data set and compared with state-of-the-art competing
methods. Our extensive experimental results show that the proposed method
significantly improves the recognition performance especially for very low
resolution probe face images (11.4% improvement in recognition accuracy).
Furthermore, it can reconstruct a high resolution image from its corresponding
low resolution probe image which is comparable with state-of-the-art
super-resolution methods in terms of visual quality.
",1,0,0,0,0,0
2813,Evidence of new twinning modes in magnesium questioning the shear paradigm,"  Twinning is an important deformation mode of hexagonal close-packed metals.
The crystallographic theory is based on the 150-years old concept of simple
shear. The habit plane of the twin is the shear plane, it is invariant. Here we
present Electron BackScatter Diffraction observations and crystallographic
analysis of a millimeter size twin in a magnesium single crystal whose straight
habit plane, unambiguously determined both the parent crystal and in its twin,
is not an invariant plane. This experimental evidence demonstrates that
macroscopic deformation twinning can be obtained by a mechanism that is not a
simple shear. Beside, this unconventional twin is often co-formed with a new
conventional twin that exhibits the lowest shear magnitude ever reported in
metals. The existence of unconventional twinning introduces a shift of paradigm
and calls for the development of a new theory for the displacive
transformations
",0,1,0,0,0,0
8197,Topological Dirac Nodal-net Fermions in AlB$_2$-type TiB$_2$ and ZrB$_2$,"  Based on first-principles calculations and effective model analysis, a Dirac
nodal-net semimetal state is recognized in AlB$_2$-type TiB$_2$ and ZrB$_2$
when spin-orbit coupling (SOC) is ignored. Taking TiB$_2$ as an example, there
are several topological excitations in this nodal-net structure including
triple point, nexus, and nodal link, which are protected by coexistence of
spatial-inversion symmetry and time reversal symmetry. This nodal-net state is
remarkably different from that of IrF$_4$, which requires sublattice chiral
symmetry. In addition, linearly and quadratically dispersed two-dimensional
surface Dirac points are identified as having emerged on the B-terminated and
Ti-terminated (001) surfaces of TiB$_2$ respectively, which are analogous to
those of monolayer and bilayer graphene.
",0,1,0,0,0,0
11585,Extensions and Exact Solutions to the Quaternion-Based RMSD Problem,"  We examine the problem of transforming matching collections of data points
into optimal correspondence. The classic RMSD (root-mean-square deviation)
method calculates a 3D rotation that minimizes the RMSD of a set of test data
points relative to a reference set of corresponding points. Similar literature
in aeronautics, photogrammetry, and proteomics employs numerical methods to
find the maximal eigenvalue of a particular $4\!\times\! 4$ quaternion-based
matrix, thus specifying the quaternion eigenvector corresponding to the optimal
3D rotation. Here we generalize this basic problem, sometimes referred to as
the ""Procrustes Problem,"" and present algebraic solutions that exhibit
properties that are inaccessible to traditional numerical methods. We begin
with the 4D data problem, a problem one dimension higher than the conventional
3D problem, but one that is also solvable by quaternion methods, we then study
the 3D and 2D data problems as special cases. In addition, we consider data
that are themselves quaternions isomorphic to orthonormal triads describing 3
coordinate frames (amino acids in proteins possess such frames). Adopting a
reasonable approximation to the exact quaternion-data minimization problem, we
find a novel closed form ""quaternion RMSD"" (QRMSD) solution for the optimal
rotation from a quaternion data set to a reference set. We observe that
composites of the RMSD and QRMSD measures, combined with problem-dependent
parameters including scaling factors to make their incommensurate dimensions
compatible, could be suitable for certain matching tasks.
",0,0,0,0,1,0
4955,Predicted novel insulating electride compound between alkali metals lithium and sodium under high pressure,"  The application of high pressure can fundamentally modify the crystalline and
electronic structures of elements as well as their chemical reactivity, which
could lead to the formation of novel materials. Here, we explore the reactivity
of lithium with sodium under high pressure, using a swarm structure searching
techniques combined with first-principles calculations, which identify a
thermodynamically stable LiNa compound adopting an orthorhombic oP8 phase at
pressure above 355 GPa. The formation of LiNa may be a consequence of strong
concentration of electrons transfer from the lithium and the sodium atoms into
the interstitial sites, which also leads to opening a relatively wide band gap
for LiNa-op8. This is substantially different from the picture that share or
exchange electrons in common compounds and alloys. In addition, lattice-dynamic
calculations indicate that LiNa-op8 remains dynamically stable when pressure
decompresses down to 70 GPa.
",0,1,0,0,0,0
8987,When is a Convolutional Filter Easy To Learn?,"  We analyze the convergence of (stochastic) gradient descent algorithm for
learning a convolutional filter with Rectified Linear Unit (ReLU) activation
function. Our analysis does not rely on any specific form of the input
distribution and our proofs only use the definition of ReLU, in contrast with
previous works that are restricted to standard Gaussian input. We show that
(stochastic) gradient descent with random initialization can learn the
convolutional filter in polynomial time and the convergence rate depends on the
smoothness of the input distribution and the closeness of patches. To the best
of our knowledge, this is the first recovery guarantee of gradient-based
algorithms for convolutional filter on non-Gaussian input distributions. Our
theory also justifies the two-stage learning rate strategy in deep neural
networks. While our focus is theoretical, we also present experiments that
illustrate our theoretical findings.
",1,0,0,1,0,0
16799,K-means Algorithm over Compressed Binary Data,"  We consider a network of binary-valued sensors with a fusion center. The
fusion center has to perform K-means clustering on the binary data transmitted
by the sensors. In order to reduce the amount of data transmitted within the
network, the sensors compress their data with a source coding scheme based on
binary sparse matrices. We propose to apply the K-means algorithm directly over
the compressed data without reconstructing the original sensors measurements,
in order to avoid potentially complex decoding operations. We provide
approximated expressions of the error probabilities of the K-means steps in the
compressed domain. From these expressions, we show that applying the K-means
algorithm in the compressed domain enables to recover the clusters of the
original domain. Monte Carlo simulations illustrate the accuracy of the
obtained approximated error probabilities, and show that the coding rate needed
to perform K-means clustering in the compressed domain is lower than the rate
needed to reconstruct all the measurements.
",1,0,1,0,0,0
13581,The Cosmic Axion Spin Precession Experiment (CASPEr): a dark-matter search with nuclear magnetic resonance,"  The Cosmic Axion Spin Precession Experiment (CASPEr) is a nuclear magnetic
resonance experiment (NMR) seeking to detect axion and axion-like particles
which could make up the dark matter present in the universe. We review the
predicted couplings of axions and axion-like particles with baryonic matter
that enable their detection via NMR. We then describe two measurement schemes
being implemented in CASPEr. The first method, presented in the original CASPEr
proposal, consists of a resonant search via continuous-wave NMR spectroscopy.
This method offers the highest sensitivity for frequencies ranging from a few
Hz to hundreds of MHz, corresponding to masses $ m_{\rm a} \sim
10^{-14}$--$10^{-6}$ eV. Sub-Hz frequencies are typically difficult to probe
with NMR due to the diminishing sensitivity of magnetometers in this region. To
circumvent this limitation, we suggest new detection and data processing
modalities. We describe a non-resonant frequency-modulation detection scheme,
enabling searches from mHz to Hz frequencies ($m_{\rm a} \sim
10^{-17}$--$10^{-14} $ eV), extending the detection bandwidth by three decades.
",0,1,0,0,0,0
2190,Extremely broadband ultralight thermally emissive metasurfaces,"  We report the design, fabrication and characterization of ultralight highly
emissive metaphotonic structures with record-low mass/area that emit thermal
radiation efficiently over a broad spectral (2 to 35 microns) and angular (0-60
degrees) range. The structures comprise one to three pairs of alternating
nanometer-scale metallic and dielectric layers, and have measured effective 300
K hemispherical emissivities of 0.7 to 0.9. To our knowledge, these structures,
which are all subwavelength in thickness are the lightest reported metasurfaces
with comparable infrared emissivity. The superior optical properties, together
with their mechanical flexibility, low outgassing, and low areal mass, suggest
that these metasurfaces are candidates for thermal management in applications
demanding of ultralight flexible structures, including aerospace applications,
ultralight photovoltaics, lightweight flexible electronics, and textiles for
thermal insulation.
",0,1,0,0,0,0
17097,"The effect of the environment on the structure, morphology and star-formation history of intermediate-redshift galaxies","  With the aim of understanding the effect of the environment on the star
formation history and morphological transformation of galaxies, we present a
detailed analysis of the colour, morphology and internal structure of cluster
and field galaxies at $0.4 \le z \le 0.8$. We use {\em HST} data for over 500
galaxies from the ESO Distant Cluster Survey (EDisCS) to quantify how the
galaxies' light distribution deviate from symmetric smooth profiles. We
visually inspect the galaxies' images to identify the likely causes for such
deviations. We find that the residual flux fraction ($RFF$), which measures the
fractional contribution to the galaxy light of the residuals left after
subtracting a symmetric and smooth model, is very sensitive to the degree of
structural disturbance but not the causes of such disturbance. On the other
hand, the asymmetry of these residuals ($A_{\rm res}$) is more sensitive to the
causes of the disturbance, with merging galaxies having the highest values of
$A_{\rm res}$. Using these quantitative parameters we find that, at a fixed
morphology, cluster and field galaxies show statistically similar degrees of
disturbance. However, there is a higher fraction of symmetric and passive
spirals in the cluster than in the field. These galaxies have smoother light
distributions than their star-forming counterparts. We also find that while
almost all field and cluster S0s appear undisturbed, there is a relatively
small population of star-forming S0s in clusters but not in the field. These
findings are consistent with relatively gentle environmental processes acting
on galaxies infalling onto clusters.
",0,1,0,0,0,0
13347,Social Media Analysis based on Semanticity of Streaming and Batch Data,"  Languages shared by people differ in different regions based on their
accents, pronunciation and word usages. In this era sharing of language takes
place mainly through social media and blogs. Every second swing of such a micro
posts exist which induces the need of processing those micro posts, in-order to
extract knowledge out of it. Knowledge extraction differs with respect to the
application in which the research on cognitive science fed the necessities for
the same. This work further moves forward such a research by extracting
semantic information of streaming and batch data in applications like Named
Entity Recognition and Author Profiling. In the case of Named Entity
Recognition context of a single micro post has been utilized and context that
lies in the pool of micro posts were utilized to identify the sociolect aspects
of the author of those micro posts. In this work Conditional Random Field has
been utilized to do the entity recognition and a novel approach has been
proposed to find the sociolect aspects of the author (Gender, Age group).
",1,0,0,0,0,0
14880,Interaction between magnetic moments and itinerant carriers in d0 ferromagnetic SiC,"  Elucidating the interaction between magnetic moments and itinerant carriers
is an important step to spintronic applications. Here, we investigate magnetic
and transport properties in d0 ferromagnetic SiC single crystals prepared by
postimplantation pulsed laser annealing. Magnetic moments are contributed by
the p states of carbon atoms, but their magnetic circular dichroism is
different from that in semi-insulating SiC samples. The anomalous Hall effect
and negative magnetoresistance indicate the influence of d0 spin order on free
carriers. The ferromagnetism is relatively weak in N-implanted SiC compared
with that in Al-implanted SiC after annealing. The results suggest that d0
magnetic moments and itinerant carriers can interact with each other, which
will facilitate the development of SiC spintronic devices with d0
ferromagnetism.
",0,1,0,0,0,0
7846,Distributed Stochastic Approximation with Local Projections,"  We propose a distributed version of a stochastic approximation scheme
constrained to remain in the intersection of a finite family of convex sets.
The projection to the intersection of these sets is also computed in a
distributed manner and a `nonlinear gossip' mechanism is employed to blend the
projection iterations with the stochastic approximation using multiple time
scales
",1,0,0,0,0,0
13601,Cyclicity in weighted $\ell^p$ spaces,"  We study the cyclicity in weighted $\ell^p(\mathbb{Z})$ spaces. For $p \geq
1$ and $\beta \geq 0$, let $\ell^p\_\beta(\mathbb{Z})$ be the space of
sequences $u=(u\_n)\_{n\in \mathbb{Z}}$ such that $(u\_n |n|^{\beta})\in
\ell^p(\mathbb{Z}) $. We obtain both necessary conditions and sufficient
conditions for $u$ to be cyclic in $\ell^p\_\beta(\mathbb{Z})$, in other words,
for $ \{(u\_{n+k})\_{n \in \mathbb{Z}},~ k \in \mathbb{Z} \}$ to span a dense
subspace of $\ell^p\_\beta(\mathbb{Z})$. The conditions are given in terms of
the Hausdorff dimension and the capacity of the zero set of the Fourier
transform of $u$.
",0,0,1,0,0,0
3965,Passive Compliance Control of Aerial Manipulators,"  This paper presents a passive compliance control for aerial manipulators to
achieve stable environmental interactions. The main challenge is the absence of
actuation along body-planar directions of the aerial vehicle which might be
required during the interaction to preserve passivity. The controller proposed
in this paper guarantees passivity of the manipulator through a proper choice
of end-effector coordinates, and that of vehicle fuselage is guaranteed by
exploiting time domain passivity technique. Simulation studies validate the
proposed approach.
",1,0,0,0,0,0
14150,A dynamic game approach to distributionally robust safety specifications for stochastic systems,"  This paper presents a new safety specification method that is robust against
errors in the probability distribution of disturbances. Our proposed
distributionally robust safe policy maximizes the probability of a system
remaining in a desired set for all times, subject to the worst possible
disturbance distribution in an ambiguity set. We propose a dynamic game
formulation of constructing such policies and identify conditions under which a
non-randomized Markov policy is optimal. Based on this existence result, we
develop a practical design approach to safety-oriented stochastic controllers
with limited information about disturbance distributions. This control method
can be used to minimize another cost function while ensuring safety in a
probabilistic way. However, an associated Bellman equation involves
infinite-dimensional minimax optimization problems since the disturbance
distribution may have a continuous density. To resolve computational issues, we
propose a duality-based reformulation method that converts the
infinite-dimensional minimax problem into a semi-infinite program that can be
solved using existing convergent algorithms. We prove that there is no duality
gap, and that this approach thus preserves optimality. The results of numerical
tests confirm that the proposed method is robust against distributional errors
in disturbances, while a standard stochastic safety specification tool is not.
",1,0,1,0,0,0
16769,SMILES Enumeration as Data Augmentation for Neural Network Modeling of Molecules,"  Simplified Molecular Input Line Entry System (SMILES) is a single line text
representation of a unique molecule. One molecule can however have multiple
SMILES strings, which is a reason that canonical SMILES have been defined,
which ensures a one to one correspondence between SMILES string and molecule.
Here the fact that multiple SMILES represent the same molecule is explored as a
technique for data augmentation of a molecular QSAR dataset modeled by a long
short term memory (LSTM) cell based neural network. The augmented dataset was
130 times bigger than the original. The network trained with the augmented
dataset shows better performance on a test set when compared to a model built
with only one canonical SMILES string per molecule. The correlation coefficient
R2 on the test set was improved from 0.56 to 0.66 when using SMILES
enumeration, and the root mean square error (RMS) likewise fell from 0.62 to
0.55. The technique also works in the prediction phase. By taking the average
per molecule of the predictions for the enumerated SMILES a further improvement
to a correlation coefficient of 0.68 and a RMS of 0.52 was found.
",1,0,0,0,0,0
641,Development of probabilistic dam breach model using Bayesian inference,"  Dam breach models are commonly used to predict outflow hydrographs of
potentially failing dams and are key ingredients for evaluating flood risk. In
this paper a new dam breach modeling framework is introduced that shall improve
the reliability of hydrograph predictions of homogeneous earthen embankment
dams. Striving for a small number of parameters, the simplified physics-based
model describes the processes of failing embankment dams by breach enlargement,
driven by progressive surface erosion. Therein the erosion rate of dam material
is modeled by empirical sediment transport formulations. Embedding the model
into a Bayesian multilevel framework allows for quantitative analysis of
different categories of uncertainties. To this end, data available in
literature of observed peak discharge and final breach width of historical dam
failures was used to perform model inversion by applying Markov Chain Monte
Carlo simulation. Prior knowledge is mainly based on non-informative
distribution functions. The resulting posterior distribution shows that the
main source of uncertainty is a correlated subset of parameters, consisting of
the residual error term and the epistemic term quantifying the breach erosion
rate. The prediction intervals of peak discharge and final breach width are
congruent with values known from literature. To finally predict the outflow
hydrograph for real case applications, an alternative residual model was
formulated that assumes perfect data and a perfect model. The fully
probabilistic fashion of hydrograph prediction has the potential to improve the
adequate risk management of downstream flooding.
",0,0,0,1,0,0
448,Direct estimation of density functionals using a polynomial basis,"  A number of fundamental quantities in statistical signal processing and
information theory can be expressed as integral functions of two probability
density functions. Such quantities are called density functionals as they map
density functions onto the real line. For example, information divergence
functions measure the dissimilarity between two probability density functions
and are useful in a number of applications. Typically, estimating these
quantities requires complete knowledge of the underlying distribution followed
by multi-dimensional integration. Existing methods make parametric assumptions
about the data distribution or use non-parametric density estimation followed
by high-dimensional integration. In this paper, we propose a new alternative.
We introduce the concept of ""data-driven basis functions"" - functions of
distributions whose value we can estimate given only samples from the
underlying distributions without requiring distribution fitting or direct
integration. We derive a new data-driven complete basis that is similar to the
deterministic Bernstein polynomial basis and develop two methods for performing
basis expansions of functionals of two distributions. We also show that the new
basis set allows us to approximate functions of distributions as closely as
desired. Finally, we evaluate the methodology by developing data driven
estimators for the Kullback-Leibler divergences and the Hellinger distance and
by constructing empirical estimates of tight bounds on the Bayes error rate.
",1,0,0,1,0,0
18905,"Inter-Operator Resource Management for Millimeter Wave, Multi-Hop Backhaul Networks","  In this paper, a novel framework is proposed for optimizing the operation and
performance of a large-scale, multi-hop millimeter wave (mmW) backhaul within a
wireless small cell network (SCN) that encompasses multiple mobile network
operators (MNOs). The proposed framework enables the small base stations (SBSs)
to jointly decide on forming the multi-hop, mmW links over backhaul
infrastructure that belongs to multiple, independent MNOs, while properly
allocating resources across those links. In this regard, the problem is
addressed using a novel framework based on matching theory that is composed to
two, highly inter-related stages: a multi-hop network formation stage and a
resource management stage. One unique feature of this framework is that it
jointly accounts for both wireless channel characteristics and economic factors
during both network formation and resource management. The multi-hop network
formation stage is formulated as a one-to-many matching game which is solved
using a novel algorithm, that builds on the so-called deferred acceptance
algorithm and is shown to yield a stable and Pareto optimal multi-hop mmW
backhaul network. Then, a one-to-many matching game is formulated to enable
proper resource allocation across the formed multi-hop network. This game is
then shown to exhibit peer effects and, as such, a novel algorithm is developed
to find a stable and optimal resource management solution that can properly
cope with these peer effects. Simulation results show that the proposed
framework yields substantial gains, in terms of the average sum rate, reaching
up to 27% and 54%, respectively, compared to a non-cooperative scheme in which
inter-operator sharing is not allowed and a random allocation approach. The
results also show that our framework provides insights on how to manage pricing
and the cost of the cooperative mmW backhaul network for the MNOs.
",1,0,0,0,0,0
13816,Relevant change points in high dimensional time series,"  This paper investigates the problem of detecting relevant change points in
the mean vector, say $\mu_t =(\mu_{1,t},\ldots ,\mu_{d,t})^T$ of a high
dimensional time series $(Z_t)_{t\in \mathbb{Z}}$.
While the recent literature on testing for change points in this context
considers hypotheses for the equality of the means $\mu_h^{(1)}$ and
$\mu_h^{(2)}$ before and after the change points in the different components,
we are interested in a null hypothesis of the form $$ H_0: |\mu^{(1)}_{h} -
\mu^{(2)}_{h} | \leq \Delta_h ~~~\mbox{ for all } ~~h=1,\ldots ,d $$ where
$\Delta_1, \ldots , \Delta_d$ are given thresholds for which a smaller
difference of the means in the $h$-th component is considered to be
non-relevant.
We propose a new test for this problem based on the maximum of squared and
integrated CUSUM statistics and investigate its properties as the sample size
$n$ and the dimension $d$ both converge to infinity. In particular, using
Gaussian approximations for the maximum of a large number of dependent random
variables, we show that on certain points of the boundary of the null
hypothesis a standardised version of the maximum converges weakly to a Gumbel
distribution.
",0,0,1,1,0,0
19523,How to Ask for Technical Help? Evidence-based Guidelines for Writing Questions on Stack Overflow,"  Context: The success of Stack Overflow and other community-based
question-and-answer (Q&A) sites depends mainly on the will of their members to
answer others' questions. In fact, when formulating requests on Q&A sites, we
are not simply seeking for information. Instead, we are also asking for other
people's help and feedback. Understanding the dynamics of the participation in
Q&A communities is essential to improve the value of crowdsourced knowledge.
Objective: In this paper, we investigate how information seekers can increase
the chance of eliciting a successful answer to their questions on Stack
Overflow by focusing on the following actionable factors: affect, presentation
quality, and time.
Method: We develop a conceptual framework of factors potentially influencing
the success of questions in Stack Overflow. We quantitatively analyze a set of
over 87K questions from the official Stack Overflow dump to assess the impact
of actionable factors on the success of technical requests. The information
seeker reputation is included as a control factor. Furthermore, to understand
the role played by affective states in the success of questions, we
qualitatively analyze questions containing positive and negative emotions.
Finally, a survey is conducted to understand how Stack Overflow users perceive
the guideline suggestions for writing questions.
Results: We found that regardless of user reputation, successful questions
are short, contain code snippets, and do not abuse with uppercase characters.
As regards affect, successful questions adopt a neutral emotional style.
Conclusion: We provide evidence-based guidelines for writing effective
questions on Stack Overflow that software engineers can follow to increase the
chance of getting technical help. As for the role of affect, we empirically
confirmed community guidelines that suggest avoiding rudeness in question
writing.
",1,0,0,0,0,0
12281,Combining Generative and Discriminative Approaches to Unsupervised Dependency Parsing via Dual Decomposition,"  Unsupervised dependency parsing aims to learn a dependency parser from
unannotated sentences. Existing work focuses on either learning generative
models using the expectation-maximization algorithm and its variants, or
learning discriminative models using the discriminative clustering algorithm.
In this paper, we propose a new learning strategy that learns a generative
model and a discriminative model jointly based on the dual decomposition
method. Our method is simple and general, yet effective to capture the
advantages of both models and improve their learning results. We tested our
method on the UD treebank and achieved a state-of-the-art performance on thirty
languages.
",1,0,0,0,0,0
3895,Expecting the Unexpected: Training Detectors for Unusual Pedestrians with Adversarial Imposters,"  As autonomous vehicles become an every-day reality, high-accuracy pedestrian
detection is of paramount practical importance. Pedestrian detection is a
highly researched topic with mature methods, but most datasets focus on common
scenes of people engaged in typical walking poses on sidewalks. But performance
is most crucial for dangerous scenarios, such as children playing in the street
or people using bicycles/skateboards in unexpected ways. Such ""in-the-tail""
data is notoriously hard to observe, making both training and testing
difficult. To analyze this problem, we have collected a novel annotated dataset
of dangerous scenarios called the Precarious Pedestrian dataset. Even given a
dedicated collection effort, it is relatively small by contemporary standards
(around 1000 images). To allow for large-scale data-driven learning, we explore
the use of synthetic data generated by a game engine. A significant challenge
is selected the right ""priors"" or parameters for synthesis: we would like
realistic data with poses and object configurations that mimic true Precarious
Pedestrians. Inspired by Generative Adversarial Networks (GANs), we generate a
massive amount of synthetic data and train a discriminative classifier to
select a realistic subset, which we deem the Adversarial Imposters. We
demonstrate that this simple pipeline allows one to synthesize realistic
training data by making use of rendering/animation engines within a GAN
framework. Interestingly, we also demonstrate that such data can be used to
rank algorithms, suggesting that Adversarial Imposters can also be used for
""in-the-tail"" validation at test-time, a notoriously difficult challenge for
real-world deployment.
",1,0,0,0,0,0
11624,Small Moving Window Calibration Models for Soft Sensing Processes with Limited History,"  Five simple soft sensor methodologies with two update conditions were
compared on two experimentally-obtained datasets and one simulated dataset. The
soft sensors investigated were moving window partial least squares regression
(and a recursive variant), moving window random forest regression, the mean
moving window of $y$, and a novel random forest partial least squares
regression ensemble (RF-PLS), all of which can be used with small sample sizes
so that they can be rapidly placed online. It was found that, on two of the
datasets studied, small window sizes led to the lowest prediction errors for
all of the moving window methods studied. On the majority of datasets studied,
the RF-PLS calibration method offered the lowest one-step-ahead prediction
errors compared to those of the other methods, and it demonstrated greater
predictive stability at larger time delays than moving window PLS alone. It was
found that both the random forest and RF-PLS methods most adequately modeled
the datasets that did not feature purely monotonic increases in property
values, but that both methods performed more poorly than moving window PLS
models on one dataset with purely monotonic property values. Other data
dependent findings are presented and discussed.
",1,0,0,1,0,0
7192,On Lie algebras responsible for zero-curvature representations of multicomponent (1+1)-dimensional evolution PDEs,"  Zero-curvature representations (ZCRs) are one of the main tools in the theory
of integrable $(1+1)$-dimensional PDEs. According to the preprint
arXiv:1212.2199, for any given $(1+1)$-dimensional evolution PDE one can define
a sequence of Lie algebras $F^p$, $p=0,1,2,3,\dots$, such that representations
of these algebras classify all ZCRs of the PDE up to local gauge equivalence.
ZCRs depending on derivatives of arbitrary finite order are allowed.
Furthermore, these algebras provide necessary conditions for existence of
Backlund transformations between two given PDEs. The algebras $F^p$ are defined
in arXiv:1212.2199 in terms of generators and relations.
In the present paper, we describe some methods to study the structure of the
algebras $F^p$ for multicomponent $(1+1)$-dimensional evolution PDEs. Using
these methods, we compute the explicit structure (up to non-essential nilpotent
ideals) of the Lie algebras $F^p$ for the Landau-Lifshitz, nonlinear
Schrodinger equations, and for the $n$-component Landau-Lifshitz system of
Golubchik and Sokolov for any $n>3$. In particular, this means that for the
$n$-component Landau-Lifshitz system we classify all ZCRs (depending on
derivatives of arbitrary finite order), up to local gauge equivalence and up to
killing nilpotent ideals in the corresponding Lie algebras.
The presented methods to classify ZCRs can be applied also to other
$(1+1)$-dimensional evolution PDEs. Furthermore, the obtained results can be
used for proving non-existence of Backlund transformations between some PDEs,
which will be described in forthcoming publications.
",0,1,1,0,0,0
13511,Weighted gevrey class regularity of euler equation in the whole space,"  In this paper we study the weighted Gevrey class regularity of Euler equation
in the whole space R 3. We first establish the local existence of Euler
equation in weighted Sobolev space, then obtain the weighted Gevrey regularity
of Euler equation. We will use the weighted Sobolev-Gevrey space method to
obtain the results of Gevrey regularity of Euler equation, and the use of the
property of singular operator in the estimate of the pressure term is the
improvement of our work.
",0,0,1,0,0,0
11498,Interpolating between $k$-Median and $k$-Center: Approximation Algorithms for Ordered $k$-Median,"  We consider a generalization of $k$-median and $k$-center, called the {\em
ordered $k$-median} problem. In this problem, we are given a metric space
$(\mathcal{D},\{c_{ij}\})$ with $n=|\mathcal{D}|$ points, and a non-increasing
weight vector $w\in\mathbb{R}_+^n$, and the goal is to open $k$ centers and
assign each point each point $j\in\mathcal{D}$ to a center so as to minimize
$w_1\cdot\text{(largest assignment cost)}+w_2\cdot\text{(second-largest
assignment cost)}+\ldots+w_n\cdot\text{($n$-th largest assignment cost)}$. We
give an $(18+\epsilon)$-approximation algorithm for this problem. Our
algorithms utilize Lagrangian relaxation and the primal-dual schema, combined
with an enumeration procedure of Aouad and Segev. For the special case of
$\{0,1\}$-weights, which models the problem of minimizing the $\ell$ largest
assignment costs that is interesting in and of by itself, we provide a novel
reduction to the (standard) $k$-median problem showing that LP-relative
guarantees for $k$-median translate to guarantees for the ordered $k$-median
problem; this yields a nice and clean $(8.5+\epsilon)$-approximation algorithm
for $\{0,1\}$ weights.
",1,0,0,0,0,0
3821,A Generalization of Quasi-twisted Codes: Multi-twisted codes,"  Cyclic codes and their various generalizations, such as quasi-twisted (QT)
codes, have a special place in algebraic coding theory. Among other things,
many of the best-known or optimal codes have been obtained from these classes.
In this work we introduce a new generalization of QT codes that we call
multi-twisted (MT) codes and study some of their basic properties. Presenting
several methods of constructing codes in this class and obtaining bounds on the
minimum distances, we show that there exist codes with good parameters in this
class that cannot be obtained as QT or constacyclic codes. This suggests that
considering this larger class in computer searches is promising for
constructing codes with better parameters than currently best-known linear
codes. Working with this new class of codes motivated us to consider a problem
about binomials over finite fields and to discover a result that is interesting
in its own right.
",1,0,1,0,0,0
19078,Follow the Compressed Leader: Faster Online Learning of Eigenvectors and Faster MMWU,"  The online problem of computing the top eigenvector is fundamental to machine
learning. In both adversarial and stochastic settings, previous results (such
as matrix multiplicative weight update, follow the regularized leader, follow
the compressed leader, block power method) either achieve optimal regret but
run slow, or run fast at the expense of loosing a $\sqrt{d}$ factor in total
regret where $d$ is the matrix dimension.
We propose a $\textit{follow-the-compressed-leader (FTCL)}$ framework which
achieves optimal regret without sacrificing the running time. Our idea is to
""compress"" the matrix strategy to dimension 3 in the adversarial setting, or
dimension 1 in the stochastic setting. These respectively resolve two open
questions regarding the design of optimal and efficient algorithms for the
online eigenvector problem.
",1,0,1,1,0,0
18153,MHD Models of Gamma-ray Emission in WR 11,"  Recent reports claiming tentative association of the massive star binary
system gamma^2 Velorum (WR 11) with a high-energy gamma-ray source observed by
Fermi-LAT contrast the so-far exclusive role of Eta Carinae as the hitherto
only detected gamma-ray emitter in the source class of particle-accelerating
colliding-wind binary systems. We aim to shed light on this claim of
association by providing dedicated model predictions for the nonthermal photon
emission spectrum of WR 11. We use three-dimensional magneto-hydrodynamic
modeling to trace the structure and conditions of the wind-collision region of
WR 11 throughout its 78.5 day orbit, including the important effect of
radiative braking in the stellar winds. A transport equation is then solved in
the wind-collision region to determine the population of relativistic electrons
and protons which are subsequently used to compute nonthermal photon emission
components. We find that - if WR 11 be indeed confirmed as the responsible
object for the observed gamma-ray emission - its radiation will unavoidably be
of hadronic origin owing to the strong radiation fields in the binary system
which inhibit the acceleration of electrons to energies suffciently high for
observable inverse Compton radiation. Different conditions in wind-collision
region near the apastron and periastron configuration lead to significant
variability on orbital time scales. The bulk of the hadronic gamma-ray emission
originates at a 400 solar radii wide region at the apex.
",0,1,0,0,0,0
13736,Real-time public transport service-level monitoring using passive WiFi: a spectral clustering approach for train timetable estimation,"  A new area in which passive WiFi analytics have promise for delivering value
is the real-time monitoring of public transport systems. One example is
determining the true (as opposed to the published) timetable of a public
transport system in real-time. In most cases, there are no other
publicly-available sources for this information. Yet, it is indispensable for
the real-time monitoring of public transport service levels. Furthermore, this
information, if accurate and temporally fine-grained, can be used for very
low-latency incident detection. In this work, we propose using spectral
clustering based on trajectories derived from passive WiFi traces of users of a
public transport system to infer the true timetable and two key performance
indicators of the transport service, namely public transport vehicle headway
and in-station dwell time. By detecting anomalous dwell times or headways, we
demonstrate that a fast and accurate real-time incident-detection procedure can
be obtained. The method we introduce makes use of the advantages of the
high-frequency WiFi data, which provides very low-latency,
universally-accessible information, while minimizing the impact of the noise in
the data.
",1,0,0,0,0,0
7572,WLAN Performance Analysis Ibrahim Group of industries Faisalabad Pakistan,"  Now a days several organizations are moving their LAN foundation towards
remote LAN frame work. The purpose for this is extremely straight forward
multinational organizations needs their clients surprise about their office
surroundings and they additionally need to make wire free environment in their
workplaces. Much IT equipment moved on Wireless for instance all in one Pc
portable workstations Wireless IP telephones. Another thing is that step by
step WLAN innovation moving towards extraordinary effectiveness. In this
exploration work Wireless LAN innovation running in Ibrahim Group gathering of
commercial enterprises Faisalabad has been investigated in term of their
equipment, Wireless signal quality, data transmission, auto channel moving, and
security in WLAN system. This examination work required physical proving
ground, some WLAN system analyzer (TamoSof throughput) software, hardware point
of interest, security testing programming. The investigation displayed in this
examination has fill two key needs. One determination is to accept this kind of
system interconnection could be broke down utilizing the exploratory models of
the two system bits (wired and remote pieces. Second key factor is to determine
the security issue in WLAN.
",1,0,0,0,0,0
15317,Healing Data Loss Problems in Android Apps,"  Android apps should be designed to cope with stop-start events, which are the
events that require stopping and restoring the execution of an app while
leaving its state unaltered. These events can be caused by run-time
configuration changes, such as a screen rotation, and by context-switches, such
as a switch from one app to another. When a stop-start event occurs, Android
saves the state of the app, handles the event, and finally restores the saved
state. To let Android save and restore the state correctly, apps must provide
the appropriate support. Unfortunately, Android developers often implement this
support incorrectly, or do not implement it at all. This bad practice makes
apps to incorrectly react to stop-start events, thus generating what we defined
data loss problems, that is Android apps that lose user data, behave
unexpectedly, and crash due to program variables that lost their values. Data
loss problems are difficult to detect because they might be observed only when
apps are in specific states and with specific inputs. Covering all the possible
cases with testing may require a large number of test cases whose execution
must be checked manually to discover whether the app under test has been
correctly restored after each stop-start event. It is thus important to
complement traditional in-house testing activities with mechanisms that can
protect apps as soon as a data loss problem occurs in the field. In this paper
we present DataLossHealer, a technique for automatically identifying and
healing data loss problems in the field as soon as they occur. DataLossHealer
is a technique that checks at run-time whether states are recovered correctly,
and heals the app when needed. DataLossHealer can learn from experience,
incrementally reducing the overhead that is introduced avoiding to monitor
interactions that have been managed correctly by the app in the past.
",1,0,0,0,0,0
8478,Extended superalgebras from twistor and Killing spinors,"  The basic first-order differential operators of spin geometry that are Dirac
operator and twistor operator are considered. Special types of spinors defined
from these operators such as twistor spinors and Killing spinors are discussed.
Symmetry operators of massless and massive Dirac equations are introduced and
relevant symmetry operators of twistor spinors and Killing spinors are
constructed from Killing-Yano (KY) and conformal Killing-Yano (CKY) forms in
constant curvature and Einstein manifolds. The squaring map of spinors gives KY
and CKY forms for Killing and twistor spinors respectively. They constitute a
graded Lie algebra structure in some special cases. By using the graded Lie
algebra structure of KY and CKY forms, extended Killing and conformal
superalgebras are constructed in constant curvature and Einstein manifolds.
",0,0,1,0,0,0
10591,The James construction and $π_4(\mathbb{S}^3)$ in homotopy type theory,"  In the first part of this paper we present a formalization in Agda of the
James construction in homotopy type theory. We include several fragments of
code to show what the Agda code looks like, and we explain several techniques
that we used in the formalization. In the second part, we use the James
construction to give a constructive proof that $\pi_4(\mathbb{S}^3)$ is of the
form $\mathbb{Z}/n\mathbb{Z}$ (but we do not compute the $n$ here).
",1,0,1,0,0,0
20876,A contemporary look at Hermann Hankel's 1861 pioneering work on Lagrangian fluid dynamics,"  The present paper is a companion to the paper by Villone and Rampf (2017),
titled ""Hermann Hankel's On the general theory of motion of fluids, an essay
including an English translation of the complete Preisschrift from 1861""
together with connected documents. Here we give a critical assessment of
Hankel's work, which covers many important aspects of fluid dynamics considered
from a Lagrangian-coordinates point of view: variational formulation in the
spirit of Hamilton for elastic (barotropic) fluids, transport (we would now say
Lie transport) of vorticity, the Lagrangian significance of Clebsch variables,
etc. Hankel's work is also put in the perspective of previous and future work.
Hence, the action spans about two centuries: from Lagrange's 1760-1761 Turin
paper on variational approaches to mechanics and fluid mechanics problems to
Arnold's 1966 founding paper on the geometrical/variational formulation of
incompressible flow. The 22-year old Hankel - who was to die 12 years later -
emerges as a highly innovative master of mathematical fluid dynamics, fully
deserving Riemann's assessment that his Preisschrift contains ""all manner of
good things.""
",0,1,1,0,0,0
5655,Sensing and Modeling Human Behavior Using Social Media and Mobile Data,"  In the past years we have witnessed the emergence of the new discipline of
computational social science, which promotes a new data-driven and
computation-based approach to social sciences. In this article we discuss how
the availability of new technologies such as online social media and mobile
smartphones has allowed researchers to passively collect human behavioral data
at a scale and a level of granularity that were just unthinkable some years
ago. We also discuss how these digital traces can then be used to prove (or
disprove) existing theories and develop new models of human behavior.
",1,1,0,0,0,0
10331,Angle-resolved photoemission spectroscopy with quantum gas microscopes,"  Quantum gas microscopes are a promising tool to study interacting quantum
many-body systems and bridge the gap between theoretical models and real
materials. So far they were limited to measurements of instantaneous
correlation functions of the form $\langle \hat{O}(t) \rangle$, even though
extensions to frequency-resolved response functions $\langle \hat{O}(t)
\hat{O}(0) \rangle$ would provide important information about the elementary
excitations in a many-body system. For example, single particle spectral
functions, which are usually measured using photoemission experiments in
electron systems, contain direct information about fractionalization and the
quasiparticle excitation spectrum. Here, we propose a measurement scheme to
experimentally access the momentum and energy resolved spectral function in a
quantum gas microscope with currently available techniques. As an example for
possible applications, we numerically calculate the spectrum of a single hole
excitation in one-dimensional $t-J$ models with isotropic and anisotropic
antiferromagnetic couplings. A sharp asymmetry in the distribution of spectral
weight appears when a hole is created in an isotropic Heisenberg spin chain.
This effect slowly vanishes for anisotropic spin interactions and disappears
completely in the case of pure Ising interactions. The asymmetry strongly
depends on the total magnetization of the spin chain, which can be tuned in
experiments with quantum gas microscopes. An intuitive picture for the observed
behavior is provided by a slave-fermion mean field theory. The key properties
of the spectra are visible at currently accessible temperatures.
",0,1,0,0,0,0
20632,Thomas Precession for Dressed Particles,"  We consider a particle dressed with boundary gravitons in three-dimensional
Minkowski space. The existence of BMS transformations implies that the
particle's wavefunction picks up a Berry phase when subjected to changes of
reference frames that trace a closed path in the asymptotic symmetry group. We
evaluate this phase and show that, for BMS superrotations, it provides a
gravitational generalization of Thomas precession. In principle, such phases
are observable signatures of asymptotic symmetries.
",0,1,1,0,0,0
912,On a binary system of Prendiville: The cubic case,"  We prove sharp decoupling inequalities for a class of two dimensional
non-degenerate surfaces in R^5, introduced by Prendiville. As a consequence, we
obtain sharp bounds on the number of integer solutions of the Diophantine
systems associated with these surfaces.
",0,0,1,0,0,0
6293,When Slepian Meets Fiedler: Putting a Focus on the Graph Spectrum,"  The study of complex systems benefits from graph models and their analysis.
In particular, the eigendecomposition of the graph Laplacian lets emerge
properties of global organization from local interactions; e.g., the Fiedler
vector has the smallest non-zero eigenvalue and plays a key role for graph
clustering. Graph signal processing focusses on the analysis of signals that
are attributed to the graph nodes. The eigendecomposition of the graph
Laplacian allows to define the graph Fourier transform and extend conventional
signal-processing operations to graphs. Here, we introduce the design of
Slepian graph signals, by maximizing energy concentration in a predefined
subgraph for a graph spectral bandlimit. We establish a novel link with
classical Laplacian embedding and graph clustering, which provides a meaning to
localized graph frequencies.
",1,0,0,0,0,0
12408,Generalized Fréchet Bounds for Cell Entries in Multidimensional Contingency Tables,"  We consider the lattice, $\mathcal{L}$, of all subsets of a multidimensional
contingency table and establish the properties of monotonicity and
supermodularity for the marginalization function, $n(\cdot)$, on $\mathcal{L}$.
We derive from the supermodularity of $n(\cdot)$ some generalized Fréchet
inequalities complementing and extending inequalities of Dobra and Fienberg.
Further, we construct new monotonic and supermodular functions from $n(\cdot)$,
and we remark on the connection between supermodularity and some correlation
inequalities for probability distributions on lattices. We also apply an
inequality of Ky Fan to derive a new approach to Fréchet inequalities for
multidimensional contingency tables.
",0,0,1,1,0,0
1053,Foundation for a series of efficient simulation algorithms,"  Compute the coarsest simulation preorder included in an initial preorder is
used to reduce the resources needed to analyze a given transition system. This
technique is applied on many models like Kripke structures, labeled graphs,
labeled transition systems or even word and tree automata. Let (Q,
$\rightarrow$) be a given transition system and Rinit be an initial preorder
over Q. Until now, algorithms to compute Rsim , the coarsest simulation
included in Rinit , are either memory efficient or time efficient but not both.
In this paper we propose the foundation for a series of efficient simulation
algorithms with the introduction of the notion of maximal transitions and the
notion of stability of a preorder with respect to a coarser one. As an
illustration we solve an open problem by providing the first algorithm with the
best published time complexity, O(|Psim |.|$\rightarrow$|), and a bit space
complexity in O(|Psim |^2. log(|Psim |) + |Q|. log(|Q|)), with Psim the
partition induced by Rsim.
",1,0,0,0,0,0
1476,On recognizing shapes of polytopes from their shadows,"  Let $P$ and $Q$ be two convex polytopes both contained in the interior of an
Euclidean ball $r\textbf{B}^{d}$. We prove that $P=Q$ provided that their sight
cones from any point on the sphere $rS^{d-1}$ are congruent. We also prove an
analogous result for spherical projections.
",0,0,1,0,0,0
5024,Noisy Networks for Exploration,"  We introduce NoisyNet, a deep reinforcement learning agent with parametric
noise added to its weights, and show that the induced stochasticity of the
agent's policy can be used to aid efficient exploration. The parameters of the
noise are learned with gradient descent along with the remaining network
weights. NoisyNet is straightforward to implement and adds little computational
overhead. We find that replacing the conventional exploration heuristics for
A3C, DQN and dueling agents (entropy reward and $\epsilon$-greedy respectively)
with NoisyNet yields substantially higher scores for a wide range of Atari
games, in some cases advancing the agent from sub to super-human performance.
",1,0,0,1,0,0
8384,Agent-based model for the origins of scaling in human language,"  Background/Introduction: The Zipf's law establishes that if the words of a
(large) text are ordered by decreasing frequency, the frequency versus the rank
decreases as a power law with exponent close to -1. Previous work has stressed
that this pattern arises from a conflict of interests of the participants of
communication: speakers and hearers. Methods: The challenge here is to define a
computational language game on a population of agents, playing games mainly
based on a parameter that measures the relative participant's interests.
Results: Numerical simulations suggest that at critical values of the parameter
a human-like vocabulary, exhibiting scaling properties, seems to appear.
Conclusions: The appearance of an intermediate distribution of frequencies at
some critical values of the parameter suggests that on a population of
artificial agents the emergence of scaling partly arises as a self-organized
process only from local interactions between agents.
",1,1,0,0,0,0
7593,"Long-range dynamical magnetic order and spin tunneling in the cooperative paramagnetic states of the pyrochlore analogous spinel antiferromagnets CdYb2X4 (X = S, Se)","  Magnetic systems with spins sitting on a lattice of corner sharing regular
tetrahedra have been particularly prolific for the discovery of new magnetic
states for the last two decades. The pyrochlore compounds have offered the
playground for these studies, while little attention has been comparatively
devoted to other compounds where the rare earth R occupies the same
sub-lattice, e.g. the spinel chalcogenides CdR2X4 (X = S, Se). Here we report
measurements performed on powder samples of this series with R = Yb using
specific heat, magnetic susceptibility, neutron diffraction and
muon-spin-relaxation measurements. The two compounds are found to be
magnetically similar. They long-range order into structures described by the
\Gamma_5 irreducible representation. The magnitude of the magnetic moment at
low temperature is 0.77 (1) and 0.62 (1) mu_B for X = S and Se, respectively.
Persistent spin dynamics is present in the ordered states. The spontaneous
field at the muon site is anomalously small, suggesting magnetic moment
fragmentation. A double spin-flip tunneling relaxation mechanism is suggested
in the cooperative paramagnetic state up to 10 K. The magnetic space groups
into which magnetic moments of systems of corner-sharing regular tetrahedra
order are provided for a number of insulating compounds characterized by null
propagation wavevectors.
",0,1,0,0,0,0
2688,A Note on Iterated Consistency and Infinite Proofs,"  Schmerl and Beklemishev's work on iterated reflection achieves two aims: It
introduces the important notion of $\Pi^0_1$-ordinal, characterizing the
$\Pi^0_1$-theorems of a theory in terms of transfinite iterations of
consistency; and it provides an innovative calculus to compute the
$\Pi^0_1$-ordinals for a range of theories. The present note demonstrates that
these achievements are independent: We read off $\Pi^0_1$-ordinals from a
Schütte-style ordinal analysis via infinite proofs, in a direct and
transparent way.
",0,0,1,0,0,0
13368,Static vs Adaptive Strategies for Optimal Execution with Signals,"  We consider an optimal execution problem in which a trader is looking at a
short-term price predictive signal while trading. In the case where the trader
is creating an instantaneous market impact, we show that transactions costs
resulting from the optimal adaptive strategy are substantially lower than the
corresponding costs of the optimal static strategy. Later, we investigate the
case where the trader is creating transient market impact. We show that
strategies in which the trader is observing the signal a number of times during
the trading period, can dramatically reduce the transaction costs and improve
the performance of the optimal static strategy. These results answer a question
which was raised by Brigo and Piat [6], by analyzing two cases where adaptive
strategies can improve the performance of the execution.
",0,0,0,0,0,1
10447,"Two-dimensional topological nodal line semimetal in layered $X_2Y$ ($X$ = Ca, Sr, and Ba; $Y$ = As, Sb, and Bi)","  In topological semimetals the Dirac points can form zero-dimensional and
one-dimensional manifolds, as predicted for Dirac/Weyl semimetals and
topological nodal line semimetals, respectively. Here, based on
first-principles calculations, we predict a topological nodal line semimetal
phase in the two-dimensional compounds $X_2Y$ ($X$=Ca, Sr, and Ba; $Y$=As, Sb,
and Bi) in the absence of spin-orbit coupling (SOC) with a band inversion at
the M point. The mirror symmetry as well as the electrostatic interaction, that
can be engineered via strain, are responsible for the nontrivial phase. In
addition, we demonstrate that the exotic edge states can be also obtained
without and with SOC although a tiny gap appears at the nodal line for the bulk
states when SOC is included.
",0,1,0,0,0,0
10335,Eliashberg theory with the external pair potential,"  Based on BCS model with the external pair potential formulated in a work
\emph{K.V. Grigorishin} arXiv:1605.07080, analogous model with electron-phonon
coupling and Coulomb coupling is proposed. The generalized Eliashberg equations
in the regime of renormalization of the order parameter are obtained. High
temperature asymptotics and influence of Coulomb pseudopotential on them are
investigated: as in the BCS model the order parameter asymptotically tends to
zero as temperature rises, but the accounting of the Coulomb pseudopotential
leads to existence of critical temperature. The effective Ginzburg-Landau
theory is formulated for such model, where the temperature dependencies near
$T_{c}$ of the basic characteristics of a superconductor (coherence length,
magnetic penetration depth, GL parameter, the thermodynamical critical field,
the first and the second critical fields) recovers to the temperature
dependencies as in the ordinary GL theory after the BCS model with the external
pair potential.
",0,1,0,0,0,0
17502,Rigidity of square-tiled interval exchange transformations,"  We look at interval exchange transformations defined as first return maps on
the set of diagonals of a flow of direction $\theta$ on a square-tiled surface:
using a combinatorial approach, we show that, when the surface has at least one
true singularity both the flow and the interval exchange are rigid if and only
if tan $\theta$ has bounded partial quotients. Moreover, if all vertices of the
squares are singularities of the flat metric, and tan $\theta$ has bounded
partial quotients, the square-tiled interval exchange transformation T is not
of rank one. Finally, for another class of surfaces, those defined by the
unfolding of billiards in Veech triangles, we build an uncountable set of rigid
directional flows and an uncountable set of rigid interval exchange
transformations.
",0,0,1,0,0,0
7412,A mode-coupling theory analysis of the rotation driven translational motion of aqueous polyatomic ions,"  In contrast to simple monatomic alkali and halide ions, complex polyatomic
ions like nitrate, acetate, nitrite, chlorate etc. have not been studied in any
great detail. Experiments have shown that diffusion of polyatomic ions exhibits
many remarkable anomalies, notable among them is the fact that polyatomic ions
with similar size show large difference in their diffusivity values. This fact
has drawn relatively little interest in scientific discussions. We show here
that a mode-coupling theory (MCT) can provide a physically meaningful
interpretation of the anomalous diffusivity of polyatomic ions in water, by
including the contribution of rotational jumps on translational friction. The
two systems discussed here, namely aqueous nitrate ion and aqueous acetate ion,
although have similar ionic radii exhibit largely different diffusivity values
due to the differences in the rate of their rotational jump motions. We have
further verified the mode-coupling theory formalism by comparing it with
experimental and simulation results that agrees well with the theoretical
prediction.
",0,1,0,0,0,0
11784,Compact linear programs for 2SAT,"  For each integer $n$ we present an explicit formulation of a compact linear
program, with $O(n^3)$ variables and constraints, which determines the
satisfiability of any 2SAT formula with $n$ boolean variables by a single
linear optimization. This contrasts with the fact that the natural polytope for
this problem, formed from the convex hull of all satisfiable formulas and their
satisfying assignments, has superpolynomial extension complexity. Our
formulation is based on multicommodity flows. We also discuss connections of
these results to the stable matching problem.
",1,0,1,0,0,0
14730,Ultra-wide plasmonic tuning of semiconductor metasurface resonators on epsilon near zero media,"  Fully reconfigurable metasurfaces would enable new classes of optical devices
that provide unprecedented control of electromagnetic beamforms. The principal
challenge for achieving reconfigurability is the need to generate large
tunability of subwavelength, low-Q metasurface resonators. Here, we demonstrate
large refractive index tuning can be efficiently facilitated at mid-infrared
wavelengths using novel temperature-dependent control over free-carrier
refraction. In doped InSb we demonstrate nearly two-fold increase in the
electron effective mass leading to a positive refractive index shift
({\Delta}n>1.5) far greater than conventional thermo-optic effects. In undoped
films we demonstrate more than 10-fold change in the thermal free-carrier
concentration producing a near-unity negative refractive index shift.
Exploiting both effects within a single resonator system, intrinsic InSb wires
on a heavily doped (epsilon near zero) InSb substrate, we demonstrate
dynamically tunable Mie resonances. The observed larger than line-width
resonance shifts ({\Delta}{\lambda}>1.5{\mu}m) suggest new avenues for highly
tunable and reconfigurable mid-infrared semiconductor metasurfaces.
",0,1,0,0,0,0
6493,Multilevel Sequential${}^2$ Monte Carlo for Bayesian Inverse Problems,"  The identification of parameters in mathematical models using noisy
observations is a common task in uncertainty quantification. We employ the
framework of Bayesian inversion: we combine monitoring and observational data
with prior information to estimate the posterior distribution of a parameter.
Specifically, we are interested in the distribution of a diffusion coefficient
of an elliptic PDE. In this setting, the sample space is high-dimensional, and
each sample of the PDE solution is expensive. To address these issues we
propose and analyse a novel Sequential Monte Carlo (SMC) sampler for the
approximation of the posterior distribution. Classical, single-level SMC
constructs a sequence of measures, starting with the prior distribution, and
finishing with the posterior distribution. The intermediate measures arise from
a tempering of the likelihood, or, equivalently, a rescaling of the noise. The
resolution of the PDE discretisation is fixed. In contrast, our estimator
employs a hierarchy of PDE discretisations to decrease the computational cost.
We construct a sequence of intermediate measures by decreasing the temperature
or by increasing the discretisation level at the same time. This idea builds on
and generalises the multi-resolution sampler proposed in [P.S. Koutsourelakis,
J. Comput. Phys., 228 (2009), pp. 6184-6211] where a bridging scheme is used to
transfer samples from coarse to fine discretisation levels. Importantly, our
choice between tempering and bridging is fully adaptive. We present numerical
experiments in 2D space, comparing our estimator to single-level SMC and the
multi-resolution sampler.
",0,0,0,1,0,0
1098,Backward Monte-Carlo applied to muon transport,"  We discuss a backward Monte-Carlo technique for muon transport problem, with
emphasis on its application in muography. Backward Monte-Carlo allows exclusive
sampling of a final state by reversing the simulation flow. In practice it can
be made analogous to an adjoint Monte-Carlo, though it is more versatile for
muon transport. A backward Monte-Carlo was implemented as a dedicated muon
transport library: PUMAS. It is shown for case studies relevant for muography
imaging that the implementations of forward and backward Monte-Carlo schemes
agree to better than 1%.
",0,1,0,0,0,0
7403,Meteorites from Phobos and Deimos at Earth?,"  We examine the conditions under which material from the martian moons Phobos
and Deimos could reach our planet in the form of meteorites. We find that the
necessary ejection speeds from these moons (900 and 600 m/s for Phobos and
Deimos respectively) are much smaller than from Mars' surface (5000 m/s). These
speeds are below typical impact speeds for asteroids and comets (10-40 km/s) at
Mars' orbit, and we conclude that the delivery of meteorites from Phobos and
Deimos to the Earth can occur.
",0,1,0,0,0,0
19101,An Analytic Formula for Numbers of Restricted Partitions from Conformal Field Theory,"  We study the correlators of irregular vertex operators in two-dimensional
conformal field theory (CFT) in order to propose an exact analytic formula for
calculating numbers of partitions, that is:
1) for given $N,k$, finding the total number $\lambda(N|k)$ of length $k$
partitions of $N$: $N=n_1+...+n_k;0<n_1\leq{n_2}...\leq{n_k}$.
2) finding the total number $\lambda(N)=\sum_{k=1}^N\lambda(N|k)$ of
partitions of a natural number $N$
We propose an exact analytic expression for $\lambda(N|k)$ by relating
two-point short-distance correlation functions of irregular vertex operators in
$c=1$ conformal field theory ( the form of the operators is established in this
paper): with the first correlator counting the partitions in the upper
half-plane and the second one obtained from the first correlator by conformal
transformations of the form $f(z)=h(z)e^{-{i\over{z}}}$ where $h(z)$ is regular
and non-vanishing at $z=0$. The final formula for $\lambda(N|k)$ is given in
terms of regularized ($\epsilon$-ordered) finite series in the generalized
higher-derivative Schwarzians and incomplete Bell polynomials of the above
conformal transformation at $z=i\epsilon$ ($\epsilon\rightarrow{0}$)
",0,0,1,0,0,0
14755,Voevodsky's conjecture for cubic fourfolds and Gushel-Mukai fourfolds via noncommutative K3 surfaces,"  In the first part of this paper we will prove the Voevodsky's nilpotence
conjecture for smooth cubic fourfolds and ordinary generic Gushel-Mukai
fourfolds. Then, making use of noncommutative motives, we will prove the
Voevodsky's nilpotence conjecture for generic Gushel-Mukai fourfolds containing
a $\tau$-plane $\G(2,3)$ and for ordinary Gushel-Mukai fourfolds containing a
quintic del Pezzo surface.
",0,0,1,0,0,0
4979,Urban Data Streams and Machine Learning: A Case of Swiss Real Estate Market,"  In this paper, we show how using publicly available data streams and machine
learning algorithms one can develop practical data driven services with no
input from domain experts as a form of prior knowledge. We report the initial
steps toward development of a real estate portal in Switzerland. Based on
continuous web crawling of publicly available real estate advertisements and
using building data from Open Street Map, we developed a system, where we
roughly estimate the rental and sale price indexes of 1.7 million buildings
across the country. In addition to these rough estimates, we developed a web
based API for accurate automated valuation of rental prices of individual
properties and spatial sensitivity analysis of rental market. We tested several
established function approximation methods against the test data to check the
quality of the rental price estimations and based on our experiments, Random
Forest gives very reasonable results with the median absolute relative error of
6.57 percent, which is comparable with the state of the art in the industry. We
argue that while recently there have been successful cases of real estate
portals, which are based on Big Data, majority of the existing solutions are
expensive, limited to certain users and mostly with non-transparent underlying
systems. As an alternative we discuss, how using the crawled data sets and
other open data sets provided from different institutes it is easily possible
to develop data driven services for spatial and temporal sensitivity analysis
in the real estate market to be used for different stakeholders. We believe
that this kind of digital literacy can disrupt many other existing business
concepts across many domains.
",1,0,0,1,0,0
3426,Inductive Representation Learning in Large Attributed Graphs,"  Graphs (networks) are ubiquitous and allow us to model entities (nodes) and
the dependencies (edges) between them. Learning a useful feature representation
from graph data lies at the heart and success of many machine learning tasks
such as classification, anomaly detection, link prediction, among many others.
Many existing techniques use random walks as a basis for learning features or
estimating the parameters of a graph model for a downstream prediction task.
Examples include recent node embedding methods such as DeepWalk, node2vec, as
well as graph-based deep learning algorithms. However, the simple random walk
used by these methods is fundamentally tied to the identity of the node. This
has three main disadvantages. First, these approaches are inherently
transductive and do not generalize to unseen nodes and other graphs. Second,
they are not space-efficient as a feature vector is learned for each node which
is impractical for large graphs. Third, most of these approaches lack support
for attributed graphs.
To make these methods more generally applicable, we propose a framework for
inductive network representation learning based on the notion of attributed
random walk that is not tied to node identity and is instead based on learning
a function $\Phi : \mathrm{\rm \bf x} \rightarrow w$ that maps a node attribute
vector $\mathrm{\rm \bf x}$ to a type $w$. This framework serves as a basis for
generalizing existing methods such as DeepWalk, node2vec, and many other
previous methods that leverage traditional random walks.
",1,0,0,1,0,0
6474,On Certain Properties of Convex Functions,"  This note deals with certain properties of convex functions. We provide
results on the convexity of the set of minima of these functions, the behaviour
of their subgradient set under restriction, and optimization of these functions
over an affine subspace.
",0,0,1,0,0,0
9051,"Stellar Absorption Line Analysis of Local Star-Forming Galaxies: The Relation Between Stellar Mass, Metallicity, Dust Attenuation and Star Formation Rate","  We analyze the optical continuum of star-forming galaxies in SDSS by fitting
stacked spectra with stellar population synthesis models to investigate the
relation between stellar mass, stellar metallicity, dust attenuation and star
formation rate. We fit models calculated with star formation and chemical
evolution histories that are derived empirically from multi-epoch observations
of the stellar mass---star formation rate and the stellar mass---gas-phase
metallicity relations, respectively. We also fit linear combinations of single
burst models with a range of metallicities and ages. Star formation and
chemical evolution histories are unconstrained for these models. The stellar
mass---stellar metallicity relations obtained from the two methods agree with
the relation measured from individual supergiant stars in nearby galaxies.
These relations are also consistent with the relation obtained from emission
line analysis of gas-phase metallicity after accounting for systematic offsets
in the gas-phase-metallicity. We measure dust attenuation of the stellar
continuum and show that its dependence on stellar mass and star formation rate
is consistent with previously reported results derived from nebular emission
lines. However, stellar continuum attenuation is smaller than nebular emission
line attenuation. The continuum-to-nebular attenuation ratio depends on stellar
mass and is smaller in more massive galaxies. Our consistent analysis of
stellar continuum and nebular emission lines paves the way for a comprehensive
investigation of stellar metallicities of star-forming and quiescent galaxies.
",0,1,0,0,0,0
10769,Estimating Historical Hourly Traffic Volumes via Machine Learning and Vehicle Probe Data: A Maryland Case Study,"  This paper focuses on the problem of estimating historical traffic volumes
between sparsely-located traffic sensors, which transportation agencies need to
accurately compute statewide performance measures. To this end, the paper
examines applications of vehicle probe data, automatic traffic recorder counts,
and neural network models to estimate hourly volumes in the Maryland highway
network, and proposes a novel approach that combines neural networks with an
existing profiling method. On average, the proposed approach yields 24% more
accurate estimates than volume profiles, which are currently used by
transportation agencies across the US to compute statewide performance
measures. The paper also quantifies the value of using vehicle probe data in
estimating hourly traffic volumes, which provides important managerial insights
to transportation agencies interested in acquiring this type of data. For
example, results show that volumes can be estimated with a mean absolute
percent error of about 21% at locations where average number of observed probes
is between 30 and 47 vehicles/hr, which provides a useful guideline for
assessing the value of probe vehicle data from different vendors.
",1,0,0,1,0,0
5506,Contracts as specifications for dynamical systems in driving variable form,"  This paper introduces assume/guarantee contracts on continuous-time control
systems, hereby extending contract theories for discrete systems to certain new
model classes and specifications. Contracts are regarded as formal
characterizations of control specifications, providing an alternative to
specifications in terms of dissipativity properties or set-invariance. The
framework has the potential to capture a richer class of specifications more
suitable for complex engineering systems. The proposed contracts are supported
by results that enable the verification of contract implementation and the
comparison of contracts. These results are illustrated by an example of a
vehicle following system.
",1,0,0,0,0,0
5863,Multi-Layer Generalized Linear Estimation,"  We consider the problem of reconstructing a signal from multi-layered
(possibly) non-linear measurements. Using non-rigorous but standard methods
from statistical physics we present the Multi-Layer Approximate Message Passing
(ML-AMP) algorithm for computing marginal probabilities of the corresponding
estimation problem and derive the associated state evolution equations to
analyze its performance. We also give the expression of the asymptotic free
energy and the minimal information-theoretically achievable reconstruction
error. Finally, we present some applications of this measurement model for
compressed sensing and perceptron learning with structured matrices/patterns,
and for a simple model of estimation of latent variables in an auto-encoder.
",1,1,0,1,0,0
17958,Neural Style Transfer: A Review,"  The seminal work of Gatys et al. demonstrated the power of Convolutional
Neural Networks (CNNs) in creating artistic imagery by separating and
recombining image content and style. This process of using CNNs to render a
content image in different styles is referred to as Neural Style Transfer
(NST). Since then, NST has become a trending topic both in academic literature
and industrial applications. It is receiving increasing attention and a variety
of approaches are proposed to either improve or extend the original NST
algorithm. In this paper, we aim to provide a comprehensive overview of the
current progress towards NST. We first propose a taxonomy of current algorithms
in the field of NST. Then, we present several evaluation methods and compare
different NST algorithms both qualitatively and quantitatively. The review
concludes with a discussion of various applications of NST and open problems
for future research. A list of papers discussed in this review, corresponding
codes, pre-trained models and more comparison results are publicly available at
this https URL.
",1,0,0,1,0,0
18899,"Optimizing the Wisdom of the Crowd: Inference, Learning, and Teaching","  The unprecedented demand for large amount of data has catalyzed the trend of
combining human insights with machine learning techniques, which facilitate the
use of crowdsourcing to enlist label information both effectively and
efficiently. The classic work on crowdsourcing mainly focuses on the label
inference problem under the categorization setting. However, inferring the true
label requires sophisticated aggregation models that usually can only perform
well under certain assumptions. Meanwhile, no matter how complicated the
aggregation model is, the true model that generated the crowd labels remains
unknown. Therefore, the label inference problem can never infer the ground
truth perfectly. Based on the fact that the crowdsourcing labels are abundant
and utilizing aggregation will lose such kind of rich annotation information
(e.g., which worker provided which labels), we believe that it is critical to
take the diverse labeling abilities of the crowdsourcing workers as well as
their correlations into consideration. To address the above challenge, we
propose to tackle three research problems, namely inference, learning, and
teaching.
",0,0,0,1,0,0
6088,Lagrangian for RLC circuits using analogy with the classical mechanics concepts,"  We study and formulate the Lagrangian for the LC, RC, RL, and RLC circuits by
using the analogy concept with the mechanical problem in classical mechanics
formulations. We found that the Lagrangian for the LC and RLC circuits are
governed by two terms i. e. kinetic energy-like and potential energy-like
terms. The Lagrangian for the RC circuit is only a contribution from the
potential energy-like term and the Lagrangian for the RL circuit is only from
the kinetic energy-like term.
",0,1,0,0,0,0
2236,On the Erasure Robustness Property of Random Matrices,"  The study of the restricted isometry property (RIP) for corrupted random
matrices is particularly important in the field of compressed sensing (CS) with
corruptions. If a matrix still satisfy RIP after a certain portion of rows are
erased, then we say that the matrix has the strong restricted isometry property
(SRIP. In the field of compressed sensing, random matrices satisfies certain
moment conditions are of particular interest. Among these matrices, those with
entries generated from i.i.d Gaussian or i.i.d $\pm1$ random variables are
often typically considered. Recent studies have shown that a matrix generated
from i.i.d Gaussian random variables satisfies the strong restricted isometry
property under arbitrary erasure of rows. In the first part of this paper we
will work on $\pm 1$ random matrices. We study the erasure robustness of $\pm
1$ random matrices show that with overwhelming probability the SRIP will still
hold. Moreover the analysis will also lead to the robust version of the
Johnson-Lindenstrauss Lemma for $\pm 1$ matrices. Then in the second part of
this paper we work on finite frames. The study of the stability of finite
frames under corruptions shares a lot of similarity to CS with corruption. We
will focus on the Gaussian finite frames as a starter. We will improve existing
results and confirm that a Gaussian random frame is numerically stable under
arbitrary erasure of rows.
",0,0,1,0,0,0
961,Complementary views on electron spectra: From Fluctuation Diagnostics to real space correlations,"  We study the relation between the microscopic properties of a many-body
system and the electron spectra, experimentally accessible by photoemission. In
a recent paper [Phys. Rev. Lett. 114, 236402 (2015)], we introduced the
""fluctuation diagnostics"" approach, to extract the dominant wave vector
dependent bosonic fluctuations from the electronic self-energy. Here, we first
reformulate the theory in terms of fermionic modes, to render its connection
with resonance valence bond (RVB) fluctuations more transparent. Secondly, by
using a large-U expansion, where U is the Coulomb interaction, we relate the
fluctuations to real space correlations. Therefore, it becomes possible to
study how electron spectra are related to charge, spin, superconductivity and
RVB-like real space correlations, broadening the analysis of an earlier work
[Phys. Rev. B 89, 245130 (2014)]. This formalism is applied to the pseudogap
physics of the two-dimensional Hubbard model, studied in the dynamical cluster
approximation. We perform calculations for embedded clusters with up to 32
sites, having three inequivalent K-points at the Fermi surface. We find that as
U is increased, correlation functions gradually attain values consistent with
an RVB state. This first happens for correlation functions involving the
antinodal point and gradually spreads to the nodal point along the Fermi
surface. Simultaneously a pseudogap opens up along the Fermi surface. We relate
this to a crossover from a Kondo-like state to an RVB-like localized cluster
state and to the presence of RVB and spin fluctuations. These changes are
caused by a strong momentum dependence in the cluster bath-couplings along the
Fermi surface. We also show, from a more algorithmic perspective, how the
time-consuming calculations in fluctuation diagnostics can be drastically
simplified.
",0,1,0,0,0,0
4992,Electrostatic gyrokinetic simulation of global tokamak boundary plasma and the generation of nonlinear intermittent turbulence,"  Boundary plasma physics plays an important role in tokamak confinement, but
is difficult to simulate in a gyrokinetic code due to the scale-inseparable
nonlocal multi-physics in magnetic separatrix and open magnetic field geometry.
Neutral particles are also an important part of the boundary plasma physics. In
the present paper, noble electrostatic gyrokinetic techniques to simulate the
flux-driven, low-beta electrostatic boundary plasma is reported. Gyrokinetic
ions and drift-kinetic electrons are utilized without scale-separation between
the neoclassical and turbulence dynamics. It is found that the nonlinear
intermittent turbulence is a natural gyrokinetic phenomenon in the boundary
plasma in the vicinity of the magnetic separatrix surface and in the scrape-off
layer.
",0,1,0,0,0,0
11021,Flavour composition and entropy increase of cosmological neutrinos after decoherence,"  We investigate the evolution of the flavour composition of the cosmic
neutrino background from neutrino decoupling until today. The decoherence of
neutrino mass states is described by means of Lindblad operators. Decoherence
goes along with the increase of neutrino family entropy, which we obtain as a
function of initial spectral distortions, mixing angles and CP-violation phase.
We also present the expected flavour composition of the cosmic neutrino
background after decoherence is completed. Decoherence is proposed to happen
after the two heaviest neutrino mass states become non-relativistic. We discuss
how the associated increase of entropy could be observed (in principle). The
physics of two- or three-flavour oscillation of cosmological neutrinos
resembles in many aspects two- or three-level systems in atomic clocks, which
were recently proposed by Weinberg for the study of decoherence phenomena.
",0,1,0,0,0,0
18505,Towards Visual Ego-motion Learning in Robots,"  Many model-based Visual Odometry (VO) algorithms have been proposed in the
past decade, often restricted to the type of camera optics, or the underlying
motion manifold observed. We envision robots to be able to learn and perform
these tasks, in a minimally supervised setting, as they gain more experience.
To this end, we propose a fully trainable solution to visual ego-motion
estimation for varied camera optics. We propose a visual ego-motion learning
architecture that maps observed optical flow vectors to an ego-motion density
estimate via a Mixture Density Network (MDN). By modeling the architecture as a
Conditional Variational Autoencoder (C-VAE), our model is able to provide
introspective reasoning and prediction for ego-motion induced scene-flow.
Additionally, our proposed model is especially amenable to bootstrapped
ego-motion learning in robots where the supervision in ego-motion estimation
for a particular camera sensor can be obtained from standard navigation-based
sensor fusion strategies (GPS/INS and wheel-odometry fusion). Through
experiments, we show the utility of our proposed approach in enabling the
concept of self-supervised learning for visual ego-motion estimation in
autonomous robots.
",1,0,0,0,0,0
9706,Molecular Beam Epitaxy Growth of [CrGe/MnGe/FeGe] Superlattices: Toward Artificial B20 Skyrmion Materials with Tunable Interactions,"  Skyrmions are localized magnetic spin textures whose stability has been shown
theoretically to depend on material parameters including bulk Dresselhaus spin
orbit coupling (SOC), interfacial Rashba SOC, and magnetic anisotropy. Here, we
establish the growth of a new class of artificial skyrmion materials, namely
B20 superlattices, where these parameters could be systematically tuned.
Specifically, we report the successful growth of B20 superlattices comprised of
single crystal thin films of FeGe, MnGe, and CrGe on Si(111) substrates. Thin
films and superlattices are grown by molecular beam epitaxy and are
characterized through a combination of reflection high energy electron
diffraction, x-ray diffraction, and cross-sectional scanning transmission
electron microscopy (STEM). X-ray energy dispersive spectroscopy (XEDS)
distinguishes layers by elemental mapping and indicates good interface quality
with relatively low levels of intermixing in the [CrGe/MnGe/FeGe] superlattice.
This demonstration of epitaxial, single-crystalline B20 superlattices is a
significant advance toward tunable skyrmion systems for fundamental scientific
studies and applications in magnetic storage and logic.
",0,1,0,0,0,0
7883,Information Directed Sampling for Stochastic Bandits with Graph Feedback,"  We consider stochastic multi-armed bandit problems with graph feedback, where
the decision maker is allowed to observe the neighboring actions of the chosen
action. We allow the graph structure to vary with time and consider both
deterministic and Erdős-Rényi random graph models. For such a graph
feedback model, we first present a novel analysis of Thompson sampling that
leads to tighter performance bound than existing work. Next, we propose new
Information Directed Sampling based policies that are graph-aware in their
decision making. Under the deterministic graph case, we establish a Bayesian
regret bound for the proposed policies that scales with the clique cover number
of the graph instead of the number of actions. Under the random graph case, we
provide a Bayesian regret bound for the proposed policies that scales with the
ratio of the number of actions over the expected number of observations per
iteration. To the best of our knowledge, this is the first analytical result
for stochastic bandits with random graph feedback. Finally, using numerical
evaluations, we demonstrate that our proposed IDS policies outperform existing
approaches, including adaptions of upper confidence bound, $\epsilon$-greedy
and Exp3 algorithms.
",1,0,0,1,0,0
17658,Arbitrage and Geometry,"  This article introduces the notion of arbitrage for a situation involving a
collection of investments and a payoff matrix describing the return to an
investor of each investment under each of a set of possible scenarios. We
explain the Arbitrage Theorem, discuss its geometric meaning, and show its
equivalence to Farkas' Lemma. We then ask a seemingly innocent question: given
a random payoff matrix, what is the probability of an arbitrage opportunity?
This question leads to some interesting geometry involving hyperplane
arrangements and related topics.
",0,0,1,1,0,0
12568,Ubiquitous quasi-Fuchsian surfaces in cusped hyperbolic 3-manifolds,"  This paper proves that every finite volume hyperbolic 3-manifold M contains a
ubiquitous collection of closed, immersed, quasi-Fuchsian surfaces. These
surfaces are ubiquitous in the sense that their preimages in the universal
cover separate any pair of disjoint, non-asymptotic geodesic planes. The proof
relies in a crucial way on the corresponding theorem of Kahn and Markovic for
closed 3-manifolds. As a corollary of this result and a companion statement
about surfaces with cusps, we recover Wise's theorem that the fundamental group
of M acts freely and cocompactly on a CAT(0) cube complex.
",0,0,1,0,0,0
11477,Exploring many body localization and thermalization using semiclassical method,"  The Discrete Truncated Wigner Approximation (DTWA) is a semi-classical phase
space method useful for the exploration of Many-body quantum dynamics. In this
work we investigate Many-Body Localization (MBL) and thermalization using DTWA
and compare its performance to exact numerical solutions. By taking as a
benchmark case a 1D random field Heisenberg spin chain with short range
interactions, and by comparing to numerically exact techniques, we show that
DTWA is able to reproduce dynamical signatures that characterize both the
thermal and the MBL phases. It exhibits the best quantitative agreement at
short times deep in each of the phases and larger mismatches close to the phase
transition. The DTWA captures the logarithmic growth of entanglement in the MBL
phase, even though a pure classical mean-field analysis would lead to no
dynamics at all. Our results suggest the DTWA can become a useful method to
investigate MBL and thermalization in experimentally relevant settings
intractable with exact numerical techniques, such as systems with long range
interactions and/or systems in higher dimensions.
",0,1,0,0,0,0
19544,A scientists' view of scientometrics: Not everything that counts can be counted,"  Like it or not, attempts to evaluate and monitor the quality of academic
research have become increasingly prevalent worldwide. Performance reviews
range from at the level of individuals, through research groups and
departments, to entire universities. Many of these are informed by, or
functions of, simple scientometric indicators and the results of such exercises
impact onto careers, funding and prestige. However, there is sometimes a
failure to appreciate that scientometrics are, at best, very blunt instruments
and their incorrect usage can be misleading. Rather than accepting the rise and
fall of individuals and institutions on the basis of such imprecise measures,
calls have been made for indicators be regularly scrutinised and for
improvements to the evidence base in this area. It is thus incumbent upon the
scientific community, especially the physics, complexity-science and
scientometrics communities, to scrutinise metric indicators. Here, we review
recent attempts to do this and show that some metrics in widespread use cannot
be used as reliable indicators research quality.
",1,1,0,0,0,0
13882,Magnon Spin-Momentum Locking: Various Spin Vortices and Dirac Magnons in Noncollinear Antiferromagnets,"  We generalize the concept of the spin-momentum locking to magnonic systems
and derive the formula to calculate the spin expectation value for one-magnon
states of general two-body spin Hamiltonians. We give no-go conditions for
magnon spin to be independent of momentum. As examples of the magnon
spin-momentum locking, we analyze a one-dimensional antiferromagnet with the
Néel order and two-dimensional kagome lattice antiferromagnets with the
120$^\circ$ structure. We find that the magnon spin depends on its momentum
even when the Hamiltonian has the $z$-axis spin rotational symmetry, which can
be explained in the context of a singular band point or a $U(1)$ symmetry
breaking. A spin vortex in momentum space generated in a kagome lattice
antiferromagnet has the winding number $Q=-2$, while the typical one observed
in topological insulator surface states is characterized by $Q=+1$. A magnonic
analogue of the surface states, the Dirac magnon with $Q=+1$, is found in
another kagome lattice antiferromagnet. We also derive the sum rule for $Q$ by
using the Poincaré-Hopf index theorem.
",0,1,0,0,0,0
12805,Dynamic Sensitivity Study of MEMS Capacitive Acceleration Transducer Based on Analytical Squeeze Film Damping and Mechanical Thermoelasticity Approaches,"  The dynamic behavior of a capacitive micro-electro-mechanical (MEMS)
accelerometer is evaluated by using a theoretical approach which makes use of a
squeeze film damping (SFD) model and ideal gas approach. The study investigates
the performance of the device as a function of the temperature, from 228 K to
398 K, and pressure, from 20 to 1000 Pa, observing the damping gas trapped
inside de mechanical transducer. Thermoelastic properties of the silicon bulk
are considered for the entire range of temperature. The damping gases
considered are Air, Helium and Argon. The global behavior of the system is
evaluated considering the electro-mechanical sensitivity (SEM) as the main
figure of merit in frequency domain. The results show the behavior of the main
mechanism losses of SFD, as well as the dynamic sensitivity of the MEMS
transducer system, and are in good agreement with experimental dynamic results
behavior.
",0,1,0,0,0,0
9118,BOOK: Storing Algorithm-Invariant Episodes for Deep Reinforcement Learning,"  We introduce a novel method to train agents of reinforcement learning (RL) by
sharing knowledge in a way similar to the concept of using a book. The recorded
information in the form of a book is the main means by which humans learn
knowledge. Nevertheless, the conventional deep RL methods have mainly focused
either on experiential learning where the agent learns through interactions
with the environment from the start or on imitation learning that tries to
mimic the teacher. Contrary to these, our proposed book learning shares key
information among different agents in a book-like manner by delving into the
following two characteristic features: (1) By defining the linguistic function,
input states can be clustered semantically into a relatively small number of
core clusters, which are forwarded to other RL agents in a prescribed manner.
(2) By defining state priorities and the contents for recording, core
experiences can be selected and stored in a small container. We call this
container as `BOOK'. Our method learns hundreds to thousand times faster than
the conventional methods by learning only a handful of core cluster
information, which shows that deep RL agents can effectively learn through the
shared knowledge from other agents.
",1,0,0,0,0,0
16150,The Wisdom of Polarized Crowds,"  As political polarization in the United States continues to rise, the
question of whether polarized individuals can fruitfully cooperate becomes
pressing. Although diversity of individual perspectives typically leads to
superior team performance on complex tasks, strong political perspectives have
been associated with conflict, misinformation and a reluctance to engage with
people and perspectives beyond one's echo chamber. It is unclear whether
self-selected teams of politically diverse individuals will create higher or
lower quality outcomes. In this paper, we explore the effect of team political
composition on performance through analysis of millions of edits to Wikipedia's
Political, Social Issues, and Science articles. We measure editors' political
alignments by their contributions to conservative versus liberal articles. A
survey of editors validates that those who primarily edit liberal articles
identify more strongly with the Democratic party and those who edit
conservative ones with the Republican party. Our analysis then reveals that
polarized teams---those consisting of a balanced set of politically diverse
editors---create articles of higher quality than politically homogeneous teams.
The effect appears most strongly in Wikipedia's Political articles, but is also
observed in Social Issues and even Science articles. Analysis of article ""talk
pages"" reveals that politically polarized teams engage in longer, more
constructive, competitive, and substantively focused but linguistically diverse
debates than political moderates. More intense use of Wikipedia policies by
politically diverse teams suggests institutional design principles to help
unleash the power of politically polarized teams.
",1,0,0,1,0,0
11321,Preference-based Teaching,"  We introduce a new model of teaching named ""preference-based teaching"" and a
corresponding complexity parameter---the preference-based teaching dimension
(PBTD)---representing the worst-case number of examples needed to teach any
concept in a given concept class. Although the PBTD coincides with the
well-known recursive teaching dimension (RTD) on finite classes, it is
radically different on infinite ones: the RTD becomes infinite already for
trivial infinite classes (such as half-intervals) whereas the PBTD evaluates to
reasonably small values for a wide collection of infinite classes including
classes consisting of so-called closed sets w.r.t. a given closure operator,
including various classes related to linear sets over $\mathbb{N}_0$ (whose RTD
had been studied quite recently) and including the class of Euclidean
half-spaces. On top of presenting these concrete results, we provide the reader
with a theoretical framework (of a combinatorial flavor) which helps to derive
bounds on the PBTD.
",1,0,0,0,0,0
10248,Mobile impurities in integrable models,"  We use a mobile impurity or depleton model to study elementary excitations in
one-dimensional integrable systems. For Lieb-Liniger and bosonic Yang-Gaudin
models we express two phenomenological parameters characterising renormalised
inter- actions of mobile impurities with superfluid background: the number of
depleted particles, $N$ and the superfluid phase drop $\pi J$ in terms of the
corresponding Bethe Ansatz solution and demonstrate, in the leading order, the
absence of two-phonon scattering resulting in vanishing rates of inelastic
processes such as viscosity experienced by the mobile impurities
",0,1,1,0,0,0
17227,"Efficient, Certifiably Optimal Clustering with Applications to Latent Variable Graphical Models","  Motivated by the task of clustering either $d$ variables or $d$ points into
$K$ groups, we investigate efficient algorithms to solve the Peng-Wei (P-W)
$K$-means semi-definite programming (SDP) relaxation. The P-W SDP has been
shown in the literature to have good statistical properties in a variety of
settings, but remains intractable to solve in practice. To this end we propose
FORCE, a new algorithm to solve this SDP relaxation. Compared to the naive
interior point method, our method reduces the computational complexity of
solving the SDP from $\tilde{O}(d^7\log\epsilon^{-1})$ to
$\tilde{O}(d^{6}K^{-2}\epsilon^{-1})$ arithmetic operations for an
$\epsilon$-optimal solution. Our method combines a primal first-order method
with a dual optimality certificate search, which when successful, allows for
early termination of the primal method. We show for certain variable clustering
problems that, with high probability, FORCE is guaranteed to find the optimal
solution to the SDP relaxation and provide a certificate of exact optimality.
As verified by our numerical experiments, this allows FORCE to solve the P-W
SDP with dimensions in the hundreds in only tens of seconds. For a variation of
the P-W SDP where $K$ is not known a priori a slight modification of FORCE
reduces the computational complexity of solving this problem as well: from
$\tilde{O}(d^7\log\epsilon^{-1})$ using a standard SDP solver to
$\tilde{O}(d^{4}\epsilon^{-1})$.
",0,0,0,1,0,0
20463,Learning from Label Proportions in Brain-Computer Interfaces: Online Unsupervised Learning with Guarantees,"  Objective: Using traditional approaches, a Brain-Computer Interface (BCI)
requires the collection of calibration data for new subjects prior to online
use. Calibration time can be reduced or eliminated e.g.~by transfer of a
pre-trained classifier or unsupervised adaptive classification methods which
learn from scratch and adapt over time. While such heuristics work well in
practice, none of them can provide theoretical guarantees. Our objective is to
modify an event-related potential (ERP) paradigm to work in unison with the
machine learning decoder to achieve a reliable calibration-less decoding with a
guarantee to recover the true class means.
Method: We introduce learning from label proportions (LLP) to the BCI
community as a new unsupervised, and easy-to-implement classification approach
for ERP-based BCIs. The LLP estimates the mean target and non-target responses
based on known proportions of these two classes in different groups of the
data. We modified a visual ERP speller to meet the requirements of the LLP. For
evaluation, we ran simulations on artificially created data sets and conducted
an online BCI study with N=13 subjects performing a copy-spelling task.
Results: Theoretical considerations show that LLP is guaranteed to minimize
the loss function similarly to a corresponding supervised classifier. It
performed well in simulations and in the online application, where 84.5% of
characters were spelled correctly on average without prior calibration.
Significance: The continuously adapting LLP classifier is the first
unsupervised decoder for ERP BCIs guaranteed to find the true class means. This
makes it an ideal solution to avoid a tedious calibration and to tackle
non-stationarities in the data. Additionally, LLP works on complementary
principles compared to existing unsupervised methods, allowing for their
further enhancement when combined with LLP.
",1,0,0,1,0,0
12651,Scalable Greedy Feature Selection via Weak Submodularity,"  Greedy algorithms are widely used for problems in machine learning such as
feature selection and set function optimization. Unfortunately, for large
datasets, the running time of even greedy algorithms can be quite high. This is
because for each greedy step we need to refit a model or calculate a function
using the previously selected choices and the new candidate.
Two algorithms that are faster approximations to the greedy forward selection
were introduced recently ([Mirzasoleiman et al. 2013, 2015]). They achieve
better performance by exploiting distributed computation and stochastic
evaluation respectively. Both algorithms have provable performance guarantees
for submodular functions.
In this paper we show that divergent from previously held opinion,
submodularity is not required to obtain approximation guarantees for these two
algorithms. Specifically, we show that a generalized concept of weak
submodularity suffices to give multiplicative approximation guarantees. Our
result extends the applicability of these algorithms to a larger class of
functions. Furthermore, we show that a bounded submodularity ratio can be used
to provide data dependent bounds that can sometimes be tighter also for
submodular functions. We empirically validate our work by showing superior
performance of fast greedy approximations versus several established baselines
on artificial and real datasets.
",1,0,0,1,0,0
20651,Low Power SI Class E Power Amplifier and RF Switch For Health Care,"  This research was to design a 2.4 GHz class E Power Amplifier (PA) for health
care, with 0.18um Semiconductor Manufacturing International Corporation CMOS
technology by using Cadence software. And also RF switch was designed at
cadence software with power Jazz 180nm SOI process. The ultimate goal for such
application is to reach high performance and low cost, and between high
performance and low power consumption design. This paper introduces the design
of a 2.4GHz class E power amplifier and RF switch design. PA consists of
cascade stage with negative capacitance. This power amplifier can transmit
16dBm output power to a 50{\Omega} load. The performance of the power amplifier
and switch meet the specification requirements of the desired.
",1,0,0,0,0,0
9200,Mirror actuation design for the interferometer control of the KAGRA gravitational wave telescope,"  KAGRA is a 3-km cryogenic interferometric gravitational wave telescope
located at an underground site in Japan. In order to achieve its target
sensitivity, the relative positions of the mirrors of the interferometer must
be finely adjusted with attached actuators. We have developed a model to
simulate the length control loops of the KAGRA interferometer with realistic
suspension responses and various noises for mirror actuation. Using our model,
we have designed the actuation parameters to have sufficient force range to
acquire lock as well as to control all the length degrees of freedom without
introducing excess noise.
",0,1,0,0,0,0
10139,Fusible HSTs and the randomized k-server conjecture,"  We exhibit an $O((\log k)^6)$-competitive randomized algorithm for the
$k$-server problem on any metric space. It is shown that a potential-based
algorithm for the fractional $k$-server problem on hierarchically separated
trees (HSTs) with competitive ratio $f(k)$ can be used to obtain a randomized
algorithm for any metric space with competitive ratio $f(k)^2 O((\log k)^2)$.
Employing the $O((\log k)^2)$-competitive algorithm for HSTs from our joint
work with Bubeck, Cohen, Lee, and Mądry (2017) yields the claimed bound.
The best previous result independent of the geometry of the underlying metric
space is the $2k-1$ competitive ratio established for the deterministic work
function algorithm by Koutsoupias and Papadimitriou (1995). Even for the
special case when the underlying metric space is the real line, the best known
competitive ratio was $k$. Since deterministic algorithms can do no better than
$k$ on any metric space with at least $k+1$ points, this establishes that for
every metric space on which the problem is non-trivial, randomized algorithms
give an exponential improvement over deterministic algorithms.
",1,0,1,0,0,0
8747,Cohomologies of locally conformally symplectic manifolds and solvmanifolds,"  We study the Morse-Novikov cohomology and its almost-symplectic counterpart
on manifolds admitting locally conformally symplectic structures. More
precisely, we introduce lcs cohomologies and we study elliptic Hodge theory,
dualities, Hard Lefschetz Condition. We consider solvmanifolds and
Oeljeklaus-Toma manifolds. In particular, we prove that Oeljeklaus-Toma
manifolds with precisely one complex place, and under an additional arithmetic
condition, satisfy the Mostow property. This holds in particular for the Inoue
surface of type $S^0$.
",0,0,1,0,0,0
11771,Verifying Security Protocols using Dynamic Strategies,"  Current formal approaches have been successfully used to find design flaws in
many security protocols. However, it is still challenging to automatically
analyze protocols due to their large or infinite state spaces. In this paper,
we propose a novel framework that can automatically verifying security
protocols without any human intervention. Experimental results show that
SmartVerif automatically verifies security protocols that cannot be
automatically verified by existing approaches. The case studies also validate
the effectiveness of our dynamic strategy.
",1,0,0,0,0,0
16197,Coupling the reduced-order model and the generative model for an importance sampling estimator,"  In this work, we develop an importance sampling estimator by coupling the
reduced-order model and the generative model in a problem setting of
uncertainty quantification. The target is to estimate the probability that the
quantity of interest (QoI) in a complex system is beyond a given threshold. To
avoid the prohibitive cost of sampling a large scale system, the reduced-order
model is usually considered for a trade-off between efficiency and accuracy.
However, the Monte Carlo estimator given by the reduced-order model is biased
due to the error from dimension reduction. To correct the bias, we still need
to sample the fine model. An effective technique to reduce the variance
reduction is importance sampling, where we employ the generative model to
estimate the distribution of the data from the reduced-order model and use it
for the change of measure in the importance sampling estimator. To compensate
the approximation errors of the reduced-order model, more data that induce a
slightly smaller QoI than the threshold need to be included into the training
set. Although the amount of these data can be controlled by a posterior error
estimate, redundant data, which may outnumber the effective data, will be kept
due to the epistemic uncertainty. To deal with this issue, we introduce a
weighted empirical distribution to process the data from the reduced-order
model. The generative model is then trained by minimizing the cross entropy
between it and the weighted empirical distribution. We also introduce a penalty
term into the objective function to deal with the overfitting for more
robustness. Numerical results are presented to demonstrate the effectiveness of
the proposed methodology.
",1,0,0,1,0,0
1720,Model-Based Control Using Koopman Operators,"  This paper explores the application of Koopman operator theory to the control
of robotic systems. The operator is introduced as a method to generate
data-driven models that have utility for model-based control methods. We then
motivate the use of the Koopman operator towards augmenting model-based
control. Specifically, we illustrate how the operator can be used to obtain a
linearizable data-driven model for an unknown dynamical process that is useful
for model-based control synthesis. Simulated results show that with increasing
complexity in the choice of the basis functions, a closed-loop controller is
able to invert and stabilize a cart- and VTOL-pendulum systems. Furthermore,
the specification of the basis function are shown to be of importance when
generating a Koopman operator for specific robotic systems. Experimental
results with the Sphero SPRK robot explore the utility of the Koopman operator
in a reduced state representation setting where increased complexity in the
basis function improve open- and closed-loop controller performance in various
terrains, including sand.
",1,0,0,0,0,0
8859,On the magnetic shield for a Vlasov-Poisson plasma,"  We study the screening of a bounded body $\Gamma$ against the effect of a
wind of charged particles, by means of a shield produced by a magnetic field
which becomes infinite on the border of $\Gamma$. The charged wind is modeled
by a Vlasov-Poisson plasma, the bounded body by a torus, and the external
magnetic field is taken close to the border of $\Gamma$. We study two models: a
plasma composed by different species with positive or negative charges, and
finite total mass of each species, and another made of many species of the same
sign, each having infinite mass. We investigate the time evolution of both
systems, showing in particular that the plasma particles cannot reach the body.
Finally we discuss possible extensions to more general initial data. We show
also that when the magnetic lines are straight lines, (that imposes an
unbounded body), the previous results can be improved.
",0,0,1,0,0,0
12439,Discrete structure of the brain rhythms,"  Neuronal activity in the brain generates synchronous oscillations of the
Local Field Potential (LFP). The traditional analyses of the LFPs are based on
decomposing the signal into simpler components, such as sinusoidal harmonics.
However, a common drawback of such methods is that the decomposition primitives
are usually presumed from the onset, which may bias our understanding of the
signal's structure. Here, we introduce an alternative approach that allows an
impartial, high resolution, hands-off decomposition of the brain waves into a
small number of discrete, frequency-modulated oscillatory processes, which we
call oscillons. In particular, we demonstrate that mouse hippocampal LFP
contain a single oscillon that occupies the $\theta$-frequency band and a
couple of $\gamma$-oscillons that correspond, respectively, to slow and fast
$\gamma$-waves. Since the oscillons were identified empirically, they may
represent the actual, physical structure of synchronous oscillations in
neuronal ensembles, whereas Fourier-defined ""brain waves"" are nothing but
poorly resolved oscillons.
",0,0,0,0,1,0
12155,Source Selection for Cluster Weak Lensing Measurements in the Hyper Suprime-Cam Survey,"  We present optimized source galaxy selection schemes for measuring cluster
weak lensing (WL) mass profiles unaffected by cluster member dilution from the
Subaru Hyper Suprime-Cam Strategic Survey Program (HSC-SSP). The ongoing
HSC-SSP survey will uncover thousands of galaxy clusters to $z\lesssim1.5$. In
deriving cluster masses via WL, a critical source of systematics is
contamination and dilution of the lensing signal by cluster {members, and by
foreground galaxies whose photometric redshifts are biased}. Using the
first-year CAMIRA catalog of $\sim$900 clusters with richness larger than 20
found in $\sim$140 deg$^2$ of HSC-SSP data, we devise and compare several
source selection methods, including selection in color-color space (CC-cut),
and selection of robust photometric redshifts by applying constraints on their
cumulative probability distribution function (PDF; P-cut). We examine the
dependence of the contamination on the chosen limits adopted for each method.
Using the proper limits, these methods give mass profiles with minimal dilution
in agreement with one another. We find that not adopting either the CC-cut or
P-cut methods results in an underestimation of the total cluster mass
($13\pm4\%$) and the concentration of the profile ($24\pm11\%$). The level of
cluster contamination can reach as high as $\sim10\%$ at $R\approx 0.24$
Mpc/$h$ for low-z clusters without cuts, while employing either the P-cut or
CC-cut results in cluster contamination consistent with zero to within the 0.5%
uncertainties. Our robust methods yield a $\sim60\sigma$ detection of the
stacked CAMIRA surface mass density profile, with a mean mass of
$M_\mathrm{200c} = (1.67\pm0.05({\rm {stat}}))\times 10^{14}\,M_\odot/h$.
",0,1,0,0,0,0
17639,Driven by Excess? Climatic Implications of New Global Mapping of Near-Surface Water-Equivalent Hydrogen on Mars,"  We present improved Mars Odyssey Neutron Spectrometer (MONS) maps of
near-surface Water-Equivalent Hydrogen (WEH) on Mars that have intriguing
implications for the global distribution of ""excess"" ice, which occurs when the
mass fraction of water ice exceeds the threshold amount needed to saturate the
pore volume in normal soils. We have refined the crossover technique of Feldman
et al. (2011) by using spatial deconvolution and Gaussian weighting to create
the first globally self-consistent map of WEH. At low latitudes, our new maps
indicate that WEH exceeds 15% in several near-equatorial regions, such as
Arabia Terra, which has important implications for the types of hydrated
minerals present at low latitudes. At high latitudes, we demonstrate that the
disparate MONS and Phoenix Robotic Arm (RA) observations of near surface WEH
can be reconciled by a three-layer model incorporating dry soil over fully
saturated pore ice over pure excess ice: such a three-layer model can also
potentially explain the strong anticorrelation of subsurface ice content and
ice table depth observed at high latitudes. At moderate latitudes, we show that
the distribution of recently formed impact craters is also consistent with our
latest MONS results, as both the shallowest ice-exposing crater and deepest
non-ice-exposing crater at each impact site are in good agreement with our
predictions of near-surface WEH. Overall, we find that our new mapping is
consistent with the widespread presence at mid-to-high Martian latitudes of
recently deposited shallow excess ice reservoirs that are not yet in
equilibrium with the atmosphere.
",0,1,0,0,0,0
4837,Faster Fuzzing: Reinitialization with Deep Neural Models,"  We improve the performance of the American Fuzzy Lop (AFL) fuzz testing
framework by using Generative Adversarial Network (GAN) models to reinitialize
the system with novel seed files. We assess performance based on the temporal
rate at which we produce novel and unseen code paths. We compare this approach
to seed file generation from a random draw of bytes observed in the training
seed files. The code path lengths and variations were not sufficiently diverse
to fully replace AFL input generation. However, augmenting native AFL with
these additional code paths demonstrated improvements over AFL alone.
Specifically, experiments showed the GAN was faster and more effective than the
LSTM and out-performed a random augmentation strategy, as measured by the
number of unique code paths discovered. GAN helps AFL discover 14.23% more code
paths than the random strategy in the same amount of CPU time, finds 6.16% more
unique code paths, and finds paths that are on average 13.84% longer. Using GAN
shows promise as a reinitialization strategy for AFL to help the fuzzer
exercise deep paths in software.
",1,0,0,0,0,0
15739,Investigating Collaboration Within Online Communities: Software Development Vs. Artistic Creation,"  Online creative communities have been able to develop large, open source
software (OSS) projects like Linux and Firefox throughout the successful
collaborations carried out over the Internet. These communities have also
expanded to creative arts domains such as animation, video games, and music.
Despite their growing popularity, the factors that lead to successful
collaborations in these communities are not entirely understood. In the
following, I describe my PhD research project aimed at improving communication,
collaboration, and retention in creative arts communities, starting from the
experience gained from the literature about OSS communities.
",1,0,0,0,0,0
13359,Learning compressed representations of blood samples time series with missing data,"  Clinical measurements collected over time are naturally represented as
multivariate time series (MTS), which often contain missing data. An
autoencoder can learn low dimensional vectorial representations of MTS that
preserve important data characteristics, but cannot deal explicitly with
missing data. In this work, we propose a new framework that combines an
autoencoder with the Time series Cluster Kernel (TCK), a kernel that accounts
for missingness patterns in MTS. Via kernel alignment, we incorporate TCK in
the autoencoder to improve the learned representations in presence of missing
data. We consider a classification problem of MTS with missing values,
representing blood samples of patients with surgical site infection. With our
approach, rather than with a standard autoencoder, we learn representations in
low dimensions that can be classified better.
",1,0,0,1,0,0
19576,Consistency Between the Luminosity Function of Resolved Millisecond Pulsars and the Galactic Center Excess,"  Fermi Large Area Telescope data reveal an excess of GeV gamma rays from the
direction of the Galactic Center and bulge. Several explanations have been
proposed for this excess including an unresolved population of millisecond
pulsars (MSPs) and self-annihilating dark matter. It has been claimed that a
key discriminant for or against the MSP explanation can be extracted from the
properties of the luminosity function describing this source population.
Specifically, is the luminosity function of the putative MSPs in the Galactic
Center consistent with that characterizing the resolved MSPs in the Galactic
disk? To investigate this we have used a Bayesian Markov Chain Monte Carlo to
evaluate the posterior distribution of the parameters of the MSP luminosity
function describing both resolved MSPs and the Galactic Center excess. At
variance with some other claims, our analysis reveals that, within current
uncertainties, both data sets can be well fit with the same luminosity
function.
",0,1,0,0,0,0
3624,Symmetric Rank Covariances: a Generalised Framework for Nonparametric Measures of Dependence,"  The need to test whether two random vectors are independent has spawned a
large number of competing measures of dependence. We are interested in
nonparametric measures that are invariant under strictly increasing
transformations, such as Kendall's tau, Hoeffding's D, and the more recently
discovered Bergsma--Dassios sign covariance. Each of these measures exhibits
symmetries that are not readily apparent from their definitions. Making these
symmetries explicit, we define a new class of multivariate nonparametric
measures of dependence that we refer to as Symmetric Rank Covariances. This new
class generalises all of the above measures and leads naturally to multivariate
extensions of the Bergsma--Dassios sign covariance. Symmetric Rank Covariances
may be estimated unbiasedly using U-statistics for which we prove results on
computational efficiency and large-sample behavior. The algorithms we develop
for their computation include, to the best of our knowledge, the first
efficient algorithms for the well-known Hoeffding's D statistic in the
multivariate setting.
",0,0,1,1,0,0
13937,Linear Parsing Expression Grammars,"  PEGs were formalized by Ford in 2004, and have several pragmatic operators
(such as ordered choice and unlimited lookahead) for better expressing modern
programming language syntax. Since these operators are not explicitly defined
in the classic formal language theory, it is significant and still challenging
to argue PEGs' expressiveness in the context of formal language theory.Since
PEGs are relatively new, there are several unsolved problems.One of the
problems is revealing a subclass of PEGs that is equivalent to DFAs. This
allows application of some techniques from the theory of regular grammar to
PEGs. In this paper, we define Linear PEGs (LPEGs), a subclass of PEGs that is
equivalent to DFAs. Surprisingly, LPEGs are formalized by only excluding some
patterns of recursive nonterminal in PEGs, and include the full set of ordered
choice, unlimited lookahead, and greedy repetition, which are characteristic of
PEGs. Although the conversion judgement of parsing expressions into DFAs is
undecidable in general, the formalism of LPEGs allows for a syntactical
judgement of parsing expressions.
",1,0,0,0,0,0
3243,Privacy Preserving Face Retrieval in the Cloud for Mobile Users,"  Recently, cloud storage and processing have been widely adopted. Mobile users
in one family or one team may automatically backup their photos to the same
shared cloud storage space. The powerful face detector trained and provided by
a 3rd party may be used to retrieve the photo collection which contains a
specific group of persons from the cloud storage server. However, the privacy
of the mobile users may be leaked to the cloud server providers. In the
meanwhile, the copyright of the face detector should be protected. Thus, in
this paper, we propose a protocol of privacy preserving face retrieval in the
cloud for mobile users, which protects the user photos and the face detector
simultaneously. The cloud server only provides the resources of storage and
computing and can not learn anything of the user photos and the face detector.
We test our protocol inside several families and classes. The experimental
results reveal that our protocol can successfully retrieve the proper photos
from the cloud server and protect the user photos and the face detector.
",1,0,0,0,0,0
10262,ADN: An Information-Centric Networking Architecture for the Internet of Things,"  Forwarding data by name has been assumed to be a necessary aspect of an
information-centric redesign of the current Internet architecture that makes
content access, dissemination, and storage more efficient. The Named Data
Networking (NDN) and Content-Centric Networking (CCNx) architectures are the
leading examples of such an approach. However, forwarding data by name incurs
storage and communication complexities that are orders of magnitude larger than
solutions based on forwarding data using addresses. Furthermore, the specific
algorithms used in NDN and CCNx have been shown to have a number of
limitations. The Addressable Data Networking (ADN) architecture is introduced
as an alternative to NDN and CCNx. ADN is particularly attractive for
large-scale deployments of the Internet of Things (IoT), because it requires
far less storage and processing in relaying nodes than NDN. ADN allows things
and data to be denoted by names, just like NDN and CCNx do. However, instead of
replacing the waist of the Internet with named-data forwarding, ADN uses an
address-based forwarding plane and introduces an information plane that
seamlessly maps names to addresses without the involvement of end-user
applications. Simulation results illustrate the order of magnitude savings in
complexity that can be attained with ADN compared to NDN.
",1,0,0,0,0,0
386,Topology of Large-Scale Structures of Galaxies in Two Dimensions - Systematic Effects,"  We study the two-dimensional topology of the galactic distribution when
projected onto two-dimensional spherical shells. Using the latest Horizon Run 4
simulation data, we construct the genus of the two-dimensional field and
consider how this statistic is affected by late-time nonlinear effects --
principally gravitational collapse and redshift space distortion (RSD). We also
consider systematic and numerical artifacts such as shot noise, galaxy bias,
and finite pixel effects. We model the systematics using a Hermite polynomial
expansion and perform a comprehensive analysis of known effects on the
two-dimensional genus, with a view toward using the statistic for cosmological
parameter estimation. We find that the finite pixel effect is dominated by an
amplitude drop and can be made less than $1\%$ by adopting pixels smaller than
$1/3$ of the angular smoothing length. Nonlinear gravitational evolution
introduces time-dependent coefficients of the zeroth, first, and second Hermite
polynomials, but the genus amplitude changes by less than $1\%$ between $z=1$
and $z=0$ for smoothing scales $R_{\rm G} > 9 {\rm Mpc/h}$. Non-zero terms are
measured up to third order in the Hermite polynomial expansion when studying
RSD. Differences in shapes of the genus curves in real and redshift space are
small when we adopt thick redshift shells, but the amplitude change remains a
significant $\sim {\cal O}(10\%)$ effect. The combined effects of galaxy
biasing and shot noise produce systematic effects up to the second Hermite
polynomial. It is shown that, when sampling, the use of galaxy mass cuts
significantly reduces the effect of shot noise relative to random sampling.
",0,1,0,0,0,0
19279,HAZMAT II: Ultraviolet Variability of Low-Mass Stars in the GALEX Archive,"  The ultraviolet (UV) light from a host star influences a planet's atmospheric
photochemistry and will affect interpretations of exoplanetary spectra from
future missions like the James Webb Space Telescope. These effects will be
particularly critical in the study of planetary atmospheres around M dwarfs,
including Earth-sized planets in the habitable zone. Given the higher activity
levels of M dwarfs compared to Sun-like stars, time resolved UV data are needed
for more accurate input conditions for exoplanet atmospheric modeling. The
Galaxy Evolution Explorer (\emph{GALEX}) provides multi-epoch photometric
observations in two UV bands: near-ultraviolet (NUV; 1771 -- 2831 \AA) and
far-ultraviolet (FUV; 1344 -- 1786 \AA). Within 30 pc of Earth, there are 357
and 303 M dwarfs in the NUV and FUV bands, respectively, with multiple\GALEX
observations. Simultaneous NUV and FUV detections exist for 145 stars in
both\GALEX bands. Our analyses of these data show that low-mass stars are
typically more variable in the FUV than the NUV. Median variability increases
with later spectral types in the NUV with no clear trend in the FUV. We find
evidence that flares increase the FUV flux density far more than the NUV flux
density, leading to variable FUV to NUV flux density ratios in the \GALEX\
bandpasses.The ratio of FUV to NUV flux is important for interpreting the
presence of atmospheric molecules in planetary atmospheres such as oxygen and
methane as a high FUV to NUV ratio may cause false-positive biosignature
detections. This ratio of flux density in the\GALEX\ bands spans three orders
of magnitude in our sample, from 0.008 to 4.6, and is 1 to 2 orders of
magnitude higher than for G dwarfs like the Sun. These results characterize the
UV behavior for the largest set of low-mass stars to date.
",0,1,0,0,0,0
5503,ASK/PSK-correspondence and the r-map,"  We formulate a correspondence between affine and projective special Kähler
manifolds of the same dimension. As an application, we show that, under this
correspondence, the affine special Kähler manifolds in the image of the rigid
r-map are mapped to one-parameter deformations of projective special Kähler
manifolds in the image of the supergravity r-map. The above one-parameter
deformations are interpreted as perturbative $\alpha'$-corrections in heterotic
and type-II string compactifications with $N=2$ supersymmetry. Also affine
special Kähler manifolds with quadratic prepotential are mapped to
one-parameter families of projective special Kähler manifolds with quadratic
prepotential. We show that the completeness of the deformed supergravity r-map
metric depends solely on the (well-understood) completeness of the undeformed
metric and the sign of the deformation parameter.
",0,0,1,0,0,0
16341,Coordination game in bidirectional flow,"  We have introduced evolutionary game dynamics to a one-dimensional
cellular-automaton to investigate evolution and maintenance of cooperative
avoiding behavior of self-driven particles in bidirectional flow. In our model,
there are two kinds of particles, which are right-going particles and
left-going particles. They often face opponent particles, so that they swerve
to the right or left stochastically in order to avoid conflicts. The particles
reinforce their preferences of the swerving direction after their successful
avoidance. The preference is also weakened by memory-loss effect.
Result of our simulation indicates that cooperative avoiding behavior is
achieved, i.e., swerving directions of the particles are unified, when the
density of particles is close to 1/2 and the memory-loss rate is small.
Furthermore, when the right-going particles occupy the majority of the system,
we observe that their flow increases when the number of left-going particles,
which prevent the smooth movement of right-going particles, becomes large. It
is also investigated that the critical memory-loss rate of the cooperative
avoiding behavior strongly depends on the size of the system. Small system can
prolong the cooperative avoiding behavior in wider range of memory-loss rate
than large system.
",1,1,0,0,0,0
475,Selecting optimal minimum spanning trees that share a topological correspondence with phylogenetic trees,"  Choi et. al (2011) introduced a minimum spanning tree (MST)-based method
called CLGrouping, for constructing tree-structured probabilistic graphical
models, a statistical framework that is commonly used for inferring
phylogenetic trees. While CLGrouping works correctly if there is a unique MST,
we observe an indeterminacy in the method in the case that there are multiple
MSTs. In this work we remove this indeterminacy by introducing so-called
vertex-ranked MSTs. We note that the effectiveness of CLGrouping is inversely
related to the number of leaves in the MST. This motivates the problem of
finding a vertex-ranked MST with the minimum number of leaves (MLVRMST). We
provide a polynomial time algorithm for the MLVRMST problem, and prove its
correctness for graphs whose edges are weighted with tree-additive distances.
",1,0,1,0,0,0
8942,Improving Neural Network Quantization using Outlier Channel Splitting,"  Quantization can improve the execution latency and energy efficiency of
neural networks on both commodity GPUs and specialized accelerators. The
majority of existing literature focuses on training quantized DNNs, while this
work examines the less-studied topic of quantizing a floating-point model
without (re)training. DNN weights and activations follow a bell-shaped
distribution post-training, while practical hardware uses a linear quantization
grid. This leads to challenges in dealing with outliers in the distribution.
Prior work has addressed this by clipping the outliers or using specialized
hardware. In this work, we propose outlier channel splitting (OCS), which
duplicates channels containing outliers, then halves the channel values. The
network remains functionally identical, but affected outliers are moved toward
the center of the distribution. OCS requires no additional training and works
on commodity hardware. Experimental evaluation on ImageNet classification and
language modeling shows that OCS can outperform state-of-the-art clipping
techniques with only minor overhead.
",1,0,0,1,0,0
10598,"Isotopes of Octonion Algebras, G2-Torsors and Triality","  Octonion algebras over rings are, in contrast to those over fields, not
determined by their norm forms. Octonion algebras whose norm is isometric to
the norm q of a given algebra C are twisted forms of C by means of the
Aut(C)-torsor O(q) ->O(q)/Aut(C). We show that, over any commutative unital
ring, these twisted forms are precisely the isotopes C(a,b) of C, with
multiplication given by x*y=(xa)(by), for unit norm octonions a,b of C. The
link is provided by the triality phenomenon, which we study from new and
classical perspectives. We then study these twisted forms using the interplay,
thus obtained, between torsor geometry and isotope computations, thus obtaining
new results on octonion algebras over e.g. rings of (Laurent) polynomials.
",0,0,1,0,0,0
17842,Ray: A Distributed Framework for Emerging AI Applications,"  The next generation of AI applications will continuously interact with the
environment and learn from these interactions. These applications impose new
and demanding systems requirements, both in terms of performance and
flexibility. In this paper, we consider these requirements and present Ray---a
distributed system to address them. Ray implements a unified interface that can
express both task-parallel and actor-based computations, supported by a single
dynamic execution engine. To meet the performance requirements, Ray employs a
distributed scheduler and a distributed and fault-tolerant store to manage the
system's control state. In our experiments, we demonstrate scaling beyond 1.8
million tasks per second and better performance than existing specialized
systems for several challenging reinforcement learning applications.
",1,0,0,1,0,0
8863,Detecting tropical defects of polynomial equations,"  We introduce the notion of tropical defects, certificates that a system of
polynomial equations is not a tropical basis, and provide algorithms for
finding them around affine spaces of complementary dimension to the zero set.
We use these techniques to solve open problems regarding del Pezzo surfaces of
degree 3 and realizability of valuated gaussoids of rank 4.
",1,0,0,0,0,0
16013,Transverse Magnetic Susceptibility of a Frustrated Spin-$\frac{1}{2}$ $J_{1}$--$J_{2}$--$J_{1}^{\perp}$ Heisenberg Antiferromagnet on a Bilayer Honeycomb Lattice,"  We use the coupled cluster method (CCM) to study a frustrated
spin-$\frac{1}{2}$ $J_{1}$--$J_{2}$--$J_{1}^{\perp}$ Heisenberg antiferromagnet
on a bilayer honeycomb lattice with $AA$ stacking. Both nearest-neighbor (NN)
and frustrating next-nearest-neighbor antiferromagnetic (AFM) exchange
interactions are present in each layer, with respective exchange coupling
constants $J_{1}>0$ and $J_{2} \equiv \kappa J_{1} > 0$. The two layers are
coupled with NN AFM exchanges with coupling strength $J_{1}^{\perp}\equiv
\delta J_{1}>0$. We calculate to high orders of approximation within the CCM
the zero-field transverse magnetic susceptibility $\chi$ in the Néel phase.
We thus obtain an accurate estimate of the full boundary of the Néel phase in
the $\kappa\delta$ plane for the zero-temperature quantum phase diagram. We
demonstrate explicitly that the phase boundary derived from $\chi$ is fully
consistent with that obtained from the vanishing of the Néel magnetic order
parameter. We thus conclude that at all points along the Néel phase boundary
quasiclassical magnetic order gives way to a nonclassical paramagnetic phase
with a nonzero energy gap. The Néel phase boundary exhibits a marked
reentrant behavior, which we discuss in detail.
",0,1,0,0,0,0
2502,"An Analysis of the Value of Information when Exploring Stochastic, Discrete Multi-Armed Bandits","  In this paper, we propose an information-theoretic exploration strategy for
stochastic, discrete multi-armed bandits that achieves optimal regret. Our
strategy is based on the value of information criterion. This criterion
measures the trade-off between policy information and obtainable rewards. High
amounts of policy information are associated with exploration-dominant searches
of the space and yield high rewards. Low amounts of policy information favor
the exploitation of existing knowledge. Information, in this criterion, is
quantified by a parameter that can be varied during search. We demonstrate that
a simulated-annealing-like update of this parameter, with a sufficiently fast
cooling schedule, leads to an optimal regret that is logarithmic with respect
to the number of episodes.
",1,0,0,1,0,0
777,A Survey on Content-Aware Video Analysis for Sports,"  Sports data analysis is becoming increasingly large-scale, diversified, and
shared, but difficulty persists in rapidly accessing the most crucial
information. Previous surveys have focused on the methodologies of sports video
analysis from the spatiotemporal viewpoint instead of a content-based
viewpoint, and few of these studies have considered semantics. This study
develops a deeper interpretation of content-aware sports video analysis by
examining the insight offered by research into the structure of content under
different scenarios. On the basis of this insight, we provide an overview of
the themes particularly relevant to the research on content-aware systems for
broadcast sports. Specifically, we focus on the video content analysis
techniques applied in sportscasts over the past decade from the perspectives of
fundamentals and general review, a content hierarchical model, and trends and
challenges. Content-aware analysis methods are discussed with respect to
object-, event-, and context-oriented groups. In each group, the gap between
sensation and content excitement must be bridged using proper strategies. In
this regard, a content-aware approach is required to determine user demands.
Finally, the paper summarizes the future trends and challenges for sports video
analysis. We believe that our findings can advance the field of research on
content-aware video analysis for broadcast sports.
",1,0,0,0,0,0
20522,Robust d-wave pairing symmetry in multi-orbital cobalt high temperature superconductors,"  The pairing symmetry of the newly proposed cobalt high temperature
(high-$T_c$) superconductors formed by vertex shared cation-anion tetrahedral
complexes is studied by the methods of mean field, random phase approximation
(RPA) and functional renormalization group (FRG) analysis. The results of all
these methods show that the $d_{x^2-y^2}$ pairing symmetry is robustly favored
near half filling. The RPA and FRG methods, which are valid in weak interaction
regions, predict that the superconducting state is also strongly orbital
selective, namely the $d_{x^2-y^2}$ orbital that has the largest density near
half filling among the three $t_{2g}$ orbitals dominates superconducting
pairing. These results suggest that the new materials, if synthesized, can
provide indisputable test to high-$T_c$ pairing mechanism and the validity of
different theoretical methods.
",0,1,0,0,0,0
6919,Revisiting Activation Regularization for Language RNNs,"  Recurrent neural networks (RNNs) serve as a fundamental building block for
many sequence tasks across natural language processing. Recent research has
focused on recurrent dropout techniques or custom RNN cells in order to improve
performance. Both of these can require substantial modifications to the machine
learning model or to the underlying RNN configurations. We revisit traditional
regularization techniques, specifically L2 regularization on RNN activations
and slowness regularization over successive hidden states, to improve the
performance of RNNs on the task of language modeling. Both of these techniques
require minimal modification to existing RNN architectures and result in
performance improvements comparable or superior to more complicated
regularization techniques or custom cell architectures. These regularization
techniques can be used without any modification on optimized LSTM
implementations such as the NVIDIA cuDNN LSTM.
",1,0,0,0,0,0
10173,Weakening of the diamagnetic shielding in FeSe$_{1-x}$S$_x$ at high pressures,"  The superconducting transition of FeSe$_{1-x}$S$_x$ with three distinct
sulphur concentrations $x$ was studied under hydrostatic pressure up to
$\sim$70 kbar via bulk AC susceptibility. The pressure dependence of the
superconducting transition temperature ($T_c$) features a small dome-shaped
variation at low pressures for $x=0.04$ and $x=0.12$, followed by a more
substantial $T_c$ enhancement to a value of around 30 K at moderate pressures.
In $x=0.21$, a similar overall pressure dependence of $T_c$ is observed, except
that the small dome at low pressures is flattened. For all three
concentrations, a significant weakening of the diamagnetic shielding is
observed beyond the pressure around which the maximum $T_c$ of 30 K is reached
near the verge of pressure-induced magnetic phase. This observation points to a
strong competition between the magnetic and high-$T_c$ superconducting states
at high pressure in this system.
",0,1,0,0,0,0
13672,Testing isotropy in the Two Micron All-Sky redshift survey with information entropy,"  We use information entropy to test the isotropy in the nearby galaxy
distribution mapped by the Two Micron All-Sky redshift survey (2MRS). We find
that the galaxy distribution is highly anisotropic on small scales. The radial
anisotropy gradually decreases with increasing length scales and the observed
anisotropy is consistent with that expected for an isotropic Poisson
distribution beyond a length scale of $90 \, h^{-1}\, {\rm Mpc}$. Using mock
catalogues from N-body simulations, we find that the galaxy distribution in the
2MRS exhibits a degree of anisotropy compatible with that of the $\Lambda$CDM
model after accounting for the clustering bias of the 2MRS galaxies. We also
quantify the polar and azimuthal anisotropies and identify two directions
$(l,b)=(150^{\circ}, -15^{\circ})$, $(l,b)=(310^{\circ},-15^{\circ})$ which are
significantly anisotropic compared to the other directions in the sky. We
suggest that their preferential orientations on the sky may indicate a possible
alignment of the Local Group with two nearby large scale structures. Despite
the differences in the degree of anisotropy on small scales, we find that the
galaxy distributions in both the 2MRS and the $\Lambda$CDM model are isotropic
on a scale of $90 \, h^{-1}\, {\rm Mpc}$.
",0,1,0,0,0,0
10303,Demagnetization of cubic Gd-Ba-Cu-O bulk superconductor by cross-fields: measurements and 3D modelling,"  Superconducting bulks, acting as high-field permanent magnets, are promising
for many applications. An important effect in bulk permanent magnets is
crossed-field demagnetization, which can reduce the magnetic field in
superconductors due to relatively small transverse fields. Crossed-field
demagnetization has not been studied in sample shapes such as rectangular
prisms or cubes. This contribution presents a study based on both 3D numerical
modelling and experiments. We study a cubic Gd-Ba-Cu-O bulk superconductor
sample of size 6 mm magnetized by field cooling in an external field of around
1.3 T, which is later submitted to crossed-field magnetic fields of up to 164
mT. Modelling results agree with experiments, except at transverse fields 50\%
or above of the initial trapped field. The current paths present a strong 3D
nature. For instance, at the mid-plane perpendicular to the initial magnetizing
field, the current density in this direction changes smoothly from the critical
magnitude, ${J_c}$, at the lateral sides to zero at a certain penetration
depth. This indicates a rotation of the current density with magnitude ${J_c}$,
and hence force free effects like flux cutting are expected to play a
significant role.
",0,1,0,0,0,0
4297,Ground-state properties of unitary bosons: from clusters to matter,"  The properties of cold Bose gases at unitarity have been extensively
investigated in the last few years both theoretically and experimentally. In
this paper we use a family of interactions tuned to two-body unitarity and very
weak three-body binding to demonstrate the universal properties of both
clusters and matter. We determine the universal properties of finite clusters
up to 60 particles and, for the first time, explicitly demonstrate the
saturation of energy and density with particle number and compare with bulk
properties. At saturation in the bulk we determine the energy, density, two-
and three-body contacts and the condensate fraction. We find that uniform
matter is more bound than three-body clusters by nearly two orders of
magnitude, the two-body contact is very large in absolute terms, and yet the
condensate fraction is also very large, greater than 90%. Equilibrium
properties of these systems may be experimentally accessible through rapid
quenching of weakly-interacting boson superfluids.
",0,1,0,0,0,0
14440,Uniform asymptotics as a stationary point approaches an endpoint,"  We obtain the rigorous uniform asymptotics of a particular integral where a
stationary point is close to an endpoint. There exists a general method
introduced by Bleistein for obtaining uniform asymptotics in this situation.
However, this method does not provide rigorous estimates for the error. Indeed,
the method of Bleistein starts with a change of variables, which implies that
the parameter governing how close the stationary point is to the endpoint
appears in several parts of the integrand, and this means that one cannot
obtain general error bounds. By adapting the above method to our particular
integral, we obtain rigorous uniform leading-order asymptotics. We also give a
rigorous derivation of the asymptotics to all orders of the same integral; the
novelty of this second approach is that it does not involve a global change of
variables.
",0,0,1,0,0,0
16382,Learning Feature Nonlinearities with Non-Convex Regularized Binned Regression,"  For various applications, the relations between the dependent and independent
variables are highly nonlinear. Consequently, for large scale complex problems,
neural networks and regression trees are commonly preferred over linear models
such as Lasso. This work proposes learning the feature nonlinearities by
binning feature values and finding the best fit in each quantile using
non-convex regularized linear regression. The algorithm first captures the
dependence between neighboring quantiles by enforcing smoothness via
piecewise-constant/linear approximation and then selects a sparse subset of
good features. We prove that the proposed algorithm is statistically and
computationally efficient. In particular, it achieves linear rate of
convergence while requiring near-minimal number of samples. Evaluations on
synthetic and real datasets demonstrate that algorithm is competitive with
current state-of-the-art and accurately learns feature nonlinearities. Finally,
we explore an interesting connection between the binning stage of our algorithm
and sparse Johnson-Lindenstrauss matrices.
",1,0,1,1,0,0
20573,Sparsity constrained split feasibility for dose-volume constraints in inverse planning of intensity-modulated photon or proton therapy,"  A split feasibility formulation for the inverse problem of
intensity-modulated radiation therapy (IMRT) treatment planning with
dose-volume constraints (DVCs) included in the planning algorithm is presented.
It involves a new type of sparsity constraint that enables the inclusion of a
percentage-violation constraint in the model problem and its handling by
continuous (as opposed to integer) methods. We propose an iterative algorithmic
framework for solving such a problem by applying the feasibility-seeking
CQ-algorithm of Byrne combined with the automatic relaxation method (ARM) that
uses cyclic projections. Detailed implementation instructions are furnished.
Functionality of the algorithm was demonstrated through the creation of an
intensity-modulated proton therapy plan for a simple 2D C-shaped geometry and
also for a realistic base-of-skull chordoma treatment site. Monte Carlo
simulations of proton pencil beams of varying energy were conducted to obtain
dose distributions for the 2D test case. A research release of the Pinnacle3
proton treatment planning system was used to extract pencil beam doses for a
clinical base-of-skull chordoma case. In both cases the beamlet doses were
calculated to satisfy dose-volume constraints according to our new algorithm.
Examination of the dose-volume histograms following inverse planning with our
algorithm demonstrated that it performed as intended. The application of our
proposed algorithm to dose-volume constraint inverse planning was successfully
demonstrated. Comparison with optimized dose distributions from the research
release of the Pinnacle3 treatment planning system showed the algorithm could
achieve equivalent or superior results.
",0,1,1,0,0,0
18928,New Bounds on the Field Size for Maximally Recoverable Codes Instantiating Grid-like Topologies,"  In recent years, the rapidly increasing amounts of data created and processed
through the internet resulted in distributed storage systems employing erasure
coding based schemes. Aiming to balance the tradeoff between data recovery for
correlated failures and efficient encoding and decoding, distributed storage
systems employing maximally recoverable codes came up. Unifying a number of
topologies considered both in theory and practice, Gopalan \cite{Gopalan2017}
initiated the study of maximally recoverable codes for grid-like topologies.
In this paper, we focus on the maximally recoverable codes that instantiate
grid-like topologies $T_{m\times n}(1,b,0)$. To characterize the property of
codes for these topologies, we introduce the notion of \emph{pseudo-parity
check matrix}. Then, using the hypergraph independent set approach, we
establish the first polynomial upper bound on the field size needed for
achieving the maximal recoverability in topologies $T_{m\times n}(1,b,0)$, when
$n$ is large enough. And we further improve this general upper bound for
topologies $T_{4\times n}(1,2,0)$ and $T_{3\times n}(1,3,0)$. By relating the
problem to generalized \emph{Sidon sets} in $\mathbb{F}_q$, we also obtain
non-trivial lower bounds on the field size for maximally recoverable codes that
instantiate topologies $T_{4\times n}(1,2,0)$ and $T_{3\times n}(1,3,0)$.
",1,0,0,0,0,0
15891,Error analysis for global minima of semilinear optimal control problems,"  In [1] we consider an optimal control problem subject to a semilinear
elliptic PDE together with its variational discretization, where we provide a
condition which allows to decide whether a solution of the necessary first
order conditions is a global minimum. This condition can be explicitly
evaluated at the discrete level. Furthermore, we prove that if the above
condition holds uniformly with respect to the discretization parameter the
sequence of discrete solutions converges to a global solution of the
corresponding limit problem. With the present work we complement our
investigations of [1] in that we prove an error estimate for those discrete
global solutions. Numerical experiments confirm our analytical findings.
",0,0,1,0,0,0
5355,Optimization and Performance of Bifacial Solar Modules: A Global Perspective,"  With the rapidly growing interest in bifacial photovoltaics (PV), a worldwide
map of their potential performance can help assess and accelerate the global
deployment of this emerging technology. However, the existing literature only
highlights optimized bifacial PV for a few geographic locations or develops
worldwide performance maps for very specific configurations, such as the
vertical installation. It is still difficult to translate these location- and
configuration-specific conclusions to a general optimized performance of this
technology. In this paper, we present a global study and optimization of
bifacial solar modules using a rigorous and comprehensive modeling framework.
Our results demonstrate that with a low albedo of 0.25, the bifacial gain of
ground-mounted bifacial modules is less than 10% worldwide. However, increasing
the albedo to 0.5 and elevating modules 1 m above the ground can boost the
bifacial gain to 30%. Moreover, we derive a set of empirical design rules,
which optimize bifacial solar modules across the world, that provide the
groundwork for rapid assessment of the location-specific performance. We find
that ground-mounted, vertical, east-west-facing bifacial modules will
outperform their south-north-facing, optimally tilted counterparts by up to 15%
below the latitude of 30 degrees, for an albedo of 0.5. The relative energy
output is the reverse of this in latitudes above 30 degrees. A detailed and
systematic comparison with experimental data from Asia, Europe, and North
America validates the model presented in this paper. An online simulation tool
(this https URL) based on the model developed in this paper is
also available for a user to predict and optimize bifacial modules in any
arbitrary location across the globe.
",0,1,0,0,0,0
8057,SMAGEXP: a galaxy tool suite for transcriptomics data meta-analysis,"  Bakground: With the proliferation of available microarray and high throughput
sequencing experiments in the public domain, the use of meta-analysis methods
increases. In these experiments, where the sample size is often limited,
meta-analysis offers the possibility to considerably enhance the statistical
power and give more accurate results. For those purposes, it combines either
effect sizes or results of single studies in a appropriate manner. R packages
metaMA and metaRNASeq perform meta-analysis on microarray and NGS data,
respectively. They are not interchangeable as they rely on statistical modeling
specific to each technology.
Results: SMAGEXP (Statistical Meta-Analysis for Gene EXPression) integrates
metaMA and metaRNAseq packages into Galaxy. We aim to propose a unified way to
carry out meta-analysis of gene expression data, while taking care of their
specificities. We have developed this tool suite to analyse microarray data
from Gene Expression Omnibus (GEO) database or custom data from affymetrix
microarrays. These data are then combined to carry out meta-analysis using
metaMA package. SMAGEXP also offers to combine raw read counts from Next
Generation Sequencing (NGS) experiments using DESeq2 and metaRNASeq package. In
both cases, key values, independent from the technology type, are reported to
judge the quality of the meta-analysis. These tools are available on the Galaxy
main tool shed. Source code, help and installation instructions are available
on github.
Conclusion: The use of Galaxy offers an easy-to-use gene expression
meta-analysis tool suite based on the metaMA and metaRNASeq packages.
",0,0,0,1,1,0
18663,Deep Reinforcement Learning for Programming Language Correction,"  Novice programmers often struggle with the formal syntax of programming
languages. To assist them, we design a novel programming language correction
framework amenable to reinforcement learning. The framework allows an agent to
mimic human actions for text navigation and editing. We demonstrate that the
agent can be trained through self-exploration directly from the raw input, that
is, program text itself, without any knowledge of the formal syntax of the
programming language. We leverage expert demonstrations for one tenth of the
training data to accelerate training. The proposed technique is evaluated on
6975 erroneous C programs with typographic errors, written by students during
an introductory programming course. Our technique fixes 14% more programs and
29% more compiler error messages relative to those fixed by a state-of-the-art
tool, DeepFix, which uses a fully supervised neural machine translation
approach.
",1,0,0,0,0,0
18034,Dynamic Mortality Risk Predictions in Pediatric Critical Care Using Recurrent Neural Networks,"  Viewing the trajectory of a patient as a dynamical system, a recurrent neural
network was developed to learn the course of patient encounters in the
Pediatric Intensive Care Unit (PICU) of a major tertiary care center. Data
extracted from Electronic Medical Records (EMR) of about 12000 patients who
were admitted to the PICU over a period of more than 10 years were leveraged.
The RNN model ingests a sequence of measurements which include physiologic
observations, laboratory results, administered drugs and interventions, and
generates temporally dynamic predictions for in-ICU mortality at user-specified
times. The RNN's ICU mortality predictions offer significant improvements over
those from two clinically-used scores and static machine learning algorithms.
",1,0,1,1,0,0
8846,An OpenCL(TM) Deep Learning Accelerator on Arria 10,"  Convolutional neural nets (CNNs) have become a practical means to perform
vision tasks, particularly in the area of image classification. FPGAs are well
known to be able to perform convolutions efficiently, however, most recent
efforts to run CNNs on FPGAs have shown limited advantages over other devices
such as GPUs. Previous approaches on FPGAs have often been memory bound due to
the limited external memory bandwidth on the FPGA device. We show a novel
architecture written in OpenCL(TM), which we refer to as a Deep Learning
Accelerator (DLA), that maximizes data reuse and minimizes external memory
bandwidth. Furthermore, we show how we can use the Winograd transform to
significantly boost the performance of the FPGA. As a result, when running our
DLA on Intel's Arria 10 device we can achieve a performance of 1020 img/s, or
23 img/s/W when running the AlexNet CNN benchmark. This comes to 1382 GFLOPs
and is 10x faster with 8.4x more GFLOPS and 5.8x better efficiency than the
state-of-the-art on FPGAs. Additionally, 23 img/s/W is competitive against the
best publicly known implementation of AlexNet on nVidia's TitanX GPU.
",1,0,0,0,0,0
12430,Optimal Packings of Two to Four Equal Circles on Any Flat Torus,"  We find explicit formulas for the radii and locations of the circles in all
the optimally dense packings of two, three or four equal circles on any flat
torus, defined to be the quotient of the Euclidean plane by the lattice
generated by two independent vectors. We prove the optimality of the
arrangements using techniques from rigidity theory and topological graph
theory.
",0,0,1,0,0,0
13620,"Proof of Riemann hypothesis, Generalized Riemann hypothesis and Ramanujan $τ$-Dirichlet series hypothesis","  We prove Riemann hypothesis, Generalized Riemann hypothesis, and Ramanujan
$\tau$-Dirichlet series hypothesis. Method is to show the convexity of function
which has zeros critical strip the same as zeta function.
",0,0,1,0,0,0
14187,Interrogation of spline surfaces with application to isogeometric design and analysis of lattice-skin structures,"  A novel surface interrogation technique is proposed to compute the
intersection of curves with spline surfaces in isogeometric analysis. The
intersection points are determined in one-shot without resorting to a
Newton-Raphson iteration or successive refinement. Surface-curve intersection
requires usually the solution of a system of nonlinear equations. It is assumed
that the surface is given in form of a spline, such as a NURBS, T-spline or
Catmull-Clark subdivision surface, and is convertible into a collection of
Bézier patches. First, a hierarchical bounding volume tree is used to
efficiently identify the Bézier patches with a convex-hull intersecting the
convex-hull of a given curve segment. For ease of implementation convex-hulls
are approximated with k-dops (discrete orientation polytopes). Subsequently,
the intersections of the identified Bézier patches with the curve segment are
determined with a matrix-based implicit representation leading to the
computation of a sequence of small singular value decompositions (SVDs). As an
application of the developed interrogation technique the isogeometric design
and analysis of lattice-skin structures is investigated. Current additive
manufacturing technologies make it possible to produce up to metre size parts
with designed geometric features reaching down to submillimetre scale. The skin
is a spline surface that is usually created in a computer-aided design (CAD)
system and the periodic lattice to be fitted consists of unit cells, each
containing a small number of struts. The lattice-skin structure is generated by
projecting selected lattice nodes onto the surface after determining the
intersection of unit cell edges with the surface. For mechanical analysis, the
skin is modelled as a Kirchhoff-Love thin-shell and the lattice as a
pin-jointed truss. The two types of structures are coupled with a standard
Lagrange multiplier approach.
",1,0,0,0,0,0
9166,"Graphical Models: An Extension to Random Graphs, Trees, and Other Objects","  In this work, we consider an extension of graphical models to random graphs,
trees, and other objects. To do this, many fundamental concepts for
multivariate random variables (e.g., marginal variables, Gibbs distribution,
Markov properties) must be extended to other mathematical objects; it turns out
that this extension is possible, as we will discuss, if we have a consistent,
complete system of projections on a given object. Each projection defines a
marginal random variable, allowing one to specify independence assumptions
between them. Furthermore, these independencies can be specified in terms of a
small subset of these marginal variables (which we call the atomic variables),
allowing the compact representation of independencies by a directed graph.
Projections also define factors, functions on the projected object space, and
hence a projection family defines a set of possible factorizations for a
distribution; these can be compactly represented by an undirected graph.
The invariances used in graphical models are essential for learning
distributions, not just on multivariate random variables, but also on other
objects. When they are applied to random graphs and random trees, the result is
a general class of models that is applicable to a broad range of problems,
including those in which the graphs and trees have complicated edge structures.
These models need not be conditioned on a fixed number of vertices, as is often
the case in the literature for random graphs, and can be used for problems in
which attributes are associated with vertices and edges. For graphs,
applications include the modeling of molecules, neural networks, and relational
real-world scenes; for trees, applications include the modeling of infectious
diseases, cell fusion, the structure of language, and the structure of objects
in visual scenes. Many classic models are particular instances of this
framework.
",1,0,0,1,0,0
2271,The Design and Implementation of Modern Online Programming Competitions,"  This paper presents a framework for the implementation of online programming
competitions, including a set of principles for the design of the multiplayer
game and a practical framework for the construction of the competition
environment. The paper presents a successful example competition, the 2016-17
Halite challenge, and briefly mentions a second competition, the Halite II
challenge, which launched in October 2017.
",1,0,0,0,0,0
3416,Learning Texture Manifolds with the Periodic Spatial GAN,"  This paper introduces a novel approach to texture synthesis based on
generative adversarial networks (GAN) (Goodfellow et al., 2014). We extend the
structure of the input noise distribution by constructing tensors with
different types of dimensions. We call this technique Periodic Spatial GAN
(PSGAN). The PSGAN has several novel abilities which surpass the current state
of the art in texture synthesis. First, we can learn multiple textures from
datasets of one or more complex large images. Second, we show that the image
generation with PSGANs has properties of a texture manifold: we can smoothly
interpolate between samples in the structured noise space and generate novel
samples, which lie perceptually between the textures of the original dataset.
In addition, we can also accurately learn periodical textures. We make multiple
experiments which show that PSGANs can flexibly handle diverse texture and
image data sources. Our method is highly scalable and it can generate output
images of arbitrary large size.
",1,0,0,1,0,0
17577,A note on primitive $1-$normal elements over finite fields,"  Let $q$ be a prime power of a prime $p$, $n$ a positive integer and $\mathbb
F_{q^n}$ the finite field with $q^n$ elements. The $k-$normal elements over
finite fields were introduced and characterized by Huczynska et al (2013).
Under the condition that $n$ is not divisible by $p$, they obtained an
existence result on primitive $1-$normal elements of $\mathbb F_{q^n}$ over
$\mathbb F_q$ for $q>2$. In this note, we extend their result to the excluded
case $q=2$.
",0,0,1,0,0,0
7229,Interacting Multi-particle Classical Szilard Engine,"  Szilard engine(SZE) is one of the best example of how information can be used
to extract work from a system. Initially, the working substance of SZE was
considered to be a single particle. Later on, researchers has extended the
studies of SZE to multi-particle systems and even to quantum regime. Here we
present a detailed study of classical SZE consisting of $N$ particles with
inter-particle interactions, i.e., the working substance is a low density
non-ideal gas and compare the work extraction with respect to SZE with
non-interacting multi particle system as working substance. We have considered
two cases of interactions namely: (i) hard core interactions and (ii) square
well interaction. Our study reveals that work extraction is less when more
particles are interacting through hard core interactions. More work is
extracted when the particles are interacting via square well interaction.
Another important result for the second case is that as we increase the
particle number the work extraction becomes independent of the initial position
of the partition, as opposed to the first case. Work extraction depends
crucially on the initial position of the partition. More work can be extracted
with larger number of particles when partition is inserted at positions near
the boundary walls.
",0,1,0,0,0,0
8511,T-duality in rational homotopy theory via $L_\infty$-algebras,"  We combine Sullivan models from rational homotopy theory with Stasheff's
$L_\infty$-algebras to describe a duality in string theory. Namely, what in
string theory is known as topological T-duality between $K^0$-cocycles in type
IIA string theory and $K^1$-cocycles in type IIB string theory, or as Hori's
formula, can be recognized as a Fourier-Mukai transform between twisted
cohomologies when looked through the lenses of rational homotopy theory. We
show this as an example of topological T-duality in rational homotopy theory,
which in turn can be completely formulated in terms of morphisms of
$L_\infty$-algebras.
",0,0,1,0,0,0
5949,"The use of Charts, Pivot Tables, and Array Formulas in two Popular Spreadsheet Corpora","  The use of spreadsheets in industry is widespread. Companies base decisions
on information coming from spreadsheets. Unfortunately, spreadsheets are
error-prone and this increases the risk that companies base their decisions on
inaccurate information, which can lead to incorrect decisions and loss of
money. In general, spreadsheet research is aimed to reduce the error-proneness
of spreadsheets. Most research is concentrated on the use of formulas. However,
there are other constructions in spreadsheets, like charts, pivot tables, and
array formulas, that are also used to present decision support information to
the user. There is almost no research about how these constructions are used.
To improve spreadsheet quality it is important to understand how spreadsheets
are used and to obtain a complete understanding, the use of charts, pivot
tables, and array formulas should be included in research. In this paper, we
analyze two popular spreadsheet corpora: Enron and EUSES on the use of the
aforementioned constructions.
",1,0,0,0,0,0
5569,Converting Your Thoughts to Texts: Enabling Brain Typing via Deep Feature Learning of EEG Signals,"  An electroencephalography (EEG) based Brain Computer Interface (BCI) enables
people to communicate with the outside world by interpreting the EEG signals of
their brains to interact with devices such as wheelchairs and intelligent
robots. More specifically, motor imagery EEG (MI-EEG), which reflects a
subjects active intent, is attracting increasing attention for a variety of BCI
applications. Accurate classification of MI-EEG signals while essential for
effective operation of BCI systems, is challenging due to the significant noise
inherent in the signals and the lack of informative correlation between the
signals and brain activities. In this paper, we propose a novel deep neural
network based learning framework that affords perceptive insights into the
relationship between the MI-EEG data and brain activities. We design a joint
convolutional recurrent neural network that simultaneously learns robust
high-level feature presentations through low-dimensional dense embeddings from
raw MI-EEG signals. We also employ an Autoencoder layer to eliminate various
artifacts such as background activities. The proposed approach has been
evaluated extensively on a large- scale public MI-EEG dataset and a limited but
easy-to-deploy dataset collected in our lab. The results show that our approach
outperforms a series of baselines and the competitive state-of-the- art
methods, yielding a classification accuracy of 95.53%. The applicability of our
proposed approach is further demonstrated with a practical BCI system for
typing.
",1,0,0,0,0,0
18738,Nonlinear learning and learning advantages in evolutionary games,"  The idea of incompetence as a learning or adaptation function was introduced
in the context of evolutionary games as a fixed parameter. However, live
organisms usually perform different nonlinear adaptation functions such as a
power law or exponential fitness growth. Here, we examine how the functional
form of the learning process may affect the social competition between
different behavioral types. Further, we extend our results for the evolutionary
games where fluctuations in the environment affect the behavioral adaptation of
competing species and demonstrate importance of the starting level of
incompetence for survival. Hence, we define a new concept of learning
advantages that becomes crucial when environments are constantly changing and
requiring rapid adaptation from species. This may lead to the evolutionarily
weak phase when even evolutionary stable populations become vulnerable to
invasions.
",0,0,0,0,1,0
6346,Estimation under group actions: recovering orbits from invariants,"  Motivated by geometric problems in signal processing, computer vision, and
structural biology, we study a class of orbit recovery problems where we
observe very noisy copies of an unknown signal, each acted upon by a random
element of some group (such as Z/p or SO(3)). The goal is to recover the orbit
of the signal under the group action in the high-noise regime. This generalizes
problems of interest such as multi-reference alignment (MRA) and the
reconstruction problem in cryo-electron microscopy (cryo-EM). We obtain
matching lower and upper bounds on the sample complexity of these problems in
high generality, showing that the statistical difficulty is intricately
determined by the invariant theory of the underlying symmetry group.
In particular, we determine that for cryo-EM with noise variance $\sigma^2$
and uniform viewing directions, the number of samples required scales as
$\sigma^6$. We match this bound with a novel algorithm for ab initio
reconstruction in cryo-EM, based on invariant features of degree at most 3. We
further discuss how to recover multiple molecular structures from heterogeneous
cryo-EM samples.
",1,0,1,0,0,0
18579,Partial constraint singularities in elastic rods,"  We present a unified classical treatment of partially constrained elastic
rods. Partial constraints often entail singularities in both shapes and
reactions. Our approach encompasses both sleeve and adhesion problems, and
provides simple and unambiguous derivations of counterintuitive results in the
literature. Relationships between reaction forces and moments, geometry, and
adhesion energies follow from the balance of energy during quasistatic motion.
We also relate our approach to the balance of material momentum and the concept
of a driving traction. The theory is generalizable and can be applied to a wide
array of contact, adhesion, gripping, and locomotion problems.
",0,1,0,0,0,0
15619,State Distribution-aware Sampling for Deep Q-learning,"  A critical and challenging problem in reinforcement learning is how to learn
the state-action value function from the experience replay buffer and
simultaneously keep sample efficiency and faster convergence to a high quality
solution. In prior works, transitions are uniformly sampled at random from the
replay buffer or sampled based on their priority measured by
temporal-difference (TD) error. However, these approaches do not fully take
into consideration the intrinsic characteristics of transition distribution in
the state space and could result in redundant and unnecessary TD updates,
slowing down the convergence of the learning procedure. To overcome this
problem, we propose a novel state distribution-aware sampling method to balance
the replay times for transitions with skew distribution, which takes into
account both the occurrence frequencies of transitions and the uncertainty of
state-action values. Consequently, our approach could reduce the unnecessary TD
updates and increase the TD updates for state-action value with more
uncertainty, making the experience replay more effective and efficient.
Extensive experiments are conducted on both classic control tasks and Atari
2600 games based on OpenAI gym platform and the experimental results
demonstrate the effectiveness of our approach in comparison with the standard
DQN approach.
",0,0,0,1,0,0
7673,Spectral up- and downshifting of Akhmediev breathers under wind forcing,"  We experimentally and numerically investigate the effect of wind forcing on
the spectral dynamics of Akhmediev breathers, a wave-type known to model the
modulation instability. We develop the wind model to the same order in
steepness as the higher order modifcation of the nonlinear Schroedinger
equation, also referred to as the Dysthe equation. This results in an
asymmetric wind term in the higher order, in addition to the leading order wind
forcing term. The derived model is in good agreement with laboratory
experiments within the range of the facility's length. We show that the leading
order forcing term amplifies all frequencies equally and therefore induces only
a broadening of the spectrum while the asymmetric higher order term in the
model enhances higher frequencies more than lower ones. Thus, the latter term
induces a permanent upshift of the spectral mean. On the other hand, in
contrast to the direct effect of wind forcing, wind can indirectly lead to
frequency downshifts, due to dissipative effects such as wave breaking, or
through amplification of the intrinsic spectral asymmetry of the Dysthe
equation. Furthermore, the definitions of the up- and downshift in terms of
peak- and mean frequencies, that are critical to relate our work to previous
results, are highlighted and discussed.
",0,1,0,0,0,0
13716,Ensemble dependence of fluctuations and the canonical/micro-canonical equivalence of ensembles,"  We study the equivalence of microcanonical and canonical ensembles in
continuous systems, in the sense of the convergence of the corresponding Gibbs
measures. This is obtained by proving a local central limit theorem and a local
large deviations principle. As an application we prove a formula due to
Lebowitz-Percus-Verlet. It gives mean square fluctuations of an extensive
observable, like the kinetic energy, in a classical micro canonical ensemble at
fixed energy.
",0,1,1,0,0,0
409,Dynamical system analysis of dark energy models in scalar coupled metric-torsion theories,"  We study the phase space dynamics of cosmological models in the theoretical
formulations of non-minimal metric-torsion couplings with a scalar field, and
investigate in particular the critical points which yield stable solutions
exhibiting cosmic acceleration driven by the {\em dark energy}. The latter is
defined in a way that it effectively has no direct interaction with the
cosmological fluid, although in an equivalent scalar-tensor cosmological setup
the scalar field interacts with the fluid (which we consider to be the
pressureless dust). Determining the conditions for the existence of the stable
critical points we check their physical viability, in both Einstein and Jordan
frames. We also verify that in either of these frames, the evolution of the
universe at the corresponding stable points matches with that given by the
respective exact solutions we have found in an earlier work (arXiv: 1611.00654
[gr-qc]). We not only examine the regions of physical relevance for the
trajectories in the phase space when the coupling parameter is varied, but also
demonstrate the evolution profiles of the cosmological parameters of interest
along fiducial trajectories in the effectively non-interacting scenarios, in
both Einstein and Jordan frames.
",0,1,0,0,0,0
12354,Wasserstein Identity Testing,"  Uniformity testing and the more general identity testing are well studied
problems in distributional property testing. Most previous work focuses on
testing under $L_1$-distance. However, when the support is very large or even
continuous, testing under $L_1$-distance may require a huge (even infinite)
number of samples. Motivated by such issues, we consider the identity testing
in Wasserstein distance (a.k.a. transportation distance and earthmover
distance) on a metric space (discrete or continuous).
In this paper, we propose the Wasserstein identity testing problem (Identity
Testing in Wasserstein distance). We obtain nearly optimal worst-case sample
complexity for the problem. Moreover, for a large class of probability
distributions satisfying the so-called ""Doubling Condition"", we provide nearly
instance-optimal sample complexity.
",1,0,1,0,0,0
14543,Sequence-to-Sequence Models Can Directly Translate Foreign Speech,"  We present a recurrent encoder-decoder deep neural network architecture that
directly translates speech in one language into text in another. The model does
not explicitly transcribe the speech into text in the source language, nor does
it require supervision from the ground truth source language transcription
during training. We apply a slightly modified sequence-to-sequence with
attention architecture that has previously been used for speech recognition and
show that it can be repurposed for this more complex task, illustrating the
power of attention-based models. A single model trained end-to-end obtains
state-of-the-art performance on the Fisher Callhome Spanish-English speech
translation task, outperforming a cascade of independently trained
sequence-to-sequence speech recognition and machine translation models by 1.8
BLEU points on the Fisher test set. In addition, we find that making use of the
training data in both languages by multi-task training sequence-to-sequence
speech translation and recognition models with a shared encoder network can
improve performance by a further 1.4 BLEU points.
",1,0,0,1,0,0
16734,Modular Representation of Layered Neural Networks,"  Layered neural networks have greatly improved the performance of various
applications including image processing, speech recognition, natural language
processing, and bioinformatics. However, it is still difficult to discover or
interpret knowledge from the inference provided by a layered neural network,
since its internal representation has many nonlinear and complex parameters
embedded in hierarchical layers. Therefore, it becomes important to establish a
new methodology by which layered neural networks can be understood.
In this paper, we propose a new method for extracting a global and simplified
structure from a layered neural network. Based on network analysis, the
proposed method detects communities or clusters of units with similar
connection patterns. We show its effectiveness by applying it to three use
cases. (1) Network decomposition: it can decompose a trained neural network
into multiple small independent networks thus dividing the problem and reducing
the computation time. (2) Training assessment: the appropriateness of a trained
result with a given hyperparameter or randomly chosen initial parameters can be
evaluated by using a modularity index. And (3) data analysis: in practical data
it reveals the community structure in the input, hidden, and output layers,
which serves as a clue for discovering knowledge from a trained neural network.
",1,0,0,1,0,0
10926,MP2-F12 Basis Set Convergence for the S66 Noncovalent Interactions Benchmark: Transferability of the Complementary Auxiliary Basis Set (CABS),"  Complementary auxiliary basis sets for F12 explicitly correlated calculations
appear to be more transferable between orbital basis sets than has been
generally assumed. We also find that aVnZ-F12 basis sets, originally developed
with anionic systems in mind, appear to be superior for noncovalent
interactions as well, and propose a suitable CABS sequence for them.
",0,1,0,0,0,0
5382,A New Wiretap Channel Model and its Strong Secrecy Capacity,"  In this paper, a new wiretap channel model is proposed, where the legitimate
transmitter and receiver communicate over a discrete memoryless channel. The
wiretapper has perfect access to a fixed-length subset of the transmitted
codeword symbols of her choosing. Additionally, she observes the remainder of
the transmitted symbols through a discrete memoryless channel. This new model
subsumes the classical wiretap channel and wiretap channel II with noisy main
channel as its special cases. The strong secrecy capacity of the proposed
channel model is identified. Achievability is established by solving a dual
secret key agreement problem in the source model, and converting the solution
to the original channel model using probability distribution approximation
arguments. In the dual problem, a source encoder and decoder, who observe
random sequences independent and identically distributed according to the input
and output distributions of the legitimate channel in the original problem,
communicate a confidential key over a public error-free channel using a single
forward transmission, in the presence of a compound wiretapping source who has
perfect access to the public discussion. The security of the key is guaranteed
for the exponentially many possibilities of the subset chosen at wiretapper by
deriving a lemma which provides a doubly-exponential convergence rate for the
probability that, for a fixed choice of the subset, the key is uniform and
independent from the public discussion and the wiretapping source's
observation. The converse is derived by using Sanov's theorem to upper bound
the secrecy capacity of the new wiretap channel model by the secrecy capacity
when the tapped subset is randomly chosen by nature.
",1,0,0,0,0,0
20964,Faithful Inversion of Generative Models for Effective Amortized Inference,"  Inference amortization methods share information across multiple
posterior-inference problems, allowing each to be carried out more efficiently.
Generally, they require the inversion of the dependency structure in the
generative model, as the modeller must learn a mapping from observations to
distributions approximating the posterior. Previous approaches have involved
inverting the dependency structure in a heuristic way that fails to capture
these dependencies correctly, thereby limiting the achievable accuracy of the
resulting approximations. We introduce an algorithm for faithfully, and
minimally, inverting the graphical model structure of any generative model.
Such inverses have two crucial properties: (a) they do not encode any
independence assertions that are absent from the model and; (b) they are local
maxima for the number of true independencies encoded. We prove the correctness
of our approach and empirically show that the resulting minimally faithful
inverses lead to better inference amortization than existing heuristic
approaches.
",1,0,0,1,0,0
2726,Making Neural Programming Architectures Generalize via Recursion,"  Empirically, neural networks that attempt to learn programs from data have
exhibited poor generalizability. Moreover, it has traditionally been difficult
to reason about the behavior of these models beyond a certain level of input
complexity. In order to address these issues, we propose augmenting neural
architectures with a key abstraction: recursion. As an application, we
implement recursion in the Neural Programmer-Interpreter framework on four
tasks: grade-school addition, bubble sort, topological sort, and quicksort. We
demonstrate superior generalizability and interpretability with small amounts
of training data. Recursion divides the problem into smaller pieces and
drastically reduces the domain of each neural network component, making it
tractable to prove guarantees about the overall system's behavior. Our
experience suggests that in order for neural architectures to robustly learn
program semantics, it is necessary to incorporate a concept like recursion.
",1,0,0,0,0,0
19214,Simulations of the Solar System's Early Dynamical Evolution with a Self-Gravitating Planetesimal Disk,"  Over the course of last decade, the Nice model has dramatically changed our
view of the solar system's formation and early evolution. Within the context of
this model, a transient period of planet-planet scattering is triggered by
gravitational interactions between the giant planets and a massive primordial
planetesimal disk, leading to a successful reproduction of the solar system's
present-day architecture. In typical realizations of the Nice model,
self-gravity of the planetesimal disk is routinely neglected, as it poses a
computational bottleneck to the calculations. Recent analyses have shown,
however, that a self-gravitating disk can exhibit behavior that is dynamically
distinct, and this disparity may have significant implications for the solar
system's evolutionary path. In this work, we explore this discrepancy utilizing
a large suite of Nice odel simulations with and without a self-gravitating
planetesimal disk, taking advantage of the inherently parallel nature of
graphic processing units. Our simulations demonstrate that self-consistent
modeling of particle interactions does not lead to significantly different
final planetary orbits from those obtained within conventional simulations.
Moreover, self-gravitating calculations show similar planetesimal evolution to
non-self-gravitating numerical experiments after dynamical instability is
triggered, suggesting that the orbital clustering observed in the distant
Kuiper belt is unlikely to have a self-gravitational origin.
",0,1,0,0,0,0
18618,Cosmic quantum optical probing of quantum gravity through a gravitational lensLens,"  We consider the nonunitary quantum dynamics of neutral massless scalar
particles used to model photons around a massive gravitational lens. The
gravitational interaction between the lensing mass and asymptotically free
particles is described by their second-quantized scattering wavefunctions.
Remarkably, the zero-point spacetime fluctuations can induce significant
decoherence of the scattered states with spontaneous emission of gravitons,
thereby reducing the particles' coherence as well as energy. This new effect
suggests that, when photon polarizations are negligible, such quantum gravity
phenomena could lead to measurable anomalous redshift of recently studied
astrophysical lasers through a gravitational lens in the range of black holes
and galaxy clusters.
",0,1,0,0,0,0
10169,Secret-Key-Aided Scheme for Securing Untrusted DF Relaying Networks,"  This paper proposes a new scheme to secure the transmissions in an untrusted
decode-and-forward (DF) relaying network. A legitimate source node, Alice,
sends her data to a legitimate destination node, Bob, with the aid of an
untrusted DF relay node, Charlie. To secure the transmissions from Charlie
during relaying time slots, each data codeword is secured using a secret-key
codeword that has been previously shared between Alice and Bob during the
perfectly secured time slots (i.e., when the channel secrecy rate is positive).
The secret-key bits exchanged between Alice and Bob are stored in a
finite-length buffer and are used to secure data transmission whenever needed.
We model the secret-key buffer as a queueing system and analyze its Markov
chain. Our numerical results show the gains of our proposed scheme relative to
benchmarks. Moreover, the proposed scheme achieves an upper bound on the secure
throughput.
",1,0,0,0,0,0
7982,Stability for gains from large investors' strategies in M1/J1 topologies,"  We prove continuity of a controlled SDE solution in Skorokhod's $M_1$ and
$J_1$ topologies and also uniformly, in probability, as a non-linear functional
of the control strategy. The functional comes from a finance problem to model
price impact of a large investor in an illiquid market. We show that
$M_1$-continuity is the key to ensure that proceeds and wealth processes from
(self-financing) càdlàg trading strategies are determined as the
continuous extensions for those from continuous strategies. We demonstrate by
examples how continuity properties are useful to solve different stochastic
control problems on optimal liquidation and to identify asymptotically
realizable proceeds.
",0,0,1,0,0,0
12297,Marginal likelihood based model comparison in Fuzzy Bayesian Learning,"  In a recent paper [1] we introduced the Fuzzy Bayesian Learning (FBL)
paradigm where expert opinions can be encoded in the form of fuzzy rule bases
and the hyper-parameters of the fuzzy sets can be learned from data using a
Bayesian approach. The present paper extends this work for selecting the most
appropriate rule base among a set of competing alternatives, which best
explains the data, by calculating the model evidence or marginal likelihood. We
explain why this is an attractive alternative over simply minimizing a mean
squared error metric of prediction and show the validity of the proposition
using synthetic examples and a real world case study in the financial services
sector.
",0,0,0,1,0,0
13296,Magnetic droplet nucleation with homochiral Neel domain wall,"  We investigate the effect of the Dzyaloshinskii Moriya interaction (DMI) on
magnetic domain nucleation in a ferromagnetic thin film with perpendicular
magnetic anisotropy. We propose an extended droplet model to determine the
nucleation field as a function of the in-plane field. The model can explain the
experimentally observed nucleation in a CoNi microstrip with the interfacial
DMI. The results are also reproduced by micromagnetic simulation based on the
string model. The electrical measurement method proposed in this study can be
widely used to quantitatively determine the DMI energy density.
",0,1,0,0,0,0
6678,Simple Classification using Binary Data,"  Binary, or one-bit, representations of data arise naturally in many
applications, and are appealing in both hardware implementations and algorithm
design. In this work, we study the problem of data classification from binary
data and propose a framework with low computation and resource costs. We
illustrate the utility of the proposed approach through stylized and realistic
numerical experiments, and provide a theoretical analysis for a simple case. We
hope that our framework and analysis will serve as a foundation for studying
similar types of approaches.
",1,0,0,1,0,0
3354,Subdeterminant Maximization via Nonconvex Relaxations and Anti-concentration,"  Several fundamental problems that arise in optimization and computer science
can be cast as follows: Given vectors $v_1,\ldots,v_m \in \mathbb{R}^d$ and a
constraint family ${\cal B}\subseteq 2^{[m]}$, find a set $S \in \cal{B}$ that
maximizes the squared volume of the simplex spanned by the vectors in $S$. A
motivating example is the data-summarization problem in machine learning where
one is given a collection of vectors that represent data such as documents or
images. The volume of a set of vectors is used as a measure of their diversity,
and partition or matroid constraints over $[m]$ are imposed in order to ensure
resource or fairness constraints. Recently, Nikolov and Singh presented a
convex program and showed how it can be used to estimate the value of the most
diverse set when ${\cal B}$ corresponds to a partition matroid. This result was
recently extended to regular matroids in works of Straszak and Vishnoi, and
Anari and Oveis Gharan. The question of whether these estimation algorithms can
be converted into the more useful approximation algorithms -- that also output
a set -- remained open.
The main contribution of this paper is to give the first approximation
algorithms for both partition and regular matroids. We present novel
formulations for the subdeterminant maximization problem for these matroids;
this reduces them to the problem of finding a point that maximizes the absolute
value of a nonconvex function over a Cartesian product of probability
simplices. The technical core of our results is a new anti-concentration
inequality for dependent random variables that allows us to relate the optimal
value of these nonconvex functions to their value at a random point. Unlike
prior work on the constrained subdeterminant maximization problem, our proofs
do not rely on real-stability or convexity and could be of independent interest
both in algorithms and complexity.
",1,0,1,1,0,0
3473,A Secular Resonant Origin for the Loneliness of Hot Jupiters,"  Despite decades of inquiry, the origin of giant planets residing within a few
tenths of an astronomical unit from their host stars remains unclear.
Traditionally, these objects are thought to have formed further out before
subsequently migrating inwards. However, the necessity of migration has been
recently called into question with the emergence of in-situ formation models of
close-in giant planets. Observational characterization of the transiting
sub-sample of close-in giants has revealed that ""warm"" Jupiters, possessing
orbital periods longer than roughly 10 days more often possess close-in,
co-transiting planetary companions than shorter period ""hot"" Jupiters, that are
usually lonely. This finding has previously been interpreted as evidence that
smooth, early migration or in situ formation gave rise to warm Jupiter-hosting
systems, whereas more violent, post-disk migration pathways sculpted hot
Jupiter-hosting systems. In this work, we demonstrate that both classes of
planet may arise via early migration or in-situ conglomeration, but that the
enhanced loneliness of hot Jupiters arises due to a secular resonant
interaction with the stellar quadrupole moment. Such an interaction tilts the
orbits of exterior, lower mass planets, removing them from transit surveys
where the hot Jupiter is detected. Warm Jupiter-hosting systems, in contrast,
retain their coplanarity due to the weaker influence of the host star's
quadrupolar potential relative to planet-disk interactions. In this way, hot
Jupiters and warm Jupiters are placed within a unified theoretical framework
that may be readily validated or falsified using data from upcoming missions
such as TESS.
",0,1,0,0,0,0
16276,Local decoding and testing of polynomials over grids,"  The well-known DeMillo-Lipton-Schwartz-Zippel lemma says that $n$-variate
polynomials of total degree at most $d$ over grids, i.e. sets of the form $A_1
\times A_2 \times \cdots \times A_n$, form error-correcting codes (of distance
at least $2^{-d}$ provided $\min_i\{|A_i|\}\geq 2$). In this work we explore
their local decodability and (tolerant) local testability. While these aspects
have been studied extensively when $A_1 = \cdots = A_n = \mathbb{F}_q$ are the
same finite field, the setting when $A_i$'s are not the full field does not
seem to have been explored before.
In this work we focus on the case $A_i = \{0,1\}$ for every $i$. We show that
for every field (finite or otherwise) there is a test whose query complexity
depends only on the degree (and not on the number of variables). In contrast we
show that decodability is possible over fields of positive characteristic (with
query complexity growing with the degree of the polynomial and the
characteristic), but not over the reals, where the query complexity must grow
with $n$. As a consequence we get a natural example of a code (one with a
transitive group of symmetries) that is locally testable but not locally
decodable.
Classical results on local decoding and testing of polynomials have relied on
the 2-transitive symmetries of the space of low-degree polynomials (under
affine transformations). Grids do not possess this symmetry: So we introduce
some new techniques to overcome this handicap and in particular use the
hypercontractivity of the (constant weight) noise operator on the Hamming cube.
",1,0,0,0,0,0
3487,On a minimal counterexample to Brauer's $k(B)$-conjecture,"  We study Brauer's long-standing $k(B)$-conjecture on the number of characters
in $p$-blocks for finite quasi-simple groups and show that their blocks do not
occur as a minimal counterexample for $p\ge5$ nor in the case of abelian
defect. For $p=3$ we obtain that the principal 3-blocks do not provide minimal
counterexamples. We also determine the precise number of irreducible characters
in unipotent blocks of classical groups for odd primes.
",0,0,1,0,0,0
13731,Theoretical and Computational Guarantees of Mean Field Variational Inference for Community Detection,"  The mean field variational Bayes method is becoming increasingly popular in
statistics and machine learning. Its iterative Coordinate Ascent Variational
Inference algorithm has been widely applied to large scale Bayesian inference.
See Blei et al. (2017) for a recent comprehensive review. Despite the
popularity of the mean field method there exist remarkably little fundamental
theoretical justifications. To the best of our knowledge, the iterative
algorithm has never been investigated for any high dimensional and complex
model. In this paper, we study the mean field method for community detection
under the Stochastic Block Model. For an iterative Batch Coordinate Ascent
Variational Inference algorithm, we show that it has a linear convergence rate
and converges to the minimax rate within $\log n$ iterations. This complements
the results of Bickel et al. (2013) which studied the global minimum of the
mean field variational Bayes and obtained asymptotic normal estimation of
global model parameters. In addition, we obtain similar optimality results for
Gibbs sampling and an iterative procedure to calculate maximum likelihood
estimation, which can be of independent interest.
",0,0,1,1,0,0
18539,Natural and Artificial Spectral Edges in Exoplanets,"  Technological civilizations may rely upon large-scale photovoltaic arrays to
harness energy from their host star. Photovoltaic materials, such as silicon,
possess distinctive spectral features, including an ""artificial edge"" that is
characteristically shifted in wavelength shortwards of the ""red edge"" of
vegetation. Future observations of reflected light from exoplanets would be
able to detect both natural and artificial edges photometrically, if a
significant fraction of the planet's surface is covered by vegetation or
photovoltaic arrays respectively. The stellar energy thus tapped can be
utilized for terraforming activities by transferring heat and light from the
day side to the night side on tidally locked exoplanets, thereby producing
detectable artifacts.
",0,1,0,0,0,0
45,LAAIR: A Layered Architecture for Autonomous Interactive Robots,"  When developing general purpose robots, the overarching software architecture
can greatly affect the ease of accomplishing various tasks. Initial efforts to
create unified robot systems in the 1990s led to hybrid architectures,
emphasizing a hierarchy in which deliberative plans direct the use of reactive
skills. However, since that time there has been significant progress in the
low-level skills available to robots, including manipulation and perception,
making it newly feasible to accomplish many more tasks in real-world domains.
There is thus renewed optimism that robots will be able to perform a wide array
of tasks while maintaining responsiveness to human operators. However, the top
layer in traditional hybrid architectures, designed to achieve long-term goals,
can make it difficult to react quickly to human interactions during goal-driven
execution. To mitigate this difficulty, we propose a novel architecture that
supports such transitions by adding a top-level reactive module which has
flexible access to both reactive skills and a deliberative control module. To
validate this architecture, we present a case study of its application on a
domestic service robot platform.
",1,0,0,0,0,0
9896,Quickest Localization of Anomalies in Power Grids: A Stochastic Graphical Framework,"  Agile localization of anomalous events plays a pivotal role in enhancing the
overall reliability of the grid and avoiding cascading failures. This is
especially of paramount significance in the large-scale grids due to their
geographical expansions and the large volume of data generated. This paper
proposes a stochastic graphical framework, by leveraging which it aims to
localize the anomalies with the minimum amount of data. This framework
capitalizes on the strong correlation structures observed among the
measurements collected from different buses. The proposed approach, at its
core, collects the measurements sequentially and progressively updates its
decision about the location of the anomaly. The process resumes until the
location of the anomaly can be identified with desired reliability. We provide
a general theory for the quickest anomaly localization and also investigate its
application for quickest line outage localization. Simulations in the IEEE
118-bus model are provided to establish the gains of the proposed approach.
",1,0,0,1,0,0
16402,Dielectric response of Anderson and pseudogapped insulators,"  Using a combination of analytic and numerical methods, we study the
polarizability of a (non-interacting) Anderson insulator in one, two, and three
dimensions and demonstrate that, in a wide range of parameters, it scales
proportionally to the square of the localization length, contrary to earlier
claims based on the effective-medium approximation. We further analyze the
effect of electron-electron interactions on the dielectric constant in
quasi-1D, quasi-2D and 3D materials with large localization length, including
both Coulomb repulsion and phonon-mediated attraction. The phonon-mediated
attraction (in the pseudogapped state on the insulating side of the
Superconductor-Insulator Transition) produces a correction to the dielectric
constant, which may be detected from a linear response of a dielectric constant
to an external magnetic field.
",0,1,0,0,0,0
18781,Hirota bilinear equations for Painlevé transcendents,"  We present some observations on the tau-function for the fourth Painlevé
equation. By considering a Hirota bilinear equation of order four for this
tau-function, we describe the general form of the Taylor expansion around an
arbitrary movable zero. The corresponding Taylor series for the tau-functions
of the first and second Painlevé equations, as well as that for the
Weierstrass sigma function, arise naturally as special cases, by setting
certain parameters to zero.
",0,1,1,0,0,0
19106,Q-analogues of the Fibo-Stirling numbers,"  Let $F_n$ denote the $n^{th}$ Fibonacci number relative to the initial
conditions $F_0=0$ and $F_1=1$. Bach, Paudyal, and Remmel introduced Fibonacci
analogues of the Stirling numbers called Fibo-Stirling numbers of the first and
second kind. These numbers serve as the connection coefficients between the
Fibo-falling factorial basis $\{(x)_{\downarrow_{F,n}}:n \geq 0\}$ and the
Fibo-rising factorial basis $\{(x)_{\uparrow_{F,n}}:n \geq 0\}$ which are
defined by $(x)_{\downarrow_{F,0}} = (x)_{\uparrow_{F,0}} = 1$ and for $k \geq
1$, $(x)_{\downarrow_{F,k}} = x(x-F_1) \cdots (x-F_{k-1})$ and
$(x)_{\uparrow_{F,k}} = x(x+F_1) \cdots (x+F_{k-1})$. We gave a general rook
theory model which allowed us to give combinatorial interpretations of the
Fibo-Stirling numbers of the first and second kind.
There are two natural $q$-analogues of the falling and rising Fibo-factorial
basis. That is, let $[x]_q = \frac{q^x-1}{q-1}$. Then we let
$[x]_{\downarrow_{q,F,0}} = \overline{[x]}_{\downarrow_{q,F,0}} =
[x]_{\uparrow_{q,F,0}} = \overline{[x]}_{\uparrow_{q,F,0}}=1$ and, for $k > 0$,
we let $[x]_{\downarrow_{q,F,k}} = [x]_q [x-F_1]_q \cdots [x-F_{k-1}]_q$,
$\overline{[x]}_{\downarrow_{q,F,k}}= [x]_q ([x]_q-[F_1]_q) \cdots
([x]_q-[F_{k-1}]_q)$, $[x]_{\uparrow_{q,F,k}}= [x]_q [x+F_1]_q \cdots
[x+F_{k-1}]_q$, and $\overline{[x]}_{\uparrow_{q,F,k}}= [x]_q ([x]_q+[F_1]_q)
\cdots ([x]_q+[F_{k-1}]_q)$.
In this paper, we show we can modify the rook theory model of Bach, Paudyal,
and Remmel to give combinatorial interpretations for the two different types
$q$-analogues of the Fibo-Stirling numbers which arise as the connection
coefficients between the two different $q$-analogues of the Fibonacci falling
and rising factorial bases. \end{abstract}
",0,0,1,0,0,0
6423,Cosmological searches for a non-cold dark matter component,"  We explore an extended cosmological scenario where the dark matter is an
admixture of cold and additional non-cold species. The mass and temperature of
the non-cold dark matter particles are extracted from a number of cosmological
measurements. Among others, we consider tomographic weak lensing data and Milky
Way dwarf satellite galaxy counts. We also study the potential of these
scenarios in alleviating the existing tensions between local measurements and
Cosmic Microwave Background (CMB) estimates of the $S_8$ parameter, with
$S_8=\sigma_8\sqrt{\Omega_m}$, and of the Hubble constant $H_0$. In principle,
a sub-dominant, non-cold dark matter particle with a mass $m_X\sim$~keV, could
achieve the goals above. However, the preferred ranges for its temperature and
its mass are different when extracted from weak lensing observations and from
Milky Way dwarf satellite galaxy counts, since these two measurements require
suppressions of the matter power spectrum at different scales. Therefore,
solving simultaneously the CMB-weak lensing tensions and the small scale crisis
in the standard cold dark matter picture via only one non-cold dark matter
component seems to be challenging.
",0,1,0,0,0,0
10761,Multipoint Radiation Induced Ignition of Dust Explosions: Turbulent Clustering of Particles and Increased Transparency,"  It is known that unconfined dust explosions consist of a relatively weak
primary (turbulent) deflagrations followed by a devastating secondary
explosion. The secondary explosion may propagate with a speed of up to 1000 m/s
producing overpressures of over 8-10 atm. Since detonation is the only
established theory that allows a rapid burning producing a high pressure that
can be sustained in open areas, the generally accepted view was that the
mechanism explaining the high rate of combustion in dust explosions is
deflagration to detonation transition. In the present work we propose a
theoretical substantiation of the alternative propagation mechanism explaining
origin of the secondary explosion producing the high speeds of combustion and
high overpressures in unconfined dust explosions. We show that clustering of
dust particles in a turbulent flow gives rise to a significant increase of the
thermal radiation absorption length ahead of the advancing flame front. This
effect ensures that clusters of dust particles are exposed to and heated by the
radiation from hot combustion products of large gaseous explosions sufficiently
long time to become multi-point ignition kernels in a large volume ahead of the
advancing flame front. The ignition times of fuel-air mixture by the
radiatively heated clusters of particles is considerably reduced compared to
the ignition time by the isolated particle. The radiation-induced multi-point
ignitions of a large volume of fuel-air ahead of the primary flame efficiently
increase the total flame area, giving rise to the secondary explosion, which
results in high rates of combustion and overpressures required to account for
the observed level of overpressures and damages in unconfined dust explosions,
such as e.g. the 2005 Buncefield explosion and several vapor cloud explosions
of severity similar to that of the Buncefield incident.
",0,1,0,0,0,0
8723,Domain Specific Semantic Validation of Schema.org Annotations,"  Since its unveiling in 2011, schema.org has become the de facto standard for
publishing semantically described structured data on the web, typically in the
form of web page annotations. The increasing adoption of schema.org facilitates
the growth of the web of data, as well as the development of automated agents
that operate on this data. Schema.org is a large heterogeneous vocabulary that
covers many domains. This is obviously not a bug, but a feature, since
schema.org aims to describe almost everything on the web, and the web is huge.
However, the heterogeneity of schema.org may cause a side effect, which is the
challenge of picking the right classes and properties for an annotation in a
certain domain, as well as keeping the annotation semantically consistent. In
this work, we introduce our rule based approach and an implementation of it for
validating schema.org annotations from two aspects: (a) the completeness of the
annotations in terms of a specified domain, (b) the semantic consistency of the
values based on pre-defined rules. We demonstrate our approach in the tourism
domain.
",1,0,0,0,0,0
9086,Spatial Random Sampling: A Structure-Preserving Data Sketching Tool,"  Random column sampling is not guaranteed to yield data sketches that preserve
the underlying structures of the data and may not sample sufficiently from
less-populated data clusters. Also, adaptive sampling can often provide
accurate low rank approximations, yet may fall short of producing descriptive
data sketches, especially when the cluster centers are linearly dependent.
Motivated by that, this paper introduces a novel randomized column sampling
tool dubbed Spatial Random Sampling (SRS), in which data points are sampled
based on their proximity to randomly sampled points on the unit sphere. The
most compelling feature of SRS is that the corresponding probability of
sampling from a given data cluster is proportional to the surface area the
cluster occupies on the unit sphere, independently from the size of the cluster
population. Although it is fully randomized, SRS is shown to provide
descriptive and balanced data representations. The proposed idea addresses a
pressing need in data science and holds potential to inspire many novel
approaches for analysis of big data.
",1,0,0,1,0,0
15123,Application of the Mixed Time-averaging Semiclassical Initial Value Representation method to Complex Molecular Spectra,"  The recently introduced mixed time-averaging semiclassical initial value
representation molecular dynamics method for spectroscopic calculations [M.
Buchholz, F. Grossmann, and M. Ceotto, J. Chem. Phys. 144, 094102 (2016)] is
applied to systems with up to 61 dimensions, ruled by a condensed phase
Caldeira-Leggett model potential. By calculating the ground state as well as
the first few excited states of the system Morse oscillator, changes of both
the harmonic frequency and the anharmonicity are determined. The method
faithfully reproduces blueshift and redshift effects and the importance of the
counter term, as previously suggested by other methods. Differently from
previous methods, the present semiclassical method does not take advantage of
the specific form of the potential and it can represent a practical tool that
opens the route to direct ab initio semiclassical simulation of condensed phase
systems.
",0,1,0,0,0,0
13817,Disentangling in Variational Autoencoders with Natural Clustering,"  Learning representations that disentangle the underlying factors of
variability in data is an intuitive precursor to AI with human-like reasoning.
Consequently, it has been the object of many efforts of the machine learning
community. This work takes a step further in this direction by addressing the
scenario where generative factors present a multimodal distribution due to the
existence of class distinction in the data. We formulate a lower bound on the
joint distribution of inputs and class labels and present N-VAE, a model which
is capable of separating factors of variation which are exclusive to certain
classes from factors that are shared among classes. This model implements the
natural clustering prior through the use of a class-conditioned latent space
and a shared latent space. We show its usefulness for detecting and
disentangling class-dependent generative factors as well as for generating rich
artificial samples.
",1,0,0,1,0,0
18993,A temperate exo-Earth around a quiet M dwarf at 3.4 parsecs,"  The combination of high-contrast imaging and high-dispersion spectroscopy,
which has successfully been used to detect the atmosphere of a giant planet, is
one of the most promising potential probes of the atmosphere of Earth-size
worlds. The forthcoming generation of extremely large telescopes (ELTs) may
obtain sufficient contrast with this technique to detect O$_2$ in the
atmosphere of those worlds that orbit low-mass M dwarfs. This is strong
motivation to carry out a census of planets around cool stars for which
habitable zones can be resolved by ELTs, i.e. for M dwarfs within $\sim$5
parsecs. Our HARPS survey has been a major contributor to that sample of nearby
planets. Here we report on our radial velocity observations of Ross 128
(Proxima Virginis, GJ447, HIP 57548), an M4 dwarf just 3.4 parsec away from our
Sun. This source hosts an exo-Earth with a projected mass $m \sin i = 1.35
M_\oplus$ and an orbital period of 9.9 days. Ross 128 b receives $\sim$1.38
times as much flux as Earth from the Sun and its equilibrium ranges in
temperature between 269 K for an Earth-like albedo and 213 K for a Venus-like
albedo. Recent studies place it close to the inner edge of the conventional
habitable zone. An 80-day long light curve from K2 campaign C01 demonstrates
that Ross~128~b does not transit. Together with the All Sky Automated Survey
(ASAS) photometry and spectroscopic activity indices, the K2 photometry shows
that Ross 128 rotates slowly and has weak magnetic activity. In a habitability
context, this makes survival of its atmosphere against erosion more likely.
Ross 128 b is the second closest known exo-Earth, after Proxima Centauri b (1.3
parsec), and the closest temperate planet known around a quiet star. The 15 mas
planet-star angular separation at maximum elongation will be resolved by ELTs
($>$ 3$\lambda/D$) in the optical bands of O$_2$.
",0,1,0,0,0,0
8731,Hybrid SGP4 orbit propagator,"  Two-Line Elements (TLEs) continue to be the sole public source of orbiter
observations. The accuracy of TLE propagations through the Simplified General
Perturbations-4 (SGP4) software decreases dramatically as the propagation
horizon increases, and thus the period of validity of TLEs is very limited. As
a result, TLEs are gradually becoming insufficient for the growing demands of
Space Situational Awareness (SSA). We propose a technique, based on the hybrid
propagation methodology, aimed at extending TLE validity with minimal changes
to the current TLE-SGP4 system in a non-intrusive way. It requires that the
institution in possession of the osculating elements distributes hybrid TLEs,
HTLEs, which encapsulate the standard TLE and the model of its propagation
error. The validity extension can be accomplished when the end user processes
HTLEs through the hybrid SGP4 propagator, HSGP4, which comprises the standard
SGP4 and an error corrector.
",0,1,0,0,0,0
1088,Multiple VLAD encoding of CNNs for image classification,"  Despite the effectiveness of convolutional neural networks (CNNs) especially
in image classification tasks, the effect of convolution features on learned
representations is still limited. It mostly focuses on the salient object of
the images, but ignores the variation information on clutter and local. In this
paper, we propose a special framework, which is the multiple VLAD encoding
method with the CNNs features for image classification. Furthermore, in order
to improve the performance of the VLAD coding method, we explore the
multiplicity of VLAD encoding with the extension of three kinds of encoding
algorithms, which are the VLAD-SA method, the VLAD-LSA and the VLAD-LLC method.
Finally, we equip the spatial pyramid patch (SPM) on VLAD encoding to add the
spatial information of CNNs feature. In particular, the power of SPM leads our
framework to yield better performance compared to the existing method.
",1,0,0,0,0,0
16066,Levels of distribution for sieve problems in prehomogeneous vector spaces,"  In a companion paper, we developed an efficient algebraic method for
computing the Fourier transforms of certain functions defined on prehomogeneous
vector spaces over finite fields, and we carried out these computations in a
variety of cases.
Here we develop a method, based on Fourier analysis and algebraic geometry,
which exploits these Fourier transform formulas to yield level of distribution
results, in the sense of analytic number theory. Such results are of the shape
typically required for a variety of sieve methods. As an example of such an
application we prove that there are $\gg$ X/log(X) quartic fields whose
discriminant is squarefree, bounded above by X, and has at most eight prime
factors.
",0,0,1,0,0,0
857,Multipath Error Correction in Radio Interferometric Positioning Systems,"  The radio interferometric positioning system (RIPS) is an accurate node
localization method featuring a novel phase-based ranging process. Multipath is
the limiting error source for RIPS in ground-deployed scenarios or indoor
applications. There are four distinct channels involved in the ranging process
for RIPS. Multipath reflections affect both the phase and amplitude of the
ranging signal for each channel. By exploiting untapped amplitude information,
we put forward a scheme to estimate each channel's multipath profile, which is
then subsequently used to correct corresponding errors in phase measurements.
Simulations show that such a scheme is very effective in reducing multipath
phase errors, which are essentially brought down to the level of receiver noise
under moderate multipath conditions. It is further demonstrated that ranging
errors in RIPS are also greatly reduced via the proposed scheme.
",1,0,0,0,0,0
4703,Early MFCC And HPCP Fusion for Robust Cover Song Identification,"  While most schemes for automatic cover song identification have focused on
note-based features such as HPCP and chord profiles, a few recent papers
surprisingly showed that local self-similarities of MFCC-based features also
have classification power for this task. Since MFCC and HPCP capture
complementary information, we design an unsupervised algorithm that combines
normalized, beat-synchronous blocks of these features using cross-similarity
fusion before attempting to locally align a pair of songs. As an added bonus,
our scheme naturally incorporates structural information in each song to fill
in alignment gaps where both feature sets fail. We show a striking jump in
performance over MFCC and HPCP alone, achieving a state of the art mean
reciprocal rank of 0.87 on the Covers80 dataset. We also introduce a new
medium-sized hand designed benchmark dataset called ""Covers 1000,"" which
consists of 395 cliques of cover songs for a total of 1000 songs, and we show
that our algorithm achieves an MRR of 0.9 on this dataset for the first
correctly identified song in a clique. We provide the precomputed HPCP and MFCC
features, as well as beat intervals, for all songs in the Covers 1000 dataset
for use in further research.
",1,0,0,0,0,0
19230,Two Categories of Indoor Interactive Dynamics of a Large-scale Human Population in a WiFi covered university campus,"  To explore large-scale population indoor interactions, we analyze 18,715
users' WiFi access logs recorded in a Chinese university campus during 3
months, and define two categories of human interactions, the event interaction
(EI) and the temporal interaction (TI). The EI helps construct a transmission
graph, and the TI helps build an interval graph. The dynamics of EIs show that
their active durations are truncated power-law distributed, which is
independent on the number of involved individuals. The transmission duration
presents a truncated power-law behavior at the daily timescale with weekly
periodicity. Besides, those `leaf' individuals in the aggregated contact
network may participate in the `super-connecting cliques' in the aggregated
transmission graph. Analyzing the dynamics of the interval graph, we find that
the probability distribution of TIs' inter-event duration also displays a
truncated power-law pattern at the daily timescale with weekly periodicity,
while the pairwise individuals with burst interactions are prone to randomly
select their interactive locations, and those individuals with periodic
interactions have preferred interactive locations.
",1,1,0,0,0,0
16068,Cluster-glass phase in pyrochlore XY antiferromagnets with quenched disorder,"  We study the impact of quenched disorder (random exchange couplings or site
dilution) on easy-plane pyrochlore antiferromagnets. In the clean system,
order-by-disorder selects a magnetically ordered state from a classically
degenerate manifold. In the presence of randomness, however, different orders
can be chosen locally depending on details of the disorder configuration. Using
a combination of analytical considerations and classical Monte-Carlo
simulations, we argue that any long-range-ordered magnetic state is destroyed
beyond a critical level of randomness where the system breaks into magnetic
domains due to random exchange anisotropies, becoming, therefore, a glass of
spin clusters, in accordance with the available experimental data. These random
anisotropies originate from off-diagonal exchange couplings in the microscopic
Hamiltonian, establishing their relevance to other magnets with strong
spin-orbit coupling.
",0,1,0,0,0,0
11920,Towards a Bootstrap approach to higher orders of epsilon expansion,"  We employ a hybrid approach in determining the anomalous dimension and OPE
coefficient of higher spin operators in the Wilson-Fisher theory. First we do a
large spin analysis for CFT data where we use results obtained from the usual
and the Mellin Bootstrap and also from Feynman diagram literature. This gives
new predictions at $O(\epsilon^4)$ and $O(\epsilon^5)$ for anomalous dimensions
and OPE coefficients, and also provides a cross-check for the results from
Mellin Bootstrap. These higher orders get contributions from all higher spin
operators in the crossed channel. We also use the Bootstrap in Mellin space
method for $\phi^3$ in $d=6-\epsilon$ CFT where we calculate general higher
spin OPE data. We demonstrate a higher loop order calculation in this approach
by summing over contributions from higher spin operators of the crossed channel
in the same spirit as before.
",0,1,0,0,0,0
11579,$N$-soliton formula and blowup result of the Wadati-Konno-Ichikawa equation,"  We formulate the $N$ soliton solution of the Wadati-Konno-Ichikawa equation
that is determined by purely algebraic equations. Derivation is based on the
matrix Riemann-Hilbert problem. We give examples of one soliton solution that
include smooth soliton, bursting soliton, and loop type soliton. In addition,
we give an explicit example for two soliton solution that blows up in a finite
time.
",0,1,1,0,0,0
11683,"Regularly Varying Functions, Generalized contents, and the spectrum of fractal strings","  We revisit the problem of characterizing the eigenvalue distribution of the
Dirichlet-Laplacian on bounded open sets $\Omega\subset\mathbb{R}$ with fractal
boundaries. It is well-known from the results of Lapidus and Pomerance
\cite{LapPo1} that the asymptotic second term of the eigenvalue counting
function can be described in terms of the Minkowski content of the boundary of
$\Omega$ provided it exists. He and Lapidus \cite{HeLap2} discussed a
remarkable extension of this characterization to sets $\Omega$ with boundaries
that are not necessarily Minkowski measurable. They employed so-called
generalized Minkowski contents given in terms of gauge functions more general
than the usual power functions. The class of valid gauge functions in their
theory is characterized by some technical conditions, the geometric meaning and
necessity of which is not obvious. Therefore, it is not completely clear how
general the approach is and which sets $\Omega$ are covered. Here we revisit
these results and put them in the context of regularly varying functions. Using
Karamata theory, it is possible to get rid of most of the technical conditions
and simplify the proofs given by He and Lapidus, revealing thus even more of
the beauty of their results. Further simplifications arise from
characterization results for Minkowski contents obtained in \cite{RW13}. We
hope our new point of view on these spectral problems will initiate some
further investigations of this beautiful theory.
",0,0,1,0,0,0
3394,Clebsch-Gordan Nets: a Fully Fourier Space Spherical Convolutional Neural Network,"  Recent work by Cohen \emph{et al.} has achieved state-of-the-art results for
learning spherical images in a rotation invariant way by using ideas from group
representation theory and noncommutative harmonic analysis. In this paper we
propose a generalization of this work that generally exhibits improved
performace, but from an implementation point of view is actually simpler. An
unusual feature of the proposed architecture is that it uses the
Clebsch--Gordan transform as its only source of nonlinearity, thus avoiding
repeated forward and backward Fourier transforms. The underlying ideas of the
paper generalize to constructing neural networks that are invariant to the
action of other compact groups.
",0,0,0,1,0,0
14106,A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks,"  We present a generalization bound for feedforward neural networks in terms of
the product of the spectral norm of the layers and the Frobenius norm of the
weights. The generalization bound is derived using a PAC-Bayes analysis.
",1,0,0,0,0,0
6321,A high resolution ion microscope for cold atoms,"  We report on an ion-optical system that serves as a microscope for ultracold
ground state and Rydberg atoms. The system is designed to achieve a
magnification of up to 1000 and a spatial resolution in the 100 nm range,
thereby surpassing many standard imaging techniques for cold atoms. The
microscope consists of four electrostatic lenses and a microchannel plate in
conjunction with a delay line detector in order to achieve single particle
sensitivity with high temporal and spatial resolution. We describe the design
process of the microscope including ion-optical simulations of the imaging
system and characterize aberrations and the resolution limit. Furthermore, we
present the experimental realization of the microscope in a cold atom setup and
investigate its performance by patterned ionization with a structure size down
to 2.7 {\mu}m. The microscope meets the requirements for studying various
many-body effects, ranging from correlations in cold quantum gases up to
Rydberg molecule formation.
",0,1,0,0,0,0
6959,Capacitated Bounded Cardinality Hub Routing Problem: Model and Solution Algorithm,"  In this paper, we address the Bounded Cardinality Hub Location Routing with
Route Capacity wherein each hub acts as a transshipment node for one directed
route. The number of hubs lies between a minimum and a maximum and the
hub-level network is a complete subgraph. The transshipment operations take
place at the hub nodes and flow transfer time from a hub-level transporter to a
spoke-level vehicle influences spoke- to-hub allocations. We propose a
mathematical model and a branch-and-cut algorithm based on Benders
decomposition to solve the problem. To accelerate convergence, our solution
framework embeds an efficient heuristic producing high-quality solutions in
short computation times. In addition, we show how symmetry can be exploited to
accelerate and improve the performance of our method.
",0,0,1,0,0,0
11779,Evaluation of Direct Haptic 4D Volume Rendering of Partially Segmented Data for Liver Puncture Simulation,"  This work presents an evaluation study using a force feedback evaluation
framework for a novel direct needle force volume rendering concept in the
context of liver puncture simulation. PTC/PTCD puncture interventions targeting
the bile ducts have been selected to illustrate this concept. The haptic
algorithms of the simulator system are based on (1) partially segmented patient
image data and (2) a non-linear spring model effective at organ borders. The
primary aim is to quantitatively evaluate force errors caused by our patient
modeling approach, in comparison to haptic force output obtained from using
gold-standard, completely manually-segmented data. The evaluation of the force
algorithms compared to a force output from fully manually segmented
gold-standard patient models, yields a low mean of 0.12 N root mean squared
force error and up to 1.6 N for systematic maximum absolute errors. Force
errors were evaluated on 31,222 preplanned test paths from 10 patients. Only
twelve percent of the emitted forces along these paths were affected by errors.
This is the first study evaluating haptic algorithms with deformable virtual
patients in silico. We prove haptic rendering plausibility on a very high
number of test paths. Important errors are below just noticeable differences
for the hand-arm system.
",1,1,0,0,0,0
17196,Doubly-Attentive Decoder for Multi-modal Neural Machine Translation,"  We introduce a Multi-modal Neural Machine Translation model in which a
doubly-attentive decoder naturally incorporates spatial visual features
obtained using pre-trained convolutional neural networks, bridging the gap
between image description and translation. Our decoder learns to attend to
source-language words and parts of an image independently by means of two
separate attention mechanisms as it generates words in the target language. We
find that our model can efficiently exploit not just back-translated in-domain
multi-modal data but also large general-domain text-only MT corpora. We also
report state-of-the-art results on the Multi30k data set.
",1,0,0,0,0,0
16374,Can MPTCP Secure Internet Communications from Man-in-the-Middle Attacks?,"  -Multipath communications at the Internet scale have been a myth for a long
time, with no actual protocol being deployed so that multiple paths could be
taken by a same connection on the way towards an Internet destination.
Recently, the Multipath Transport Control Protocol (MPTCP) extension was
standardized and is undergoing a quick adoption in many use-cases, from mobile
to fixed access networks, from data-centers to core networks. Among its major
benefits -- i.e., reliability thanks to backup path rerouting; throughput
increase thanks to link aggregation; and confidentiality thanks to harder
capacity to intercept a full connection -- the latter has attracted lower
attention. How interesting would it be using MPTCP to exploit multiple
Internet-scale paths hence decreasing the probability of man-in-the-middle
(MITM) attacks is a question to which we try to answer. By analyzing the
Autonomous System (AS) level graph, we identify which countries and regions
show a higher level of robustness against MITM AS-level attacks, for example
due to core cable tapping or route hijacking practices.
",1,0,0,0,0,0
8171,The Unheralded Value of the Multiway Rendezvous: Illustration with the Production Cell Benchmark,"  The multiway rendezvous introduced in Theoretical CSP is a powerful paradigm
to achieve synchronization and communication among a group of (possibly more
than two) processes. We illustrate the advantages of this paradigm on the
production cell benchmark, a model of a real metal processing plant, for which
we propose a compositional software controller, which is written in LNT and
LOTOS, and makes intensive use of the multiway rendezvous.
",1,0,0,0,0,0
16398,Accurate Single Stage Detector Using Recurrent Rolling Convolution,"  Most of the recent successful methods in accurate object detection and
localization used some variants of R-CNN style two stage Convolutional Neural
Networks (CNN) where plausible regions were proposed in the first stage then
followed by a second stage for decision refinement. Despite the simplicity of
training and the efficiency in deployment, the single stage detection methods
have not been as competitive when evaluated in benchmarks consider mAP for high
IoU thresholds. In this paper, we proposed a novel single stage end-to-end
trainable object detection network to overcome this limitation. We achieved
this by introducing Recurrent Rolling Convolution (RRC) architecture over
multi-scale feature maps to construct object classifiers and bounding box
regressors which are ""deep in context"". We evaluated our method in the
challenging KITTI dataset which measures methods under IoU threshold of 0.7. We
showed that with RRC, a single reduced VGG-16 based model already significantly
outperformed all the previously published results. At the time this paper was
written our models ranked the first in KITTI car detection (the hard level),
the first in cyclist detection and the second in pedestrian detection. These
results were not reached by the previous single stage methods. The code is
publicly available.
",1,0,0,0,0,0
15937,On the Relation of External and Internal Feature Interactions: A Case Study,"  Detecting feature interactions is imperative for accurately predicting
performance of highly-configurable systems. State-of-the-art performance
prediction techniques rely on supervised machine learning for detecting feature
interactions, which, in turn, relies on time consuming performance measurements
to obtain training data. By providing information about potentially interacting
features, we can reduce the number of required performance measurements and
make the overall performance prediction process more time efficient. We expect
that the information about potentially interacting features can be obtained by
statically analyzing the source code of a highly-configurable system, which is
computationally cheaper than performing multiple performance measurements. To
this end, we conducted a qualitative case study in which we explored the
relation between control-flow feature interactions (detected through static
program analysis) and performance feature interactions (detected by performance
prediction techniques using performance measurements). We found that a relation
exists, which can potentially be exploited to predict performance interactions.
",1,0,0,0,0,0
7528,Active Inductive Logic Programming for Code Search,"  Modern search techniques either cannot efficiently incorporate human feedback
to refine search results or to express structural or semantic properties of
desired code. The key insight of our interactive code search technique ALICE is
that user feedback could be actively incorporated to allow users to easily
express and refine search queries. We design a query language to model the
structure and semantics of code as logic facts. Given a code example with user
annotations, ALICE automatically extracts a logic query from features that are
tagged as important. Users can refine the search query by labeling one or more
examples as desired (positive) or irrelevant (negative). ALICE then infers a
new logic query that separates the positives from negative examples via active
inductive logic programming. Our comprehensive and systematic simulation
experiment shows that ALICE removes a large number of false positives quickly
by actively incorporating user feedback. Its search algorithm is also robust to
noise and user labeling mistakes. Our choice of leveraging both positive and
negative examples and the nested containment structure of selected code is
effective in refining search queries. Compared with an existing technique,
Critics, ALICE does not require a user to manually construct a search pattern
and yet achieves comparable precision and recall with fewer search iterations
on average. A case study with users shows that ALICE is easy to use and helps
express complex code patterns.
",1,0,0,0,0,0
13475,Inferring network connectivity from event timing patterns,"  Reconstructing network connectivity from the collective dynamics of a system
typically requires access to its complete continuous-time evolution although
these are often experimentally inaccessible. Here we propose a theory for
revealing physical connectivity of networked systems only from the event time
series their intrinsic collective dynamics generate. Representing the patterns
of event timings in an event space spanned by inter-event and cross-event
intervals, we reveal which other units directly influence the inter-event times
of any given unit. For illustration, we linearize an event space mapping
constructed from the spiking patterns in model neural circuits to reveal the
presence or absence of synapses between any pair of neurons as well as whether
the coupling acts in an inhibiting or activating (excitatory) manner. The
proposed model-independent reconstruction theory is scalable to larger networks
and may thus play an important role in the reconstruction of networks from
biology to social science and engineering.
",0,0,0,1,1,0
19108,On the martingale property in the rough Bergomi model,"  We consider a class of fractional stochastic volatility models (including the
so-called rough Bergomi model), where the volatility is a superlinear function
of a fractional Gaussian process. We show that the stock price is a true
martingale if and only if the correlation $\rho$ between the driving Brownian
motions of the stock and the volatility is nonpositive. We also show that for
each $\rho<0$ and $m> \frac{1}{1-\rho^2}$, the $m$-th moment of the stock
price is infinite at each positive time.
",0,0,0,0,0,1
15509,Multi-Objective Learning and Mask-Based Post-Processing for Deep Neural Network Based Speech Enhancement,"  We propose a multi-objective framework to learn both secondary targets not
directly related to the intended task of speech enhancement (SE) and the
primary target of the clean log-power spectra (LPS) features to be used
directly for constructing the enhanced speech signals. In deep neural network
(DNN) based SE we introduce an auxiliary structure to learn secondary
continuous features, such as mel-frequency cepstral coefficients (MFCCs), and
categorical information, such as the ideal binary mask (IBM), and integrate it
into the original DNN architecture for joint optimization of all the
parameters. This joint estimation scheme imposes additional constraints not
available in the direct prediction of LPS, and potentially improves the
learning of the primary target. Furthermore, the learned secondary information
as a byproduct can be used for other purposes, e.g., the IBM-based
post-processing in this work. A series of experiments show that joint LPS and
MFCC learning improves the SE performance, and IBM-based post-processing
further enhances listening quality of the reconstructed speech.
",1,0,0,0,0,0
8764,Fast evaluation of solid harmonic Gaussian integrals for local resolution-of-the-identity methods and range-separated hybrid functionals,"  An integral scheme for the efficient evaluation of two-center integrals over
contracted solid harmonic Gaussian functions is presented. Integral expressions
are derived for local operators that depend on the position vector of one of
the two Gaussian centers. These expressions are then used to derive the formula
for three-index overlap integrals where two of the three Gaussians are located
at the same center. The efficient evaluation of the latter is essential for
local resolution-of-the-identity techniques that employ an overlap metric. We
compare the performance of our integral scheme to the widely used Cartesian
Gaussian-based method of Obara and Saika (OS). Non-local interaction potentials
such as standard Coulomb, modified Coulomb and Gaussian-type operators, that
occur in range-separated hybrid functionals, are also included in the
performance tests. The speed-up with respect to the OS scheme is up to three
orders of magnitude for both, integrals and their derivatives. In particular,
our method is increasingly efficient for large angular momenta and highly
contracted basis sets.
",0,1,0,0,0,0
3406,Signal coupling to embedded pitch adapters in silicon sensors,"  We have examined the effects of embedded pitch adapters on signal formation
in n-substrate silicon microstrip sensors with data from beam tests and
simulation. According to simulation, the presence of the pitch adapter metal
layer changes the electric field inside the sensor, resulting in slowed signal
formation on the nearby strips and a pick-up effect on the pitch adapter. This
can result in an inefficiency to detect particles passing through the pitch
adapter region. All these effects have been observed in the beam test data.
",0,1,0,0,0,0
3818,Compressive Sensing Approaches for Autonomous Object Detection in Video Sequences,"  Video analytics requires operating with large amounts of data. Compressive
sensing allows to reduce the number of measurements required to represent the
video using the prior knowledge of sparsity of the original signal, but it
imposes certain conditions on the design matrix. The Bayesian compressive
sensing approach relaxes the limitations of the conventional approach using the
probabilistic reasoning and allows to include different prior knowledge about
the signal structure. This paper presents two Bayesian compressive sensing
methods for autonomous object detection in a video sequence from a static
camera. Their performance is compared on the real datasets with the
non-Bayesian greedy algorithm. It is shown that the Bayesian methods can
provide the same accuracy as the greedy algorithm but much faster; or if the
computational time is not critical they can provide more accurate results.
",1,0,0,1,0,0
1183,Mining Illegal Insider Trading of Stocks: A Proactive Approach,"  Illegal insider trading of stocks is based on releasing non-public
information (e.g., new product launch, quarterly financial report, acquisition
or merger plan) before the information is made public. Detecting illegal
insider trading is difficult due to the complex, nonlinear, and non-stationary
nature of the stock market. In this work, we present an approach that detects
and predicts illegal insider trading proactively from large heterogeneous
sources of structured and unstructured data using a deep-learning based
approach combined with discrete signal processing on the time series data. In
addition, we use a tree-based approach that visualizes events and actions to
aid analysts in their understanding of large amounts of unstructured data.
Using existing data, we have discovered that our approach has a good success
rate in detecting illegal insider trading patterns.
",0,0,0,1,0,1
2054,Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks,"  Deep neural networks are commonly developed and trained in 32-bit floating
point format. Significant gains in performance and energy efficiency could be
realized by training and inference in numerical formats optimized for deep
learning. Despite advances in limited precision inference in recent years,
training of neural networks in low bit-width remains a challenging problem.
Here we present the Flexpoint data format, aiming at a complete replacement of
32-bit floating point format training and inference, designed to support modern
deep network topologies without modifications. Flexpoint tensors have a shared
exponent that is dynamically adjusted to minimize overflows and maximize
available dynamic range. We validate Flexpoint by training AlexNet, a deep
residual network and a generative adversarial network, using a simulator
implemented with the neon deep learning framework. We demonstrate that 16-bit
Flexpoint closely matches 32-bit floating point in training all three models,
without any need for tuning of model hyperparameters. Our results suggest
Flexpoint as a promising numerical format for future hardware for training and
inference.
",1,0,0,1,0,0
14793,Orbit classification in the Hill problem: I. The classical case,"  The case of the classical Hill problem is numerically investigated by
performing a thorough and systematic classification of the initial conditions
of the orbits. More precisely, the initial conditions of the orbits are
classified into four categories: (i) non-escaping regular orbits; (ii) trapped
chaotic orbits; (iii) escaping orbits; and (iv) collision orbits. In order to
obtain a more general and complete view of the orbital structure of the
dynamical system our exploration takes place in both planar (2D) and the
spatial (3D) version of the Hill problem. For the 2D system we numerically
integrate large sets of initial conditions in several types of planes, while
for the system with three degrees of freedom, three-dimensional distributions
of initial conditions of orbits are examined. For distinguishing between
ordered and chaotic bounded motion the Smaller ALingment Index (SALI) method is
used. We managed to locate the several bounded basins, as well as the basins of
escape and collision and also to relate them with the corresponding escape and
collision time of the orbits. Our numerical calculations indicate that the
overall orbital dynamics of the Hamiltonian system is a complicated but highly
interested problem. We hope our contribution to be useful for a further
understanding of the orbital properties of the classical Hill problem.
",0,1,0,0,0,0
16741,How Sensitive are Sensitivity-Based Explanations?,"  We propose a simple objective evaluation measure for explanations of a
complex black-box machine learning model. While most such model explanations
have largely been evaluated via qualitative measures, such as how humans might
qualitatively perceive the explanations, it is vital to also consider objective
measures such as the one we propose in this paper. Our evaluation measure that
we naturally call sensitivity is simple: it characterizes how an explanation
changes as we vary the test input, and depending on how we measure these
changes, and how we vary the input, we arrive at different notions of
sensitivity. We also provide a calculus for deriving sensitivity of complex
explanations in terms of that for simpler explanations, which thus allows an
easy computation of sensitivities for yet to be proposed explanations. One
advantage of an objective evaluation measure is that we can optimize the
explanation with respect to the measure: we show that (1) any given explanation
can be simply modified to improve its sensitivity with just a modest deviation
from the original explanation, and (2) gradient based explanations of an
adversarially trained network are less sensitive. Perhaps surprisingly, our
experiments show that explanations optimized to have lower sensitivity can be
more faithful to the model predictions.
",1,0,0,1,0,0
5217,Practical Processing of Mobile Sensor Data for Continual Deep Learning Predictions,"  We present a practical approach for processing mobile sensor time series data
for continual deep learning predictions. The approach comprises data cleaning,
normalization, capping, time-based compression, and finally classification with
a recurrent neural network. We demonstrate the effectiveness of the approach in
a case study with 279 participants. On the basis of sparse sensor events, the
network continually predicts whether the participants would attend to a
notification within 10 minutes. Compared to a random baseline, the classifier
achieves a 40% performance increase (AUC of 0.702) on a withheld test set. This
approach allows to forgo resource-intensive, domain-specific, error-prone
feature engineering, which may drastically increase the applicability of
machine learning to mobile phone sensor data.
",1,0,0,0,0,0
16267,Entombed: An archaeological examination of an Atari 2600 game,"  The act and experience of programming is, at its heart, a fundamentally human
activity that results in the production of artifacts. When considering
programming, therefore, it would be a glaring omission to not involve people
who specialize in studying artifacts and the human activity that yields them:
archaeologists. Here we consider this with respect to computer games, the focus
of archaeology's nascent subarea of archaeogaming.
One type of archaeogaming research is digital excavation, a technical
examination of the code and techniques used in old games' implementation. We
apply that in a case study of Entombed, an Atari 2600 game released in 1982 by
US Games. The player in this game is, appropriately, an archaeologist who must
make their way through a zombie-infested maze. Maze generation is a fruitful
area for comparative retrogame archaeology, because a number of early games on
different platforms featured mazes, and their variety of approaches can be
compared. The maze in Entombed is particularly interesting: it is shaped in
part by the extensive real-time constraints of the Atari 2600 platform, and
also had to be generated efficiently and use next to no memory. We reverse
engineered key areas of the game's code to uncover its unusual maze-generation
algorithm, which we have also built a reconstruction of, and analyzed the
mysterious table that drives it. In addition, we discovered what appears to be
a 35-year-old bug in the code, as well as direct evidence of code-reuse
practices amongst game developers.
What further makes this game's development interesting is that, in an era
where video games were typically solo projects, a total of five people were
involved in various ways with Entombed. We piece together some of the backstory
of the game's development and intoxicant-fueled design using interviews to
complement our technical work.
Finally, we contextualize this example in archaeology and lay the groundwork
for a broader interdisciplinary discussion about programming, one that includes
both computer scientists and archaeologists.
",1,0,0,0,0,0
16455,Ultra-Wideband Aided Fast Localization and Mapping System,"  This paper proposes an ultra-wideband (UWB) aided localization and mapping
system that leverages on inertial sensor and depth camera. Inspired by the fact
that visual odometry (VO) system, regardless of its accuracy in the short term,
still faces challenges with accumulated errors in the long run or under
unfavourable environments, the UWB ranging measurements are fused to remove the
visual drift and improve the robustness. A general framework is developed which
consists of three parallel threads, two of which carry out the visual-inertial
odometry (VIO) and UWB localization respectively. The other mapping thread
integrates visual tracking constraints into a pose graph with the proposed
smooth and virtual range constraints, such that an optimization is performed to
provide robust trajectory estimation. Experiments show that the proposed system
is able to create dense drift-free maps in real-time even running on an
ultra-low power processor in featureless environments.
",1,0,0,0,0,0
7328,The Dynamical History of Chariklo and its Rings,"  Chariklo is the only small Solar system body confirmed to have rings. Given
the instability of its orbit, the presence of rings is surprising, and their
origin remains poorly understood. In this work, we study the dynamical history
of the Chariklo system by integrating almost 36,000 Chariklo clones backwards
in time for one Gyr under the influence of the Sun and the four giant planets.
By recording all close encounters between the clones and planets, we
investigate the likelihood that Chariklo's rings could have survived since its
capture to the Centaur population. Our results reveal that Chariklo's orbit
occupies a region of stable chaos, resulting in its orbit being marginally more
stable than those of the other Centaurs. Despite this, we find that it was most
likely captured to the Centaur population within the last 20 Myr, and that its
orbital evolution has been continually punctuated by regular close encounters
with the giant planets. The great majority (> 99%) of those encounters within
one Hill radius of the planet have only a small effect on the rings. We
conclude that close encounters with giant planets have not had a significant
effect on the ring structure. Encounters within the Roche limit of the giant
planets are rare, making ring creation through tidal disruption unlikely.
",0,1,0,0,0,0
9982,MPC meets SNA: A Privacy Preserving Analysis of Distributed Sensitive Social Networks,"  In this paper, we formalize the notion of distributed sensitive social
networks (DSSNs), which encompasses networks like enmity networks, financial
transaction networks, supply chain networks and sexual relationship networks.
Compared to the well studied traditional social networks, DSSNs are often more
challenging to study, given the privacy concerns of the individuals on whom the
network is knit. In the current work, we envision the use of secure multiparty
tools and techniques for performing privacy preserving social network analysis
over DSSNs. As a step towards realizing this, we design efficient
data-oblivious algorithms for computing the K-shell decomposition and the
PageRank centrality measure for a given DSSN. The designed data-oblivious
algorithms can be translated into equivalent secure computation protocols. We
also list a string of challenges that are needed to be addressed, for employing
secure computation protocols as a practical solution for studying DSSNs.
",1,0,0,0,0,0
7949,Piecewise Deterministic Markov Processes and their invariant measure,"  Piecewise Deterministic Markov Processes (PDMPs) are studied in a general
framework. First, different constructions are proven to be equivalent. Second,
we introduce a coupling between two PDMPs following the same differential flow
which implies quantitative bounds on the total variation between the marginal
distributions of the two processes. Finally two results are established
regarding the invariant measures of PDMPs. A practical condition to show that a
probability measure is invariant for the associated PDMP semi-group is
presented. In a second time, a bound on the invariant probability measures in
$V$-norm of two PDMPs following the same differential flow is established. This
last result is then applied to study the asymptotic bias of some non-exact PDMP
MCMC methods.
",0,0,0,1,0,0
9926,Automatic classification of trees using a UAV onboard camera and deep learning,"  Automatic classification of trees using remotely sensed data has been a dream
of many scientists and land use managers. Recently, Unmanned aerial vehicles
(UAV) has been expected to be an easy-to-use, cost-effective tool for remote
sensing of forests, and deep learning has attracted attention for its ability
concerning machine vision. In this study, using a commercially available UAV
and a publicly available package for deep learning, we constructed a machine
vision system for the automatic classification of trees. In our method, we
segmented a UAV photography image of forest into individual tree crowns and
carried out object-based deep learning. As a result, the system was able to
classify 7 tree types at 89.0% accuracy. This performance is notable because we
only used basic RGB images from a standard UAV. In contrast, most of previous
studies used expensive hardware such as multispectral imagers to improve the
performance. This result means that our method has the potential to classify
individual trees in a cost-effective manner. This can be a usable tool for many
forest researchers and managements.
",0,0,0,1,0,0
15081,"Inverse scattering transform for the nonlocal reverse space-time Sine-Gordon, Sinh-Gordon and nonlinear Schrödinger equations with nonzero boundary conditions","  The reverse space-time (RST) Sine-Gordon, Sinh-Gordon and nonlinear
Schrödinger equations were recently introduced and shown to be integrable
infinite-dimensional dynamical systems. The inverse scattering transform (IST)
for rapidly decaying data was also constructed. In this paper, IST for these
equations with nonzero boundary conditions (NZBCs) at infinity is presented.
The NZBC problem is more complicated due to the associated branching structure
of the associated linear eigenfunctions. With constant amplitude at infinity,
four cases are analyzed; they correspond to two different signs of nonlinearity
and two different values of the phase at infinity. Special soliton solutions
are discussed and explicit 1-soliton and 2-soliton solutions are found. In
terms of IST, the difference between the RST Sine-Gordon/Sinh-Gordon equations
and the RST NLS equation is the time dependence of the scattering data.
Spatially dependent boundary conditions are also briefly considered.
",0,1,0,0,0,0
14078,Identifying Harm Events in Clinical Care through Medical Narratives,"  Preventable medical errors are estimated to be among the leading causes of
injury and death in the United States. To prevent such errors, healthcare
systems have implemented patient safety and incident reporting systems. These
systems enable clinicians to report unsafe conditions and cases where patients
have been harmed due to errors in medical care. These reports are narratives in
natural language and while they provide detailed information about the
situation, it is non-trivial to perform large scale analysis for identifying
common causes of errors and harm to the patients. In this work, we present a
method based on attentive convolutional and recurrent networks for identifying
harm events in patient care and categorize the harm based on its severity
level. We demonstrate that our methods can significantly improve the
performance over existing methods in identifying harm in clinical care.
",1,0,0,0,0,0
17121,Recovery guarantees for compressed sensing with unknown errors,"  From a numerical analysis perspective, assessing the robustness of
l1-minimization is a fundamental issue in compressed sensing and sparse
regularization. Yet, the recovery guarantees available in the literature
usually depend on a priori estimates of the noise, which can be very hard to
obtain in practice, especially when the noise term also includes unknown
discrepancies between the finite model and data. In this work, we study the
performance of l1-minimization when these estimates are not available,
providing robust recovery guarantees for quadratically constrained basis
pursuit and random sampling in bounded orthonormal systems. Several
applications of this work are approximation of high-dimensional functions,
infinite-dimensional sparse regularization for inverse problems, and fast
algorithms for non-Cartesian Magnetic Resonance Imaging.
",0,0,1,0,0,0
813,Fulde-Ferrell-Larkin-Ovchinnikov state in spin-orbit-coupled superconductors,"  We show that in the presence of magnetic field, two superconducting phases
with the center-of-mass momentum of Cooper pair parallel to the magnetic field
are induced in spin-orbit-coupled superconductor Li$_2$Pd$_3$B. Specifically,
at small magnetic field, the center-of-mass momentum is induced due to the
energy-spectrum distortion and no unpairing region with vanishing singlet
correlation appears. We refer to this superconducting state as the drift-BCS
state. By further increasing the magnetic field, the superconducting state
falls into the Fulde-Ferrell-Larkin-Ovchinnikov state with the emergence of the
unpairing regions. The observed abrupt enhancement of the center-of-mass
momenta and suppression on the order parameters during the crossover indicate
the first-order phase transition. Enhanced Pauli limit and hence enlarged
magnetic-field regime of the Fulde-Ferrell-Larkin-Ovchinnikov state, due to the
spin-flip terms of the spin-orbit coupling, are revealed. We also address the
triplet correlations induced by the spin-orbit coupling, and show that the
Cooper-pair spin polarizations, generated by the magnetic field and
center-of-mass momentum with the triplet correlations, exhibit totally
different magnetic-field dependences between the drift-BCS and
Fulde-Ferrell-Larkin-Ovchinnikov states.
",0,1,0,0,0,0
9273,Matrix Completion Methods for Causal Panel Data Models,"  In this paper we study methods for estimating causal effects in settings with
panel data, where a subset of units are exposed to a treatment during a subset
of periods, and the goal is estimating counterfactual (untreated) outcomes for
the treated unit/period combinations. We develop a class of matrix completion
estimators that uses the observed elements of the matrix of control outcomes
corresponding to untreated unit/periods to predict the ""missing"" elements of
the matrix, corresponding to treated units/periods. The approach estimates a
matrix that well-approximates the original (incomplete) matrix, but has lower
complexity according to the nuclear norm for matrices. From a technical
perspective, we generalize results from the matrix completion literature by
allowing the patterns of missing data to have a time series dependency
structure. We also present novel insights concerning the connections between
the matrix completion literature, the literature on interactive fixed effects
models and the literatures on program evaluation under unconfoundedness and
synthetic control methods.
",0,0,1,0,0,0
564,SimProp v2r4: Monte Carlo simulation code for UHECR propagation,"  We introduce the new version of SimProp, a Monte Carlo code for simulating
the propagation of ultra-high energy cosmic rays in intergalactic space. This
version, SimProp v2r4, together with an overall improvement of the code
capabilities with a substantial reduction in the computation time, also
computes secondary cosmogenic particles such as electron-positron pairs and
gamma rays produced during the propagation of ultra-high energy cosmic rays. As
recently pointed out by several authors, the flux of this secondary radiation
and its products, within reach of the current observatories, provides useful
information about models of ultra-high energy cosmic ray sources which would be
hard to discriminate otherwise.
",0,1,0,0,0,0
6530,Simultaneous diagonalisation of the covariance and complementary covariance matrices in quaternion widely linear signal processing,"  Recent developments in quaternion-valued widely linear processing have
established that the exploitation of complete second-order statistics requires
consideration of both the standard covariance and the three complementary
covariance matrices. Although such matrices have a tremendous amount of
structure and their decomposition is a powerful tool in a variety of
applications, the non-commutative nature of the quaternion product has been
prohibitive to the development of quaternion uncorrelating transforms. To this
end, we introduce novel techniques for a simultaneous decomposition of the
covariance and complementary covariance matrices in the quaternion domain,
whereby the quaternion version of the Takagi factorisation is explored to
diagonalise symmetric quaternion-valued matrices. This gives new insights into
the quaternion uncorrelating transform (QUT) and forms a basis for the proposed
quaternion approximate uncorrelating transform (QAUT) which simultaneously
diagonalises all four covariance matrices associated with improper quaternion
signals. The effectiveness of the proposed uncorrelating transforms is
validated by simulations on both synthetic and real-world quaternion-valued
signals.
",1,0,0,0,0,0
18710,The area of the Mandelbrot set and Zagier's conjecture,"  We prove Zagier's conjecture regarding the 2-adic valuation of the
coefficients $\{b_m\}$ that appear in Ewing and Schober's series formula for
the area of the Mandelbrot set in the case where $m\equiv 2 \mod 4$.
",0,0,1,0,0,0
3316,Regulating Highly Automated Robot Ecologies: Insights from Three User Studies,"  Highly automated robot ecologies (HARE), or societies of independent
autonomous robots or agents, are rapidly becoming an important part of much of
the world's critical infrastructure. As with human societies, regulation,
wherein a governing body designs rules and processes for the society, plays an
important role in ensuring that HARE meet societal objectives. However, to
date, a careful study of interactions between a regulator and HARE is lacking.
In this paper, we report on three user studies which give insights into how to
design systems that allow people, acting as the regulatory authority, to
effectively interact with HARE. As in the study of political systems in which
governments regulate human societies, our studies analyze how interactions
between HARE and regulators are impacted by regulatory power and individual
(robot or agent) autonomy. Our results show that regulator power, decision
support, and adaptive autonomy can each diminish the social welfare of HARE,
and hint at how these seemingly desirable mechanisms can be designed so that
they become part of successful HARE.
",1,0,0,0,0,0
17877,Characterization of Lipschitz functions in terms of variable exponent Lebesgue spaces,"  Our aim is to characterize the Lipschitz functions by variable exponent
Lebesgue spaces. We give some characterizations of the boundedness of the
maximal or nonlinear commutators of the Hardy-Littlewood maximal function and
sharp maximal function in variable exponent Lebesgue spaces when the symbols
$b$ belong to the Lipschitz spaces, by which some new characterizations of
Lipschitz spaces and nonnegative Lipschitz functions are obtained. Some
equivalent relations between the Lipschitz norm and the variable exponent
Lebesgue norm are also given.
",0,0,1,0,0,0
16204,Non-Convex Rank/Sparsity Regularization and Local Minima,"  This paper considers the problem of recovering either a low rank matrix or a
sparse vector from observations of linear combinations of the vector or matrix
elements. Recent methods replace the non-convex regularization with $\ell_1$ or
nuclear norm relaxations. It is well known that this approach can be guaranteed
to recover a near optimal solutions if a so called restricted isometry property
(RIP) holds. On the other hand it is also known to perform soft thresholding
which results in a shrinking bias which can degrade the solution.
In this paper we study an alternative non-convex regularization term. This
formulation does not penalize elements that are larger than a certain threshold
making it much less prone to small solutions. Our main theoretical results show
that if a RIP holds then the stationary points are often well separated, in the
sense that their differences must be of high cardinality/rank. Thus, with a
suitable initial solution the approach is unlikely to fall into a bad local
minima. Our numerical tests show that the approach is likely to converge to a
better solution than standard $\ell_1$/nuclear-norm relaxation even when
starting from trivial initializations. In many cases our results can also be
used to verify global optimality of our method.
",0,0,1,0,0,0
12325,Kernel k-Groups via Hartigan's Method,"  Energy statistics was proposed by Székely in the 80's inspired by
Newton's gravitational potential in classical mechanics, and it provides a
model-free hypothesis test for equality of distributions. In its original form,
energy statistics was formulated in Euclidean spaces. More recently, it was
generalized to metric spaces of negative type. In this paper, we consider a
formulation for the clustering problem using a weighted version of energy
statistics in spaces of negative type. We show that this approach leads to a
quadratically constrained quadratic program in the associated kernel space,
establishing connections with graph partitioning problems and kernel methods in
unsupervised machine learning. To find local solutions of such an optimization
problem, we propose an extension of Hartigan's method to kernel spaces. Our
method has the same computational cost as kernel k-means algorithm, which is
based on Lloyd's heuristic, but our numerical results show an improved
performance, especially in high dimensions.
",1,0,1,1,0,0
3209,Bounds for the completely positive rank of a symmetric matrix over a tropical semiring,"  In this paper, we find an upper bound for the CP-rank of a matrix over a
tropical semiring, according to the vertex clique cover of the graph prescribed
by the pattern of the matrix. We study the graphs that beget the patterns of
matrices with the lowest possible CP-ranks and prove that any such graph must
have its diameter equal to 2.
",0,0,1,0,0,0
3703,Comparison moduli spaces of Riemann surfaces,"  We define a kind of moduli space of nested surfaces and mappings, which we
call a comparison moduli space. We review examples of such spaces in geometric
function theory and modern Teichmueller theory, and illustrate how a wide range
of phenomena in complex analysis are captured by this notion of moduli space.
The paper includes a list of open problems in classical and modern function
theory and Teichmueller theory ranging from general theoretical questions to
specific technical problems.
",0,0,1,0,0,0
12177,Generic Axiomatization of Families of Noncrossing Graphs in Dependency Parsing,"  We present a simple encoding for unlabeled noncrossing graphs and show how
its latent counterpart helps us to represent several families of directed and
undirected graphs used in syntactic and semantic parsing of natural language as
context-free languages. The families are separated purely on the basis of
forbidden patterns in latent encoding, eliminating the need to differentiate
the families of non-crossing graphs in inference algorithms: one algorithm
works for all when the search space can be controlled in parser input.
",1,0,0,0,0,0
16721,"Self-Trapping of G-Mode Oscillations in Relativistic Thin Disks, Revisited","  We examine by a perturbation method how the self-trapping of g-mode
oscillations in geometrically thin relativistic disks is affected by uniform
vertical magnetic fields. Disks which we consider are isothermal in the
vertical direction, but are truncated at a certain height by presence of hot
coronae. We find that the characteristics of self-trapping of axisymmetric
g-mode oscillations in non-magnetized disks is kept unchanged in magnetized
disks at least till a strength of the fields, depending on vertical thickness
of disks. These magnetic fields become stronger as the disk becomes thinner.
This result suggests that trapped g-mode oscillations still remain as one of
possible candidates of quasi-periodic oscillations observed in black-hole and
neutron-star X-ray binaries in the cases where vertical magnetic fields in
disks are weak.
",0,1,0,0,0,0
8355,A computational approach to calculate the heat of transport of aqueous solutions,"  Thermal gradients induce concentration gradients in alkali halide solutions,
and the salt migrates towards hot or cold regions depending on the average
temperature of the solution. This effect has been interpreted using the heat of
transport, which provides a route to rationalize thermophoretic phenomena.
Early theories provide estimates of the heat of transport at infinite dilution.
These values are used to interpret thermodiffusion (Soret) and thermoelectric
(Seebeck) effects. However, accessing heats of transport of individual ions at
finite concentration remains an outstanding question both theoretically and
experimentally. Here we discuss a computational approach to calculate heats of
transport of aqueous solutions at finite concentrations, and apply our method
to study lithium chloride solutions at concentrations $>0.5$~M. The heats of
transport are significantly different for Li$^+$ and Cl$^-$ ions, unlike what
is expected at infinite dilution. We find theoretical evidence for the
existence of minima in the Soret coefficient of LiCl, where the magnitude of
the heat of transport is maximized. The Seebeck coefficient obtained from the
ionic heats of transport varies significantly with temperature and
concentration. We identify thermodynamic conditions leading to a maximization
of the thermoelectric response of aqueous solutions.
",0,1,0,0,0,0
13308,A novel online scheduling protocol for energy-efficient TWDM-OLT design,"  Design of energy-efficient access networks has emerged as an important area
of research, since access networks consume $80-90\%$ of the overall Internet
power consumption. TWDM-PON is envisaged to be one of the widely accepted
future access technologies. TWDM-PON offers an additional opportunity to save
energy at the OLT along with the existing energy-efficient ONU design. In this
paper, we focus on the energy-efficient OLT design in a TWDM-PON. While most of
the conventional methods employ a minimization of the number of wavelengths, we
propose a novel approach which aims at minimizing the number of voids created
due to scheduling. In the process, for the first time, we present a
low-complexity on-line scheduling algorithm for the upstream traffic
considering delay constraints. Our extensive simulations demonstrate a
significant improvement in energy efficiency of $\sim 25\%$ for high load at
the OLT receivers. Furthermore, we provide an analytical upper-bound on the
energy-efficiency of the OLT receivers and demonstrate that the proposed
protocol achieves an energy efficiency very close to the bound with a maximum
deviation $\sim 2\%$ for $64$ ONUs.
",1,0,0,0,0,0
19662,A Constrained Shortest Path Scheme for Virtual Network Service Management,"  Virtual network services that span multiple data centers are important to
support emerging data-intensive applications in fields such as bioinformatics
and retail analytics. Successful virtual network service composition and
maintenance requires flexible and scalable 'constrained shortest path
management' both in the management plane for virtual network embedding (VNE) or
network function virtualization service chaining (NFV-SC), as well as in the
data plane for traffic engineering (TE). In this paper, we show analytically
and empirically that leveraging constrained shortest paths within recent VNE,
NFV-SC and TE algorithms can lead to network utilization gains (of up to 50%)
and higher energy efficiency. The management of complex VNE, NFV-SC and TE
algorithms can be, however, intractable for large scale substrate networks due
to the NP-hardness of the constrained shortest path problem. To address such
scalability challenges, we propose a novel, exact constrained shortest path
algorithm viz., 'Neighborhoods Method' (NM). Our NM uses novel search space
reduction techniques and has a theoretical quadratic speed-up making it
practically faster (by an order of magnitude) than recent branch-and-bound
exhaustive search solutions. Finally, we detail our NM-based SDN controller
implementation in a real-world testbed to further validate practical NM
benefits for virtual network services.
",1,0,0,0,0,0
14420,Quasi Maximum-Likelihood Estimation of Dynamic Panel Data Models,"  This paper establishes the almost sure convergence and asymptotic normality
of levels and differenced quasi maximum-likelihood (QML) estimators of dynamic
panel data models. The QML estimators are robust with respect to initial
conditions, conditional and time-series heteroskedasticity, and
misspecification of the log-likelihood. The paper also provides an ECME
algorithm for calculating levels QML estimates. Finally, it uses Monte Carlo
experiments to compare the finite sample performance of levels and differenced
QML estimators, the differenced GMM estimator, and the system GMM estimator. In
these experiments the QML estimators usually have smaller --- typically
substantially smaller --- bias and root mean squared errors than the panel data
GMM estimators.
",0,0,1,1,0,0
11819,Finite presheaves and $A$-finite generation of unstable algebras mod nilpotents,"  Inspired by the work of Henn, Lannes and Schwartz on unstable algebras over
the Steenrod algebra modulo nilpotents, a characterization of unstable algebras
that are $A$-finitely generated up to nilpotents is given in terms of the
associated presheaf, by introducing the notion of a finite presheaf. In
particular, this gives the natural characterization of the (co)analytic
presheaves that are important in the theory of Henn, Lannes and Schwartz.
However, finite presheaves remain imperfectly understood, as illustrated by
examples. One important class of examples is shown to be provided by unstable
algebras of finite transcendence degree (under a necessary weak finiteness
condition).
For unstable Hopf algebras, it is shown that the situation is much better:
the associated presheaf is finite if and only if its growth function is
polynomial. This leads to a description of unstable Hopf algebras modulo
nilpotents in the spirit of Henn, Lannes and Schwartz.
",0,0,1,0,0,0
4116,Observation of Spatio-temporal Instability of Femtosecond Pulses in Normal Dispersion Multimode Graded-Index Fiber,"  We study the spatio-temporal instability generated by a universal unstable
attractor in normal dispersion graded-index multimode fiber (GRIN MMF) for
femtosecond pulses. Our results present the generation of geometric parametric
instability (GPI) sidebands with ultrashort input pulse for the first time.
Observed GPI sidebands are 91 THz detuned from the pump wavelength, 800 nm.
Detailed analysis carried out numerically by employing coupled-mode pulse
propagation model including optical shock and Raman nonlinearity terms. A
simplified theoretical model and numerically calculated spectra are
well-aligned with experimental results. For input pulses of 200-fs duration,
formation and evolution of GPI are shown in both spatial and temporal domains.
The spatial intensity distribution of the total field and GPI sidebands are
calculated. Numerically and experimentally obtained beam shapes of first GPI
features a Gaussian-like beam profile. Our numerical results verify the unique
feature of GPI and generated sidebands preserve their inherited spatial
intensity profile from the input pulse for different propagation distances
particularly for focused and spread the total field inside the GRIN MMF.
",0,1,0,0,0,0
9049,Candidate Hα emission and absorption line sources in the Galactic Bulge Survey,"  We present a catalogue of candidate H{\alpha} emission and absorption line
sources and blue objects in the Galactic Bulge Survey (GBS) region. We use a
point source catalogue of the GBS fields (two strips of (l x b) = (6 x 1)
degrees centred at b = 1.5 above and below the Galactic centre), covering the
magnitude range 16 < r' < 22.5. We utilize (r'-i', r'-H{\alpha}) colour-colour
diagrams to select H{\alpha} emission and absorption line candidates, and also
identify blue objects (compared to field stars) using the r'-i' colour index.
We identify 1337 H{\alpha} emission line candidates and 336 H{\alpha}
absorption line candidates. These catalogues likely contain a plethora of
sources, ranging from active (binary) stars, early-type emission line objects,
cataclysmic variables (CVs) and low-mass X-ray binaries (LMXBs) to background
active galactic nuclei (AGN). The 389 blue objects we identify are likely
systems containing a compact object, such as CVs, planetary nebulae and LMXBs.
Hot subluminous dwarfs (sdO/B stars) are also expected to be found as blue
outliers. Crossmatching our outliers with the GBS X-ray catalogue yields
sixteen sources, including seven (magnetic) CVs and one qLMXB candidate among
the emission line candidates, and one background AGN for the absorption line
candidates. One of the blue outliers is a high state AM CVn system.
Spectroscopic observations combined with the multi-wavelength coverage of this
area, including X-ray, ultraviolet and (time-resolved) optical and infrared
observations, can be used to further constrain the nature of individual
sources.
",0,1,0,0,0,0
7525,Positive semi-definite embedding for dimensionality reduction and out-of-sample extensions,"  In machine learning or statistics, it is often desirable to reduce the
dimensionality of high dimensional data. We propose to obtain the low
dimensional embedding coordinates as the eigenvectors of a positive
semi-definite kernel matrix. This kernel matrix is the solution of a
semi-definite program promoting a low rank solution and defined with the help
of a diffusion kernel. Besides, we also discuss an infinite dimensional
analogue of the same semi-definite program. From a practical perspective, a
main feature of our approach is the existence of a non-linear out-of-sample
extension formula of the embedding coordinates that we call a projected
Nyström approximation. This extension formula yields an extension of the
kernel matrix to a data-dependent Mercer kernel function. Although the
semi-definite program may be solved directly, we propose another strategy based
on a rank constrained formulation solved thanks to a projected power method
algorithm followed by a singular value decomposition. This strategy allows for
a reduced computational time.
",1,0,0,1,0,0
7182,A Central Limit Theorem for Wasserstein type distances between two different laws,"  This article is dedicated to the estimation of Wasserstein distances and
Wasserstein costs between two distinct continuous distributions $F$ and $G$ on
$\mathbb R$. The estimator is based on the order statistics of (possibly
dependent) samples of $F$ resp. $G$. We prove the consistency and the
asymptotic normality of our estimators. \begin{it}Keywords:\end{it} Central
Limit Theorems- Generelized Wasserstein distances- Empirical processes- Strong
approximation- Dependent samples.
",0,0,1,1,0,0
18796,Temporal Stable Community in Time-Varying Networks,"  Identifying community structure of a complex network provides insight to the
interdependence between the network topology and emergent collective behaviors
of networks, while detecting such invariant communities in a time-varying
network is more challenging. In this paper, we define the temporal stable
community and newly propose the concept of dynamic modularity to evaluate the
stable community structures in time-varying networks, which is robust against
small changes as verified by several empirical time-varying network datasets.
Besides, using the volatility features of temporal stable communities in
functional brain networks, we successfully differentiate the ADHD (Attention
Deficit Hyperactivity Disorder) patients and healthy controls efficiently.
",0,0,0,0,1,0
15444,Development and evaluation of a deep learning model for protein-ligand binding affinity prediction,"  Structure based ligand discovery is one of the most successful approaches for
augmenting the drug discovery process. Currently, there is a notable shift
towards machine learning (ML) methodologies to aid such procedures. Deep
learning has recently gained considerable attention as it allows the model to
""learn"" to extract features that are relevant for the task at hand. We have
developed a novel deep neural network estimating the binding affinity of
ligand-receptor complexes. The complex is represented with a 3D grid, and the
model utilizes a 3D convolution to produce a feature map of this
representation, treating the atoms of both proteins and ligands in the same
manner. Our network was tested on the CASF ""scoring power"" benchmark and Astex
Diverse Set and outperformed classical scoring functions. The model, together
with usage instructions and examples, is available as a git repository at
this http URL
",1,0,0,1,0,0
10423,In silico evolution of signaling networks using rule-based models: bistable response dynamics,"  One of the ultimate goals in biology is to understand the design principles
of biological systems. Such principles, if they exist, can help us better
understand complex, natural biological systems and guide the engineering of de
novo ones. Towards deciphering design principles, in silico evolution of
biological systems with proper abstraction is a promising approach. Here, we
demonstrate the application of in silico evolution combined with rule-based
modelling for exploring design principles of cellular signaling networks. This
application is based on a computational platform, called BioJazz, which allows
in silico evolution of signaling networks with unbounded complexity. We provide
a detailed introduction to BioJazz architecture and implementation and describe
how it can be used to evolve and/or design signaling networks with defined
dynamics. For the latter, we evolve signaling networks with switch-like
response dynamics and demonstrate how BioJazz can result in new biological
insights on network structures that can endow bistable response dynamics. This
example also demonstrated both the power of BioJazz in evolving and designing
signaling networks and its limitations at the current stage of development.
",0,0,0,0,1,0
745,Artificial Intelligence Based Malware Analysis,"  Artificial intelligence methods have often been applied to perform specific
functions or tasks in the cyber-defense realm. However, as adversary methods
become more complex and difficult to divine, piecemeal efforts to understand
cyber-attacks, and malware-based attacks in particular, are not providing
sufficient means for malware analysts to understand the past, present and
future characteristics of malware.
In this paper, we present the Malware Analysis and Attributed using Genetic
Information (MAAGI) system. The underlying idea behind the MAAGI system is that
there are strong similarities between malware behavior and biological organism
behavior, and applying biologically inspired methods to corpora of malware can
help analysts better understand the ecosystem of malware attacks. Due to the
sophistication of the malware and the analysis, the MAAGI system relies heavily
on artificial intelligence techniques to provide this capability. It has
already yielded promising results over its development life, and will hopefully
inspire more integration between the artificial intelligence and cyber--defense
communities.
",1,0,0,0,0,0
9283,A new concept multi-stage Zeeman decelerator: experimental implementation,"  We demonstrate the successful experimental implementation of a multi-stage
Zeeman decelerator utilizing the new concept described in the accompanying
paper. The decelerator consists of an array of 25 hexapoles and 24 solenoids.
The performance of the decelerator in acceleration, deceleration and guiding
modes is characterized using beams of metastable Helium ($^3S$) atoms. Up to
60% of the kinetic energy was removed for He atoms that have an initial
velocity of 520 m/s. The hexapoles consist of permanent magnets, whereas the
solenoids are produced from a single hollow copper capillary through which
cooling liquid is passed. The solenoid design allows for excellent thermal
properties, and enables the use of readily available and cheap electronics
components to pulse high currents through the solenoids. The Zeeman decelerator
demonstrated here is mechanically easy to build, can be operated with
cost-effective electronics, and can run at repetition rates up to 10 Hz.
",0,1,0,0,0,0
3726,Mammography Assessment using Multi-Scale Deep Classifiers,"  Applying deep learning methods to mammography assessment has remained a
challenging topic. Dense noise with sparse expressions, mega-pixel raw data
resolution, lack of diverse examples have all been factors affecting
performance. The lack of pixel-level ground truths have especially limited
segmentation methods in pushing beyond approximately bounding regions. We
propose a classification approach grounded in high performance tissue
assessment as an alternative to all-in-one localization and assessment models
that is also capable of pinpointing the causal pixels. First, the objective of
the mammography assessment task is formalized in the context of local tissue
classifiers. Then, the accuracy of a convolutional neural net is evaluated on
classifying patches of tissue with suspicious findings at varying scales, where
highest obtained AUC is above $0.9$. The local evaluations of one such expert
tissue classifier is used to augment the results of a heatmap regression model
and additionally recover the exact causal regions at high resolution as a
saliency image suitable for clinical settings.
",0,0,0,1,0,0
14948,Hochschild Cohomology and Deformation Quantization of Affine Toric Varieties,"  For an affine toric variety $\mathrm{Spec}(A)$, we give a convex geometric
description of the Hodge decomposition of its Hochschild cohomology. Under
certain assumptions we compute the dimensions of the Hodge summands
$T^1_{(i)}(A)$, generalizing the existing results about the Andre-Quillen
cohomology group $T^1_{(1)}(A)$. We prove that every Poisson structure on a
possibly singular affine toric variety can be quantized in the sense of
deformation quantization.
",0,0,1,0,0,0
17264,Factorization Machines Leveraging Lightweight Linked Open Data-enabled Features for Top-N Recommendations,"  With the popularity of Linked Open Data (LOD) and the associated rise in
freely accessible knowledge that can be accessed via LOD, exploiting LOD for
recommender systems has been widely studied based on various approaches such as
graph-based or using different machine learning models with LOD-enabled
features. Many of the previous approaches require construction of an additional
graph to run graph-based algorithms or to extract path-based features by
combining user- item interactions (e.g., likes, dislikes) and background
knowledge from LOD. In this paper, we investigate Factorization Machines (FMs)
based on particularly lightweight LOD-enabled features which can be directly
obtained via a public SPARQL Endpoint without any additional effort to
construct a graph. Firstly, we aim to study whether using FM with these
lightweight LOD-enabled features can provide competitive performance compared
to a learning-to-rank approach leveraging LOD as well as other well-established
approaches such as kNN-item and BPRMF. Secondly, we are interested in finding
out to what extent each set of LOD-enabled features contributes to the
recommendation performance. Experimental evaluation on a standard dataset shows
that our proposed approach using FM with lightweight LOD-enabled features
provides the best performance compared to other approaches in terms of five
evaluation metrics. In addition, the study of the recommendation performance
based on different sets of LOD-enabled features indicate that property-object
lists and PageRank scores of items are useful for improving the performance,
and can provide the best performance through using them together for FM. We
observe that subject-property lists of items does not contribute to the
recommendation performance but rather decreases the performance.
",1,0,0,0,0,0
10500,COREclust: a new package for a robust and scalable analysis of complex data,"  In this paper, we present a new R package COREclust dedicated to the
detection of representative variables in high dimensional spaces with a
potentially limited number of observations. Variable sets detection is based on
an original graph clustering strategy denoted CORE-clustering algorithm that
detects CORE-clusters, i.e. variable sets having a user defined size range and
in which each variable is very similar to at least another variable.
Representative variables are then robustely estimate as the CORE-cluster
centers. This strategy is entirely coded in C++ and wrapped by R using the Rcpp
package. A particular effort has been dedicated to keep its algorithmic cost
reasonable so that it can be used on large datasets. After motivating our work,
we will explain the CORE-clustering algorithm as well as a greedy extension of
this algorithm. We will then present how to use it and results obtained on
synthetic and real data.
",0,0,0,1,0,0
804,Essentially No Barriers in Neural Network Energy Landscape,"  Training neural networks involves finding minima of a high-dimensional
non-convex loss function. Knowledge of the structure of this energy landscape
is sparse. Relaxing from linear interpolations, we construct continuous paths
between minima of recent neural network architectures on CIFAR10 and CIFAR100.
Surprisingly, the paths are essentially flat in both the training and test
landscapes. This implies that neural networks have enough capacity for
structural changes, or that these changes are small between minima. Also, each
minimum has at least one vanishing Hessian eigenvalue in addition to those
resulting from trivial invariance.
",0,0,0,1,0,0
15556,High-dimensional ABC,"  This Chapter, ""High-dimensional ABC"", is to appear in the forthcoming
Handbook of Approximate Bayesian Computation (2018). It details the main ideas
and concepts behind extending ABC methods to higher dimensions, with supporting
examples and illustrations.
",0,0,0,1,0,0
3616,Stochastic Assume-Guarantee Contracts for Cyber-Physical System Design Under Probabilistic Requirements,"  We develop an assume-guarantee contract framework for the design of
cyber-physical systems, modeled as closed-loop control systems, under
probabilistic requirements. We use a variant of signal temporal logic, namely,
Stochastic Signal Temporal Logic (StSTL) to specify system behaviors as well as
contract assumptions and guarantees, thus enabling automatic reasoning about
requirements of stochastic systems. Given a stochastic linear system
representation and a set of requirements captured by bounded StSTL contracts,
we propose algorithms that can check contract compatibility, consistency, and
refinement, and generate a controller to guarantee that a contract is
satisfied, following a stochastic model predictive control approach. Our
algorithms leverage encodings of the verification and control synthesis tasks
into mixed integer optimization problems, and conservative approximations of
probabilistic constraints that produce both sound and tractable problem
formulations. We illustrate the effectiveness of our approach on a few
examples, including the design of embedded controllers for aircraft power
distribution networks.
",1,0,0,0,0,0
2309,Feature Learning for Meta-Paths in Knowledge Graphs,"  In this thesis, we study the problem of feature learning on heterogeneous
knowledge graphs. These features can be used to perform tasks such as link
prediction, classification and clustering on graphs. Knowledge graphs provide
rich semantics encoded in the edge and node types. Meta-paths consist of these
types and abstract paths in the graph. Until now, meta-paths can only be used
as categorical features with high redundancy and are therefore unsuitable for
machine learning models. We propose meta-path embeddings to solve this problem
by learning semantical and compact vector representations of them. Current
graph embedding methods only embed nodes and edge types and therefore miss
semantics encoded in the combination of them. Our method embeds meta-paths
using the skipgram model with an extension to deal with the redundancy and high
amount of meta-paths in big knowledge graphs. We critically evaluate our
embedding approach by predicting links on Wikidata. The experiments indicate
that we learn a sensible embedding of the meta-paths but can improve it
further.
",1,0,0,1,0,0
15060,Criteria for strict monotonicity of the mixed volume of convex polytopes,"  Let $P_1,\dots, P_n$ and $Q_1,\dots, Q_n$ be convex polytopes in
$\mathbb{R}^n$ such that $P_i\subset Q_i$. It is well-known that the mixed
volume has the monotonicity property: $V(P_1,\dots,P_n)\leq V(Q_1,\dots,Q_n)$.
We give two criteria for when this inequality is strict in terms of essential
collections of faces as well as mixed polyhedral subdivisions. This geometric
result allows us to characterize sparse polynomial systems with Newton
polytopes $P_1,\dots,P_n$ whose number of isolated solutions equals the
normalized volume of the convex hull of $P_1\cup\dots\cup P_n$. In addition, we
obtain an analog of Cramer's rule for sparse polynomial systems.
",0,0,1,0,0,0
19053,Positive and Unlabeled Learning through Negative Selection and Imbalance-aware Classification,"  Motivated by applications in protein function prediction, we consider a
challenging supervised classification setting in which positive labels are
scarce and there are no explicit negative labels. The learning algorithm must
thus select which unlabeled examples to use as negative training points,
possibly ending up with an unbalanced learning problem. We address these issues
by proposing an algorithm that combines active learning (for selecting negative
examples) with imbalance-aware learning (for mitigating the label imbalance).
In our experiments we observe that these two techniques operate
synergistically, outperforming state-of-the-art methods on standard protein
function prediction benchmarks.
",0,0,0,0,1,0
7505,Simulating Cosmic Microwave Background anisotropy measurements for Microwave Kinetic Inductance Devices,"  Microwave Kinetic Inductance Devices (MKIDs) are poised to allow for
massively and natively multiplexed photon detectors arrays and are a natural
choice for the next-generation CMB-Stage 4 experiment which will require 105
detectors. In this proceed- ing we discuss what noise performance of present
generation MKIDs implies for CMB measurements. We consider MKID noise spectra
and simulate a telescope scan strategy which projects the detector noise onto
the CMB sky. We then analyze the simulated CMB + MKID noise to understand
particularly low frequency noise affects the various features of the CMB, and
thusly set up a framework connecting MKID characteristics with scan strategies,
to the type of CMB signals we may probe with such detectors.
",0,1,0,0,0,0
2347,FLUX: Progressive State Estimation Based on Zakai-type Distributed Ordinary Differential Equations,"  We propose a homotopy continuation method called FLUX for approximating
complicated probability density functions. It is based on progressive
processing for smoothly morphing a given density into the desired one.
Distributed ordinary differential equations (DODEs) with an artificial time
$\gamma \in [0,1]$ are derived for describing the evolution from the initial
density to the desired final density. For a finite-dimensional parametrization,
the DODEs are converted to a system of ordinary differential equations (SODEs),
which are solved for $\gamma \in [0,1]$ and return the desired result for
$\gamma=1$. This includes parametric representations such as Gaussians or
Gaussian mixtures and nonparametric setups such as sample sets. In the latter
case, we obtain a particle flow between the two densities along the artificial
time.
FLUX is applied to state estimation in stochastic nonlinear dynamic systems
by gradual inclusion of measurement information. The proposed approximation
method (1) is fast, (2) can be applied to arbitrary nonlinear systems and is
not limited to additive noise, (3) allows for target densities that are only
known at certain points, (4) does not require optimization, (5) does not
require the solution of partial differential equations, and (6) works with
standard procedures for solving SODEs. This manuscript is limited to the
one-dimensional case and a fixed number of parameters during the progression.
Future extensions will include consideration of higher dimensions and on the
fly adaption of the number of parameters.
",1,0,0,0,0,0
7822,Cosmological model discrimination with Deep Learning,"  We demonstrate the potential of Deep Learning methods for measurements of
cosmological parameters from density fields, focusing on the extraction of
non-Gaussian information. We consider weak lensing mass maps as our dataset. We
aim for our method to be able to distinguish between five models, which were
chosen to lie along the $\sigma_8$ - $\Omega_m$ degeneracy, and have nearly the
same two-point statistics. We design and implement a Deep Convolutional Neural
Network (DCNN) which learns the relation between five cosmological models and
the mass maps they generate. We develop a new training strategy which ensures
the good performance of the network for high levels of noise. We compare the
performance of this approach to commonly used non-Gaussian statistics, namely
the skewness and kurtosis of the convergence maps. We find that our
implementation of DCNN outperforms the skewness and kurtosis statistics,
especially for high noise levels. The network maintains the mean discrimination
efficiency greater than $85\%$ even for noise levels corresponding to ground
based lensing observations, while the other statistics perform worse in this
setting, achieving efficiency less than $70\%$. This demonstrates the ability
of CNN-based methods to efficiently break the $\sigma_8$ - $\Omega_m$
degeneracy with weak lensing mass maps alone. We discuss the potential of this
method to be applied to the analysis of real weak lensing data and other
datasets.
",0,1,0,1,0,0
18843,An extension problem and trace Hardy inequality for the sublaplacian on $H$-type groups,"  In this paper we study the extension problem for the sublaplacian on a
$H$-type group and use the solutions to prove trace Hardy and Hardy
inequalities for fractional powers of the sublaplacian.
",0,0,1,0,0,0
15923,Automatic symbolic computation for discontinuous Galerkin finite element methods,"  The implementation of discontinuous Galerkin finite element methods (DGFEMs)
represents a very challenging computational task, particularly for systems of
coupled nonlinear PDEs, including multiphysics problems, whose parameters may
consist of power series or functionals of the solution variables. Thereby, the
exploitation of symbolic algebra to express a given DGFEM approximation of a
PDE problem within a high level language, whose syntax closely resembles the
mathematical definition, is an invaluable tool. Indeed, this then facilitates
the automatic assembly of the resulting system of (nonlinear) equations, as
well as the computation of Fréchet derivative(s) of the DGFEM scheme, needed,
for example, within a Newton-type solver. However, even exploiting symbolic
algebra, the discretisation of coupled systems of PDEs can still be extremely
verbose and hard to debug. Thereby, in this article we develop a further layer
of abstraction by designing a class structure for the automatic computation of
DGFEM formulations. This work has been implemented within the FEniCS package,
based on exploiting the Unified Form Language. Numerical examples are presented
which highlight the simplicity of implementation of DGFEMs for the numerical
approximation of a range of PDE problems.
",1,0,0,0,0,0
4986,BézierGAN: Automatic Generation of Smooth Curves from Interpretable Low-Dimensional Parameters,"  Many real-world objects are designed by smooth curves, especially in the
domain of aerospace and ship, where aerodynamic shapes (e.g., airfoils) and
hydrodynamic shapes (e.g., hulls) are designed. To facilitate the design
process of those objects, we propose a deep learning based generative model
that can synthesize smooth curves. The model maps a low-dimensional latent
representation to a sequence of discrete points sampled from a rational
Bézier curve. We demonstrate the performance of our method in completing both
synthetic and real-world generative tasks. Results show that our method can
generate diverse and realistic curves, while preserving consistent shape
variation in the latent space, which is favorable for latent space design
optimization or design space exploration.
",0,0,0,1,0,0
13367,The Hasse Norm Principle For Biquadratic Extensions,"  We give an asymptotic formula for the number of biquadratic extensions of the
rationals of bounded discriminant that fail the Hasse norm principle.
",0,0,1,0,0,0
1458,Photodetector figures of merit in terms of POVMs,"  A photodetector may be characterized by various figures of merit such as
response time, bandwidth, dark count rate, efficiency, wavelength resolution,
and photon-number resolution. On the other hand, quantum theory says that any
measurement device is fully described by its POVM, which stands for
Positive-Operator-Valued Measure, and which generalizes the textbook notion of
the eigenstates of the appropriate hermitian operator (the ""observable"") as
measurement outcomes. Here we show how to define a multitude of photodetector
figures of merit in terms of a given POVM. We distinguish classical and quantum
figures of merit and issue a conjecture regarding trade-off relations between
them. We discuss the relationship between POVM elements and photodetector
clicks, and how models of photodetectors may be tested by measuring either POVM
elements or figures of merit. Finally, the POVM is advertised as a
platform-independent way of comparing different types of photodetectors, since
any such POVM refers to the Hilbert space of the incoming light, and not to any
Hilbert space internal to the detector.
",0,1,0,0,0,0
2004,Bayes-Optimal Entropy Pursuit for Active Choice-Based Preference Learning,"  We analyze the problem of learning a single user's preferences in an active
learning setting, sequentially and adaptively querying the user over a finite
time horizon. Learning is conducted via choice-based queries, where the user
selects her preferred option among a small subset of offered alternatives.
These queries have been shown to be a robust and efficient way to learn an
individual's preferences. We take a parametric approach and model the user's
preferences through a linear classifier, using a Bayesian prior to encode our
current knowledge of this classifier. The rate at which we learn depends on the
alternatives offered at every time epoch. Under certain noise assumptions, we
show that the Bayes-optimal policy for maximally reducing entropy of the
posterior distribution of this linear classifier is a greedy policy, and that
this policy achieves a linear lower bound when alternatives can be constructed
from the continuum. Further, we analyze a different metric called
misclassification error, proving that the performance of the optimal policy
that minimizes misclassification error is bounded below by a linear function of
differential entropy. Lastly, we numerically compare the greedy entropy
reduction policy with a knowledge gradient policy under a number of scenarios,
examining their performance under both differential entropy and
misclassification error.
",1,0,0,1,0,0
7496,Vestigial nematic order and superconductivity in the doped topological insulator Cu$_{x}$Bi$_{2}$Se$_{3}$,"  If the topological insulator Bi$_{2}$Se$_{3}$ is doped with electrons,
superconductivity with $T_{\rm c}\approx3-4\:{\rm K}$ emerges for a low
density of carriers ($n\approx10^{20}{\rm cm}^{-3}$) and with a small ratio of
the superconducting coherence length and Fermi wave length:
$\xi/\lambda_{F}\approx2\cdots4$. These values make fluctuations of the
superconducting order parameter increasingly important, to the extend that the
$T_{c}$-value is surprisingly large. Strong spin-orbit interaction led to the
proposal of an odd-parity pairing state. This begs the question of the nature
of the transition in an unconventional superconductor with strong pairing
fluctuations. We show that for a multi-component order parameter, these
fluctuations give rise to a nematic phase at $T_{\rm nem}>T_{c}$. Below
$T_{c}$ several experiments demonstrated a rotational symmetry breaking where
the Cooper pair wave function is locked to the lattice. Our theory shows that
this rotational symmetry breaking, as vestige of the superconducting state,
already occurs above $T_{c}$. The nematic phase is characterized by vanishing
off-diagonal long range order, yet with anisotropic superconducting
fluctuations. It can be identified through direction-dependent
para-conductivity, lattice softening, and an enhanced Raman response in the
$E_{g}$ symmetry channel. In addition, nematic order partially avoids the usual
fluctuation suppression of $T_{c}$.
",0,1,0,0,0,0
10076,Two forms of minimality in ASPIC+,"  Many systems of structured argumentation explicitly require that the facts
and rules that make up the argument for a conclusion be the minimal set
required to derive the conclusion. ASPIC+ does not place such a requirement on
arguments, instead requiring that every rule and fact that are part of an
argument be used in its construction. Thus ASPIC+ arguments are minimal in the
sense that removing any element of the argument would lead to a structure that
is not an argument. In this brief note we discuss these two types of minimality
and show how the first kind of minimality can, if desired, be recovered in
ASPIC+.
",1,0,0,0,0,0
10614,Collapsed Dark Matter Structures,"  The distributions of dark matter and baryons in the Universe are known to be
very different: the dark matter resides in extended halos, while a significant
fraction of the baryons have radiated away much of their initial energy and
fallen deep into the potential wells. This difference in morphology leads to
the widely held conclusion that dark matter cannot cool and collapse on any
scale. We revisit this assumption, and show that a simple model where dark
matter is charged under a ""dark electromagnetism"" can allow dark matter to form
gravitationally collapsed objects with characteristic mass scales much smaller
than that of a Milky Way-type galaxy. Though the majority of the dark matter in
spiral galaxies would remain in the halo, such a model opens the possibility
that galaxies and their associated dark matter play host to a significant
number of collapsed substructures. The observational signatures of such
structures are not well explored, but potentially interesting.
",0,1,0,0,0,0
18128,Entanglement entropy and computational complexity of the Anderson impurity model out of equilibrium I: quench dynamics,"  We study the growth of entanglement entropy in density matrix renormalization
group calculations of the real-time quench dynamics of the Anderson impurity
model. We find that with appropriate choice of basis, the entropy growth is
logarithmic in both the interacting and noninteracting single-impurity models.
The logarithmic entropy growth is understood from a noninteracting chain model
as a critical behavior separating regimes of linear growth and saturation of
entropy, corresponding respectively to an overlapping and gapped energy spectra
of the set of bath states. We find that with an appropriate choices of basis
(energy-ordered bath orbitals), logarithmic entropy growth is the generic
behavior of quenched impurity models. A noninteracting calculation of a
double-impurity Anderson model supports the conclusion in the multi-impurity
case. The logarithmic growth of entanglement entropy enables studies of quench
dynamics to very long times.
",0,1,0,0,0,0
10870,Mining Application-aware Community Organization with Expanded Feature Subspaces from Concerned Attributes in Social Networks,"  Social networks are typical attributed networks with node attributes.
Different from traditional attribute community detection problem aiming at
obtaining the whole set of communities in the network, we study an
application-oriented problem of mining an application-aware community
organization with respect to specific concerned attributes. The concerned
attributes are designated based on the requirements of any application by a
user in advance. The application-aware community organization w.r.t. concerned
attributes consists of the communities with feature subspaces containing these
concerned attributes. Besides concerned attributes, feature subspace of each
required community may contain some other relevant attributes. All relevant
attributes of a feature subspace jointly describe and determine the community
embedded in such subspace. Thus the problem includes two subproblems, i.e., how
to expand the set of concerned attributes to complete feature subspaces and how
to mine the communities embedded in the expanded subspaces. Two subproblems are
jointly solved by optimizing a quality function called subspace fitness. An
algorithm called ACM is proposed. In order to locate the communities
potentially belonging to the application-aware community organization, cohesive
parts of a network backbone composed of nodes with similar concerned attributes
are detected and set as the community seeds. The set of concerned attributes is
set as the initial subspace for all community seeds. Then each community seed
and its attribute subspace are adjusted iteratively to optimize the subspace
fitness. Extensive experiments on synthetic datasets demonstrate the
effectiveness and efficiency of our method and applications on real-world
networks show its application values.
",1,1,0,0,0,0
2352,Modification of low-temperature silicon dioxide films under the influence of technology factors,"  The structure, composition and electrophysical characteristics of
low-temperature silicon dioxide films under influence of various technology
factors, such as ion implantation, laser irradiation, thermal and photonic
annealing, have been studied. Silicon dioxide films have been obtained by
monosilane oxidation using plasma chemical method, reactive cathode sputtering,
and tetraethoxysilane pyrolysis. In the capacity of substrates, germanium,
silicon, gallium arsenide and gallium nitride were used. Structure and
composition of the dielectric films were analyzed by methods of infrared
transmission spectroscopy and frustrated internal reflectance spectroscopy.
Analysis of modification efficiency of low-temperature silicon dioxide films
has been made depending on the substrate type, structure and properties of the
films, their moisture permeability, dielectric deposition technique, type and
dose of implantation ions, temperature and kind of annealing.
",0,1,0,0,0,0
18155,New ADS Functionality for the Curator,"  In this paper we provide an update concerning the operations of the NASA
Astrophysics Data System (ADS), its services and user interface, and the
content currently indexed in its database. As the primary information system
used by researchers in Astronomy, the ADS aims to provide a comprehensive index
of all scholarly resources appearing in the literature. With the current effort
in our community to support data and software citations, we discuss what steps
the ADS is taking to provide the needed infrastructure in collaboration with
publishers and data providers. A new API provides access to the ADS search
interface, metrics, and libraries allowing users to programmatically automate
discovery and curation tasks. The new ADS interface supports a greater
integration of content and services with a variety of partners, including ORCID
claiming, indexing of SIMBAD objects, and article graphics from a variety of
publishers. Finally, we highlight how librarians can facilitate the ingest of
gray literature that they curate into our system.
",1,1,0,0,0,0
342,Greedy-Merge Degrading has Optimal Power-Law,"  Consider a channel with a given input distribution. Our aim is to degrade it
to a channel with at most L output letters. One such degradation method is the
so called ""greedy-merge"" algorithm. We derive an upper bound on the reduction
in mutual information between input and output. For fixed input alphabet size
and variable L, the upper bound is within a constant factor of an
algorithm-independent lower bound. Thus, we establish that greedy-merge is
optimal in the power-law sense.
",1,0,1,0,0,0
3509,Proof Reduction of Fair Stuttering Refinement of Asynchronous Systems and Applications,"  We present a series of definitions and theorems demonstrating how to reduce
the requirements for proving system refinements ensuring containment of fair
stuttering runs. A primary result of the work is the ability to reduce the
requisite proofs on runs of a system of interacting state machines to a set of
definitions and checks on single steps of a small number of state machines
corresponding to the intuitive notions of freedom from starvation and deadlock.
We further refine the definitions to afford an efficient explicit-state
checking procedure in certain finite state cases. We demonstrate the proof
reduction on versions of the Bakery Algorithm.
",1,0,0,0,0,0
3296,Detail-revealing Deep Video Super-resolution,"  Previous CNN-based video super-resolution approaches need to align multiple
frames to the reference. In this paper, we show that proper frame alignment and
motion compensation is crucial for achieving high quality results. We
accordingly propose a `sub-pixel motion compensation' (SPMC) layer in a CNN
framework. Analysis and experiments show the suitability of this layer in video
SR. The final end-to-end, scalable CNN framework effectively incorporates the
SPMC layer and fuses multiple frames to reveal image details. Our
implementation can generate visually and quantitatively high-quality results,
superior to current state-of-the-arts, without the need of parameter tuning.
",1,0,0,0,0,0
7614,A physiology--based parametric imaging method for FDG--PET data,"  Parametric imaging is a compartmental approach that processes nuclear imaging
data to estimate the spatial distribution of the kinetic parameters governing
tracer flow. The present paper proposes a novel and efficient computational
method for parametric imaging which is potentially applicable to several
compartmental models of diverse complexity and which is effective in the
determination of the parametric maps of all kinetic coefficients. We consider
applications to [{18}F]-fluorodeoxyglucose Positron Emission Tomography
(FDG-PET) data and analyze the two-compartment catenary model describing the
standard FDG metabolization by an homogeneous tissue and the three-compartment
non-catenary model representing the renal physiology. We show uniqueness
theorems for both models. The proposed imaging method starts from the
reconstructed FDG-PET images of tracer concentration and preliminarily applies
image processing algorithms for noise reduction and image segmentation. The
optimization procedure solves pixelwise the non-linear inverse problem of
determining the kinetic parameters from dynamic concentration data through a
regularized Gauss-Newton iterative algorithm. The reliability of the method is
validated against synthetic data, for the two-compartment system, and
experimental real data of murine models, for the renal three-compartment
system.
",0,1,1,0,0,0
20897,Von Neumann Regular Cellular Automata,"  For any group $G$ and any set $A$, a cellular automaton (CA) is a
transformation of the configuration space $A^G$ defined via a finite memory set
and a local function. Let $\text{CA}(G;A)$ be the monoid of all CA over $A^G$.
In this paper, we investigate a generalisation of the inverse of a CA from the
semigroup-theoretic perspective. An element $\tau \in \text{CA}(G;A)$ is von
Neumann regular (or simply regular) if there exists $\sigma \in \text{CA}(G;A)$
such that $\tau \circ \sigma \circ \tau = \tau$ and $\sigma \circ \tau \circ
\sigma = \sigma$, where $\circ$ is the composition of functions. Such an
element $\sigma$ is called a generalised inverse of $\tau$. The monoid
$\text{CA}(G;A)$ itself is regular if all its elements are regular. We
establish that $\text{CA}(G;A)$ is regular if and only if $\vert G \vert = 1$
or $\vert A \vert = 1$, and we characterise all regular elements in
$\text{CA}(G;A)$ when $G$ and $A$ are both finite. Furthermore, we study
regular linear CA when $A= V$ is a vector space over a field $\mathbb{F}$; in
particular, we show that every regular linear CA is invertible when $G$ is
torsion-free elementary amenable (e.g. when $G=\mathbb{Z}^d, \ d \in
\mathbb{N}$) and $V=\mathbb{F}$, and that every linear CA is regular when $V$
is finite-dimensional and $G$ is locally finite with $\text{Char}(\mathbb{F})
\nmid o(g)$ for all $g \in G$.
",1,0,1,0,0,0
5529,Correlative cellular ptychography with functionalized nanoparticles at the Fe L-edge,"  Precise localization of nanoparticles within a cell is crucial to the
understanding of cell-particle interactions and has broad applications in
nanomedicine. Here, we report a proof-of-principle experiment for imaging
individual functionalized nanoparticles within a mammalian cell by correlative
microscopy. Using a chemically-fixed, HeLa cell labeled with fluorescent
core-shell nanoparticles as a model system, we implemented a graphene-oxide
layer as a substrate to significantly reduce background scattering. We
identified cellular features of interest by fluorescence microscopy, followed
by scanning transmission X-ray tomography to localize the particles in 3D, and
ptychographic coherent diffractive imaging of the fine features in the region
at high resolution. By tuning the X-ray energy to the Fe L-edge, we
demonstrated sensitive detection of nanoparticles composed of a 22 nm magnetic
Fe3O4 core encased by a 25-nm-thick fluorescent silica (SiO2) shell. These
fluorescent core-shell nanoparticles act as landmarks and offer clarity in a
cellular context. Our correlative microscopy results confirmed a subset of
particles to be fully internalized, and high-contrast ptychographic images
showed two oxidation states of individual nanoparticles with a resolution of
~16.5 nm. The ability to precisely localize individual fluorescent
nanoparticles within mammalian cells will expand our understanding of the
structure/function relationships for functionalized nanoparticles.
",0,1,0,0,0,0
2860,Changing Fashion Cultures,"  The paper presents a novel concept that analyzes and visualizes worldwide
fashion trends. Our goal is to reveal cutting-edge fashion trends without
displaying an ordinary fashion style. To achieve the fashion-based analysis, we
created a new fashion culture database (FCDB), which consists of 76 million
geo-tagged images in 16 cosmopolitan cities. By grasping a fashion trend of
mixed fashion styles,the paper also proposes an unsupervised fashion trend
descriptor (FTD) using a fashion descriptor, a codeword vetor, and temporal
analysis. To unveil fashion trends in the FCDB, the temporal analysis in FTD
effectively emphasizes consecutive features between two different times. In
experiments, we clearly show the analysis of fashion trends and fashion-based
city similarity. As the result of large-scale data collection and an
unsupervised analyzer, the proposed approach achieves world-level fashion
visualization in a time series. The code, model, and FCDB will be publicly
available after the construction of the project page.
",1,0,0,0,0,0
347,Combining learned and analytical models for predicting action effects,"  One of the most basic skills a robot should possess is predicting the effect
of physical interactions with objects in the environment. This enables optimal
action selection to reach a certain goal state. Traditionally, dynamics are
approximated by physics-based analytical models. These models rely on specific
state representations that may be hard to obtain from raw sensory data,
especially if no knowledge of the object shape is assumed. More recently, we
have seen learning approaches that can predict the effect of complex physical
interactions directly from sensory input. It is however an open question how
far these models generalize beyond their training data. In this work, we
investigate the advantages and limitations of neural network based learning
approaches for predicting the effects of actions based on sensory input and
show how analytical and learned models can be combined to leverage the best of
both worlds. As physical interaction task, we use planar pushing, for which
there exists a well-known analytical model and a large real-world dataset. We
propose to use a convolutional neural network to convert raw depth images or
organized point clouds into a suitable representation for the analytical model
and compare this approach to using neural networks for both, perception and
prediction. A systematic evaluation of the proposed approach on a very large
real-world dataset shows two main advantages of the hybrid architecture.
Compared to a pure neural network, it significantly (i) reduces required
training data and (ii) improves generalization to novel physical interaction.
",1,0,0,0,0,0
17243,Learning Spatial Regularization with Image-level Supervisions for Multi-label Image Classification,"  Multi-label image classification is a fundamental but challenging task in
computer vision. Great progress has been achieved by exploiting semantic
relations between labels in recent years. However, conventional approaches are
unable to model the underlying spatial relations between labels in multi-label
images, because spatial annotations of the labels are generally not provided.
In this paper, we propose a unified deep neural network that exploits both
semantic and spatial relations between labels with only image-level
supervisions. Given a multi-label image, our proposed Spatial Regularization
Network (SRN) generates attention maps for all labels and captures the
underlying relations between them via learnable convolutions. By aggregating
the regularized classification results with original results by a ResNet-101
network, the classification performance can be consistently improved. The whole
deep neural network is trained end-to-end with only image-level annotations,
thus requires no additional efforts on image annotations. Extensive evaluations
on 3 public datasets with different types of labels show that our approach
significantly outperforms state-of-the-arts and has strong generalization
capability. Analysis of the learned SRN model demonstrates that it can
effectively capture both semantic and spatial relations of labels for improving
classification performance.
",1,0,0,0,0,0
6291,Perpetual points: New tool for localization of co-existing attractors in dynamical systems,"  Perpetual points (PPs) are special critical points for which the magnitude of
acceleration describing dynamics drops to zero, while the motion is still
possible (stationary points are excluded), e.g. considering the motion of the
particle in the potential field, at perpetual point it has zero acceleration
and non-zero velocity. We show that using PPs we can trace all the stable fixed
points in the system, and that the structure of trajectories leading from
former points to stable equilibria may be similar to orbits obtained from
unstable stationary points. Moreover, we argue that the concept of perpetual
points may be useful in tracing unexpected attractors (hidden or rare
attractors with small basins of attraction). We show potential applicability of
this approach by analysing several representative systems of physical
significance, including the damped oscillator, pendula and the Henon map. We
suggest that perpetual points may be a useful tool for localization of
co-existing attractors in dynamical systems.
",0,1,0,0,0,0
13822,Thickening and sickening the SYK model,"  We discuss higher dimensional generalizations of the 0+1-dimensional
Sachdev-Ye-Kitaev (SYK) model that has recently become the focus of intensive
interdisciplinary studies by, both, the condensed matter and field-theoretical
communities. Unlike the previous constructions where multiple SYK copies would
be coupled to each other and/or hybridized with itinerant fermions via
spatially short-ranged random hopping processes, we study algebraically varying
long-range (spatially and/or temporally) correlated random couplings in the
general d+1 dimensions. Such pertinent topics as translationally-invariant
strong-coupling solutions, emergent reparametrization symmetry, effective
action for fluctuations, chaotic behavior, and diffusive transport (or a lack
thereof) are all addressed. We find that the most appealing properties of the
original SYK model that suggest the existence of its 1+1-dimensional
holographic gravity dual do not survive the aforementioned generalizations,
thus lending no additional support to the hypothetical broad (including
'non-AdS/non-CFT') holographic correspondence.
",0,1,0,0,0,0
8785,A parametric level-set method for partially discrete tomography,"  This paper introduces a parametric level-set method for tomographic
reconstruction of partially discrete images. Such images consist of a
continuously varying background and an anomaly with a constant (known)
grey-value. We represent the geometry of the anomaly using a level-set
function, which we represent using radial basis functions. We pose the
reconstruction problem as a bi-level optimization problem in terms of the
background and coefficients for the level-set function. To constrain the
background reconstruction we impose smoothness through Tikhonov regularization.
The bi-level optimization problem is solved in an alternating fashion; in each
iteration we first reconstruct the background and consequently update the
level-set function. We test our method on numerical phantoms and show that we
can successfully reconstruct the geometry of the anomaly, even from limited
data. On these phantoms, our method outperforms Total Variation reconstruction,
DART and P-DART.
",1,0,0,0,0,0
5048,"Fluid-Structure Interaction for the Classroom: Interpolation, Hearts, and Swimming!","  While students may find spline interpolation easily digestible, based on
their familiarity with continuity of a function and its derivatives, some of
its inherent value may be missed when students only see it applied to standard
data interpolation exercises. In this paper, we offer alternatives where
students can qualitatively and quantitatively witness the resulting dynamical
differences when objects are driven through a fluid using different spline
interpolation methods. They say, seeing is believing; here we showcase the
differences between linear and cubic spline interpolation using examples from
fluid pumping and aquatic locomotion. Moreover, students can define their own
interpolation functions and visualize the dynamics unfold. To solve the
fluid-structure interaction system, the open source software IB2d is used. In
that vein, all simulation codes, analysis scripts, and movies are provided for
streamlined use.
",0,0,0,0,1,0
3499,Converting Cascade-Correlation Neural Nets into Probabilistic Generative Models,"  Humans are not only adept in recognizing what class an input instance belongs
to (i.e., classification task), but perhaps more remarkably, they can imagine
(i.e., generate) plausible instances of a desired class with ease, when
prompted. Inspired by this, we propose a framework which allows transforming
Cascade-Correlation Neural Networks (CCNNs) into probabilistic generative
models, thereby enabling CCNNs to generate samples from a category of interest.
CCNNs are a well-known class of deterministic, discriminative NNs, which
autonomously construct their topology, and have been successful in giving
accounts for a variety of psychological phenomena. Our proposed framework is
based on a Markov Chain Monte Carlo (MCMC) method, called the
Metropolis-adjusted Langevin algorithm, which capitalizes on the gradient
information of the target distribution to direct its explorations towards
regions of high probability, thereby achieving good mixing properties. Through
extensive simulations, we demonstrate the efficacy of our proposed framework.
",1,0,0,1,0,0
2606,Nonlinear transport associated with spin-density-wave dynamics in Ca$_3$Co$_{4}$O$_9$,"  We have carried out the transient nonlinear transport measurements on the
layered cobalt oxide Ca$_3$Co$_{4}$O$_9$, in which a spin density wave (SDW)
transition is proposed at $T_{\rm SDW} \simeq 30$ K. We find that, below
$T_{\rm SDW}$, the electrical conductivity systematically varies with both the
applied current and the time, indicating a close relationship between the
observed nonlinear conduction and the SDW order in this material. The time
dependence of the conductivity is well analyzed by considering the dynamics of
SDW which involves a low-field deformation and a sliding motion above a
threshold field. We also measure the transport properties of the isovalent
Sr-substituted systems to examine an impurity effect on the nonlinear response,
and discuss the obtained threshold fields in terms of thermal fluctuations of
the SDW order parameter.
",0,1,0,0,0,0
14137,Precision Interfaces,"  Building interactive tools to support data analysis is hard because it is not
always clear what to build and how to build it. To address this problem, we
present Precision Interfaces, a semi-automatic system to generate task-specific
data analytics interfaces. Precision Interface can turn a log of executed
programs into an interface, by identifying micro-variations between the
programs and mapping them to interface components. This paper focuses on SQL
query logs, but we can generalize the approach to other languages. Our system
operates in two steps: it first build an interaction graph, which describes how
the queries can be transformed into each other. Then, it finds a set of UI
components that covers a maximal number of transformations. To restrict the
domain of changes to be detected, our system uses a domain-specific language,
PILang. We give a full description of Precision Interface's components,
showcase an early prototype on real program logs and discuss future research
opportunities.
",1,0,0,0,0,0
5976,Collective excitations and supersolid behavior of bosonic atoms inside two crossed optical cavities,"  We discuss the nature of symmetry breaking and the associated collective
excitations for a system of bosons coupled to the electromagnetic field of two
optical cavities. For the specific configuration realized in a recent
experiment at ETH, we show that, in absence of direct intercavity scattering
and for parameters chosen such that the atoms couple symmetrically to both
cavities, the system possesses an approximate $U(1)$ symmetry which holds
asymptotically for vanishing cavity field intensity. It corresponds to the
invariance with respect to redistributing the total intensity $I=I_1+I_2$
between the two cavities. The spontaneous breaking of this symmetry gives rise
to a broken continuous translation-invariance for the atoms, creating a
supersolid-like order in the presence of a Bose-Einstein condensate. In
particular, we show that atom-mediated scattering between the two cavities,
which favors the state with equal light intensities $I_1=I_2$ and reduces the
symmetry to $\mathbf{Z}_2\otimes \mathbf{Z}_2$, gives rise to a finite value
$\sim \sqrt{I}$ of the effective Goldstone mass. For strong atom driving, this
low energy mode is clearly separated from an effective Higgs excitation
associated with changes of the total intensity $I$. In addition, we compute the
spectral distribution of the cavity light field and show that both the Higgs
and Goldstone mode acquire a finite lifetime due to Landau damping at non-zero
temperature.
",0,1,0,0,0,0
7585,Restriction of representations of metaplectic $GL_{2}(F)$ to tori,"  Let $F$ be a non-Archimedean local field. We study the restriction of an
irreducible admissible genuine representations of the two fold metaplectic
cover $\widetilde{GL}_{2}(F)$ of $GL_{2}(F)$ to the inverse image in
$\widetilde{GL}_{2}(F)$ of a maximal torus in $GL_{2}(F)$.
",0,0,1,0,0,0
2146,Gas Adsorption and Dynamics in Pillared Graphene Frameworks,"  Pillared Graphene Frameworks are a novel class of microporous materials made
by graphene sheets separated by organic spacers. One of their main features is
that the pillar type and density can be chosen to tune the material properties.
In this work, we present a computer simulation study of adsorption and dynamics
of H$_{4}$, CH$_{2}$, CO$_{2}$, N$_{2}$ and O$_{2}$ and binary mixtures
thereof, in Pillared Graphene Frameworks with nitrogen-containing organic
spacers. In general, we find that pillar density plays the most important role
in determining gas adsorption. In the low-pressure regime (< 10 bar) the amount
of gas adsorbed is an increasing function of pillar density. At higher
pressures the opposite trend is observed. Diffusion coefficients were computed
for representative structures taking into account the framework flexibility
that is essential in assessing the dynamical properties of the adsorbed gases.
Good performance for the gas separation in CH$_{4}$/H$_{2}$, CO$_{2}$/H$_{2}$
and CO$_{2}$/N$_{2}$ mixtures was found with values comparable to those of
metal-organic frameworks and zeolites.
",0,1,0,0,0,0
12544,Pipelined Parallel FFT Architecture,"  In this paper, an optimized efficient VLSI architecture of a pipeline Fast
Fourier transform (FFT) processor capable of producing the reverse output order
sequence is presented. Paper presents Radix-2 multipath delay architecture for
FFT calculation. The implementation of FFT in hardware is very critical because
for calculation of FFT number of butterfly operations i.e. number of
multipliers requires due to which hardware gets increased means indirectly cost
of hardware is automatically gets increased. Also multiplier operations are
slow that's why it limits the speed of operation of architecture. The optimized
VLSI implementation of FFT algorithm is presented in this paper. Here
architecture is pipelined to optimize it and to increase the speed of
operation. Also to increase the speed of operation 2 levels parallel processing
is used.
",1,0,1,0,0,0
20306,Survivable Probability of SDN-enabled Cloud Networking with Random Physical Link Failure,"  Software-driven cloud networking is a new paradigm in orchestrating physical
resources (CPU, network bandwidth, energy, storage) allocated to network
functions, services, and applications, which is commonly modeled as a
cross-layer network. This model carries a physical network representing the
physical infrastructure, a logical network showing demands, and
logical-to-physical node/link mappings. In such networks, a single failure in
the physical network may trigger cascading failures in the logical network and
disable network services and connectivity. In this paper, we propose an
evaluation metric, survivable probability, to evaluate the reliability of such
networks under random physical link failure(s). We propose the concept of base
protecting spanning tree and prove the necessary and sufficient conditions for
its existence and relation to survivability. We then develop mathematical
programming formulations for reliable cross-layer network routing design with
the maximal reliable probability. Computation results demonstrate the viability
of our approach.
",1,0,0,0,0,0
10611,Constraining Reionization with the $z \sim 5-6$ Lyman-$α$ Forest Power Spectrum: the Outlook after Planck,"  The latest measurements of CMB electron scattering optical depth reported by
Planck significantly reduces the allowed space of HI reionization models,
pointing towards a later ending and/or less extended phase transition than
previously believed. Reionization impulsively heats the intergalactic medium
(IGM) to $\sim10^4$ K, and owing to long cooling and dynamical times in the
diffuse gas, comparable to the Hubble time, memory of reionization heating is
retained. Therefore, a late ending reionization has significant implications
for the structure of the $z\sim5-6$ Lyman-$\alpha$ (ly$\alpha$) forest. Using
state-of-the-art hydrodynamical simulations that allow us to vary the timing of
reionization and its associated heat injection, we argue that extant thermal
signatures from reionization can be detected via the ly$\alpha$ forest power
spectrum at $5< z<6$. This arises because the small-scale cutoff in the power
depends not only the the IGMs temperature at these epochs, but is also
particularly sensitive to the pressure smoothing scale set by the IGMs full
thermal history. Comparing our different reionization models with existing
measurements of the ly$\alpha$ forest flux power spectrum at $z=5.0-5.4$, we
find that models satisfying Planck's $\tau_e$ constraint, favor a moderate
amount of heat injection consistent with galaxies driving reionization, but
disfavoring quasar driven scenarios. We explore the impact of different
reionization histories and heating models on the shape of the power spectrum,
and find that they can produce similar effects, but argue that this degeneracy
can be broken with high enough quality data. We study the feasibility of
measuring the flux power spectrum at $z\simeq 6$ using mock quasar spectra and
conclude that a sample of $\sim10$ high-resolution spectra with attainable S/N
ratio will allow to discriminate between different reionization scenarios.
",0,1,0,0,0,0
19655,Introduction to finite mixtures,"  Mixture models have been around for over 150 years, as an intuitively simple
and practical tool for enriching the collection of probability distributions
available for modelling data. In this chapter we describe the basic ideas of
the subject, present several alternative representations and perspectives on
these models, and discuss some of the elements of inference about the unknowns
in the models. Our focus is on the simplest set-up, of finite mixture models,
but we discuss also how various simplifying assumptions can be relaxed to
generate the rich landscape of modelling and inference ideas traversed in the
rest of this book.
",0,0,0,1,0,0
1356,Retrosynthetic reaction prediction using neural sequence-to-sequence models,"  We describe a fully data driven model that learns to perform a retrosynthetic
reaction prediction task, which is treated as a sequence-to-sequence mapping
problem. The end-to-end trained model has an encoder-decoder architecture that
consists of two recurrent neural networks, which has previously shown great
success in solving other sequence-to-sequence prediction tasks such as machine
translation. The model is trained on 50,000 experimental reaction examples from
the United States patent literature, which span 10 broad reaction types that
are commonly used by medicinal chemists. We find that our model performs
comparably with a rule-based expert system baseline model, and also overcomes
certain limitations associated with rule-based expert systems and with any
machine learning approach that contains a rule-based expert system component.
Our model provides an important first step towards solving the challenging
problem of computational retrosynthetic analysis.
",1,0,0,1,0,0
10126,Matched bipartite block model with covariates,"  Community detection or clustering is a fundamental task in the analysis of
network data. Many real networks have a bipartite structure which makes
community detection challenging. In this paper, we consider a model which
allows for matched communities in the bipartite setting, in addition to node
covariates with information about the matching. We derive a simple fast
algorithm for fitting the model based on variational inference ideas and show
its effectiveness on both simulated and real data. A variation of the model to
allow for degree-correction is also considered, in addition to a novel approach
to fitting such degree-corrected models.
",1,0,0,1,0,0
3456,Understanding Organizational Approach towards End User Privacy,"  End user privacy is a critical concern for all organizations that collect,
process and store user data as a part of their business. Privacy concerned
users, regulatory bodies and privacy experts continuously demand organizations
provide users with privacy protection. Current research lacks an understanding
of organizational characteristics that affect an organization's motivation
towards user privacy. This has resulted in a ""one solution fits all"" approach,
which is incapable of providing sustainable solutions for organizational issues
related to user privacy. In this work, we have empirically investigated 40
diverse organizations on their motivations and approaches towards user privacy.
Resources such as newspaper articles, privacy policies and internal privacy
reports that display information about organizational motivations and
approaches towards user privacy were used in the study. We could observe
organizations to have two primary motivations to provide end users with privacy
as voluntary driven inherent motivation, and risk driven compliance motivation.
Building up on these findings we developed a taxonomy of organizational privacy
approaches and further explored the taxonomy through limited exclusive
interviews. With his work, we encourage authorities and scholars to understand
organizational characteristics that define an organization's approach towards
privacy, in order to effectively communicate regulations that enforce and
encourage organizations to consider privacy within their business practices.
",1,0,0,0,0,0
7327,Gaussian Graphical Models: An Algebraic and Geometric Perspective,"  Gaussian graphical models are used throughout the natural sciences, social
sciences, and economics to model the statistical relationships between
variables of interest in the form of a graph. We here provide a pedagogic
introduction to Gaussian graphical models and review recent results on maximum
likelihood estimation for such models. Throughout, we highlight the rich
algebraic and geometric properties of Gaussian graphical models and explain how
these properties relate to convex optimization and ultimately result in
insights on the existence of the maximum likelihood estimator (MLE) and
algorithms for computing the MLE.
",0,0,1,1,0,0
14345,Analytical history,"  The purpose of this note is to explain what is ""analytical history"", a
modular and testable analysis of historical events introduced in a book
published in 2002 (Roehner and Syme 2002). Broadly speaking, it is a
comparative methodology for the analysis of historical events. Comparison is
the keystone and hallmark of science. For instance, the extrasolar planets are
crucial for understanding our own solar system. Until their discovery,
astronomers could observe only one instance. Single instances can be described
but they cannot be understood in a testable way. In other words, if one accepts
that, as many historians say, ""historical events are unique"", then no testable
understanding can be developed.
",0,1,0,0,0,0
10960,Emergence of Leadership in Communication,"  We study a neuro-inspired model that mimics a discussion (or information
dissemination) process in a network of agents. During their interaction, agents
redistribute activity and network weights, resulting in emergence of leader(s).
The model is able to reproduce the basic scenarios of leadership known in
nature and society: laissez-faire (irregular activity, weak leadership, sizable
inter-follower interaction, autonomous sub-leaders); participative or
democratic (strong leadership, but with feedback from followers); and
autocratic (no feedback, one-way influence). Several pertinent aspects of these
scenarios are found as well---e.g., hidden leadership (a hidden clique of
agents driving the official autocratic leader), and successive leadership (two
leaders influence followers by turns). We study how these scenarios emerge from
inter-agent dynamics and how they depend on behavior rules of agents---in
particular, on their inertia against state changes.
",0,1,0,0,0,0
13459,Exact completion and constructive theories of sets,"  In the present paper we use the theory of exact completions to study
categorical properties of small setoids in Martin-Löf type theory and, more
generally, of models of the Constructive Elementary Theory of the Category of
Sets, in terms of properties of their subcategories of choice objects (i.e.
objects satisfying the axiom of choice). Because of these intended
applications, we deal with categories that lack equalisers and just have weak
ones, but whose objects can be regarded as collections of global elements. In
this context, we study the internal logic of the categories involved, and
employ this analysis to give a sufficient condition for the local cartesian
closure of an exact completion. Finally, we apply these results to show when an
exact completion produces a model of CETCS.
",0,0,1,0,0,0
5306,Semantic Code Repair using Neuro-Symbolic Transformation Networks,"  We study the problem of semantic code repair, which can be broadly defined as
automatically fixing non-syntactic bugs in source code. The majority of past
work in semantic code repair assumed access to unit tests against which
candidate repairs could be validated. In contrast, the goal here is to develop
a strong statistical model to accurately predict both bug locations and exact
fixes without access to information about the intended correct behavior of the
program. Achieving such a goal requires a robust contextual repair model, which
we train on a large corpus of real-world source code that has been augmented
with synthetically injected bugs. Our framework adopts a two-stage approach
where first a large set of repair candidates are generated by rule-based
processors, and then these candidates are scored by a statistical model using a
novel neural network architecture which we refer to as Share, Specialize, and
Compete. Specifically, the architecture (1) generates a shared encoding of the
source code using an RNN over the abstract syntax tree, (2) scores each
candidate repair using specialized network modules, and (3) then normalizes
these scores together so they can compete against one another in comparable
probability space. We evaluate our model on a real-world test set gathered from
GitHub containing four common categories of bugs. Our model is able to predict
the exact correct repair 41\% of the time with a single guess, compared to 13\%
accuracy for an attentional sequence-to-sequence model.
",1,0,0,0,0,0
13994,DLTK: State of the Art Reference Implementations for Deep Learning on Medical Images,"  We present DLTK, a toolkit providing baseline implementations for efficient
experimentation with deep learning methods on biomedical images. It builds on
top of TensorFlow and its high modularity and easy-to-use examples allow for a
low-threshold access to state-of-the-art implementations for typical medical
imaging problems. A comparison of DLTK's reference implementations of popular
network architectures for image segmentation demonstrates new top performance
on the publicly available challenge data ""Multi-Atlas Labeling Beyond the
Cranial Vault"". The average test Dice similarity coefficient of $81.5$ exceeds
the previously best performing CNN ($75.7$) and the accuracy of the challenge
winning method ($79.0$).
",1,0,0,0,0,0
3112,Diversification-Based Learning in Computing and Optimization,"  Diversification-Based Learning (DBL) derives from a collection of principles
and methods introduced in the field of metaheuristics that have broad
applications in computing and optimization. We show that the DBL framework goes
significantly beyond that of the more recent Opposition-based learning (OBL)
framework introduced in Tizhoosh (2005), which has become the focus of numerous
research initiatives in machine learning and metaheuristic optimization. We
unify and extend earlier proposals in metaheuristic search (Glover, 1997,
Glover and Laguna, 1997) to give a collection of approaches that are more
flexible and comprehensive than OBL for creating intensification and
diversification strategies in metaheuristic search. We also describe potential
applications of DBL to various subfields of machine learning and optimization.
",1,0,0,0,0,0
972,Performance analysis of smart digital signage system based on software-defined IoT and invisible image sensor communication,"  Everything in the world is being connected, and things are becoming
interactive. The future of the interactive world depends on the future Internet
of Things (IoT). Software-defined networking (SDN) technology, a new paradigm
in the networking area, can be useful in creating an IoT because it can handle
interactivity by controlling physical devices, transmission of data among them,
and data acquisition. However, digital signage can be one of the promising
technologies in this era of technology that is progressing toward the
interactive world, connecting users to the IoT network through device-to-device
communication technology. This article illustrates a novel prototype that is
mainly focused on a smart digital signage system comprised of software-defined
IoT (SD-IoT) and invisible image sensor communication technology. We have
proposed an SDN scheme with a view to initiating its flexibility and
compatibility for an IoT network-based smart digital signage system. The idea
of invisible communication can make the users of the technology trendier to it,
and the usage of unused resources such as images and videos can be ensured. In
addition, this communication has paved the way for interactivity between the
user and digital signage, where the digital signage and the camera of a
smartphone can be operated as a transmitter and a receiver, respectively. The
proposed scheme might be applicable to real-world applications because SDN has
the flexibility to adapt with the alteration of network status without any
hardware modifications while displays and smartphones are available everywhere.
A performance analysis of this system showed the advantages of an SD-IoT
network over an Internet protocol-based IoT network considering a queuing
analysis for a dynamic link allocation process in the case of user access to
the IoT network.
",1,0,0,0,0,0
7443,Heavy-Tailed Universality Predicts Trends in Test Accuracies for Very Large Pre-Trained Deep Neural Networks,"  Given two or more Deep Neural Networks (DNNs) with the same or similar
architectures, and trained on the same dataset, but trained with different
solvers, parameters, hyper-parameters, regularization, etc., can we predict
which DNN will have the best test accuracy, and can we do so without peeking at
the test data? In this paper, we show how to use a new Theory of Heavy-Tailed
Self-Regularization (HT-SR) to answer this. HT-SR suggests, among other things,
that modern DNNs exhibit what we call Heavy-Tailed Mechanistic Universality
(HT-MU), meaning that the correlations in the layer weight matrices can be fit
to a power law with exponents that lie in common Universality classes from
Heavy-Tailed Random Matrix Theory (HT-RMT). From this, we develop a Universal
capacity control metric that is a weighted average of these PL exponents.
Rather than considering small toy NNs, we examine over 50 different,
large-scale pre-trained DNNs, ranging over 15 different architectures, trained
on ImagetNet, each of which has been reported to have different test
accuracies. We show that this new capacity metric correlates very well with the
reported test accuracies of these DNNs, looking across each architecture
(VGG16/.../VGG19, ResNet10/.../ResNet152, etc.). We also show how to
approximate the metric by the more familiar Product Norm capacity measure, as
the average of the log Frobenius norm of the layer weight matrices. Our
approach requires no changes to the underlying DNN or its loss function, it
does not require us to train a model (although it could be used to monitor
training), and it does not even require access to the ImageNet data.
",1,0,0,1,0,0
20783,A Study of MAC Address Randomization in Mobile Devices and When it Fails,"  MAC address randomization is a privacy technique whereby mobile devices
rotate through random hardware addresses in order to prevent observers from
singling out their traffic or physical location from other nearby devices.
Adoption of this technology, however, has been sporadic and varied across
device manufacturers. In this paper, we present the first wide-scale study of
MAC address randomization in the wild, including a detailed breakdown of
different randomization techniques by operating system, manufacturer, and model
of device.
We then identify multiple flaws in these implementations which can be
exploited to defeat randomization as performed by existing devices. First, we
show that devices commonly make improper use of randomization by sending
wireless frames with the true, global address when they should be using a
randomized address. We move on to extend the passive identification techniques
of Vanhoef et al. to effectively defeat randomization in ~96% of Android
phones. Finally, we show a method that can be used to track 100% of devices
using randomization, regardless of manufacturer, by exploiting a previously
unknown flaw in the way existing wireless chipsets handle low-level control
frames.
",1,0,0,0,0,0
6351,Numerical analysis of nonlocal fracture models in Hölder space,"  In this work, we calculate the convergence rate of the finite difference
approximation for a class of nonlocal fracture models. We consider two point
force interactions characterized by a double well potential. We show the
existence of a evolving displacement field in Hölder space with Hölder
exponent $\gamma \in (0,1]$. The rate of convergence of the finite difference
approximation depends on the factor $C_s h^\gamma/\epsilon^2$ where $\epsilon$
gives the length scale of nonlocal interaction, $h$ is the discretization
length and $C_s$ is the maximum of Hölder norm of the solution and its second
derivatives during the evolution. It is shown that the rate of convergence
holds for both the forward Euler scheme as well as general single step implicit
schemes. A stability result is established for the semi-discrete approximation.
The Hölder continuous evolutions are seen to converge to a brittle fracture
evolution in the limit of vanishing nonlocality.
",0,0,1,0,0,0
11437,Double Sparsity Kernel Learning with Automatic Variable Selection and Data Extraction,"  Learning with Reproducing Kernel Hilbert Spaces (RKHS) has been widely used
in many scientific disciplines. Because a RKHS can be very flexible, it is
common to impose a regularization term in the optimization to prevent
overfitting. Standard RKHS learning employs the squared norm penalty of the
learning function. Despite its success, many challenges remain. In particular,
one cannot directly use the squared norm penalty for variable selection or data
extraction. Therefore, when there exists noise predictors, or the underlying
function has a sparse representation in the dual space, the performance of
standard RKHS learning can be suboptimal. In the literature,work has been
proposed on how to perform variable selection in RKHS learning, and a data
sparsity constraint was considered for data extraction. However, how to learn
in a RKHS with both variable selection and data extraction simultaneously
remains unclear. In this paper, we propose a unified RKHS learning method,
namely, DOuble Sparsity Kernel (DOSK) learning, to overcome this challenge. An
efficient algorithm is provided to solve the corresponding optimization
problem. We prove that under certain conditions, our new method can
asymptotically achieve variable selection consistency. Simulated and real data
results demonstrate that DOSK is highly competitive among existing approaches
for RKHS learning.
",0,0,0,1,0,0
6249,The length of excitable knots,"  The FitzHugh-Nagumo equation provides a simple mathematical model of cardiac
tissue as an excitable medium hosting spiral wave vortices. Here we present
extensive numerical simulations studying long-term dynamics of knotted vortex
string solutions for all torus knots up to crossing number 11. We demonstrate
that FitzHugh-Nagumo evolution preserves the knot topology for all the examples
presented, thereby providing a novel field theory approach to the study of
knots. Furthermore, the evolution yields a well-defined minimal length for each
knot that is comparable to the ropelength of ideal knots. We highlight the role
of the medium boundary in stabilizing the length of the knot and discuss the
implications beyond torus knots. By applying Moffatt's test we are able to show
that there is not a unique attractor within a given knot topology.
",0,1,1,0,0,0
6149,Bridging the Gap Between Value and Policy Based Reinforcement Learning,"  We establish a new connection between value and policy based reinforcement
learning (RL) based on a relationship between softmax temporal value
consistency and policy optimality under entropy regularization. Specifically,
we show that softmax consistent action values correspond to optimal entropy
regularized policy probabilities along any action sequence, regardless of
provenance. From this observation, we develop a new RL algorithm, Path
Consistency Learning (PCL), that minimizes a notion of soft consistency error
along multi-step action sequences extracted from both on- and off-policy
traces. We examine the behavior of PCL in different scenarios and show that PCL
can be interpreted as generalizing both actor-critic and Q-learning algorithms.
We subsequently deepen the relationship by showing how a single model can be
used to represent both a policy and the corresponding softmax state values,
eliminating the need for a separate critic. The experimental evaluation
demonstrates that PCL significantly outperforms strong actor-critic and
Q-learning baselines across several benchmarks.
",1,0,0,1,0,0
4008,Iterative Machine Teaching,"  In this paper, we consider the problem of machine teaching, the inverse
problem of machine learning. Different from traditional machine teaching which
views the learners as batch algorithms, we study a new paradigm where the
learner uses an iterative algorithm and a teacher can feed examples
sequentially and intelligently based on the current performance of the learner.
We show that the teaching complexity in the iterative case is very different
from that in the batch case. Instead of constructing a minimal training set for
learners, our iterative machine teaching focuses on achieving fast convergence
in the learner model. Depending on the level of information the teacher has
from the learner model, we design teaching algorithms which can provably reduce
the number of teaching examples and achieve faster convergence than learning
without teachers. We also validate our theoretical findings with extensive
experiments on different data distribution and real image datasets.
",1,0,0,1,0,0
7536,Exotic pairing symmetry of interacting Dirac fermions on a $π$ flux lattice,"  The pairing symmetry of interacting Dirac fermions on the $\pi$-flux lattice
is studied with the determinant quantum Monte Carlo and numerical linked
cluster expansion methods. The extended $s^*$- (i.e. extended $s$-) and d-wave
pairing symmetries, which are distinct in the conventional square lattice, are
degenerate under the Landau gauge. We demonstrate that the dominant pairing
channel at strong interactions is an exotic $ds^*$-wave phase consisting of
alternating stripes of $s^*$- and d-wave phases. A complementary mean-field
analysis shows that while the $s^*$- and d-wave symmetries individually have
nodes in the energy spectrum, the $ds^*$ channel is fully gapped. The results
represent a new realization of pairing in Dirac systems, connected to the
problem of chiral d-wave pairing on the honeycomb lattice, which might be more
readily accessed by cold-atom experiments.
",0,1,0,0,0,0
719,Database Learning: Toward a Database that Becomes Smarter Every Time,"  In today's databases, previous query answers rarely benefit answering future
queries. For the first time, to the best of our knowledge, we change this
paradigm in an approximate query processing (AQP) context. We make the
following observation: the answer to each query reveals some degree of
knowledge about the answer to another query because their answers stem from the
same underlying distribution that has produced the entire dataset. Exploiting
and refining this knowledge should allow us to answer queries more
analytically, rather than by reading enormous amounts of raw data. Also,
processing more queries should continuously enhance our knowledge of the
underlying distribution, and hence lead to increasingly faster response times
for future queries.
We call this novel idea---learning from past query answers---Database
Learning. We exploit the principle of maximum entropy to produce answers, which
are in expectation guaranteed to be more accurate than existing sample-based
approximations. Empowered by this idea, we build a query engine on top of Spark
SQL, called Verdict. We conduct extensive experiments on real-world query
traces from a large customer of a major database vendor. Our results
demonstrate that Verdict supports 73.7% of these queries, speeding them up by
up to 23.0x for the same accuracy level compared to existing AQP systems.
",1,0,0,0,0,0
3129,On Data-Dependent Random Features for Improved Generalization in Supervised Learning,"  The randomized-feature approach has been successfully employed in large-scale
kernel approximation and supervised learning. The distribution from which the
random features are drawn impacts the number of features required to
efficiently perform a learning task. Recently, it has been shown that employing
data-dependent randomization improves the performance in terms of the required
number of random features. In this paper, we are concerned with the
randomized-feature approach in supervised learning for good generalizability.
We propose the Energy-based Exploration of Random Features (EERF) algorithm
based on a data-dependent score function that explores the set of possible
features and exploits the promising regions. We prove that the proposed score
function with high probability recovers the spectrum of the best fit within the
model class. Our empirical results on several benchmark datasets further verify
that our method requires smaller number of random features to achieve a certain
generalization error compared to the state-of-the-art while introducing
negligible pre-processing overhead. EERF can be implemented in a few lines of
code and requires no additional tuning parameters.
",1,0,0,1,0,0
10765,Compile-Time Extensions to Hybrid ODEs,"  Reachability analysis for hybrid systems is an active area of development and
has resulted in many promising prototype tools. Most of these tools allow users
to express hybrid system as automata with a set of ordinary differential
equations (ODEs) associated with each state, as well as rules for transitions
between states. Significant effort goes into developing and verifying and
correctly implementing those tools. As such, it is desirable to expand the
scope of applicability tools of such as far as possible. With this goal, we
show how compile-time transformations can be used to extend the basic hybrid
ODE formalism traditionally supported in hybrid reachability tools such as
SpaceEx or Flow*. The extension supports certain types of partial derivatives
and equational constraints. These extensions allow users to express, among
other things, the Euler-Lagrangian equation, and to capture practically
relevant constraints that arise naturally in mechanical systems. Achieving this
level of expressiveness requires using a binding time-analysis (BTA), program
differentiation, symbolic Gaussian elimination, and abstract interpretation
using interval analysis. Except for BTA, the other components are either
readily available or can be easily added to most reachability tools. The paper
therefore focuses on presenting both the declarative and algorithmic
specifications for the BTA phase, and establishes the soundness of the
algorithmic specifications with respect to the declarative one.
",1,0,0,0,0,0
19304,Normalized Maximum Likelihood with Luckiness for Multivariate Normal Distributions,"  The normalized maximum likelihood (NML) is one of the most important
distribution in coding theory and statistics. NML is the unique solution (if
exists) to the pointwise minimax regret problem. However, NML is not defined
even for simple family of distributions such as the normal distributions. Since
there does not exist any meaningful minimax-regret distribution for such case,
it is pointed out that NML with luckiness (LNML) can be employed as an
alternative to NML. In this paper, we develop the closed form of LNMLs for
multivariate normal distributions.
",0,0,1,1,0,0
2520,Method for Computationally Efficient Design of Dielectric Laser Accelerators,"  Dielectric microstructures have generated much interest in recent years as a
means of accelerating charged particles when powered by solid state lasers. The
acceleration gradient (or particle energy gain per unit length) is an important
figure of merit. To design structures with high acceleration gradients, we
explore the adjoint variable method, a highly efficient technique used to
compute the sensitivity of an objective with respect to a large number of
parameters. With this formalism, the sensitivity of the acceleration gradient
of a dielectric structure with respect to its entire spatial permittivity
distribution is calculated by the use of only two full-field electromagnetic
simulations, the original and adjoint. The adjoint simulation corresponds
physically to the reciprocal situation of a point charge moving through the
accelerator gap and radiating. Using this formalism, we perform numerical
optimizations aimed at maximizing acceleration gradients, which generate
fabricable structures of greatly improved performance in comparison to
previously examined geometries.
",0,1,0,0,0,0
9252,Complexity Dichotomies for the Minimum F-Overlay Problem,"  For a (possibly infinite) fixed family of graphs F, we say that a graph G
overlays F on a hypergraph H if V(H) is equal to V(G) and the subgraph of G
induced by every hyperedge of H contains some member of F as a spanning
subgraph.While it is easy to see that the complete graph on |V(H)| overlays F
on a hypergraph H whenever the problem admits a solution, the Minimum F-Overlay
problem asks for such a graph with the minimum number of edges.This problem
allows to generalize some natural problems which may arise in practice. For
instance, if the family F contains all connected graphs, then Minimum F-Overlay
corresponds to the Minimum Connectivity Inference problem (also known as Subset
Interconnection Design problem) introduced for the low-resolution
reconstruction of macro-molecular assembly in structural biology, or for the
design of networks.Our main contribution is a strong dichotomy result regarding
the polynomial vs. NP-hard status with respect to the considered family F.
Roughly speaking, we show that the easy cases one can think of (e.g. when
edgeless graphs of the right sizes are in F, or if F contains only cliques) are
the only families giving rise to a polynomial problem: all others are
NP-complete.We then investigate the parameterized complexity of the problem and
give similar sufficient conditions on F that give rise to W[1]-hard, W[2]-hard
or FPT problems when the parameter is the size of the solution.This yields an
FPT/W[1]-hard dichotomy for a relaxed problem, where every hyperedge of H must
contain some member of F as a (non necessarily spanning) subgraph.
",1,0,0,0,0,0
16644,Long range scattering for nonlinear Schrödinger equations with critical homogeneous nonlinearity in three space dimensions,"  In this paper, we consider the final state problem for the nonlinear
Schrödinger equation with a homogeneous nonlinearity of the critical order
which is not necessarily a polynomial. In [10], the first and the second
authors consider one- and two-dimensional cases and gave a sufficient condition
on the nonlinearity for that the corresponding equation admits a solution that
behaves like a free solution with or without a logarithmic phase correction.
The present paper is devoted to the study of the three-dimensional case, in
which it is required that a solution converges to a given asymptotic profile in
a faster rate than in the lower dimensional cases. To obtain the necessary
convergence rate, we employ the end-point Strichartz estimate and modify a
time-dependent regularizing operator, introduced in [10]. Moreover, we present
a candidate of the second asymptotic profile to the solution.
",0,0,1,0,0,0
3923,Stable and unstable vortex knots in a trapped Bose-Einstein condensate,"  The dynamics of a quantum vortex torus knot ${\cal T}_{P,Q}$ and similar
knots in an atomic Bose-Einstein condensate at zero temperature in the
Thomas-Fermi regime has been considered in the hydrodynamic approximation. The
condensate has a spatially nonuniform equilibrium density profile $\rho(z,r)$
due to an external axisymmetric potential. It is assumed that $z_*=0$, $r_*=1$
is a maximum point for function $r\rho(z,r)$, with $\delta
(r\rho)\approx-(\alpha-\epsilon) z^2/2 -(\alpha+\epsilon) (\delta r)^2/2$ at
small $z$ and $\delta r$. Configuration of knot in the cylindrical coordinates
is specified by a complex $2\pi P$-periodic function
$A(\varphi,t)=Z(\varphi,t)+i [R(\varphi,t)-1]$. In the case $|A|\ll 1$ the
system is described by relatively simple approximate equations for re-scaled
functions $W_n(\varphi)\propto A(2\pi n+\varphi)$, where $n=0,\dots,P-1$, and
$iW_{n,t}=-(W_{n,\varphi\varphi}+\alpha W_n -\epsilon W_n^*)/2-\sum_{j\neq
n}1/(W_n^*-W_j^*)$. At $\epsilon=0$, numerical examples of stable solutions as
$W_n=\theta_n(\varphi-\gamma t)\exp(-i\omega t)$ with non-trivial topology have
been found for $P=3$. Besides that, dynamics of various non-stationary knots
with $P=3$ was simulated, and in some cases a tendency towards a finite-time
singularity has been detected. For $P=2$ at small $\epsilon\neq 0$, rotating
around $z$ axis configurations of the form $(W_0-W_1)\approx
B_0\exp(i\zeta)+\epsilon C(B_0,\alpha)\exp(-i\zeta) + \epsilon
D(B_0,\alpha)\exp(3i\zeta)$ have been investigated, where $B_0>0$ is an
arbitrary constant, $\zeta=k_0\varphi -\Omega_0 t+\zeta_0$, $k_0=Q/2$,
$\Omega_0=(k_0^2-\alpha)/2-2/B_0^2$. In the parameter space $(\alpha, B_0)$,
wide stability regions for such solutions have been found. In unstable bands, a
recurrence of the vortex knot to a weakly excited state has been noted to be
possible.
",0,1,0,0,0,0
6406,A Decision Procedure for Herbrand Formulae without Skolemization,"  This paper describes a decision procedure for disjunctions of conjunctions of
anti-prenex normal forms of pure first-order logic (FOLDNFs) that do not
contain $\vee$ within the scope of quantifiers. The disjuncts of these FOLDNFs
are equivalent to prenex normal forms whose quantifier-free parts are
conjunctions of atomic and negated atomic formulae (= Herbrand formulae). In
contrast to the usual algorithms for Herbrand formulae, neither skolemization
nor unification algorithms with function symbols are applied. Instead, a
procedure is described that rests on nothing but equivalence transformations
within pure first-order logic (FOL). This procedure involves the application of
a calculus for negative normal forms (the NNF-calculus) with $A \dashv\vdash A
\wedge A$ (= $\wedge$I) as the sole rule that increases the complexity of given
FOLDNFs. The described algorithm illustrates how, in the case of Herbrand
formulae, decision problems can be solved through a systematic search for
proofs that reduce the number of applications of the rule $\wedge$I to a
minimum in the NNF-calculus. In the case of Herbrand formulae, it is even
possible to entirely abstain from applying $\wedge$I. Finally, it is shown how
the described procedure can be used within an optimized general search for
proofs of contradiction and what kind of questions arise for a
$\wedge$I-minimal proof strategy in the case of a general search for proofs of
contradiction.
",1,0,1,0,0,0
12691,Viconmavlink: A software tool for indoor positioning using a motion capture system,"  Motion capture is a widely-used technology in robotics research thanks to its
precise posi tional measurements with real-time performance. This paper
presents ViconMAVLink, a cross-platform open-source software tool that provides
indoor positioning services to networked robots. ViconMAVLink converts Vicon
motion capture data into proper pose and motion data formats and send
localization information to robots using the MAVLink protocol. The software is
a convenient tool for mobile robotics researchers to conduct experiments in a
controlled indoor environment.
",1,0,0,0,0,0
7899,The quest for H$_3^+$ at Neptune: deep burn observations with NASA IRTF iSHELL,"  Emission from the molecular ion H$_3^+$ is a powerful diagnostic of the upper
atmosphere of Jupiter, Saturn, and Uranus, but it remains undetected at
Neptune. In search of this emission, we present near-infrared spectral
observations of Neptune between 3.93 and 4.00 $\mu$m taken with the newly
commissioned iSHELL instrument on the NASA Infrared Telescope Facility in
Hawaii, obtained 17-20 August 2017. We spent 15.4 h integrating across the disk
of the planet, yet were unable to unambiguously identify any H$_3^+$ line
emissions. Assuming a temperature of 550 K, we derive an upper limit on the
column integrated density of $1.0^{+1.2}_{-0.8}\times10^{13}$ m$^{-2}$, which
is an improvement of 30\% on the best previous observational constraint. This
result means that models are over-estimating the density by at least a factor
of 5, highlighting the need for renewed modelling efforts. A potential solution
is strong vertical mixing of polyatomic neutral species from Neptune's upper
stratosphere to the thermosphere, reacting with H$_3^+$, thus greatly reducing
the column integrated H$_3^+$ densities. This upper limit also provide
constraints on future attempts at detecting H$_3^+$ using the James Webb Space
Telescope.
",0,1,0,0,0,0
8597,Convergence rates for nonequilibrium Langevin dynamics,"  We study the exponential convergence to the stationary state for
nonequilibrium Langevin dynamics, by a perturbative approach based on
hypocoercive techniques developed for equilibrium Langevin dynamics. The
Hamiltonian and overdamped limits (corresponding respectively to frictions
going to zero or infinity) are carefully investigated. In particular, the
maximal magnitude of admissible perturbations are quantified as a function of
the friction. Numerical results based on a Galerkin discretization of the
generator of the dynamics confirm the theoretical lower bounds on the spectral
gap.
",0,1,1,0,0,0
3959,Doubly Accelerated Stochastic Variance Reduced Dual Averaging Method for Regularized Empirical Risk Minimization,"  In this paper, we develop a new accelerated stochastic gradient method for
efficiently solving the convex regularized empirical risk minimization problem
in mini-batch settings. The use of mini-batches is becoming a golden standard
in the machine learning community, because mini-batch settings stabilize the
gradient estimate and can easily make good use of parallel computing. The core
of our proposed method is the incorporation of our new ""double acceleration""
technique and variance reduction technique. We theoretically analyze our
proposed method and show that our method much improves the mini-batch
efficiencies of previous accelerated stochastic methods, and essentially only
needs size $\sqrt{n}$ mini-batches for achieving the optimal iteration
complexities for both non-strongly and strongly convex objectives, where $n$ is
the training set size. Further, we show that even in non-mini-batch settings,
our method achieves the best known convergence rate for both non-strongly and
strongly convex objectives.
",1,0,1,1,0,0
13527,Motivic modular forms from equivariant stable homotopy theory,"  In this paper, we produce a cellular motivic spectrum of motivic modular
forms over $\R$ and $\C$, answering positively to a conjecture of Dan Isaksen.
This spectrum is constructed to have the appropriate cohomology, as a module
over the relevant motivic Steenrod algebra. We first produce a $\G$-equivariant
version of this spectrum, and then use a machinery to construct a motivic
spectrum from an equivariant one. We believe that this machinery will be of
independent interest.
",0,0,1,0,0,0
11499,Statistical Challenges in Modeling Big Brain Signals,"  Brain signal data are inherently big: massive in amount, complex in
structure, and high in dimensions. These characteristics impose great
challenges for statistical inference and learning. Here we review several key
challenges, discuss possible solutions, and highlight future research
directions.
",0,0,0,1,0,0
12537,Understanding Web Archiving Services and Their (Mis)Use on Social Media,"  Web archiving services play an increasingly important role in today's
information ecosystem, by ensuring the continuing availability of information,
or by deliberately caching content that might get deleted or removed. Among
these, the Wayback Machine has been proactively archiving, since 2001, versions
of a large number of Web pages, while newer services like archive.is allow
users to create on-demand snapshots of specific Web pages, which serve as time
capsules that can be shared across the Web. In this paper, we present a
large-scale analysis of Web archiving services and their use on social media,
shedding light on the actors involved in this ecosystem, the content that gets
archived, and how it is shared. We crawl and study: 1) 21M URLs from
archive.is, spanning almost two years, and 2) 356K archive.is plus 391K Wayback
Machine URLs that were shared on four social networks: Reddit, Twitter, Gab,
and 4chan's Politically Incorrect board (/pol/) over 14 months. We observe that
news and social media posts are the most common types of content archived,
likely due to their perceived ephemeral and/or controversial nature. Moreover,
URLs of archiving services are extensively shared on ""fringe"" communities
within Reddit and 4chan to preserve possibly contentious content. Lastly, we
find evidence of moderators nudging or even forcing users to use archives,
instead of direct links, for news sources with opposing ideologies, potentially
depriving them of ad revenue.
",1,0,0,0,0,0
14242,Integral Chow motives of threefolds with $K$-motives of unit type,"  We prove that if a smooth projective algebraic variety of dimension less or
equal to three has a unit type integral $K$-motive, then its integral Chow
motive is of Lefschetz type. As a consequence, the integral Chow motive is of
Lefschetz type for a smooth projective variety of dimension less or equal to
three that admits a full exceptional collection.
",0,0,1,0,0,0
7947,Nonasymptotic estimation and support recovery for high dimensional sparse covariance matrices,"  We propose a general framework for nonasymptotic covariance matrix estimation
making use of concentration inequality-based confidence sets. We specify this
framework for the estimation of large sparse covariance matrices through
incorporation of past thresholding estimators with key emphasis on support
recovery. This technique goes beyond past results for thresholding estimators
by allowing for a wide range of distributional assumptions beyond merely
sub-Gaussian tails. This methodology can furthermore be adapted to a wide range
of other estimators and settings. The usage of nonasymptotic dimension-free
confidence sets yields good theoretical performance. Through extensive
simulations, it is demonstrated to have superior performance when compared with
other such methods. In the context of support recovery, we are able to specify
a false positive rate and optimize to maximize the true recoveries.
",0,0,1,1,0,0
15595,Current-driven skyrmion dynamics in disordered films,"  A theoretical study of the current-driven dynamics of magnetic skyrmions in
disordered perpendicularly-magnetized ultrathin films is presented. The
disorder is simulated as a granular structure in which the local anisotropy
varies randomly from grain to grain. The skyrmion velocity is computed for
different disorder parameters and ensembles. Similar behavior is seen for
spin-torques due to in-plane currents and the spin Hall effect, where a pinning
regime can be identified at low currents with a transition towards the
disorder-free case at higher currents, similar to domain wall motion in
disordered films. Moreover, a current-dependent skyrmion Hall effect and
fluctuations in the core radius are found, which result from the interaction
with the pinning potential.
",0,1,0,0,0,0
18104,Shiba Bound States across the mobility edge in doped InAs nanowires,"  We present a study of Andreev Quantum Dots (QDots) fabricated with
small-diameter (30 nm) Si-doped InAs nanowires where the Fermi level can be
tuned across a mobility edge separating localized states from delocalized
states. The transition to the insulating phase is identified by a drop in the
amplitude and width of the excited levels and is found to have remarkable
consequences on the spectrum of superconducting SubGap Resonances (SGRs). While
at deeply localized levels, only quasiparticles co-tunneling is observed, for
slightly delocalized levels, Shiba bound states form and a parity changing
quantum phase transition is identified by a crossing of the bound states at
zero energy. Finally, in the metallic regime, single Andreev resonances are
observed.
",0,1,0,0,0,0
4897,Far-from-equilibrium transport of excited carriers in nanostructures,"  Transport of charged carriers in regimes of strong non-equilibrium is
critical in a wide array of applications ranging from solar energy conversion
and semiconductor devices to quantum information. Plasmonic hot-carrier science
brings this regime of transport physics to the forefront since photo-excited
carriers must be extracted far from equilibrium to harvest their energy
efficiently. Here, we present a theoretical and computational framework,
Non-Equilibrium Scattering in Space and Energy (NESSE), to predict the spatial
evolution of carrier energy distributions that combines the best features of
phase-space (Boltzmann) and particle-based (Monte Carlo) methods. Within the
NESSE framework, we bridge first-principles electronic structure predictions of
plasmon decay and carrier collision integrals at the atomic scale, with
electromagnetic field simulations at the nano- to mesoscale. Finally, we apply
NESSE to predict spatially-resolved energy distributions of photo-excited
carriers that impact the surface of experimentally realizable plasmonic
nanostructures, enabling first-principles design of hot carrier devices.
",0,1,0,0,0,0
2042,Chaotic Dynamic S Boxes Based Substitution Approach for Digital Images,"  In this paper, we propose an image encryption algorithm based on the chaos,
substitution boxes, nonlinear transformation in Galois field and Latin square.
Initially, the dynamic S boxes are generated using Fisher Yates shuffle method
and piece wise linear chaotic map. The algorithm utilizes advantages of keyed
Latin square and transformation to substitute highly correlated digital images
and yield encrypted image with valued performance. The chaotic behavior is
achieved using Logistic map which is used to select one of thousand S boxes and
also decides the row and column of selected S box. The selected S box value is
transformed using nonlinear transformation. Along with the keyed Latin square
generated using a 256 bit external key, used to substitute secretly plain image
pixels in cipher block chaining mode. To further strengthen the security of
algorithm, round operation are applied to obtain final ciphered image. The
experimental results are performed to evaluate algorithm and the anticipated
algorithm is compared with a recent encryption scheme. The analyses demonstrate
algorithms effectiveness in providing high security to digital media.
",1,0,0,0,0,0
13038,Hilbert Bases and Lecture Hall Partitions,"  In the interest of finding the minimum additive generating set for the set of
$\boldsymbol{s}$-lecture hall partitions, we compute the Hilbert bases for the
$\boldsymbol{s}$-lecture hall cones in certain cases. In particular, we compute
the Hilbert bases for two well-studied families of sequences, namely the $1\mod
k$ sequences and the $\ell$-sequences. Additionally, we provide a
characterization of the Hilbert bases for $\boldsymbol{u}$-generated Gorenstein
$\boldsymbol{s}$-lecture hall cones in low dimensions.
",0,0,1,0,0,0
8442,Local-ring network automata and the impact of hyperbolic geometry in complex network link-prediction,"  Topological link-prediction can exploit the entire network topology (global
methods) or only the neighbourhood (local methods) of the link to predict.
Global methods are believed the best. Is this common belief well-founded?
Stochastic-Block-Model (SBM) is a global method believed as one of the best
link-predictors, therefore it is considered a reference for comparison. But,
our results suggest that SBM, whose computational time is high, cannot in
general overcome the Cannistraci-Hebb (CH) network automaton model that is a
simple local-learning-rule of topological self-organization proved as the
current best local-based and parameter-free deterministic rule for
link-prediction. To elucidate the reasons of this unexpected result, we
formally introduce the notion of local-ring network automata models and their
relation with the nature of common-neighbours' definition in complex network
theory. After extensive tests, we recommend Structural-Perturbation-Method
(SPM) as the new best global method baseline. However, even SPM overall does
not outperform CH and in several evaluation frameworks we astonishingly found
the opposite. In particular, CH was the best predictor for synthetic networks
generated by the Popularity-Similarity-Optimization (PSO) model, and its
performance in PSO networks with community structure was even better than using
the original internode-hyperbolic-distance as link-predictor. Interestingly,
when tested on non-hyperbolic synthetic networks the performance of CH
significantly dropped down indicating that this rule of network
self-organization could be strongly associated to the rise of hyperbolic
geometry in complex networks. The superiority of global methods seems a
""misleading belief"" caused by a latent geometry bias of the few small networks
used as benchmark in previous studies. We propose to found a latent geometry
theory of link-prediction in complex networks.
",1,0,0,0,0,0
15842,Advertising and Brand Attitudes: Evidence from 575 Brands over Five Years,"  Little is known about how different types of advertising affect brand
attitudes. We investigate the relationships between three brand attitude
variables (perceived quality, perceived value and recent satisfaction) and
three types of advertising (national traditional, local traditional and
digital). The data represent ten million brand attitude surveys and $264
billion spent on ads by 575 regular advertisers over a five-year period,
approximately 37% of all ad spend measured between 2008 and 2012. Inclusion of
brand/quarter fixed effects and industry/week fixed effects brings parameter
estimates closer to expectations without major reductions in estimation
precision. The findings indicate that (i) national traditional ads increase
perceived quality, perceived value, and recent satisfaction; (ii) local
traditional ads increase perceived quality and perceived value; (iii) digital
ads increase perceived value; and (iv) competitor ad effects are generally
negative.
",0,0,0,0,0,1
5712,Shape optimization in laminar flow with a label-guided variational autoencoder,"  Computational design optimization in fluid dynamics usually requires to solve
non-linear partial differential equations numerically. In this work, we explore
a Bayesian optimization approach to minimize an object's drag coefficient in
laminar flow based on predicting drag directly from the object shape. Jointly
training an architecture combining a variational autoencoder mapping shapes to
latent representations and Gaussian process regression allows us to generate
improved shapes in the two dimensional case we consider.
",1,0,0,0,0,0
13837,BPjs --- a framework for modeling reactive systems using a scripting language and BP,"  We describe some progress towards a new common framework for model driven
engineering, based on behavioral programming. The tool we have developed
unifies almost all of the work done in behavioral programming so far, under a
common set of interfaces. Its architecture supports pluggable event selection
strategies, which can make models more intuitive and compact. Program state
space can be traversed using various algorithms, such as DFS and A*.
Furthermore, program state is represented in a way that enables scanning a
state space using parallel and distributed algorithms. Executable models
created with this tool can be directly embedded in Java applications, enabling
a model-first approach to system engineering, where initially a model is
created and verified, and then a working application is gradually built around
the model. The model itself consists of a collection of small scripts written
in JavaScript (hence ""BPjs""). Using a variety of case-studies, this paper shows
how the combination of a lenient programming language with formal model
analysis tools creates an efficient way of developing robust complex systems.
Additionally, as we learned from an experimental course we ran, the usage of
JavaScript make practitioners more amenable to using this system and, thus,
model checking and model driven engineering. In addition to providing
infrastructure for development and case-studies in behavioral programming, the
tool is designed to serve as a common platform for research and innovation in
behavioral programming and in model driven engineering in general.
",1,0,0,0,0,0
7027,Non-asymptotic theory for nonparametric testing,"  We consider nonparametric testing in a non-asymptotic framework. Our
statistical guarantees are exact in the sense that Type I and II errors are
controlled for any finite sample size. Meanwhile, one proposed test is shown to
achieve minimax optimality in the asymptotic sense. An important consequence of
this non-asymptotic theory is a new and practically useful formula for
selecting the optimal smoothing parameter in nonparametric testing. The leading
example in this paper is smoothing spline models under Gaussian errors. The
results obtained therein can be further generalized to the kernel ridge
regression framework under possibly non-Gaussian errors. Simulations
demonstrate that our proposed test improves over the conventional asymptotic
test when sample size is small to moderate.
",0,0,1,1,0,0
11300,Mosquito detection with low-cost smartphones: data acquisition for malaria research,"  Mosquitoes are a major vector for malaria, causing hundreds of thousands of
deaths in the developing world each year. Not only is the prevention of
mosquito bites of paramount importance to the reduction of malaria transmission
cases, but understanding in more forensic detail the interplay between malaria,
mosquito vectors, vegetation, standing water and human populations is crucial
to the deployment of more effective interventions. Typically the presence and
detection of malaria-vectoring mosquitoes is only quantified by hand-operated
insect traps or signified by the diagnosis of malaria. If we are to gather
timely, large-scale data to improve this situation, we need to automate the
process of mosquito detection and classification as much as possible. In this
paper, we present a candidate mobile sensing system that acts as both a
portable early warning device and an automatic acoustic data acquisition
pipeline to help fuel scientific inquiry and policy. The machine learning
algorithm that powers the mobile system achieves excellent off-line
multi-species detection performance while remaining computationally efficient.
Further, we have conducted preliminary live mosquito detection tests using
low-cost mobile phones and achieved promising results. The deployment of this
system for field usage in Southeast Asia and Africa is planned in the near
future. In order to accelerate processing of field recordings and labelling of
collected data, we employ a citizen science platform in conjunction with
automated methods, the former implemented using the Zooniverse platform,
allowing crowdsourcing on a grand scale.
",1,0,0,1,0,0
2623,Multirole Logic (Extended Abstract),"  We identify multirole logic as a new form of logic in which
conjunction/disjunction is interpreted as an ultrafilter on the power set of
some underlying set (of roles) and the notion of negation is generalized to
endomorphisms on this underlying set. We formalize both multirole logic (MRL)
and linear multirole logic (LMRL) as natural generalizations of classical logic
(CL) and classical linear logic (CLL), respectively, and also present a
filter-based interpretation for intuitionism in multirole logic. Among various
meta-properties established for MRL and LMRL, we obtain one named multiparty
cut-elimination stating that every cut involving one or more sequents (as a
generalization of a (binary) cut involving exactly two sequents) can be
eliminated, thus extending the celebrated result of cut-elimination by Gentzen.
",1,0,1,0,0,0
12539,Training Deep Networks without Learning Rates Through Coin Betting,"  Deep learning methods achieve state-of-the-art performance in many
application scenarios. Yet, these methods require a significant amount of
hyperparameters tuning in order to achieve the best results. In particular,
tuning the learning rates in the stochastic optimization process is still one
of the main bottlenecks. In this paper, we propose a new stochastic gradient
descent procedure for deep networks that does not require any learning rate
setting. Contrary to previous methods, we do not adapt the learning rates nor
we make use of the assumed curvature of the objective function. Instead, we
reduce the optimization process to a game of betting on a coin and propose a
learning-rate-free optimal algorithm for this scenario. Theoretical convergence
is proven for convex and quasi-convex functions and empirical evidence shows
the advantage of our algorithm over popular stochastic gradient algorithms.
",1,0,1,1,0,0
3114,Towards Neural Co-Processors for the Brain: Combining Decoding and Encoding in Brain-Computer Interfaces,"  The field of brain-computer interfaces is poised to advance from the
traditional goal of controlling prosthetic devices using brain signals to
combining neural decoding and encoding within a single neuroprosthetic device.
Such a device acts as a ""co-processor"" for the brain, with applications ranging
from inducing Hebbian plasticity for rehabilitation after brain injury to
reanimating paralyzed limbs and enhancing memory. We review recent progress in
simultaneous decoding and encoding for closed-loop control and plasticity
induction. To address the challenge of multi-channel decoding and encoding, we
introduce a unifying framework for developing brain co-processors based on
artificial neural networks and deep learning. These ""neural co-processors"" can
be used to jointly optimize cost functions with the nervous system to achieve
desired behaviors ranging from targeted neuro-rehabilitation to augmentation of
brain function.
",0,0,0,0,1,0
14976,Parameter Estimation for Thurstone Choice Models,"  We consider the estimation accuracy of individual strength parameters of a
Thurstone choice model when each input observation consists of a choice of one
item from a set of two or more items (so called top-1 lists). This model
accommodates the well-known choice models such as the Luce choice model for
comparison sets of two or more items and the Bradley-Terry model for pair
comparisons.
We provide a tight characterization of the mean squared error of the maximum
likelihood parameter estimator. We also provide similar characterizations for
parameter estimators defined by a rank-breaking method, which amounts to
deducing one or more pair comparisons from a comparison of two or more items,
assuming independence of these pair comparisons, and maximizing a likelihood
function derived under these assumptions. We also consider a related binary
classification problem where each individual parameter takes value from a set
of two possible values and the goal is to correctly classify all items within a
prescribed classification error.
",0,0,1,1,0,0
18549,Introduction to compact and discrete quantum groups,"  These are notes from introductory lectures at the graduate school
""Topological Quantum Groups"" in Będlewo (June 28--July 11, 2015). The notes
present the passage from Hopf algebras to compact quantum groups and sketch the
notion of discrete quantum groups viewed as duals of compact quantum groups.
",0,0,1,0,0,0
13339,Optimal DoF region of the K-User MISO BC with Partial CSIT,"  We consider the $K$-User Multiple-Input-Single-Output (MISO) Broadcast
Channel (BC) where the transmitter, equipped with $M$ antennas, serves $K$
users, with $K \leq M$. The transmitter has access to a partial channel state
information of the users. This is modelled by letting the variance of the
Channel State Information at the Transmitter (CSIT) error of user $i$ scale as
$O(P^{-\alpha_i}$) for the Signal-to-Noise Ratio (SNR) $P$ and some constant
$\alpha_i \geq 0$. In this work we derive the optimal Degrees-of-Freedom (DoF)
region in such setting and we show that Rate-Splitting (RS) is the key scheme
to achieve such a region.
",1,0,0,0,0,0
13219,Small Telescope Exoplanet Transit Surveys: XO,"  The XO project aims at detecting transiting exoplanets around bright stars
from the ground using small telescopes. The original configuration of XO
(McCullough et al. 2005) has been changed and extended as described here. The
instrumental setup consists of three identical units located at different
sites, each composed of two lenses equipped with CCD cameras mounted on the
same mount. We observed two strips of the sky covering an area of 520 deg$^2$
for twice nine months. We build lightcurves for ~20,000 stars up to magnitude
R~12.5 using a custom-made photometric data reduction pipeline. The photometric
precision is around 1-2% for most stars, and the large quantity of data allows
us to reach a millimagnitude precision when folding the lightcurves on
timescales that are relevant to exoplanetary transits. We search for periodic
signals and identify several hundreds of variable stars and a few tens of
transiting planet candidates. Follow-up observations are underway to confirm or
reject these candidates. We found two close-in gas giant planets so far, in
line with the expected yield.
",0,1,0,0,0,0
3486,FeaStNet: Feature-Steered Graph Convolutions for 3D Shape Analysis,"  Convolutional neural networks (CNNs) have massively impacted visual
recognition in 2D images, and are now ubiquitous in state-of-the-art
approaches. CNNs do not easily extend, however, to data that are not
represented by regular grids, such as 3D shape meshes or other graph-structured
data, to which traditional local convolution operators do not directly apply.
To address this problem, we propose a novel graph-convolution operator to
establish correspondences between filter weights and graph neighborhoods with
arbitrary connectivity. The key novelty of our approach is that these
correspondences are dynamically computed from features learned by the network,
rather than relying on predefined static coordinates over the graph as in
previous work. We obtain excellent experimental results that significantly
improve over previous state-of-the-art shape correspondence results. This shows
that our approach can learn effective shape representations from raw input
coordinates, without relying on shape descriptors.
",1,0,0,0,0,0
8916,Themis-ml: A Fairness-aware Machine Learning Interface for End-to-end Discrimination Discovery and Mitigation,"  As more industries integrate machine learning into socially sensitive
decision processes like hiring, loan-approval, and parole-granting, we are at
risk of perpetuating historical and contemporary socioeconomic disparities.
This is a critical problem because on the one hand, organizations who use but
do not understand the discriminatory potential of such systems will facilitate
the widening of social disparities under the assumption that algorithms are
categorically objective. On the other hand, the responsible use of machine
learning can help us measure, understand, and mitigate the implicit historical
biases in socially sensitive data by expressing implicit decision-making mental
models in terms of explicit statistical models. In this paper we specify,
implement, and evaluate a ""fairness-aware"" machine learning interface called
themis-ml, which is intended for use by individual data scientists and
engineers, academic research teams, or larger product teams who use machine
learning in production systems.
",1,0,0,0,0,0
10982,Unifying Map and Landmark Based Representations for Visual Navigation,"  This works presents a formulation for visual navigation that unifies map
based spatial reasoning and path planning, with landmark based robust plan
execution in noisy environments. Our proposed formulation is learned from data
and is thus able to leverage statistical regularities of the world. This allows
it to efficiently navigate in novel environments given only a sparse set of
registered images as input for building representations for space. Our
formulation is based on three key ideas: a learned path planner that outputs
path plans to reach the goal, a feature synthesis engine that predicts features
for locations along the planned path, and a learned goal-driven closed loop
controller that can follow plans given these synthesized features. We test our
approach for goal-driven navigation in simulated real world environments and
report performance gains over competitive baseline approaches.
",1,0,0,0,0,0
2725,Hyperfine state entanglement of spinor BEC and scattering atom,"  Condensate of spin-1 atoms frozen in a unique spatial mode may possess large
internal degrees of freedom. The scattering amplitudes of polarized cold atoms
scattered by the condensate are obtained with the method of fractional
parentage coefficients that treats the spin degrees of freedom rigorously.
Channels with scattering cross sections enhanced by square of atom number of
the condensate are found. Entanglement between the condensate and the
propagating atom can be established by the scattering. The entanglement entropy
is analytically obtained for arbitrary initial states. Our results also give
hint for the establishment of quantum thermal ensembles in the hyperfine space.
",0,1,0,0,0,0
4150,Independent Set Size Approximation in Graph Streams,"  We study the problem of estimating the size of independent sets in a graph
$G$ defined by a stream of edges. Our approach relies on the Caro-Wei bound,
which expresses the desired quantity in terms of a sum over nodes of the
reciprocal of their degrees, denoted by $\beta(G)$. Our results show that
$\beta(G)$ can be approximated accurately, based on a provided lower bound on
$\beta$. Stronger results are possible when the edges are promised to arrive
grouped by an incident node. In this setting, we obtain a value that is at most
a logarithmic factor below the true value of $\beta$ and no more than the true
independent set size. To justify the form of this bound, we also show an
$\Omega(n/\beta)$ lower bound on any algorithm that approximates $\beta$ up to
a constant factor.
",1,0,0,0,0,0
18745,Improving phase II oncology trials using best observed RECIST response as an endpoint by modelling continuous tumour measurements,"  In many phase II trials in solid tumours, patients are assessed using
endpoints based on the Response Evaluation Criteria in Solid Tumours (RECIST)
scale. Often, analyses are based on the response rate. This is the proportion
of patients who have an observed tumour shrinkage above a pre-defined level and
no new tumour lesions. The augmented binary method has been proposed to improve
the precision of the estimator of the response rate. The method involves
modelling the tumour shrinkage to avoid dichotomising it. However, in many
trials the best observed response is used as the primary outcome. In such
trials, patients are followed until progression, and their best observed RECIST
outcome is used as the primary endpoint. In this paper, we propose a method
that extends the augmented binary method so that it can be used when the
outcome is best observed response. We show through simulated data and data from
a real phase II cancer trial that this method improves power in both single-arm
and randomised trials. The average gain in power compared to the traditional
analysis is equivalent to approximately a 35% increase in sample size. A
modified version of the method is proposed to reduce the computational effort
required. We show this modified method maintains much of the efficiency
advantages.
",0,0,0,1,0,0
20117,A Multi-frequency analysis of possible Dark Matter Contributions to M31 Gamma-Ray Emissions,"  We examine the possibility of a dark matter (DM) contribution to the recently
observed gamma-ray spectrum seen in the M31 galaxy. In particular, we apply
limits on Weakly Interacting Massive Particle DM annihilation cross-sections
derived from the Coma galaxy cluster and the Reticulum II dwarf galaxy to
determine the maximal flux contribution by DM annihilation to both the M31
gamma-ray spectrum and that of the Milky-Way galactic centre. We limit the
energy range between 1 and 12 GeV in M31 and galactic centre spectra due to the
limited range of former's data, as well as to encompass the high-energy
gamma-ray excess observed in the latter target. In so doing, we will make use
of Fermi-LAT data for all mentioned targets, as well as diffuse radio data for
the Coma cluster. The multi-target strategy using both Coma and Reticulum II to
derive cross-section limits, as well as multi-frequency data, ensures that our
results are robust against the various uncertainties inherent in modelling of
indirect DM emissions.
Our results indicate that, when a Navarro-Frenk-White (or shallower) radial
density profile is assumed, severe constraints can be imposed upon the fraction
of the M31 and galactic centre spectra that can be accounted for by DM, with
the best limits arising from cross-section constraints from Coma radio data and
Reticulum II gamma-ray limits. These particular limits force all the studied
annihilation channels to contribute 1% or less to the total integrated
gamma-ray flux within both M31 and galactic centre targets. In contrast,
considerably more, 10-100%, of the flux can be attributed to DM when a
contracted Navarro-Frenk-White profile is assumed. This demonstrates how
sensitive DM contributions to gamma-ray emissions are to the possibility of
cored profiles in galaxies.
",0,1,0,0,0,0
14609,A shared latent space matrix factorisation method for recommending new trial evidence for systematic review updates,"  Clinical trial registries can be used to monitor the production of trial
evidence and signal when systematic reviews become out of date. However, this
use has been limited to date due to the extensive manual review required to
search for and screen relevant trial registrations. Our aim was to evaluate a
new method that could partially automate the identification of trial
registrations that may be relevant for systematic review updates. We identified
179 systematic reviews of drug interventions for type 2 diabetes, which
included 537 clinical trials that had registrations in ClinicalTrials.gov. We
tested a matrix factorisation approach that uses a shared latent space to learn
how to rank relevant trial registrations for each systematic review, comparing
the performance to document similarity to rank relevant trial registrations.
The two approaches were tested on a holdout set of the newest trials from the
set of type 2 diabetes systematic reviews and an unseen set of 141 clinical
trial registrations from 17 updated systematic reviews published in the
Cochrane Database of Systematic Reviews. The matrix factorisation approach
outperformed the document similarity approach with a median rank of 59 and
recall@100 of 60.9%, compared to a median rank of 138 and recall@100 of 42.8%
in the document similarity baseline. In the second set of systematic reviews
and their updates, the highest performing approach used document similarity and
gave a median rank of 67 (recall@100 of 62.9%). The proposed method was useful
for ranking trial registrations to reduce the manual workload associated with
finding relevant trials for systematic review updates. The results suggest that
the approach could be used as part of a semi-automated pipeline for monitoring
potentially new evidence for inclusion in a review update.
",1,0,0,0,0,0
6435,PIMKL: Pathway Induced Multiple Kernel Learning,"  Reliable identification of molecular biomarkers is essential for accurate
patient stratification. While state-of-the-art machine learning approaches for
sample classification continue to push boundaries in terms of performance, most
of these methods are not able to integrate different data types and lack
generalization power, limiting their application in a clinical setting.
Furthermore, many methods behave as black boxes, and we have very little
understanding about the mechanisms that lead to the prediction. While
opaqueness concerning machine behaviour might not be a problem in deterministic
domains, in health care, providing explanations about the molecular factors and
phenotypes that are driving the classification is crucial to build trust in the
performance of the predictive system. We propose Pathway Induced Multiple
Kernel Learning (PIMKL), a novel methodology to reliably classify samples that
can also help gain insights into the molecular mechanisms that underlie the
classification. PIMKL exploits prior knowledge in the form of a molecular
interaction network and annotated gene sets, by optimizing a mixture of
pathway-induced kernels using a Multiple Kernel Learning (MKL) algorithm, an
approach that has demonstrated excellent performance in different machine
learning applications. After optimizing the combination of kernels for
prediction of a specific phenotype, the model provides a stable molecular
signature that can be interpreted in the light of the ingested prior knowledge
and that can be used in transfer learning tasks.
",0,0,0,1,1,0
12632,Impossibility results on stability of phylogenetic consensus methods,"  We answer two questions raised by Bryant, Francis and Steel in their work on
consensus methods in phylogenetics. Consensus methods apply to every practical
instance where it is desired to aggregate a set of given phylogenetic trees
(say, gene evolution trees) into a resulting, ""consensus"" tree (say, a species
tree). Various stability criteria have been explored in this context, seeking
to model desirable consistency properties of consensus methods as the
experimental data is updated (e.g., more taxa, or more trees, are mapped).
However, such stability conditions can be incompatible with some basic
regularity properties that are widely accepted to be essential in any
meaningful consensus method. Here, we prove that such an incompatibility does
arise in the case of extension stability on binary trees and in the case of
associative stability. Our methods combine general theoretical considerations
with the use of computer programs tailored to the given stability requirements.
",0,0,0,0,1,0
3474,An Incremental Slicing Method for Functional Programs,"  Several applications of slicing require a program to be sliced with respect
to more than one slicing criterion. Program specialization, parallelization and
cohesion measurement are examples of such applications. These applications can
benefit from an incremental static slicing method in which a significant extent
of the computations for slicing with respect to one criterion could be reused
for another. In this paper, we consider the problem of incremental slicing of
functional programs. We first present a non-incremental version of the slicing
algorithm which does a polyvariant analysis 1 of functions. Since polyvariant
analyses tend to be costly, we compute a compact context-independent summary of
each function and then use this summary at the call sites of the function. The
construction of the function summary is non-trivial and helps in the
development of the incremental version. The incremental method, on the other
hand, consists of a one-time pre-computation step that uses the non-incremental
version to slice the program with respect to a fixed default slicing criterion
and processes the results further to a canonical form. Presented with an actual
slicing criterion, the incremental step involves a low-cost computation that
uses the results of the pre-computation to obtain the slice. We have
implemented a prototype of the slicer for a pure subset of Scheme, with pairs
and lists as the only algebraic data types. Our experiments show that the
incremental step of the slicer runs orders of magnitude faster than the
non-incremental version. We have also proved the correctness of our incremental
algorithm with respect to the non-incremental version.
",1,0,0,0,0,0
16165,Learning Navigation Behaviors End to End,"  A longstanding goal of behavior-based robotics is to solve high-level
navigation tasks using end to end navigation behaviors that directly map
sensors to actions. Navigation behaviors, such as reaching a goal or following
a path without collisions, can be learned from exploration and interaction with
the environment, but are constrained by the type and quality of a robot's
sensors, dynamics, and actuators. Traditional motion planning handles varied
robot geometry and dynamics, but typically assumes high-quality observations.
Modern vision-based navigation typically considers imperfect or partial
observations, but simplifies the robot action space. With both approaches, the
transition from simulation to reality can be difficult. Here, we learn two end
to end navigation behaviors that avoid moving obstacles: point to point and
path following. These policies receive noisy lidar observations and output
robot linear and angular velocities. We train these policies in small, static
environments with Shaped-DDPG, an adaptation of the Deep Deterministic Policy
Gradient (DDPG) reinforcement learning method which optimizes reward and
network architecture. Over 500 meters of on-robot experiments show , these
policies generalize to new environments and moving obstacles, are robust to
sensor, actuator, and localization noise, and can serve as robust building
blocks for larger navigation tasks. The path following and point and point
policies are 83% and 56% more successful than the baseline, respectively.
",1,0,0,0,0,0
5447,Contagion dynamics of extremist propaganda in social networks,"  Recent terrorist attacks carried out on behalf of ISIS on American and
European soil by lone wolf attackers or sleeper cells remind us of the
importance of understanding the dynamics of radicalization mediated by social
media communication channels. In this paper, we shed light on the social media
activity of a group of twenty-five thousand users whose association with ISIS
online radical propaganda has been manually verified. By using a computational
tool known as dynamical activity-connectivity maps, based on network and
temporal activity patterns, we investigate the dynamics of social influence
within ISIS supporters. We finally quantify the effectiveness of ISIS
propaganda by determining the adoption of extremist content in the general
population and draw a parallel between radical propaganda and epidemics
spreading, highlighting that information broadcasters and influential ISIS
supporters generate highly-infectious cascades of information contagion. Our
findings will help generate effective countermeasures to combat the group and
other forms of online extremism.
",1,1,0,0,0,0
17001,Miraculous cancellations for quantum $SL_2$,"  In earlier work, Helen Wong and the author discovered certain ""miraculous
cancellations"" for the quantum trace map connecting the Kauffman bracket skein
algebra of a surface to its quantum Teichmueller space, occurring when the
quantum parameter $q$ is a root of unity. The current paper is devoted to
giving a more representation theoretic interpretation of this phenomenon, in
terms of the quantum group $U_q(sl_2)$ and its dual Hopf algebra $SL_2^q$.
",0,0,1,0,0,0
7135,Data-driven Probabilistic Atlases Capture Whole-brain Individual Variation,"  Probabilistic atlases provide essential spatial contextual information for
image interpretation, Bayesian modeling, and algorithmic processing. Such
atlases are typically constructed by grouping subjects with similar demographic
information. Importantly, use of the same scanner minimizes inter-group
variability. However, generalizability and spatial specificity of such
approaches is more limited than one might like. Inspired by Commowick
""Frankenstein's creature paradigm"" which builds a personal specific anatomical
atlas, we propose a data-driven framework to build a personal specific
probabilistic atlas under the large-scale data scheme. The data-driven
framework clusters regions with similar features using a point distribution
model to learn different anatomical phenotypes. Regional structural atlases and
corresponding regional probabilistic atlases are used as indices and targets in
the dictionary. By indexing the dictionary, the whole brain probabilistic
atlases adapt to each new subject quickly and can be used as spatial priors for
visualization and processing. The novelties of this approach are (1) it
provides a new perspective of generating personal specific whole brain
probabilistic atlases (132 regions) under data-driven scheme across sites. (2)
The framework employs the large amount of heterogeneous data (2349 images). (3)
The proposed framework achieves low computational cost since only one affine
registration and Pearson correlation operation are required for a new subject.
Our method matches individual regions better with higher Dice similarity value
when testing the probabilistic atlases. Importantly, the advantage the
large-scale scheme is demonstrated by the better performance of using
large-scale training data (1888 images) than smaller training set (720 images).
",0,0,0,1,1,0
18857,Hypergames and Cyber-Physical Security for Control Systems,"  The identification of the Stuxnet worm in 2010 provided a highly publicized
example of a cyber attack used to damage an industrial control system
physically. This raised public awareness about the possibility of similar
attacks against other industrial targets -- including critical infrastructure.
In this paper, we use hypergames to analyze how adversarial perturbations can
be used to manipulate a system using optimal control. Hypergames form an
extension of game theory that enables us to model strategic interactions where
the players may have significantly different perceptions of the game(s) they
are playing. Past work with hypergames has been limited to relatively simple
interactions consisting of a small set of discrete choices for each player, but
here, we apply hypergames to larger systems with continuous variables. We find
that manipulating constraints can be a more effective attacker strategy than
directly manipulating objective function parameters. Moreover, the attacker
need not change the underlying system to carry out a successful attack -- it
may be sufficient to deceive the defender controlling the system. It is
possible to scale our approach up to even larger systems, but the ability to do
so will depend on the characteristics of the system in question, and we
identify several characteristics that will make those systems amenable to
hypergame analysis.
",1,0,0,0,0,0
13959,Power series expansions for the planar monomer-dimer problem,"  We compute the free energy of the planar monomer-dimer model. Unlike the
classical planar dimer model, an exact solution is not known in this case. Even
the computation of the low-density power series expansion requires heavy and
nontrivial computations. Despite of the exponential computational complexity,
we compute almost three times more terms than were previously known. Such an
expansion provides both lower and upper bound for the free energy, and allows
to obtain more accurate numerical values than previously possible. We expect
that our methods can be applied to other similar problems.
",1,0,0,0,0,0
12344,Compressing Green's function using intermediate representation between imaginary-time and real-frequency domains,"  New model-independent compact representations of imaginary-time data are
presented in terms of the intermediate representation (IR) of analytical
continuation. This is motivated by a recent numerical finding by the authors
[J. Otsuki et al., arXiv:1702.03056]. We demonstrate the efficiency of the IR
through continuous-time quantum Monte Carlo calculations of an Anderson
impurity model. We find that the IR yields a significantly compact form of
various types of correlation functions. The present framework will provide
general ways to boost the power of cutting-edge diagrammatic/quantum Monte
Carlo treatments of many-body systems.
",0,1,0,1,0,0
3683,English-Japanese Neural Machine Translation with Encoder-Decoder-Reconstructor,"  Neural machine translation (NMT) has recently become popular in the field of
machine translation. However, NMT suffers from the problem of repeating or
missing words in the translation. To address this problem, Tu et al. (2017)
proposed an encoder-decoder-reconstructor framework for NMT using
back-translation. In this method, they selected the best forward translation
model in the same manner as Bahdanau et al. (2015), and then trained a
bi-directional translation model as fine-tuning. Their experiments show that it
offers significant improvement in BLEU scores in Chinese-English translation
task. We confirm that our re-implementation also shows the same tendency and
alleviates the problem of repeating and missing words in the translation on a
English-Japanese task too. In addition, we evaluate the effectiveness of
pre-training by comparing it with a jointly-trained model of forward
translation and back-translation.
",1,0,0,0,0,0
2362,Harmonic density interpolation methods for high-order evaluation of Laplace layer potentials in 2D and 3D,"  We present an effective harmonic density interpolation method for the
numerical evaluation of singular and nearly singular Laplace boundary integral
operators and layer potentials in two and three spatial dimensions. The method
relies on the use of Green's third identity and local Taylor-like
interpolations of density functions in terms of harmonic polynomials. The
proposed technique effectively regularizes the singularities present in
boundary integral operators and layer potentials, and recasts the latter in
terms of integrands that are bounded or even more regular, depending on the
order of the density interpolation. The resulting boundary integrals can then
be easily, accurately, and inexpensively evaluated by means of standard
quadrature rules. A variety of numerical examples demonstrate the effectiveness
of the technique when used in conjunction with the classical trapezoidal rule
(to integrate over smooth curves) in two-dimensions, and with a Chebyshev-type
quadrature rule (to integrate over surfaces given as unions of non-overlapping
quadrilateral patches) in three-dimensions.
",0,1,0,0,0,0
13865,Power Allocation for Full-Duplex Relay Selection in Underlay Cognitive Radio Networks: Coherent versus Non-Coherent Scenarios,"  This paper investigates power control and relay selection in Full Duplex
Cognitive Relay Networks (FDCRNs), where the secondary-user (SU) relays can
simultaneously receive data from the SU source and forward them to the SU
destination. We study both non-coherent and coherent scenarios. In the
non-coherent case, the SU relay forwards the signal from the SU source without
regulating the phase; while in the coherent scenario, the SU relay regulates
the phase when forwarding the signal to minimize the interference at the
primary-user (PU) receiver. We consider the problem of maximizing the
transmission rate from the SU source to the SU destination subject to the
interference constraint at the PU receiver and power constraints at both the SU
source and SU relay. We then develop a mathematical model to analyze the data
rate performance of the FDCRN considering the self-interference effects at the
FD relay. We develop low-complexity and high-performance joint power control
and relay selection algorithms. Extensive numerical results are presented to
illustrate the impacts of power level parameters and the self-interference
cancellation quality on the rate performance. Moreover, we demonstrate the
significant gain of phase regulation at the SU relay.
",1,0,1,1,0,0
20469,Algebraic entropy of (integrable) lattice equations and their reductions,"  We study the growth of degrees in many autonomous and non-autonomous lattice
equations defined by quad rules with corner boundary values, some of which are
known to be integrable by other characterisations. Subject to an enabling
conjecture, we prove polynomial growth for a large class of equations which
includes the Adler-Bobenko-Suris equations and Viallet's $Q_V$ and its
non-autonomous generalization. Our technique is to determine the ambient degree
growth of the projective version of the lattice equations and to conjecture the
growth of their common factors at each lattice vertex, allowing the true degree
growth to be found. The resulting degrees satisfy a linear partial difference
equation which is universal, i.e. the same for all the integrable lattice
equations considered. When we take periodic reductions of these equations,
which includes staircase initial conditions, we obtain from this linear partial
difference equation an ordinary difference equation for degrees that implies
quadratic or linear degree growth. We also study growth of degree of several
non-integrable lattice equations. Exponential growth of degrees of these
equations, and their mapping reductions, is also proved subject to a
conjecture.
",0,1,0,0,0,0
539,"Characterizing videos, audience and advertising in Youtube channels for kids","  Online video services, messaging systems, games and social media services are
tremendously popular among young people and children in many countries. Most of
the digital services offered on the internet are advertising funded, which
makes advertising ubiquitous in children's everyday life. To understand the
impact of advertising-based digital services on children, we study the
collective behavior of users of YouTube for kids channels and present the
demographics of a large number of users. We collected data from 12,848 videos
from 17 channels in US and UK and 24 channels in Brazil. The channels in
English have been viewed more than 37 billion times. We also collected more
than 14 million comments made by users. Based on a combination of text-analysis
and face recognition tools, we show the presence of racial and gender biases in
our large sample of users. We also identify children actively using YouTube,
although the minimum age for using the service is 13 years in most countries.
We provide comparisons of user behavior among the three countries, which
represent large user populations in the global North and the global South.
",1,0,0,0,0,0
19213,The Effect of Temperature on Cu-K-In-Se Thin Films,"  Films of Cu-K-In-Se were co-evaporated at varied K/(K+Cu) compositions and
substrate temperatures (with constant (K+Cu)/In ~ 0.85). Increased Na
composition on the substrate's surface and decreased growth temperature were
both found to favor Cu1-xKxInSe2 (CKIS) alloy formation, relative to
mixed-phase CuInSe2 + KInSe2 formation. Structures from X-ray diffraction
(XRD), band gaps, resistivities, minority carrier lifetimes and carrier
concentrations from time-resolved photoluminescence were in agreement with
previous reports, where low K/(K+Cu) composition films exhibited properties
promising for photovoltaic (PV) absorbers. Films grown at 400-500 C were then
annealed to 600 C under Se, which caused K loss by evaporation in proportion to
initial K/(K+Cu) composition. Similar to growth temperature, annealing drove
CKIS alloy consumption and CuInSe2 + KInSe2 production, as evidenced by high
temperature XRD. Annealing also decomposed KInSe2 and formed K2In12Se19. At
high temperature the KInSe2 crystal lattice gradually contracted as temperature
and time increased, as well as just time. Evaporative loss of K during
annealing could accompany the generation of vacancies on K lattice sites, and
may explain the KInSe2 lattice contraction. This knowledge of Cu-K-In-Se
material chemistry may be used to predict and control minor phase impurities in
Cu(In,Ga)(Se,S)2 PV absorbers-where impurities below typical detection limits
may have played a role in recent world record PV efficiencies that utilized KF
post-deposition treatments.
",0,1,0,0,0,0
16943,Order preserving pattern matching on trees and DAGs,"  The order preserving pattern matching (OPPM) problem is, given a pattern
string $p$ and a text string $t$, find all substrings of $t$ which have the
same relative orders as $p$. In this paper, we consider two variants of the
OPPM problem where a set of text strings is given as a tree or a DAG. We show
that the OPPM problem for a single pattern $p$ of length $m$ and a text tree
$T$ of size $N$ can be solved in $O(m+N)$ time if the characters of $p$ are
drawn from an integer alphabet of polynomial size. The time complexity becomes
$O(m \log m + N)$ if the pattern $p$ is over a general ordered alphabet. We
then show that the OPPM problem for a single pattern and a text DAG is
NP-complete.
",1,0,0,0,0,0
10910,Optimized Bacteria are Environmental Prediction Engines,"  Experimentalists have observed phenotypic variability in isogenic bacteria
populations. We explore the hypothesis that in fluctuating environments this
variability is tuned to maximize a bacterium's expected log growth rate,
potentially aided by epigenetic markers that store information about past
environments. We show that, in a complex, memoryful environment, the maximal
expected log growth rate is linear in the instantaneous predictive
information---the mutual information between a bacterium's epigenetic markers
and future environmental states. Hence, under resource constraints, optimal
epigenetic markers are causal states---the minimal sufficient statistics for
prediction. This is the minimal amount of information about the past needed to
predict the future as well as possible. We suggest new theoretical
investigations into and new experiments on bacteria phenotypic bet-hedging in
fluctuating complex environments.
",0,0,0,0,1,0
4443,Effects of Images with Different Levels of Familiarity on EEG,"  Evaluating human brain potentials during watching different images can be
used for memory evaluation, information retrieving, guilty-innocent
identification and examining the brain response. In this study, the effects of
watching images, with different levels of familiarity, on subjects'
Electroencephalogram (EEG) have been studied. Three different groups of images
with three familiarity levels of ""unfamiliar"", ""familiar"" and ""very familiar""
have been considered for this study. EEG signals of 21 subjects (14 men) were
recorded. After signal acquisition, pre-processing, including noise and
artifact removal, were performed on epochs of data. Features, including
spatial-statistical, wavelet, frequency and harmonic parameters, and also
correlation between recording channels, were extracted from the data. Then, we
evaluated the efficiency of the extracted features by using p-value and also an
orthogonal feature selection method (combination of Gram-Schmitt method and
Fisher discriminant ratio) for feature dimensional reduction. As the final step
of feature selection, we used 'add-r take-away l' method for choosing the most
discriminative features. For data classification, including all two-class and
three-class cases, we applied Support Vector Machine (SVM) on the extracted
features. The correct classification rates (CCR) for ""unfamiliar-familiar"",
""unfamiliar-very familiar"" and ""familiar-very familiar"" cases were 85.6%,
92.6%, and 70.6%, respectively. The best results of classifications were
obtained in pre-frontal and frontal regions of brain. Also, wavelet, frequency
and harmonic features were among the most discriminative features. Finally, in
three-class case, the best CCR was 86.8%.
",0,0,0,1,0,0
1605,Group chasing tactics: how to catch a faster prey?,"  We propose a bio-inspired, agent-based approach to describe the natural
phenomenon of group chasing in both two and three dimensions. Using a set of
local interaction rules we created a continuous-space and discrete-time model
with time delay, external noise and limited acceleration. We implemented a
unique collective chasing strategy, optimized its parameters and studied its
properties when chasing a much faster, erratic escaper. We show that collective
chasing strategies can significantly enhance the chasers' success rate. Our
realistic approach handles group chasing within closed, soft boundaries -
contrasting most of those published in the literature with periodic ones -- and
resembles several properties of pursuits observed in nature, such as the
emergent encircling or the escaper's zigzag motion.
",0,1,0,1,0,0
12944,Optimization over Degree Sequences,"  We introduce and study the problem of optimizing arbitrary functions over
degree sequences of hypergraphs and multihypergraphs. We show that over
multihypergraphs the problem can be solved in polynomial time. For hypergraphs,
we show that deciding if a given sequence is the degree sequence of a
3-hypergraph is NP-complete, thereby solving a 30 year long open problem. This
implies that optimization over hypergraphs is hard already for simple concave
functions. In contrast, we show that for graphs, if the functions at vertices
are the same, then the problem is polynomial time solvable. We also provide
positive results for convex optimization over multihypergraphs and graphs and
exploit connections to degree sequence polytopes and threshold graphs. We then
elaborate on connections to the emerging theory of shifted combinatorial
optimization.
",1,0,1,0,0,0
4697,Approximate Steepest Coordinate Descent,"  We propose a new selection rule for the coordinate selection in coordinate
descent methods for huge-scale optimization. The efficiency of this novel
scheme is provably better than the efficiency of uniformly random selection,
and can reach the efficiency of steepest coordinate descent (SCD), enabling an
acceleration of a factor of up to $n$, the number of coordinates. In many
practical applications, our scheme can be implemented at no extra cost and
computational efficiency very close to the faster uniform selection. Numerical
experiments with Lasso and Ridge regression show promising improvements, in
line with our theoretical guarantees.
",1,0,1,0,0,0
4141,Particle Identification with the TOP and ARICH detectors at Belle II,"  Particle identification at the Belle II experiment will be provided by two
ring imaging Cherenkov devices, the time of propagation counters in the central
region and the proximity focusing RICH with aerogel radiator in the forward
end-cap region. The key features of these two detectors, the performance
studies, and the construction progress is presented.
",0,1,0,0,0,0
6310,Social Media Would Not Lie: Prediction of the 2016 Taiwan Election via Online Heterogeneous Data,"  The prevalence of online media has attracted researchers from various domains
to explore human behavior and make interesting predictions. In this research,
we leverage heterogeneous social media data collected from various online
platforms to predict Taiwan's 2016 presidential election. In contrast to most
existing research, we take a ""signal"" view of heterogeneous information and
adopt the Kalman filter to fuse multiple signals into daily vote predictions
for the candidates. We also consider events that influenced the election in a
quantitative manner based on the so-called event study model that originated in
the field of financial research. We obtained the following interesting
findings. First, public opinions in online media dominate traditional polls in
Taiwan election prediction in terms of both predictive power and timeliness.
But offline polls can still function on alleviating the sample bias of online
opinions. Second, although online signals converge as election day approaches,
the simple Facebook ""Like"" is consistently the strongest indicator of the
election result. Third, most influential events have a strong connection to
cross-strait relations, and the Chou Tzu-yu flag incident followed by the
apology video one day before the election increased the vote share of Tsai
Ing-Wen by 3.66%. This research justifies the predictive power of online media
in politics and the advantages of information fusion. The combined use of the
Kalman filter and the event study method contributes to the data-driven
political analytics paradigm for both prediction and attribution purposes.
",1,0,0,1,0,0
16356,Empirical Bayes Estimators for High-Dimensional Sparse Vectors,"  The problem of estimating a high-dimensional sparse vector
$\boldsymbol{\theta} \in \mathbb{R}^n$ from an observation in i.i.d. Gaussian
noise is considered. The performance is measured using squared-error loss. An
empirical Bayes shrinkage estimator, derived using a Bernoulli-Gaussian prior,
is analyzed and compared with the well-known soft-thresholding estimator. We
obtain concentration inequalities for the Stein's unbiased risk estimate and
the loss function of both estimators. The results show that for large $n$, both
the risk estimate and the loss function concentrate on deterministic values
close to the true risk.
Depending on the underlying $\boldsymbol{\theta}$, either the proposed
empirical Bayes (eBayes) estimator or soft-thresholding may have smaller loss.
We consider a hybrid estimator that attempts to pick the better of the
soft-thresholding estimator and the eBayes estimator by comparing their risk
estimates. It is shown that: i) the loss of the hybrid estimator concentrates
on the minimum of the losses of the two competing estimators, and ii) the risk
of the hybrid estimator is within order $\frac{1}{\sqrt{n}}$ of the minimum of
the two risks. Simulation results are provided to support the theoretical
results. Finally, we use the eBayes and hybrid estimators as denoisers in the
approximate message passing (AMP) algorithm for compressed sensing, and show
that their performance is superior to the soft-thresholding denoiser in a wide
range of settings.
",0,0,1,1,0,0
4772,A FEL Based on a Superlattice,"  The motion and photon emission of electrons in a superlattice may be
described as in an undulator. Therefore, there is a close analogy between
ballistic electrons in a superlattice and electrons in a free electron laser
(FEL). Touching upon this analogy the intensity of photon emission in the IR
region and the gain are calculated. It is shown that the amplification can be
significant, reaching tens of percent.
",0,1,0,0,0,0
4376,A deep learning-based method for prostate segmentation in T2-weighted magnetic resonance imaging,"  We propose a novel automatic method for accurate segmentation of the prostate
in T2-weighted magnetic resonance imaging (MRI). Our method is based on
convolutional neural networks (CNNs). Because of the large variability in the
shape, size, and appearance of the prostate and the scarcity of annotated
training data, we suggest training two separate CNNs. A global CNN will
determine a prostate bounding box, which is then resampled and sent to a local
CNN for accurate delineation of the prostate boundary. This way, the local CNN
can effectively learn to segment the fine details that distinguish the prostate
from the surrounding tissue using the small amount of available training data.
To fully exploit the training data, we synthesize additional data by deforming
the training images and segmentations using a learned shape model. We apply the
proposed method on the PROMISE12 challenge dataset and achieve state of the art
results. Our proposed method generates accurate, smooth, and artifact-free
segmentations. On the test images, we achieve an average Dice score of 90.6
with a small standard deviation of 2.2, which is superior to all previous
methods. Our two-step segmentation approach and data augmentation strategy may
be highly effective in segmentation of other organs from small amounts of
annotated medical images.
",1,0,0,1,0,0
2798,Image Reconstruction using Matched Wavelet Estimated from Data Sensed Compressively using Partial Canonical Identity Matrix,"  This paper proposes a joint framework wherein lifting-based, separable,
image-matched wavelets are estimated from compressively sensed (CS) images and
used for the reconstruction of the same. Matched wavelet can be easily designed
if full image is available. Also matched wavelet may provide better
reconstruction results in CS application compared to standard wavelet
sparsifying basis. Since in CS application, we have compressively sensed image
instead of full image, existing methods of designing matched wavelet cannot be
used. Thus, we propose a joint framework that estimates matched wavelet from
the compressively sensed images and also reconstructs full images. This paper
has three significant contributions. First, lifting-based, image-matched
separable wavelet is designed from compressively sensed images and is also used
to reconstruct the same. Second, a simple sensing matrix is employed to sample
data at sub-Nyquist rate such that sensing and reconstruction time is reduced
considerably without any noticeable degradation in the reconstruction
performance. Third, a new multi-level L-Pyramid wavelet decomposition strategy
is provided for separable wavelet implementation on images that leads to
improved reconstruction performance. Compared to CS-based reconstruction using
standard wavelets with Gaussian sensing matrix and with existing wavelet
decomposition strategy, the proposed methodology provides faster and better
image reconstruction in compressive sensing application.
",1,0,0,0,0,0
10301,VQABQ: Visual Question Answering by Basic Questions,"  Taking an image and question as the input of our method, it can output the
text-based answer of the query question about the given image, so called Visual
Question Answering (VQA). There are two main modules in our algorithm. Given a
natural language question about an image, the first module takes the question
as input and then outputs the basic questions of the main given question. The
second module takes the main question, image and these basic questions as input
and then outputs the text-based answer of the main question. We formulate the
basic questions generation problem as a LASSO optimization problem, and also
propose a criterion about how to exploit these basic questions to help answer
main question. Our method is evaluated on the challenging VQA dataset and
yields state-of-the-art accuracy, 60.34% in open-ended task.
",1,0,0,0,0,0
14896,Observation of spin superfluidity: YIG magnetic films and beyond,"  From topology of the order parameter of the magnon condensate observed in
yttrium-iron-garnet (YIG) magnetic films one must not expect energetic barriers
making spin supercurrents metastable. But we show that some barriers of
dynamical origin are possible nevertheless until the gradient of the phase
(angle of spin precession) does not exceed the critical value (analog of the
Landau critical velocity in superfluids). On the other hand, recently published
claims of experimental detection of spin superfluidity in YIG films and
antiferromagnets are not justified, and spin superfluidity in magnetically
ordered solids has not yet been experimentally confirmed.
",0,1,0,0,0,0
17294,On Certain Analytical Representations of Cellular Automata,"  We extend a previously introduced semi-analytical representation of a
decomposition of CA dynamics in arbitrary dimensions and neighborhood schemes
via the use of certain universal maps in which CA rule vectors are derivable
from the equivalent of superpotentials. The results justify the search for
alternative analog models of computation and their possible physical
connections.
",0,1,0,0,0,0
20073,Quantitative analysis of the influence of keV He ion bombardment on exchange bias layer systems,"  The mechanism of ion bombardment induced magnetic patterning of exchange bias
layer systems for creating engineered magnetic stray field landscapes is still
unclear. We compare results from vectorial magneto-optic Kerr effect
measurements to a recently proposed model with time dependent rotatable
magnetic anisotropy. Results show massive reduction of rotational magnetic
anisotropy compared to all other magnetic anisotropies. We disprove the
assumption of comparable weakening of all magnetic anisotropies and show that
ion bombardment mainly influences smaller grains in the antiferromagnet.
",0,1,0,0,0,0
15302,Nb3Sn wire shape and cross sectional area inhomogeneity in Rutherford cables,"  During Rutherford cable production the wires are plastically deformed and
their initially round shape is distorted. Using X-ray absorption tomography we
have determined the 3D shape of an unreacted Nb3Sn 11 T dipole Rutherford
cable, and of a reacted and impregnated Nb3Sn cable double stack.
State-of-the-art image processing was applied to correct for tomographic
artefacts caused by the large cable aspect ratio, for the segmentation of the
individual wires and subelement bundles inside the wires, and for the
calculation of the wire cross sectional area and shape variations. The 11 T
dipole cable cross section oscillates by 2% with a frequency of 1.24 mm (1/80
of the transposition pitch length of the 40 wire cable). A comparatively
stronger cross sectional area variation is observed in the individual wires at
the thin edge of the keystoned cable where the wire aspect ratio is largest.
",0,1,0,0,0,0
15113,Psychological and Personality Profiles of Political Extremists,"  Global recruitment into radical Islamic movements has spurred renewed
interest in the appeal of political extremism. Is the appeal a rational
response to material conditions or is it the expression of psychological and
personality disorders associated with aggressive behavior, intolerance,
conspiratorial imagination, and paranoia? Empirical answers using surveys have
been limited by lack of access to extremist groups, while field studies have
lacked psychological measures and failed to compare extremists with contrast
groups. We revisit the debate over the appeal of extremism in the U.S. context
by comparing publicly available Twitter messages written by over 355,000
political extremist followers with messages written by non-extremist U.S.
users. Analysis of text-based psychological indicators supports the moral
foundation theory which identifies emotion as a critical factor in determining
political orientation of individuals. Extremist followers also differ from
others in four of the Big Five personality traits.
",1,1,0,0,0,0
6872,Quivers with potentials for cluster varieties associated to braid semigroups,"  Let $C$ be a simply laced generalized Cartan matrix. Given an element $b$ of
the generalized braid semigroup related to $C$, we construct a collection of
mutation-equivalent quivers with potentials. A quiver with potential in such a
collection corresponds to an expression of $b$ in terms of the standard
generators. For two expressions that differ by a braid relation, the
corresponding quivers with potentials are related by a mutation.
The main application of this result is a construction of a family of $CY_3$
$A_\infty$-categories associated to elements of the braid semigroup related to
$C$. In particular, we construct a canonical up to equivalence $CY_3$
$A_\infty$-category associated to quotient of any Double Bruhat cell
$G^{u,v}/{\rm Ad} H$ in a simply laced reductive Lie group $G$.
We describe the full set of parameters these categories depend on by defining
a 2-dimensional CW-complex and proving that the set of parameters is identified
with second cohomology group of this complex.
",0,0,1,0,0,0
18550,An optimization approach to adaptive multi-dimensional capital management,"  Firms should keep capital to offer sufficient protection against the risks
they are facing. In the insurance context methods have been developed to
determine the minimum capital level required, but less so in the context of
firms with multiple business lines including allocation. The individual capital
reserve of each line can be represented by means of classical models, such as
the conventional Cramér-Lundberg model, but the challenge lies in soundly
modelling the correlations between the business lines. We propose a simple yet
versatile approach that allows for dependence by introducing a common
environmental factor. We present a novel Bayesian approach to calibrate the
latent environmental state distribution based on observations concerning the
claim processes. The calibration approach is adjusted for an environmental
factor that changes over time. The convergence of the calibration procedure
towards the true environmental state is deduced. We then point out how to
determine the optimal initial capital of the different business lines under
specific constraints on the ruin probability of subsets of business lines. Upon
combining the above findings, we have developed an easy-to-implement approach
to capital risk management in a multi-dimensional insurance risk model.
",0,0,0,0,0,1
17758,Neutrino mass and dark energy constraints from redshift-space distortions,"  Cosmology in the near future promises a measurement of the sum of neutrino
masses, a fundamental Standard Model parameter, as well as
substantially-improved constraints on the dark energy. We use the shape of the
BOSS redshift-space galaxy power spectrum, in combination with CMB and
supernova data, to constrain the neutrino masses and the dark energy. Essential
to this calculation are several recent advances in non-linear cosmological
perturbation theory, including FFT methods, redshift space distortions, and
scale-dependent growth. Our 95% confidence upper bound of 200 meV on the sum of
masses degrades substantially to 770 meV when the dark energy equation of state
and its first derivative are also allowed to vary, representing a significant
challenge to current constraints. We also study the impact of additional galaxy
bias parameters, finding that a velocity bias or a more complicated
scale-dependent density bias shift the preferred neutrino mass values 20%-30%
lower while minimally impacting the other cosmological parameters.
",0,1,0,0,0,0
4725,A maximum principle for free boundary minimal varieties of arbitrary codimension,"  We establish a boundary maximum principle for free boundary minimal
submanifolds in a Riemannian manifold with boundary, in any dimension and
codimension. Our result holds more generally in the context of varifolds.
",0,0,1,0,0,0
291,A XGBoost risk model via feature selection and Bayesian hyper-parameter optimization,"  This paper aims to explore models based on the extreme gradient boosting
(XGBoost) approach for business risk classification. Feature selection (FS)
algorithms and hyper-parameter optimizations are simultaneously considered
during model training. The five most commonly used FS methods including weight
by Gini, weight by Chi-square, hierarchical variable clustering, weight by
correlation, and weight by information are applied to alleviate the effect of
redundant features. Two hyper-parameter optimization approaches, random search
(RS) and Bayesian tree-structured Parzen Estimator (TPE), are applied in
XGBoost. The effect of different FS and hyper-parameter optimization methods on
the model performance are investigated by the Wilcoxon Signed Rank Test. The
performance of XGBoost is compared to the traditionally utilized logistic
regression (LR) model in terms of classification accuracy, area under the curve
(AUC), recall, and F1 score obtained from the 10-fold cross validation. Results
show that hierarchical clustering is the optimal FS method for LR while weight
by Chi-square achieves the best performance in XG-Boost. Both TPE and RS
optimization in XGBoost outperform LR significantly. TPE optimization shows a
superiority over RS since it results in a significantly higher accuracy and a
marginally higher AUC, recall and F1 score. Furthermore, XGBoost with TPE
tuning shows a lower variability than the RS method. Finally, the ranking of
feature importance based on XGBoost enhances the model interpretation.
Therefore, XGBoost with Bayesian TPE hyper-parameter optimization serves as an
operative while powerful approach for business risk modeling.
",1,0,0,1,0,0
13632,Decoding the spectroscopic features and timescales of aqueous proton defects,"  Acid solutions exhibit a variety of complex structural and dynamical features
arising from the presence of multiple interacting reactive proton defects and
counterions. However, disentangling the transient structural motifs of proton
defects in the water hydrogen bond network and the mechanisms for their
interconversion remains a formidable challenge. Here, we use simulations
treating the quantum nature of both the electrons and nuclei to show how the
experimentally observed spectroscopic features and relaxation timescales can be
elucidated using a physically transparent coordinate that encodes the overall
asymmetry of the solvation environment of the proton defect. We demonstrate
that this coordinate can be used both to discriminate the extremities of the
features observed in the linear vibrational spectrum and to explain the
molecular motions that give rise to the interconversion timescales observed in
recent nonlinear experiments. This analysis provides a unified condensed-phase
picture of proton structure and dynamics that, at its extrema, encompasses
proton sharing and spectroscopic features resembling the limiting Eigen
[H$_{3}$O(H$_{2}$O)$_{3}$]$^{+}$ and Zundel [H(H$_{2}$O)$_{2}$]$^{+}$ gas-phase
structures, while also describing the rich variety of interconverting
environments in the liquid phase.
",0,1,0,0,0,0
8305,Out-of-time-order Operators and the Butterfly Effect,"  Out-of-time-order (OTO) operators have recently become popular diagnostics of
quantum chaos in many-body systems. The usual way they are introduced is via a
quantization of classical Lyapunov growth, which measures the divergence of
classical trajectories in phase space due to the butterfly effect. However, it
is not obvious how exactly they capture the sensitivity of a quantum system to
its initial conditions beyond the classical limit. In this paper, we analyze
sensitivity to initial conditions in the quantum regime by recasting OTO
operators for many-body systems using various formulations of quantum
mechanics. Notably, we utilize the Wigner phase space formulation to derive an
$\hbar$-expansion of the OTO operator for spatial degrees of freedom, and a
large spin $1/s$-expansion for spin degrees of freedom. We find in each case
that the leading term is the Lyapunov growth for the classical limit of the
system and argue that quantum corrections become dominant at around the
scrambling time, which is also when we expect the OTO operator to saturate. We
also express the OTO operator in terms of propagators and see from a different
point of view how it is a quantum generalization of the divergence of classical
trajectories.
",0,1,0,0,0,0
18689,Machine learning quantum mechanics: solving quantum mechanics problems using radial basis function networks,"  Inspired by the recent work of Carleo and Troyer[1], we apply machine
learning methods to quantum mechanics in this article. The radial basis
function network in a discrete basis is used as the variational wavefunction
for the ground state of a quantum system. Variational Monte Carlo(VMC)
calculations are carried out for some simple Hamiltonians. The results are in
good agreements with theoretical values. The smallest eigenvalue of a Hermitian
matrix can also be acquired using VMC calculations. Our results demonstrate
that machine learning techniques are capable of solving quantum mechanical
problems.
",0,1,0,0,0,0
3489,Separator Reconnection at Earth's Dayside Magnetopause: MMS Observations Compared to Global Simulations,"  We compare a global high resolution resistive magnetohydrodynamics (MHD)
simulation of Earth's magnetosphere with observations from the Magnetospheric
Multiscale (MMS) constellation for a southward IMF magnetopause crossing during
October 16, 2015 that was previously identified as an electron diffusion region
(EDR) event. The simulation predicts a complex time-dependent magnetic topology
consisting of multiple separators and flux ropes. Despite the topological
complexity, the predicted distance between MMS and the primary separator is
less than 0.5 Earth radii. These results suggest that global magnetic topology,
rather than local magnetic geometry alone, determines the location of the
electron diffusion region at the dayside magnetopause.
",0,1,0,0,0,0
12725,Effect of annealing on the magnetic properties of zinc ferrite thin films,"  We report on the magnetic properties of zinc ferrite thin film deposited on
SrTiO$_3$ single crystal using pulsed laser deposition. X-ray diffraction
result indicates the highly oriented single phase growth of the film along with
the presence of the strain. In comparison to the bulk antiferromagnetic order,
the as-deposited film has been found to exhibit ferrimagnetic ordering with a
coercive field of 1140~Oe at 5~K. A broad maximum, at $\approx$105~K, observed
in zero-field cooled magnetization curve indicates the wide grain size
distribution for the as-deposited film. Reduction in magnetization and blocking
temperature has been observed after annealing in both argon as well as oxygen
atmospheres, where the variation was found to be dependent on the annealing
temperature.
",0,1,0,0,0,0
9202,A new computational method for a model of C. elegans biomechanics: Insights into elasticity and locomotion performance,"  An organism's ability to move freely is a fundamental behaviour in the animal
kingdom. To understand animal locomotion requires a characterisation of the
material properties, as well as the biomechanics and physiology. We present a
biomechanical model of C. elegans locomotion together with a novel finite
element method. We formulate our model as a nonlinear initial-boundary value
problem which allows the study of the dynamics of arbitrary body shapes,
undulation gaits and the link between the animal's material properties and its
performance across a range of environments. Our model replicates behaviours
across a wide range of environments. It makes strong predictions on the viable
range of the worm's Young's modulus and suggests that animals can control speed
via the known mechanism of gait modulation that is observed across different
media.
",0,1,0,0,0,0
4741,From Strings to Sets,"  A complete proof is given of relative interpretability of Adjunctive Set
Theory with Extensionality in an elementary concatenation theory.
",0,0,1,0,0,0
12140,Characterization theorems for $Q$-independent random variables with values in a locally compact Abelian group,"  Let $X$ be a locally compact Abelian group, $Y$ be its character group.
Following A. Kagan and G. Székely we introduce a notion of $Q$-independence
for random variables with values in $X$. We prove group analogues of the
Cramér, Kac-Bernstein, Skitovich-Darmois and Heyde theorems for
$Q$-independent random variables with values in $X$. The proofs of these
theorems are reduced to solving some functional equations on the group $Y$.
",0,0,1,0,0,0
17848,Temperature induced phase transition from cycloidal to collinear antiferromagnetism in multiferroic Bi$_{0.9}$Sm$_{0.1}$FeO$_3$ driven by $f$-$d$ induced magnetic anisotropy,"  In multiferroic BiFeO$_3$ a cycloidal antiferromagnetic structure is coupled
to a large electric polarization at room temperature, giving rise to
magnetoelectric functionality that may be exploited in novel multiferroic-based
devices. In this paper, we demonstrate that by substituting samarium for 10% of
the bismuth ions the periodicity of the room temperature cycloid is increased,
and by cooling below $\sim15$ K the magnetic structure tends towards a simple
G-type antiferromagnet, which is fully established at 1.5 K. We show that this
transition results from $f-d$ exchange coupling, which induces a local
anisotropy on the iron magnetic moments that destroys the cycloidal order - a
result of general significance regarding the stability of non-collinear
magnetic structures in the presence of multiple magnetic sublattices.
",0,1,0,0,0,0
7069,Spin - Phonon Coupling in Nickel Oxide Determined from Ultraviolet Raman Spectroscopy,"  Nickel oxide (NiO) has been studied extensively for various applications
ranging from electrochemistry to solar cells [1,2]. In recent years, NiO
attracted much attention as an antiferromagnetic (AF) insulator material for
spintronic devices [3-10]. Understanding the spin - phonon coupling in NiO is a
key to its functionalization, and enabling AF spintronics' promise of
ultra-high-speed and low-power dissipation [11,12]. However, despite its status
as an exemplary AF insulator and a benchmark material for the study of
correlated electron systems, little is known about the spin - phonon
interaction, and the associated energy dissipation channel, in NiO. In
addition, there is a long-standing controversy over the large discrepancies
between the experimental and theoretical values for the electron, phonon, and
magnon energies in NiO [13-23]. This gap in knowledge is explained by NiO
optical selection rules, high Neel temperature and dominance of the magnon band
in the visible Raman spectrum, which precludes a conventional approach for
investigating such interaction. Here we show that by using ultraviolet (UV)
Raman spectroscopy one can extract the spin - phonon coupling coefficients in
NiO. We established that unlike in other materials, the spins of Ni atoms
interact more strongly with the longitudinal optical (LO) phonons than with the
transverse optical (TO) phonons, and produce opposite effects on the phonon
energies. The peculiarities of the spin - phonon coupling are consistent with
the trends given by density functional theory calculations. The obtained
results shed light on the nature of the spin - phonon coupling in AF insulators
and may help in developing innovative spintronic devices.
",0,1,0,0,0,0
1216,Towards a scientific blockchain framework for reproducible data analysis,"  Publishing reproducible analyses is a long-standing and widespread challenge
for the scientific community, funding bodies and publishers. Although a
definitive solution is still elusive, the problem is recognized to affect all
disciplines and lead to a critical system inefficiency. Here, we propose a
blockchain-based approach to enhance scientific reproducibility, with a focus
on life science studies and precision medicine. While the interest of encoding
permanently into an immutable ledger all the study key information-including
endpoints, data and metadata, protocols, analytical methods and all
findings-has been already highlighted, here we apply the blockchain approach to
solve the issue of rewarding time and expertise of scientists that commit to
verify reproducibility. Our mechanism builds a trustless ecosystem of
researchers, funding bodies and publishers cooperating to guarantee digital and
permanent access to information and reproducible results. As a natural
byproduct, a procedure to quantify scientists' and institutions' reputation for
ranking purposes is obtained.
",1,0,0,0,0,0
2413,"Fast Inverse Nonlinear Fourier Transformation using Exponential One-Step Methods, Part I: Darboux Transformation","  This paper considers the non-Hermitian Zakharov-Shabat (ZS) scattering
problem which forms the basis for defining the SU$(2)$-nonlinear Fourier
transformation (NFT). The theoretical underpinnings of this generalization of
the conventional Fourier transformation is quite well established in the
Ablowitz-Kaup-Newell-Segur (AKNS) formalism; however, efficient numerical
algorithms that could be employed in practical applications are still
unavailable.
In this paper, we present a unified framework for the forward and inverse NFT
using exponential one-step methods which are amenable to FFT-based fast
polynomial arithmetic. Within this discrete framework, we propose a fast
Darboux transformation (FDT) algorithm having an operational complexity of
$\mathscr{O}\left(KN+N\log^2N\right)$ such that the error in the computed
$N$-samples of the $K$-soliton vanishes as $\mathscr{O}\left(N^{-p}\right)$
where $p$ is the order of convergence of the underlying one-step method. For
fixed $N$, this algorithm outperforms the the classical DT (CDT) algorithm
which has a complexity of $\mathscr{O}\left(K^2N\right)$. We further present
extension of these algorithms to the general version of DT which allows one to
add solitons to arbitrary profiles that are admissible as scattering potentials
in the ZS-problem. The general CDT/FDT algorithms have the same operational
complexity as that of the $K$-soliton case and the order of convergence matches
that of the underlying one-step method. A comparative study of these algorithms
is presented through exhaustive numerical tests.
",0,1,0,0,0,0
19513,A Deep Learning Based 6 Degree-of-Freedom Localization Method for Endoscopic Capsule Robots,"  We present a robust deep learning based 6 degrees-of-freedom (DoF)
localization system for endoscopic capsule robots. Our system mainly focuses on
localization of endoscopic capsule robots inside the GI tract using only visual
information captured by a mono camera integrated to the robot. The proposed
system is a 23-layer deep convolutional neural network (CNN) that is capable to
estimate the pose of the robot in real time using a standard CPU. The dataset
for the evaluation of the system was recorded inside a surgical human stomach
model with realistic surface texture, softness, and surface liquid properties
so that the pre-trained CNN architecture can be transferred confidently into a
real endoscopic scenario. An average error of 7:1% and 3:4% for translation and
rotation has been obtained, respectively. The results accomplished from the
experiments demonstrate that a CNN pre-trained with raw 2D endoscopic images
performs accurately inside the GI tract and is robust to various challenges
posed by reflection distortions, lens imperfections, vignetting, noise, motion
blur, low resolution, and lack of unique landmarks to track.
",1,0,0,0,0,0
1115,Asynchronous Distributed Variational Gaussian Processes for Regression,"  Gaussian processes (GPs) are powerful non-parametric function estimators.
However, their applications are largely limited by the expensive computational
cost of the inference procedures. Existing stochastic or distributed
synchronous variational inferences, although have alleviated this issue by
scaling up GPs to millions of samples, are still far from satisfactory for
real-world large applications, where the data sizes are often orders of
magnitudes larger, say, billions. To solve this problem, we propose ADVGP, the
first Asynchronous Distributed Variational Gaussian Process inference for
regression, on the recent large-scale machine learning platform,
PARAMETERSERVER. ADVGP uses a novel, flexible variational framework based on a
weight space augmentation, and implements the highly efficient, asynchronous
proximal gradient optimization. While maintaining comparable or better
predictive performance, ADVGP greatly improves upon the efficiency of the
existing variational methods. With ADVGP, we effortlessly scale up GP
regression to a real-world application with billions of samples and demonstrate
an excellent, superior prediction accuracy to the popular linear models.
",0,0,0,1,0,0
11667,A short proof of the middle levels theorem,"  Consider the graph that has as vertices all bitstrings of length $2n+1$ with
exactly $n$ or $n+1$ entries equal to 1, and an edge between any two bitstrings
that differ in exactly one bit. The well-known middle levels conjecture asserts
that this graph has a Hamilton cycle for any $n\geq 1$. In this paper we
present a new proof of this conjecture, which is much shorter and more
accessible than the original proof.
",1,0,0,0,0,0
15697,Competition between disorder and interaction effects in 3D Weyl semimetals,"  We investigate the low-energy scaling behavior of an interacting 3D Weyl
semimetal in the presence of disorder. In order to achieve a renormalization
group analysis of the theory, we focus on the effects of a
short-ranged-correlated disorder potential, checking nevertheless that this
choice is not essential to locate the different phases of the Weyl semimetal.
We show that there is a line of fixed-points in the renormalization group flow
of the interacting theory, corresponding to the disorder-driven transition to a
diffusive metal phase. Along that boundary, the critical disorder strength
undergoes a strong increase with respect to the noninteracting theory, as a
consequence of the unconventional screening of the Coulomb and disorder-induced
interactions. A complementary resolution of the Schwinger-Dyson equations
allows us to determine the full phase diagram of the system, showing the
prevalence of a renormalized semimetallic phase in the regime of intermediate
interaction strength, and adjacent to the non-Fermi liquid phase characteristic
of the strong interaction regime of 3D Weyl semimetals.
",0,1,0,0,0,0
20226,Assembly Bias and Splashback in Galaxy Clusters,"  We use publicly available data for the Millennium Simulation to explore the
implications of the recent detection of assembly bias and splashback signatures
in a large sample of galaxy clusters. These were identified in the SDSS/DR8
photometric data by the redMaPPer algorithm and split into high- and
low-concentration subsamples based on the projected positions of cluster
members. We use simplified versions of these procedures to build cluster
samples of similar size from the simulation data. These match the observed
samples quite well and show similar assembly bias and splashback signals.
Previous theoretical work has found the logarithmic slope of halo density
profiles to have a well-defined minimum whose depth decreases and whose radius
increases with halo concentration. Projected profiles for the observed and
simulated cluster samples show trends with concentration which are opposite to
these predictions. In addition, for high-concentration clusters the minimum
slope occurs at significantly smaller radius than predicted. We show that these
discrepancies all reflect confusion between splashback features and features
imposed on the profiles by the cluster identification and concentration
estimation procedures. The strong apparent assembly bias is not reflected in
the three-dimensional distribution of matter around clusters. Rather it is a
consequence of the preferential contamination of low-concentration clusters by
foreground or background groups.
",0,1,0,0,0,0
16444,Quasi-Static Internal Magnetic Field Detected in the Pseudogap Phase of Bi$_{2+x}$Sr$_{2-x}$CaCu$_2$O$_{8+δ}$ by $μ$SR,"  We report muon spin relaxation ($\mu$SR) measurements of optimally-doped and
overdoped Bi$_{2+x}$Sr$_{2-x}$CaCu$_2$O$_{8+\delta}$ (Bi2212) single crystals
that reveal the presence of a weak temperature-dependent quasi-static internal
magnetic field of electronic origin in the superconducting (SC) and pseudogap
(PG) phases. In both samples the internal magnetic field persists up to 160~K,
but muon diffusion prevents following the evolution of the field to higher
temperatures. We consider the evidence from our measurments in support of PG
order parameter candidates, namely, electronic loop currents and
magnetoelectric quadrupoles.
",0,1,0,0,0,0
5338,NetSciEd: Network Science and Education for the Interconnected World,"  This short article presents a summary of the NetSciEd (Network Science and
Education) initiative that aims to address the need for curricula, resources,
accessible materials, and tools for introducing K-12 students and the general
public to the concept of networks, a crucial framework in understanding
complexity. NetSciEd activities include (1) the NetSci High educational
outreach program (since 2010), which connects high school students and their
teachers with regional university research labs and provides them with the
opportunity to work on network science research projects; (2) the NetSciEd
symposium series (since 2012), which brings network science researchers and
educators together to discuss how network science can help and be integrated
into formal and informal education; and (3) the Network Literacy: Essential
Concepts and Core Ideas booklet (since 2014), which was created collaboratively
and subsequently translated into 18 languages by an extensive group of network
science researchers and educators worldwide.
",1,1,0,0,0,0
20373,The LOFAR window on star-forming galaxies and AGN - curved radio SEDs and IR-radio correlation at $0 < z < 2.5$,"  We present a study of the low-frequency radio properties of star forming (SF)
galaxies and active galactic nuclei (AGN) up to redshift $z=2.5$. The new
spectral window probed by the Low Frequency Array (LOFAR) allows us to
reconstruct the radio continuum emission from 150 MHz to 1.4 GHz to an
unprecedented depth for a radio-selected sample of $1542$ galaxies in $\sim 7~
\rm{deg}^2$ of the LOFAR Boötes field. Using the extensive multi-wavelength
dataset available in Boötes and detailed modelling of the FIR to UV spectral
energy distribution (SED), we are able to separate the star-formation (N=758)
and the AGN (N=784) dominated populations. We study the shape of the radio SEDs
and their evolution across cosmic time and find significant differences in the
spectral curvature between the SF galaxy and AGN populations. While the radio
spectra of SF galaxies exhibit a weak but statistically significant flattening,
AGN SEDs show a clear trend to become steeper towards lower frequencies. No
evolution of the spectral curvature as a function of redshift is found for SF
galaxies or AGN. We investigate the redshift evolution of the infrared-radio
correlation (IRC) for SF galaxies and find that the ratio of total infrared to
1.4 GHz radio luminosities decreases with increasing redshift: $ q_{\rm 1.4GHz}
= (2.45 \pm 0.04) \times (1+z)^{-0.15 \pm 0.03} $. Similarly, $q_{\rm 150MHz}$
shows a redshift evolution following $ q_{\rm 150GHz} = (1.72 \pm 0.04) \times
(1+z)^{-0.22 \pm 0.05}$. Calibration of the 150 MHz radio luminosity as a star
formation rate tracer suggests that a single power-law extrapolation from
$q_{\rm 1.4GHz}$ is not an accurate approximation at all redshifts.
",0,1,0,0,0,0
1625,Multiband Superconductivity in the time reversal symmetry broken superconductor Re6Zr,"  We report point contact Andreev Reflection (PCAR) measurements on a
high-quality single crystal of the non-centrosymmetric superconductor Re6Zr. We
observe that the PCAR spectra can be fitted by taking two isotropic
superconducting gaps with Delta_1 ~ 0.79 meV and Delta_2 ~ 0.22 meV
respectively, suggesting that there are at least two bands which contribute to
superconductivity. Combined with the observation of time reversal symmetry
breaking at the superconducting transition from muon spin relaxation
measurements (Phys. Rev. Lett. 112, 107002 (2014)), our results imply an
unconventional superconducting order in this compound: A multiband singlet
state that breaks time reversal symmetry or a triplet state dominated by
interband pairing.
",0,1,0,0,0,0
13327,Multi-Task Feature Learning for Knowledge Graph Enhanced Recommendation,"  Collaborative filtering often suffers from sparsity and cold start problems
in real recommendation scenarios, therefore, researchers and engineers usually
use side information to address the issues and improve the performance of
recommender systems. In this paper, we consider knowledge graphs as the source
of side information. We propose MKR, a Multi-task feature learning approach for
Knowledge graph enhanced Recommendation. MKR is a deep end-to-end framework
that utilizes knowledge graph embedding task to assist recommendation task. The
two tasks are associated by cross&compress units, which automatically share
latent features and learn high-order interactions between items in recommender
systems and entities in the knowledge graph. We prove that cross&compress units
have sufficient capability of polynomial approximation, and show that MKR is a
generalized framework over several representative methods of recommender
systems and multi-task learning. Through extensive experiments on real-world
datasets, we demonstrate that MKR achieves substantial gains in movie, book,
music, and news recommendation, over state-of-the-art baselines. MKR is also
shown to be able to maintain a decent performance even if user-item
interactions are sparse.
",1,0,0,1,0,0
16271,Disorder Dependent Valley Properties in Monolayer WSe2,"  We investigate the effect on disorder potential on exciton valley
polarization and valley coherence in monolayer WSe2. By analyzing polarization
properties of photoluminescence, the valley coherence (VC) and valley
polarization (VP) is quantified across the inhomogeneously broadened exciton
resonance. We find that disorder plays a critical role in the exciton VC, while
minimally affecting VP. For different monolayer samples with disorder
characterized by their Stokes Shift (SS), VC decreases in samples with higher
SS while VP again remains unchanged. These two methods consistently demonstrate
that VC as defined by the degree of linearly polarized photoluminescence is
more sensitive to disorder potential, motivating further theoretical studies.
",0,1,0,0,0,0
15859,Comparing multiple networks using the Co-expression Differential Network Analysis (CoDiNA),"  Biomedical sciences are increasingly recognising the relevance of gene
co-expression-networks for analysing complex-systems, phenotypes or diseases.
When the goal is investigating complex-phenotypes under varying conditions, it
comes naturally to employ comparative network methods. While approaches for
comparing two networks exist, this is not the case for multiple networks. Here
we present a method for the systematic comparison of an unlimited number of
networks: Co-expression Differential Network Analysis (CoDiNA) for detecting
links and nodes that are common, specific or different to the networks.
Applying CoDiNA to a neurogenesis study identified genes for neuron
differentiation. Experimentally overexpressing one candidate resulted in
significant disturbance in the underlying neurogenesis' gene regulatory
network. We compared data from adults and children with active tuberculosis to
test for signatures of HIV. We also identified common and distinct network
features for particular cancer types with CoDiNA. These studies show that
CoDiNA successfully detects genes associated with the diseases.
",0,0,0,1,1,0
4905,Harnessing functional segregation across brain rhythms as a means to detect EEG oscillatory multiplexing during music listening,"  Music, being a multifaceted stimulus evolving at multiple timescales,
modulates brain function in a manifold way that encompasses not only the
distinct stages of auditory perception but also higher cognitive processes like
memory and appraisal. Network theory is apparently a promising approach to
describe the functional reorganization of brain oscillatory dynamics during
music listening. However, the music induced changes have so far been examined
within the functional boundaries of isolated brain rhythms. Using naturalistic
music, we detected the functional segregation patterns associated with
different cortical rhythms, as these were reflected in the surface EEG
measurements. The emerged structure was compared across frequency bands to
quantify the interplay among rhythms. It was also contrasted against the
structure from the rest and noise listening conditions to reveal the specific
components stemming from music listening. Our methodology includes an efficient
graph-partitioning algorithm, which is further utilized for mining prototypical
modular patterns, and a novel algorithmic procedure for identifying switching
nodes that consistently change module during music listening. Our results
suggest the multiplex character of the music-induced functional reorganization
and particularly indicate the dependence between the networks reconstructed
from the {\delta} and {\beta}H rhythms. This dependence is further justified
within the framework of nested neural oscillations and fits perfectly within
the context of recently introduced cortical entrainment to music. Considering
its computational efficiency, and in conjunction with the flexibility of in
situ electroencephalography, it may lead to novel assistive tools for real-life
applications.
",0,0,0,0,1,0
11882,Multiscale Residual Mixture of PCA: Dynamic Dictionaries for Optimal Basis Learning,"  In this paper we are interested in the problem of learning an over-complete
basis and a methodology such that the reconstruction or inverse problem does
not need optimization. We analyze the optimality of the presented approaches,
their link to popular already known techniques s.a. Artificial Neural
Networks,k-means or Oja's learning rule. Finally, we will see that one approach
to reach the optimal dictionary is a factorial and hierarchical approach. The
derived approach lead to a formulation of a Deep Oja Network. We present
results on different tasks and present the resulting very efficient learning
algorithm which brings a new vision on the training of deep nets. Finally, the
theoretical work shows that deep frameworks are one way to efficiently have
over-complete (combinatorially large) dictionary yet allowing easy
reconstruction. We thus present the Deep Residual Oja Network (DRON). We
demonstrate that a recursive deep approach working on the residuals allow
exponential decrease of the error w.r.t. the depth.
",1,0,0,1,0,0
8744,Analysing Temporal Evolution of Interlingual Wikipedia Article Pairs,"  Wikipedia articles representing an entity or a topic in different language
editions evolve independently within the scope of the language-specific user
communities. This can lead to different points of views reflected in the
articles, as well as complementary and inconsistent information. An analysis of
how the information is propagated across the Wikipedia language editions can
provide important insights in the article evolution along the temporal and
cultural dimensions and support quality control. To facilitate such analysis,
we present MultiWiki - a novel web-based user interface that provides an
overview of the similarities and differences across the article pairs
originating from different language editions on a timeline. MultiWiki enables
users to observe the changes in the interlingual article similarity over time
and to perform a detailed visual comparison of the article snapshots at a
particular time point.
",1,0,0,0,0,0
6429,"Structure, magnetic susceptibility and specific heat of the spin-orbital-liquid candidate FeSc2S4 : Influence of fe off-stoichiometry","  We report structural, susceptibility and specific heat studies of
stoichiometric and off-stoichiometric poly- and single crystals of the A-site
spinel compound FeSc2S4. In stoichiometric samples no long-range magnetic order
is found down to 1.8 K. The magnetic susceptibility of these samples is field
independent in the temperature range 10 - 400 K and does not show irreversible
effects at low temperatures. In contrast, the magnetic susceptibility of
samples with iron excess shows substantial field dependence at high
temperatures and manifests a pronounced magnetic irreversibility at low
temperatures with a difference between ZFC and FC susceptibilities and a
maximum at 10 K reminiscent of a magnetic transition. Single crystal x-ray
diffraction of the stoichiometric samples revealed a single phase spinel
structure without site inversion. In single crystalline samples with Fe excess
besides the main spinel phase a second ordered single-crystal phase was
detected with the diffraction pattern of a vacancy-ordered superstructure of
iron sulfide, close to the 5C polytype Fe9S10. Specific heat studies reveal a
broad anomaly, which evolves below 20 K in both stoichiometric and
off-stoichiometric crystals. We show that the low-temperature specific heat can
be well described by considering the low-lying spin-orbital electronic levels
of Fe2+ ions. Our results demonstrate significant influence of excess Fe ions
on intrinsic magnetic behavior of FeSc2S4 and provide support for the
spin-orbital liquid scenario proposed in earlier studies for the stoichiometric
compound.
",0,1,0,0,0,0
11093,Predict Responsibly: Improving Fairness and Accuracy by Learning to Defer,"  In many machine learning applications, there are multiple decision-makers
involved, both automated and human. The interaction between these agents often
goes unaddressed in algorithmic development. In this work, we explore a simple
version of this interaction with a two-stage framework containing an automated
model and an external decision-maker. The model can choose to say ""Pass"", and
pass the decision downstream, as explored in rejection learning. We extend this
concept by proposing ""learning to defer"", which generalizes rejection learning
by considering the effect of other agents in the decision-making process. We
propose a learning algorithm which accounts for potential biases held by
external decision-makers in a system. Experiments demonstrate that learning to
defer can make systems not only more accurate but also less biased. Even when
working with inconsistent or biased users, we show that deferring models still
greatly improve the accuracy and/or fairness of the entire system.
",1,0,0,1,0,0
17262,Adaptive Real-Time Software Defined MIMO Visible Light Communications using Spatial Multiplexing and Spatial Diversity,"  In this paper, we experimentally demonstrate a real-time software defined
multiple input multiple output (MIMO) visible light communication (VLC) system
employing link adaptation of spatial multiplexing and spatial diversity.
Real-time MIMO signal processing is implemented by using the Field Programmable
Gate Array (FPGA) based Universal Software Radio Peripheral (USRP) devices.
Software defined implantation of MIMO VLC can assist in enabling an adaptive
and reconfigurable communication system without hardware changes. We measured
the error vector magnitude (EVM), bit error rate (BER) and spectral efficiency
performance for single carrier M-QAM MIMO VLC using spatial diversity and
spatial multiplexing. Results show that spatial diversity MIMO VLC improves
error performance at the cost of spectral efficiency that spatial multiplexing
should enhance. We propose the adaptive MIMO solution that both modulation
schema and MIMO schema are dynamically adapted to the changing channel
conditions for enhancing the error performance and spectral efficiency. The
average error-free spectral efficiency of adaptive 2x2 MIMO VLC achieved 12
b/s/Hz over 2 meters indoor dynamic transmission.
",1,0,1,0,0,0
14978,Uniqueness and radial symmetry of minimizers for a nonlocal variational problem,"  In this paper we prove the uniqueness and radial symmetry of minimizers for
variational problems that model several phenomena. The uniqueness is a
consequence of the convexity of the functional. The main technique is Fourier
transform of tempered distributions.
",0,0,1,0,0,0
1863,The ellipse law: Kirchhoff meets dislocations,"  In this paper we consider a nonlocal energy $I_\alpha$ whose kernel is
obtained by adding to the Coulomb potential an anisotropic term weighted by a
parameter $\alpha\in \R$. The case $\alpha=0$ corresponds to purely logarithmic
interactions, minimised by the celebrated circle law for a quadratic
confinement; $\alpha=1$ corresponds to the energy of interacting dislocations,
minimised by the semi-circle law. We show that for $\alpha\in (0,1)$ the
minimiser can be computed explicitly and is the normalised characteristic
function of the domain enclosed by an \emph{ellipse}. To prove our result we
borrow techniques from fluid dynamics, in particular those related to
Kirchhoff's celebrated result that domains enclosed by ellipses are rotating
vortex patches, called \emph{Kirchhoff ellipses}. Therefore we show a
surprising connection between vortices and dislocations.
",0,0,1,0,0,0
13813,Adaptive Multilevel Monte Carlo Approximation of Distribution Functions,"  We analyse a multilevel Monte Carlo method for the approximation of
distribution functions of univariate random variables. Since, by assumption,
the target distribution is not known explicitly, approximations have to be
used. We provide an asymptotic analysis of the error and the cost of the
algorithm. Furthermore we construct an adaptive version of the algorithm that
does not require any a priori knowledge on weak or strong convergence rates. We
apply the adaptive algorithm to smooth path-independent and path-dependent
functionals and to stopped exit times of SDEs.
",0,0,1,1,0,0
14709,Zeroth-Order Online Alternating Direction Method of Multipliers: Convergence Analysis and Applications,"  In this paper, we design and analyze a new zeroth-order online algorithm,
namely, the zeroth-order online alternating direction method of multipliers
(ZOO-ADMM), which enjoys dual advantages of being gradient-free operation and
employing the ADMM to accommodate complex structured regularizers. Compared to
the first-order gradient-based online algorithm, we show that ZOO-ADMM requires
$\sqrt{m}$ times more iterations, leading to a convergence rate of
$O(\sqrt{m}/\sqrt{T})$, where $m$ is the number of optimization variables, and
$T$ is the number of iterations. To accelerate ZOO-ADMM, we propose two
minibatch strategies: gradient sample averaging and observation averaging,
resulting in an improved convergence rate of $O(\sqrt{1+q^{-1}m}/\sqrt{T})$,
where $q$ is the minibatch size. In addition to convergence analysis, we also
demonstrate ZOO-ADMM to applications in signal processing, statistics, and
machine learning.
",1,0,0,1,0,0
16963,Automatic Generation of Typographic Font from a Small Font Subset,"  This paper addresses the automatic generation of a typographic font from a
subset of characters. Specifically, we use a subset of a typographic font to
extrapolate additional characters. Consequently, we obtain a complete font
containing a number of characters sufficient for daily use. The automated
generation of Japanese fonts is in high demand because a Japanese font requires
over 1,000 characters. Unfortunately, professional typographers create most
fonts, resulting in significant financial and time investments for font
generation. The proposed method can be a great aid for font creation because
designers do not need to create the majority of the characters for a new font.
The proposed method uses strokes from given samples for font generation. The
strokes, from which we construct characters, are extracted by exploiting a
character skeleton dataset. This study makes three main contributions: a novel
method of extracting strokes from characters, which is applicable to both
standard fonts and their variations; a fully automated approach for
constructing characters; and a selection method for sample characters. We
demonstrate our proposed method by generating 2,965 characters in 47 fonts.
Objective and subjective evaluations verify that the generated characters are
similar to handmade characters.
",1,0,0,0,0,0
2085,Out-of-Sample Testing for GANs,"  We propose a new method to evaluate GANs, namely EvalGAN. EvalGAN relies on a
test set to directly measure the reconstruction quality in the original sample
space (no auxiliary networks are necessary), and it also computes the
(log)likelihood for the reconstructed samples in the test set. Further, EvalGAN
is agnostic to the GAN algorithm and the dataset. We decided to test it on
three state-of-the-art GANs over the well-known CIFAR-10 and CelebA datasets.
",1,0,0,1,0,0
4775,Efficient Dense Labeling of Human Activity Sequences from Wearables using Fully Convolutional Networks,"  Recognizing human activities in a sequence is a challenging area of research
in ubiquitous computing. Most approaches use a fixed size sliding window over
consecutive samples to extract features---either handcrafted or learned
features---and predict a single label for all samples in the window. Two key
problems emanate from this approach: i) the samples in one window may not
always share the same label. Consequently, using one label for all samples
within a window inevitably lead to loss of information; ii) the testing phase
is constrained by the window size selected during training while the best
window size is difficult to tune in practice. We propose an efficient algorithm
that can predict the label of each sample, which we call dense labeling, in a
sequence of human activities of arbitrary length using a fully convolutional
network. In particular, our approach overcomes the problems posed by the
sliding window step. Additionally, our algorithm learns both the features and
classifier automatically. We release a new daily activity dataset based on a
wearable sensor with hospitalized patients. We conduct extensive experiments
and demonstrate that our proposed approach is able to outperform the
state-of-the-arts in terms of classification and label misalignment measures on
three challenging datasets: Opportunity, Hand Gesture, and our new dataset.
",1,0,0,0,0,0
13875,"The earliest phases of high-mass star formation, as seen in NGC 6334 by \emph{Herschel}","  To constrain models of high-mass star formation, the Herschel/HOBYS KP aims
at discovering massive dense cores (MDCs) able to host the high-mass analogs of
low-mass prestellar cores, which have been searched for over the past decade.
We here focus on NGC6334, one of the best-studied HOBYS molecular cloud
complexes.
We used Herschel PACS and SPIRE 70-500mu images of the NGC6334 complex
complemented with (sub)millimeter and mid-infrared data. We built a complete
procedure to extract ~0.1 pc dense cores with the getsources software, which
simultaneously measures their far-infrared to millimeter fluxes. We carefully
estimated the temperatures and masses of these dense cores from their SEDs.
A cross-correlation with high-mass star formation signposts suggests a mass
threshold of 75Msun for MDCs in NGC6334. MDCs have temperatures of 9.5-40K,
masses of 75-1000Msun, and densities of 10^5-10^8cm-3. Their mid-IR emission is
used to separate 6 IR-bright and 10 IR-quiet protostellar MDCs while their 70mu
emission strength, with respect to fitted SEDs, helps identify 16 starless MDC
candidates. The ability of the latter to host high-mass prestellar cores is
investigated here and remains questionable. An increase in mass and density
from the starless to the IR-quiet and IR-bright phases suggests that the
protostars and MDCs simultaneously grow in mass. The statistical lifetimes of
the high-mass prestellar and protostellar core phases, estimated to be
1-7x10^4yr and at most 3x10^5yr respectively, suggest a dynamical scenario of
high-mass star formation.
The present study provides good mass estimates for a statistically
significant sample, covering the earliest phases of high-mass star formation.
High-mass prestellar cores may not exist in NGC6334, favoring a scenario
presented here, which simultaneously forms clouds and high-mass protostars.
",0,1,0,0,0,0
5457,Deconvolutional Latent-Variable Model for Text Sequence Matching,"  A latent-variable model is introduced for text matching, inferring sentence
representations by jointly optimizing generative and discriminative objectives.
To alleviate typical optimization challenges in latent-variable models for
text, we employ deconvolutional networks as the sequence decoder (generator),
providing learned latent codes with more semantic information and better
generalization. Our model, trained in an unsupervised manner, yields stronger
empirical predictive performance than a decoder based on Long Short-Term Memory
(LSTM), with less parameters and considerably faster training. Further, we
apply it to text sequence-matching problems. The proposed model significantly
outperforms several strong sentence-encoding baselines, especially in the
semi-supervised setting.
",1,0,0,1,0,0
3822,On the Successive Cancellation Decoding of Polar Codes with Arbitrary Linear Binary Kernels,"  A method for efficiently successive cancellation (SC) decoding of polar codes
with high-dimensional linear binary kernels (HDLBK) is presented and analyzed.
We devise a $l$-expressions method which can obtain simplified recursive
formulas of SC decoder in likelihood ratio form for arbitrary linear binary
kernels to reduce the complexity of corresponding SC decoder. By considering
the bit-channel transition probabilities $W_{G}^{(\cdot)}(\cdot|0)$ and
$W_{G}^{(\cdot)}(\cdot|1)$ separately, a $W$-expressions method is proposed to
further reduce the complexity of HDLBK based SC decoder. For a $m\times m$
binary kernel, the complexity of straightforward SC decoder is $O(2^{m}N\log
N)$. With $W$-expressions, we reduce the complexity of straightforward SC
decoder to $O(m^{2}N\log N)$ when $m\leq 16$. Simulation results show that
$16\times16$ kernel polar codes offer significant advantages in terms of error
performances compared with $2\times2$ kernel polar codes under SC and list SC
decoders.
",1,0,1,0,0,0
3670,Least-Squares Temporal Difference Learning for the Linear Quadratic Regulator,"  Reinforcement learning (RL) has been successfully used to solve many
continuous control tasks. Despite its impressive results however, fundamental
questions regarding the sample complexity of RL on continuous problems remain
open. We study the performance of RL in this setting by considering the
behavior of the Least-Squares Temporal Difference (LSTD) estimator on the
classic Linear Quadratic Regulator (LQR) problem from optimal control. We give
the first finite-time analysis of the number of samples needed to estimate the
value function for a fixed static state-feedback policy to within
$\varepsilon$-relative error. In the process of deriving our result, we give a
general characterization for when the minimum eigenvalue of the empirical
covariance matrix formed along the sample path of a fast-mixing stochastic
process concentrates above zero, extending a result by Koltchinskii and
Mendelson in the independent covariates setting. Finally, we provide
experimental evidence indicating that our analysis correctly captures the
qualitative behavior of LSTD on several LQR instances.
",1,0,0,1,0,0
19554,Learning to Play Othello with Deep Neural Networks,"  Achieving superhuman playing level by AlphaGo corroborated the capabilities
of convolutional neural architectures (CNNs) for capturing complex spatial
patterns. This result was to a great extent due to several analogies between Go
board states and 2D images CNNs have been designed for, in particular
translational invariance and a relatively large board. In this paper, we verify
whether CNN-based move predictors prove effective for Othello, a game with
significantly different characteristics, including a much smaller board size
and complete lack of translational invariance. We compare several CNN
architectures and board encodings, augment them with state-of-the-art
extensions, train on an extensive database of experts' moves, and examine them
with respect to move prediction accuracy and playing strength. The empirical
evaluation confirms high capabilities of neural move predictors and suggests a
strong correlation between prediction accuracy and playing strength. The best
CNNs not only surpass all other 1-ply Othello players proposed to date but
defeat (2-ply) Edax, the best open-source Othello player.
",1,0,0,1,0,0
7376,The Enemy Among Us: Detecting Hate Speech with Threats Based 'Othering' Language Embeddings,"  Offensive or antagonistic language targeted at individuals and social groups
based on their personal characteristics (also known as cyber hate speech or
cyberhate) has been frequently posted and widely circulated viathe World Wide
Web. This can be considered as a key risk factor for individual and societal
tension linked toregional instability. Automated Web-based cyberhate detection
is important for observing and understandingcommunity and regional societal
tension - especially in online social networks where posts can be rapidlyand
widely viewed and disseminated. While previous work has involved using
lexicons, bags-of-words orprobabilistic language parsing approaches, they often
suffer from a similar issue which is that cyberhate can besubtle and indirect -
thus depending on the occurrence of individual words or phrases can lead to a
significantnumber of false negatives, providing inaccurate representation of
the trends in cyberhate. This problemmotivated us to challenge thinking around
the representation of subtle language use, such as references toperceived
threats from ""the other"" including immigration or job prosperity in a hateful
context. We propose anovel framework that utilises language use around the
concept of ""othering"" and intergroup threat theory toidentify these subtleties
and we implement a novel classification method using embedding learning to
computesemantic distances between parts of speech considered to be part of an
""othering"" narrative. To validate ourapproach we conduct several experiments on
different types of cyberhate, namely religion, disability, race andsexual
orientation, with F-measure scores for classifying hateful instances obtained
through applying ourmodel of 0.93, 0.86, 0.97 and 0.98 respectively, providing
a significant improvement in classifier accuracy overthe state-of-the-art
",1,0,0,0,0,0
1490,Gradient Method With Inexact Oracle for Composite Non-Convex Optimization,"  In this paper, we develop new first-order method for composite non-convex
minimization problems with simple constraints and inexact oracle. The objective
function is given as a sum of ""`hard""', possibly non-convex part, and
""`simple""' convex part. Informally speaking, oracle inexactness means that, for
the ""`hard""' part, at any point we can approximately calculate the value of the
function and construct a quadratic function, which approximately bounds this
function from above. We give several examples of such inexactness: smooth
non-convex functions with inexact Hölder-continuous gradient, functions given
by auxiliary uniformly concave maximization problem, which can be solved only
approximately. For the introduced class of problems, we propose a gradient-type
method, which allows to use different proximal setup to adapt to geometry of
the feasible set, adaptively chooses controlled oracle error, allows for
inexact proximal mapping. We provide convergence rate for our method in terms
of the norm of generalized gradient mapping and show that, in the case of
inexact Hölder-continuous gradient, our method is universal with respect to
Hölder parameters of the problem. Finally, in a particular case, we show that
small value of the norm of generalized gradient mapping at a point means that a
necessary condition of local minimum approximately holds at that point.
",0,0,1,0,0,0
20061,Asymptotic network models of subwavelength metamaterials formed by closely packed photonic and phononic crystals,"  We demonstrate that photonic and phononic crystals consisting of closely
spaced inclusions constitute a versatile class of subwavelength metamaterials.
Intuitively, the voids and narrow gaps that characterise the crystal form an
interconnected network of Helmholtz-like resonators. We use this intuition to
argue that these continuous photonic (phononic) crystals are in fact
asymptotically equivalent, at low frequencies, to discrete capacitor-inductor
(mass-spring) networks whose lumped parameters we derive explicitly. The
crystals are tantamount to metamaterials as their entire acoustic branch, or
branches when the discrete analogue is polyatomic, is squeezed into a
subwavelength regime where the ratio of wavelength to period scales like the
ratio of period to gap width raised to the power 1/4; at yet larger wavelengths
we accordingly find a comparably large effective refractive index. The fully
analytical dispersion relations predicted by the discrete models yield
dispersion curves that agree with those from finite-element simulations of the
continuous crystals. The insight gained from the network approach is used to
show that, surprisingly, the continuum created by a closely packed hexagonal
lattice of cylinders is represented by a discrete honeycomb lattice. The
analogy is utilised to show that the hexagonal continuum lattice has a
Dirac-point degeneracy that is lifted in a controlled manner by specifying the
area of a symmetry-breaking defect.
",0,1,0,0,0,0
10088,A gentle introduction to the minimal Naming Game,"  Social conventions govern countless behaviors all of us engage in every day,
from how we greet each other to the languages we speak. But how can shared
conventions emerge spontaneously in the absence of a central coordinating
authority? The Naming Game model shows that networks of locally interacting
individuals can spontaneously self-organize to produce global coordination.
Here, we provide a gentle introduction to the main features of the model, from
the dynamics observed in homogeneously mixing populations to the role played by
more complex social networks, and to how slight modifications of the basic
interaction rules give origin to a richer phenomenology in which more
conventions can co-exist indefinitely.
",1,1,0,0,0,0
20279,Simplified Minimal Gated Unit Variations for Recurrent Neural Networks,"  Recurrent neural networks with various types of hidden units have been used
to solve a diverse range of problems involving sequence data. Two of the most
recent proposals, gated recurrent units (GRU) and minimal gated units (MGU),
have shown comparable promising results on example public datasets. In this
paper, we introduce three model variants of the minimal gated unit (MGU) which
further simplify that design by reducing the number of parameters in the
forget-gate dynamic equation. These three model variants, referred to simply as
MGU1, MGU2, and MGU3, were tested on sequences generated from the MNIST dataset
and from the Reuters Newswire Topics (RNT) dataset. The new models have shown
similar accuracy to the MGU model while using fewer parameters and thus
lowering training expense. One model variant, namely MGU2, performed better
than MGU on the datasets considered, and thus may be used as an alternate to
MGU or GRU in recurrent neural networks.
",1,0,0,1,0,0
2409,Microfluidics for Chemical Synthesis: Flow Chemistry,"  Klavs F. Jensen is Warren K. Lewis Professor in Chemical Engineering and
Materials Science and Engineering at the Massachusetts Institute of Technology.
Here he describes the use of microfluidics for chemical synthesis, from the
early demonstration examples to the current efforts with automated droplet
microfluidic screening and optimization techniques.
",0,0,0,0,1,0
14930,High-throughput nanofluidic device for one-dimensional confined detection of single fluorophores,"  Ensemble averaging experiments may conceal many fundamental molecular
interactions. To overcome that, a high-throughput detection of single molecules
or colloidal nanodots is crucial for biomedical, nanoelectronic, and
solid-state applications. One-dimensional (1D) discrete flow of nanoscale
objects is an efficient approach in this direction. The development of simple
and cost-effective nanofluidic devices is a critical step to realise 1D flow.
This letter presents a nanofabrication technique using
shadow-angle-electron-beam-deposition for a high-throughput preparation of
parallel nanofluidic channels. These were used to flow and detect DNA,
carbon-nanodots, and organic fluorophores. The 1D molecular mass transport was
performed using electro-osmotic flow. The 1D flow behaviour was identified and
analysed using two-focus fluorescence correlation spectroscopy (2fFCS). A range
of flow velocities of single molecules was achieved. The transitions of single
molecules or nanodots through the two foci were quantitatively analysed using
confocal scanning imaging, correlative photon detection, and burst size
distribution analysis. The results suggest an efficient nanofabrication
technique is developed to prepare nanofluidic devices. This first demonstration
of high-throughput nanochannel fabrication process and using 2fFCS-based single
molecule flow detection should have a potential impact on ultra-sensitive
biomedical diagnostics and studying biomolecular interactions as well as
nanomaterials.
",0,1,0,0,0,0
10781,Anomalous Magnetism for Dirac Electrons in Two Dimensional Rashba Systems,"  Spin-spin correlation function response in the low electronic density regime
and externally applied electric field is evaluated for 2D metallic crystals
under Rashba-type coupling, fixed number of particles and two-fold energy band
structure. Intrinsic Zeeman-like effect on electron spin polarization, density
of states, Fermi surface topology and transverse magnetic susceptibility are
analyzed in the zero temperature limit. A possible magnetic state for Dirac
electrons depending on the zero field band gap magnitude under this conditions
is found.
",0,1,0,0,0,0
13416,Persistence paths and signature features in topological data analysis,"  We introduce a new feature map for barcodes that arise in persistent homology
computation. The main idea is to first realize each barcode as a path in a
convenient vector space, and to then compute its path signature which takes
values in the tensor algebra of that vector space. The composition of these two
operations - barcode to path, path to tensor series - results in a feature map
that has several desirable properties for statistical learning, such as
universality and characteristicness, and achieves state-of-the-art results on
common classification benchmarks.
",0,0,0,1,0,0
8600,"Fractional Operators with Inhomogeneous Boundary Conditions: Analysis, Control, and Discretization","  In this paper we introduce new characterizations of spectral fractional
Laplacian to incorporate nonhomogeneous Dirichlet and Neumann boundary
conditions. The classical cases with homogeneous boundary conditions arise as a
special case. We apply our definition to fractional elliptic equations of order
$s \in (0,1)$ with nonzero Dirichlet and Neumann boundary condition. Here the
domain $\Omega$ is assumed to be a bounded, quasi-convex Lipschitz domain. To
impose the nonzero boundary conditions, we construct fractional harmonic
extensions of the boundary data. It is shown that solving for the fractional
harmonic extension is equivalent to solving for the standard harmonic extension
in the very-weak form. The latter result is of independent interest as well.
The remaining fractional elliptic problem (with homogeneous boundary data) can
be realized using the existing techniques. We introduce finite element
discretizations and derive discretization error estimates in natural norms,
which are confirmed by numerical experiments. We also apply our
characterizations to Dirichlet and Neumann boundary optimal control problems
with fractional elliptic equation as constraints.
",0,0,1,0,0,0
10836,Fractional Patlak-Keller-Segel equations for chemotactic superdiffusion,"  The long range movement of certain organisms in the presence of a
chemoattractant can be governed by long distance runs, according to an
approximate Levy distribution. This article clarifies the form of biologically
relevant model equations: We derive Patlak-Keller-Segel-like equations
involving nonlocal, fractional Laplacians from a microscopic model for cell
movement. Starting from a power-law distribution of run times, we derive a
kinetic equation in which the collision term takes into account the long range
behaviour of the individuals. A fractional chemotactic equation is obtained in
a biologically relevant regime. Apart from chemotaxis, our work has
implications for biological diffusion in numerous processes.
",0,1,0,0,0,0
9657,Edge N-Level Sparse Visibility Graphs: Fast Optimal Any-Angle Pathfinding Using Hierarchical Taut Paths,"  In the Any-Angle Pathfinding problem, the goal is to find the shortest path
between a pair of vertices on a uniform square grid, that is not constrained to
any fixed number of possible directions over the grid. Visibility Graphs are a
known optimal algorithm for solving the problem with the use of pre-processing.
However, Visibility Graphs are known to perform poorly in terms of running
time, especially on large, complex maps. In this paper, we introduce two
improvements over the Visibility Graph Algorithm to compute optimal paths.
Sparse Visibility Graphs (SVGs) are constructed by pruning unnecessary edges
from the original Visibility Graph. Edge N-Level Sparse Visibility Graphs
(ENLSVGs) is a hierarchical SVG built by iteratively pruning non-taut paths. We
also introduce Line-of-Sight Scans, a faster algorithm for building Visibility
Graphs over a grid. SVGs run much faster than Visibility Graphs by reducing the
average vertex degree. ENLSVGs, a hierarchical algorithm, improves this
further, especially on larger maps. On large maps, with the use of
pre-processing, these algorithms are orders of magnitude faster than existing
algorithms like Visibility Graphs and Theta*.
",1,0,0,0,0,0
13306,Analytic solutions of the Madelung equation,"  We present analytic self-similar solutions for the one, two and three
dimensional Madelung hydrodynamical equation for a free particle. There is a
direct connection between the zeros of the Madelung fluid density and the
magnitude of the quantum potential.
",0,0,1,0,0,0
17763,High-transmissivity Silicon Visible-wavelength Metasurface Designs based on Truncated-cone Nanoantennae,"  High-transmissivity all-dielectric metasurfaces have recently attracted
attention towards the realization of ultra-compact optical devices and systems.
Silicon based metasurfaces, in particular, are highly promising considering the
possibility of monolithic integration with VLSI circuits. Realization of
silicon based metasurfaces operational in the visible wavelengths remains a
challenge. A numerical study of silicon metasurfaces based on stepped truncated
cone shaped nanoantenna elements is presented. Metasurfaces based on the
stepped conical geometry can be designed for operation in the 700nm to 800nm
wavelength window and achieve full cycle phase response (0 to pi with an
improved transmittance in comparison with previously reported cylindrical
geometry [1]. A systematic parameter study of the influence of various
geometrical parameters on the achievable amplitude and phase coverage is
reported.
",0,1,0,0,0,0
9227,Automatic Gradient Boosting,"  Automatic machine learning performs predictive modeling with high performing
machine learning tools without human interference. This is achieved by making
machine learning applications parameter-free, i.e. only a dataset is provided
while the complete model selection and model building process is handled
internally through (often meta) optimization. Projects like Auto-WEKA and
auto-sklearn aim to solve the Combined Algorithm Selection and Hyperparameter
optimization (CASH) problem resulting in huge configuration spaces. However,
for most real-world applications, the optimization over only a few different
key learning algorithms can not only be sufficient, but also potentially
beneficial. The latter becomes apparent when one considers that models have to
be validated, explained, deployed and maintained. Here, less complex model are
often preferred, for validation or efficiency reasons, or even a strict
requirement. Automatic gradient boosting simplifies this idea one step further,
using only gradient boosting as a single learning algorithm in combination with
model-based hyperparameter tuning, threshold optimization and encoding of
categorical features. We introduce this general framework as well as a concrete
implementation called autoxgboost. It is compared to current AutoML projects on
16 datasets and despite its simplicity is able to achieve comparable results on
about half of the datasets as well as performing best on two.
",0,0,0,1,0,0
18467,Hardy-Sobolev-Maz'ya inequalities for higher order derivatives on half spaces,"  By using, among other things, the Fourier analysis techniques on hyperbolic
and symmetric spaces, we establish the Hardy-Sobolev-Maz'ya inequalities for
higher order derivatives on half spaces. The proof relies on a
Hardy-Littlewood-Sobolev inequality on hyperbolic spaces which is of its
independent interest. We also give an alternative proof of Benguria, Frank and
Loss' work concerning the sharp constant in the Hardy-Sobolev-Maz'ya inequality
in the three dimensional upper half space. Finally, we show the sharp constant
in the Hardy-Sobolev-Maz'ya inequality for bi-Laplacian in the upper half space
of dimension five coincides with the Sobolev constant.
",0,0,1,0,0,0
5733,Motion Segmentation via Global and Local Sparse Subspace Optimization,"  In this paper, we propose a new framework for segmenting feature-based moving
objects under affine subspace model. Since the feature trajectories in practice
are high-dimensional and contain a lot of noise, we firstly apply the sparse
PCA to represent the original trajectories with a low-dimensional global
subspace, which consists of the orthogonal sparse principal vectors.
Subsequently, the local subspace separation will be achieved via automatically
searching the sparse representation of the nearest neighbors for each projected
data. In order to refine the local subspace estimation result and deal with the
missing data problem, we propose an error estimation to encourage the projected
data that span a same local subspace to be clustered together. In the end, the
segmentation of different motions is achieved through the spectral clustering
on an affinity matrix, which is constructed with both the error estimation and
sparse neighbors optimization. We test our method extensively and compare it
with state-of-the-art methods on the Hopkins 155 dataset and Freiburg-Berkeley
Motion Segmentation dataset. The results show that our method is comparable
with the other motion segmentation methods, and in many cases exceed them in
terms of precision and computation time.
",1,0,0,0,0,0
5164,The Rice-Shapiro theorem in Computable Topology,"  We provide requirements on effectively enumerable topological spaces which
guarantee that the Rice-Shapiro theorem holds for the computable elements of
these spaces. We show that the relaxation of these requirements leads to the
classes of effectively enumerable topological spaces where the Rice-Shapiro
theorem does not hold. We propose two constructions that generate effectively
enumerable topological spaces with particular properties from wn--families and
computable trees without computable infinite paths. Using them we propose
examples that give a flavor of this class.
",1,0,1,0,0,0
11733,Accurate spectroscopic redshift of the multiply lensed quasar PSOJ0147 from the Pan-STARRS survey,"  Context: The gravitational lensing time delay method provides a one-step
determination of the Hubble constant (H0) with an uncertainty level on par with
the cosmic distance ladder method. However, to further investigate the nature
of the dark energy, a H0 estimate down to 1% level is greatly needed. This
requires dozens of strongly lensed quasars that are yet to be delivered by
ongoing and forthcoming all-sky surveys.
Aims: In this work we aim to determine the spectroscopic redshift of
PSOJ0147, the first strongly lensed quasar candidate found in the Pan-STARRS
survey. The main goal of our work is to derive an accurate redshift estimate of
the background quasar for cosmography.
Methods: To obtain timely spectroscopically follow-up, we took advantage of
the fast-track service programme that is carried out by the Nordic Optical
Telescope. Using a grism covering 3200 - 9600 A, we identified prominent
emission line features, such as Ly-alpha, N V, O I, C II, Si IV, C IV, and [C
III] in the spectra of the background quasar of the PSOJ0147 lens system. This
enables us to determine accurately the redshift of the background quasar.
Results: The spectrum of the background quasar exhibits prominent absorption
features bluewards of the strong emission lines, such as Ly-alpha, N V, and C
IV. These blue absorption lines indicate that the background source is a broad
absorption line (BAL) quasar. Unfortunately, the BAL features hamper an
accurate determination of redshift using the above-mentioned strong emission
lines. Nevertheless, we are able to determine a redshift of 2.341+/-0.001 from
three of the four lensed quasar images with the clean forbidden line [C III].
In addition, we also derive a maximum outflow velocity of ~ 9800 km/s with the
broad absorption features bluewards of the C IV emission line. This value of
maximum outflow velocity is in good agreement with other BAL quasars.
",0,1,0,0,0,0
1513,Distributed Event-Triggered Control for Global Consensus of Multi-Agent Systems with Input Saturation,"  We consider the global consensus problem for multi-agent systems with input
saturation over digraphs. Under a mild connectivity condition that the
underlying digraph has a directed spanning tree, we use Lyapunov methods to
show that the widely used distributed consensus protocol, which solves the
consensus problem for the case without input saturation constraints, also
solves the global consensus problem for the case with input saturation
constraints. In order to reduce the overall need of communication and system
updates, we then propose a distributed event-triggered control law. Global
consensus is still realized and Zeno behavior is excluded. Numerical
simulations are provided to illustrate the effectiveness of the theoretical
results.
",0,0,1,0,0,0
14975,Offline Biases in Online Platforms: a Study of Diversity and Homophily in Airbnb,"  How diverse are sharing economy platforms? Are they fair marketplaces, where
all participants operate on a level playing field, or are they large-scale
online aggregators of offline human biases? Often portrayed as easy-to-access
digital spaces whose participants receive equal opportunities, such platforms
have recently come under fire due to reports of discriminatory behaviours among
their users, and have been associated with gentrification phenomena that
exacerbate preexisting inequalities along racial lines. In this paper, we focus
on the Airbnb sharing economy platform, and analyse the diversity of its user
base across five large cities. We find it to be predominantly young, female,
and white. Notably, we find this to be true even in cities with a diverse
racial composition. We then introduce a method based on the statistical
analysis of networks to quantify behaviours of homophily, heterophily and
avoidance between Airbnb hosts and guests. Depending on cities and property
types, we do find signals of such behaviours relating both to race and gender.
We use these findings to provide platform design recommendations, aimed at
exposing and possibly reducing the biases we detect, in support of a more
inclusive growth of sharing economy platforms.
",1,0,0,0,0,0
4144,Conceptualization of Object Compositions Using Persistent Homology,"  A topological shape analysis is proposed and utilized to learn concepts that
reflect shape commonalities. Our approach is two-fold: i) a spatial topology
analysis of point cloud segment constellations within objects. Therein
constellations are decomposed and described in an hierarchical manner - from
single segments to segment groups until a single group reflects an entire
object. ii) a topology analysis of the description space in which segment
decompositions are exposed in. Inspired by Persistent Homology, hidden groups
of shape commonalities are revealed from object segment decompositions.
Experiments show that extracted persistent groups of commonalities can
represent semantically meaningful shape concepts. We also show the
generalization capability of the proposed approach considering samples of
external datasets.
",1,0,0,0,0,0
5006,Point-Cloud-Based Aerial Fragmentation Analysis for Application in the Minerals Industry,"  This work investigates the application of Unmanned Aerial Vehicle (UAV)
technology for measurement of rock fragmentation without placement of scale
objects in the scene to determine image scale. Commonly practiced image-based
rock fragmentation analysis requires a technician to walk to a rock pile, place
a scale object of known size in the area of interest, and capture individual 2D
images. Our previous work has used UAV technology for the first time to acquire
real-time rock fragmentation data and has shown comparable quality of results;
however, it still required the (potentially dangerous) placement of scale
objects, and continued to make the assumption that the rock pile surface is
planar and that the scale objects lie on the surface plane. This work improves
our UAV-based approach to enable rock fragmentation measurement without
placement of scale objects and without the assumption of planarity. This is
achieved by first generating a point cloud of the rock pile from 2D images,
taking into account intrinsic and extrinsic camera parameters, and then taking
2D images for fragmentation analysis. This work represents an important step
towards automating post-blast rock fragmentation analysis. In experiments, a
rock pile with known size distribution was photographed by the UAV with and
without using scale objects. For fragmentation analysis without scale objects,
a point cloud of the rock pile was generated and used to compute image scale.
Comparison of the rock size distributions show that this point-cloud-based
method enables producing measurements with better or comparable accuracy
(within 10% of the ground truth) to the manual method with scale objects.
",1,0,0,0,0,0
8239,A generalized quantum Slepian-Wolf,"  In this work we consider a quantum generalization of the task considered by
Slepian and Wolf [1973] regarding distributed source compression. In our task
Alice, Bob, Charlie and Reference share a joint pure state. Alice and Bob wish
to send a part of their respective systems to Charlie without collaborating
with each other. We give achievability bounds for this task in the one-shot
setting and provide the asymptotic and i.i.d. analysis in the case when there
is no side information with Charlie.
Our result implies the result of Abeyesinghe, Devetak, Hayden and Winter
[2009] who studied a special case of this problem. As another special case
wherein Bob holds trivial registers, we recover the result of Devetak and Yard
[2008] regarding quantum state redistribution.
",1,0,0,0,0,0
8603,Disorder-protected topological entropy after a quantum quench,"  Topological phases of matter are considered the bedrock of novel quantum
materials as well as ideal candidates for quantum computers that possess
robustness at the physical level. The robustness of the topological phase at
finite temperature or away from equilibrium is therefore a very desirable
feature. Disorder can improve the lifetime of the encoded topological qubits.
Here we tackle the problem of the survival of the topological phase as detected
by topological entropy, after a sudden quantum quench. We introduce a method to
study analytically the time evolution of the system after a quantum quench and
show that disorder in the couplings of the Hamiltonian of the toric code and
the resulting Anderson localization can make the topological entropy resilient.
",0,1,0,0,0,0
10225,The absolutely Koszul property of Veronese subrings and Segre products,"  Absolutely Koszul algebras are a class of rings over which any finite graded
module has a rational Poincaré series. We provide a criterion to detect
non-absolutely Koszul rings. Combining the criterion with machine computations,
we identify large families of Veronese subrings and Segre products of
polynomial rings which are not absolutely Koszul. In particular, we classify
completely the absolutely Koszul algebras among Segre products of polynomial
rings, at least in characteristic $0$.
",0,0,1,0,0,0
9385,Human-Robot Trust Integrated Task Allocation and Symbolic Motion planning for Heterogeneous Multi-robot Systems,"  This paper presents a human-robot trust integrated task allocation and motion
planning framework for multi-robot systems (MRS) in performing a set of tasks
concurrently. A set of task specifications in parallel are conjuncted with MRS
to synthesize a task allocation automaton. Each transition of the task
allocation automaton is associated with the total trust value of human in
corresponding robots. Here, the human-robot trust model is constructed with a
dynamic Bayesian network (DBN) by considering individual robot performance,
safety coefficient, human cognitive workload and overall evaluation of task
allocation. Hence, a task allocation path with maximum encoded human-robot
trust can be searched based on the current trust value of each robot in the
task allocation automaton. Symbolic motion planning (SMP) is implemented for
each robot after they obtain the sequence of actions. The task allocation path
can be intermittently updated with this DBN based trust model. The overall
strategy is demonstrated by a simulation with 5 robots and 3 parallel subtask
automata.
",1,0,0,0,0,0
17804,UCB Exploration via Q-Ensembles,"  We show how an ensemble of $Q^*$-functions can be leveraged for more
effective exploration in deep reinforcement learning. We build on well
established algorithms from the bandit setting, and adapt them to the
$Q$-learning setting. We propose an exploration strategy based on
upper-confidence bounds (UCB). Our experiments show significant gains on the
Atari benchmark.
",1,0,0,1,0,0
7874,Riemann-Hilbert problems for the resolved conifold,"  We study the Riemann-Hilbert problems associated to the Donaldson-Thomas
theory of the resolved conifold. We give explicit solutions in terms of the
Barnes double and triple sine functions. We show that the corresponding tau
function is a non-perturbative partition function, in the sense that its
asymptotic expansion coincides with the topological string partition function.
",0,0,1,0,0,0
2782,Klein-Gordonization: mapping superintegrable quantum mechanics to resonant spacetimes,"  We describe a procedure naturally associating relativistic Klein-Gordon
equations in static curved spacetimes to non-relativistic quantum motion on
curved spaces in the presence of a potential. Our procedure is particularly
attractive in application to (typically, superintegrable) problems whose energy
spectrum is given by a quadratic function of the energy level number, since for
such systems the spacetimes one obtains possess evenly spaced, resonant spectra
of frequencies for scalar fields of a certain mass. This construction emerges
as a generalization of the previously studied correspondence between the Higgs
oscillator and Anti-de Sitter spacetime, which has been useful for both
understanding weakly nonlinear dynamics in Anti-de Sitter spacetime and
algebras of conserved quantities of the Higgs oscillator. Our conversion
procedure (""Klein-Gordonization"") reduces to a nonlinear elliptic equation
closely reminiscent of the one emerging in relation to the celebrated Yamabe
problem of differential geometry. As an illustration, we explicitly demonstrate
how to apply this procedure to superintegrable Rosochatius systems, resulting
in a large family of spacetimes with resonant spectra for massless wave
equations.
",0,1,1,0,0,0
6224,Sparse Randomized Kaczmarz for Support Recovery of Jointly Sparse Corrupted Multiple Measurement Vectors,"  While single measurement vector (SMV) models have been widely studied in
signal processing, there is a surging interest in addressing the multiple
measurement vectors (MMV) problem. In the MMV setting, more than one
measurement vector is available and the multiple signals to be recovered share
some commonalities such as a common support. Applications in which MMV is a
naturally occurring phenomenon include online streaming, medical imaging, and
video recovery. This work presents a stochastic iterative algorithm for the
support recovery of jointly sparse corrupted MMV. We present a variant of the
Sparse Randomized Kaczmarz algorithm for corrupted MMV and compare our proposed
method with an existing Kaczmarz type algorithm for MMV problems. We also
showcase the usefulness of our approach in the online (streaming) setting and
provide empirical evidence that suggests the robustness of the proposed method
to the distribution of the corruption and the number of corruptions occurring.
",1,0,0,0,0,0
17195,Algebras of Quasi-Plücker Coordinates are Koszul,"  Motivated by the theory of quasi-determinants, we study non-commutative
algebras of quasi-Plücker coordinates. We prove that these algebras provide
new examples of non-homogeneous quadratic Koszul algebras by showing that their
quadratic duals have quadratic Gröbner bases.
",0,0,1,0,0,0
14982,Modular invariant representations of the $\mathcal{N}=2$ superconformal algebra,"  We compute the modular transformation formula of the characters for a certain
family of (finitely or uncountably many) simple modules over the simple
$\mathcal{N}=2$ vertex operator superalgebra of central charge
$c_{p,p'}=3\left(1-\frac{2p'}{p}\right),$ where $(p,p')$ is a pair of coprime
positive integers such that $p\geq2$. When $p'=1$, the formula coincides with
that of the $\mathcal{N}=2$ unitary minimal series found by F. Ravanini and
S.-K. Yang. In addition, we study the properties of the corresponding ""modular
$S$-matrix"", which is no longer a matrix if $p'\geq2$.
",0,0,1,0,0,0
10990,Electroweak Vacuum Metastability and Low-scale Inflation,"  We study the stability of the electroweak vacuum in low-scale inflation
models whose Hubble parameter is much smaller than the instability scale of the
Higgs potential. In general, couplings between the inflaton and Higgs are
present, and hence we study effects of these couplings during and after
inflation. We derive constraints on the couplings between the inflaton and
Higgs by requiring that they do not lead to catastrophic electroweak vacuum
decay, in particular, via resonant production of the Higgs particles.
",0,1,0,0,0,0
18800,"On $C$-bases, partition pairs and filtrations for induced or restricted Specht modules","  We obtain alternative explicit Specht filtrations for the induced and the
restricted Specht modules in the Hecke algebra of the symmetric group (defined
over the ring $A=\mathbb Z[q^{1/2},q^{-1/2}]$ where $q$ is an indeterminate)
using $C$-bases for these modules. Moreover, we provide a link between a
certain $C$-basis for the induced Specht module and the notion of pairs of
partitions.
",0,0,1,0,0,0
16298,Sketched Subspace Clustering,"  The immense amount of daily generated and communicated data presents unique
challenges in their processing. Clustering, the grouping of data without the
presence of ground-truth labels, is an important tool for drawing inferences
from data. Subspace clustering (SC) is a relatively recent method that is able
to successfully classify nonlinearly separable data in a multitude of settings.
In spite of their high clustering accuracy, SC methods incur prohibitively high
computational complexity when processing large volumes of high-dimensional
data. Inspired by random sketching approaches for dimensionality reduction, the
present paper introduces a randomized scheme for SC, termed Sketch-SC, tailored
for large volumes of high-dimensional data. Sketch-SC accelerates the
computationally heavy parts of state-of-the-art SC approaches by compressing
the data matrix across both dimensions using random projections, thus enabling
fast and accurate large-scale SC. Performance analysis as well as extensive
numerical tests on real data corroborate the potential of Sketch-SC and its
competitive performance relative to state-of-the-art scalable SC approaches.
",1,0,0,1,0,0
3634,Network Design with Probabilistic Capacities,"  We consider a network design problem with random arc capacities and give a
formulation with a probabilistic capacity constraint on each cut of the
network. To handle the exponentially-many probabilistic constraints a
separation procedure that solves a nonlinear minimum cut problem is introduced.
For the case with independent arc capacities, we exploit the supermodularity of
the set function defining the constraints and generate cutting planes based on
the supermodular covering knapsack polytope. For the general correlated case,
we give a reformulation of the constraints that allows to uncover and utilize
the submodularity of a related function. The computational results indicate
that exploiting the underlying submodularity and supermodularity arising with
the probabilistic constraints provides significant advantages over the
classical approaches.
",0,0,1,0,0,0
5953,"Character Distributions of Classical Chinese Literary Texts: Zipf's Law, Genres, and Epochs","  We collect 14 representative corpora for major periods in Chinese history in
this study. These corpora include poetic works produced in several dynasties,
novels of the Ming and Qing dynasties, and essays and news reports written in
modern Chinese. The time span of these corpora ranges between 1046 BCE and 2007
CE. We analyze their character and word distributions from the viewpoint of the
Zipf's law, and look for factors that affect the deviations and similarities
between their Zipfian curves. Genres and epochs demonstrated their influences
in our analyses. Specifically, the character distributions for poetic works of
between 618 CE and 1644 CE exhibit striking similarity. In addition, although
texts of the same dynasty may tend to use the same set of characters, their
character distributions still deviate from each other.
",1,0,0,0,0,0
17142,"Fundamental groups, slalom curves and extremal length","  We define the extremal length of elements of the fundamental group of the
twice punctured complex plane and give upper and lower bounds for this
invariant. The bounds differ by a multiplicative constant. The main motivation
comes from $3$-braid invariants and their application.
",0,0,1,0,0,0
1612,High-dimensional Linear Regression for Dependent Observations with Application to Nowcasting,"  In the last few years, an extensive literature has been focused on the
$\ell_1$ penalized least squares (Lasso) estimators of high dimensional linear
regression when the number of covariates $p$ is considerably larger than the
sample size $n$. However, there is limited attention paid to the properties of
the estimators when the errors or/and the covariates are serially dependent. In
this study, we investigate the theoretical properties of the Lasso estimators
for linear regression with random design under serially dependent and/or
non-sub-Gaussian errors and covariates. In contrast to the traditional case in
which the errors are i.i.d and have finite exponential moments, we show that
$p$ can at most be a power of $n$ if the errors have only polynomial moments.
In addition, the rate of convergence becomes slower due to the serial
dependencies in errors and the covariates. We also consider sign consistency
for model selection via Lasso when there are serial correlations in the errors
or the covariates or both. Adopting the framework of functional dependence
measure, we provide a detailed description on how the rates of convergence and
the selection consistencies of the estimators depend on the dependence measures
and moment conditions of the errors and the covariates. Simulation results show
that Lasso regression can be substantially more powerful than the mixed
frequency data sampling regression (MIDAS) in the presence of irrelevant
variables. We apply the results obtained for the Lasso method to nowcasting
mixing frequency data in which serially correlated errors and a large number of
covariates are common. In real examples, the Lasso procedure outperforms the
MIDAS in both forecasting and nowcasting.
",0,0,1,1,0,0
2894,Quantum Mechanical Approach to Modelling Reliability of Sensor Reports,"  Dempster-Shafer evidence theory is wildly applied in multi-sensor data
fusion. However, lots of uncertainty and interference exist in practical
situation, especially in the battle field. It is still an open issue to model
the reliability of sensor reports. Many methods are proposed based on the
relationship among collected data. In this letter, we proposed a quantum
mechanical approach to evaluate the reliability of sensor reports, which is
based on the properties of a sensor itself. The proposed method is used to
modify the combining of evidences.
",1,0,0,0,0,0
15273,Efficient Algorithms for Non-convex Isotonic Regression through Submodular Optimization,"  We consider the minimization of submodular functions subject to ordering
constraints. We show that this optimization problem can be cast as a convex
optimization problem on a space of uni-dimensional measures, with ordering
constraints corresponding to first-order stochastic dominance. We propose new
discretization schemes that lead to simple and efficient algorithms based on
zero-th, first, or higher order oracles; these algorithms also lead to
improvements without isotonic constraints. Finally, our experiments show that
non-convex loss functions can be much more robust to outliers for isotonic
regression, while still leading to an efficient optimization problem.
",1,0,0,1,0,0
8819,Stable determination of a Lamé coefficient by one internal measurement of displacement,"  In this paper we show that the shear modulus $\mu$ of an isotropic elastic
body can be stably recovered by the knowledge of one single displacement field
whose boundary data can be assigned independently of the unknown elasticity
tensor.
",0,0,1,0,0,0
3948,NFFT meets Krylov methods: Fast matrix-vector products for the graph Laplacian of fully connected networks,"  The graph Laplacian is a standard tool in data science, machine learning, and
image processing. The corresponding matrix inherits the complex structure of
the underlying network and is in certain applications densely populated. This
makes computations, in particular matrix-vector products, with the graph
Laplacian a hard task. A typical application is the computation of a number of
its eigenvalues and eigenvectors. Standard methods become infeasible as the
number of nodes in the graph is too large. We propose the use of the fast
summation based on the nonequispaced fast Fourier transform (NFFT) to perform
the dense matrix-vector product with the graph Laplacian fast without ever
forming the whole matrix. The enormous flexibility of the NFFT algorithm allows
us to embed the accelerated multiplication into Lanczos-based eigenvalues
routines or iterative linear system solvers and even consider other than the
standard Gaussian kernels. We illustrate the feasibility of our approach on a
number of test problems from image segmentation to semi-supervised learning
based on graph-based PDEs. In particular, we compare our approach with the
Nyström method. Moreover, we present and test an enhanced, hybrid version of
the Nyström method, which internally uses the NFFT.
",0,0,0,1,0,0
10731,Nearly circular domains which are integrable close to the boundary are ellipses,"  The Birkhoff conjecture says that the boundary of a strictly convex
integrable billiard table is necessarily an ellipse. In this article, we
consider a stronger notion of integrability, namely integrability close to the
boundary, and prove a local version of this conjecture: a small perturbation of
an ellipse of small eccentricity which preserves integrability near the
boundary, is itself an ellipse. This extends the result in [1], where
integrability was assumed on a larger set. In particular, it shows that (local)
integrability near the boundary implies global integrability. One of the
crucial ideas in the proof consists in analyzing Taylor expansion of the
corresponding action-angle coordinates with respect to the eccentricity
parameter, deriving and studying higher order conditions for the preservation
of integrable rational caustics.
",0,0,1,0,0,0
19921,The Compressed Overlap Index,"  For analysing text algorithms, for computing superstrings, or for testing
random number generators, one needs to compute all overlaps between any pairs
of words in a given set. The positions of overlaps of a word onto itself, or of
two words, are needed to compute the absence probability of a word in a random
text, or the numbers of common words shared by two random texts. In all these
contexts, one needs to compute or to query overlaps between pairs of words in a
given set. For this sake, we designed COvI, a compressed overlap index that
supports multiple queries on overlaps: like computing the correlation of two
words, or listing pairs of words whose longest overlap is maximal among all
possible pairs. COvI stores overlaps in a hierarchical and non-redundant
manner. We propose an implementation that can handle datasets of millions of
words and still answer queries efficiently. Comparison with a baseline solution
- called FullAC - relying on the Aho-Corasick automaton shows that COvI
provides significant advantages. For similar construction times, COvI requires
half the memory FullAC, and still solves complex queries much faster.
",1,0,0,0,0,0
11075,Quivers with additive labelings: classification and algebraic entropy,"  We show that Zamolodchikov dynamics of a recurrent quiver has zero algebraic
entropy only if the quiver has a weakly subadditive labeling, and conjecture
the converse. By assigning a pair of generalized Cartan matrices of affine type
to each quiver with an additive labeling, we completely classify such quivers,
obtaining $40$ infinite families and $13$ exceptional quivers. This completes
the program of classifying Zamolodchikov periodic and integrable quivers.
",0,0,1,0,0,0
14522,Intersection of conjugate solvable subgroups in symmetric groups,"  It is shown that for a solvable subgroup $G$ of an almost simple group $S$
which socle is isomorphic to $A_n$ $ (n\ge5)$ there are $x,y,z,t \in S$ such
that $G \cap G^x \cap G^y \cap G^z \cap G^t =1.$
",0,0,1,0,0,0
13266,A two-phase gradient method for quadratic programming problems with a single linear constraint and bounds on the variables,"  We propose a gradient-based method for quadratic programming problems with a
single linear constraint and bounds on the variables. Inspired by the GPCG
algorithm for bound-constrained convex quadratic programming [J.J. Moré and
G. Toraldo, SIAM J. Optim. 1, 1991], our approach alternates between two phases
until convergence: an identification phase, which performs gradient projection
iterations until either a candidate active set is identified or no reasonable
progress is made, and an unconstrained minimization phase, which reduces the
objective function in a suitable space defined by the identification phase, by
applying either the conjugate gradient method or a recently proposed spectral
gradient method. However, the algorithm differs from GPCG not only because it
deals with a more general class of problems, but mainly for the way it stops
the minimization phase. This is based on a comparison between a measure of
optimality in the reduced space and a measure of bindingness of the variables
that are on the bounds, defined by extending the concept of proportioning,
which was proposed by some authors for box-constrained problems. If the
objective function is bounded, the algorithm converges to a stationary point
thanks to a suitable application of the gradient projection method in the
identification phase. For strictly convex problems, the algorithm converges to
the optimal solution in a finite number of steps even in case of degeneracy.
Extensive numerical experiments show the effectiveness of the proposed
approach.
",0,0,1,0,0,0
1730,A Correspondence Between Random Neural Networks and Statistical Field Theory,"  A number of recent papers have provided evidence that practical design
questions about neural networks may be tackled theoretically by studying the
behavior of random networks. However, until now the tools available for
analyzing random neural networks have been relatively ad-hoc. In this work, we
show that the distribution of pre-activations in random neural networks can be
exactly mapped onto lattice models in statistical physics. We argue that
several previous investigations of stochastic networks actually studied a
particular factorial approximation to the full lattice model. For random linear
networks and random rectified linear networks we show that the corresponding
lattice models in the wide network limit may be systematically approximated by
a Gaussian distribution with covariance between the layers of the network. In
each case, the approximate distribution can be diagonalized by Fourier
transformation. We show that this approximation accurately describes the
results of numerical simulations of wide random neural networks. Finally, we
demonstrate that in each case the large scale behavior of the random networks
can be approximated by an effective field theory.
",1,1,0,1,0,0
3510,Steady Galactic Dynamos and Observational Consequences I: Halo Magnetic Fields,"  We study the global consequences in the halos of spiral galaxies of the
steady, axially symmetric, mean field dynamo. We use the classical theory but
add the possibility of using the velocity field components as parameters in
addition to the helicity and diffusivity. The analysis is based on the simplest
version of the theory and uses scale-invariant solutions. The velocity field
(subject to restrictions) is a scale invariant field in a `pattern' frame, in
place of a full dynamical theory. The `pattern frame' of reference may either
be the systemic frame or some rigidly rotating spiral pattern frame. One type
of solution for the magnetic field yields off-axis, spirally wound, magnetic
field lines. These predict sign changes in the Faraday screen rotation measure
in every quadrant of the halo of an edge-on galaxy. Such rotation measure
oscillations have been observed in the CHANG-ES survey.
",0,1,0,0,0,0
9004,Comments on avalanche flow models based on the concept of random kinetic energy,"  In a series of papers, Bartelt and co-workers developed novel snow-avalanche
models in which \emph{random kinetic energy} $R_K$ (a.k.a.\ granular
temperature) is a key concept. The earliest models were for a single, constant
density layer, using a Voellmy model but with $R_K$-dependent friction
parameters. This was then extended to variable density, and finally a
suspension layer (powder-snow cloud) was added. The physical basis and
mathematical formulation of these models is critically reviewed here, with the
following main findings: (i) Key assumptions in the original RKE model differ
substantially from established results on dense granular flows; in particular,
the effective friction coefficient decreases to zero with velocity in the RKE
model. (ii) In the variable-density model, non-canonical interpretation of the
energy balance leads to a third-order evolution equation for the flow depth or
density, whereas the stated assumptions imply a first-order equation. (iii) The
model for the suspension layer neglects gravity and disregards well established
theoretical and experimental results on particulate gravity currents. Some
options for improving these aspects are discussed.
",0,1,0,0,0,0
20201,Pitfalls and Best Practices in Algorithm Configuration,"  Good parameter settings are crucial to achieve high performance in many areas
of artificial intelligence (AI), such as propositional satisfiability solving,
AI planning, scheduling, and machine learning (in particular deep learning).
Automated algorithm configuration methods have recently received much attention
in the AI community since they replace tedious, irreproducible and error-prone
manual parameter tuning and can lead to new state-of-the-art performance.
However, practical applications of algorithm configuration are prone to several
(often subtle) pitfalls in the experimental design that can render the
procedure ineffective. We identify several common issues and propose best
practices for avoiding them. As one possibility for automatically handling as
many of these as possible, we also propose a tool called GenericWrapper4AC.
",1,0,0,0,0,0
14539,APO Time Resolved Color Photometry of Highly-Elongated Interstellar Object 1I/'Oumuamua,"  We report on $g$, $r$ and $i$ band observations of the Interstellar Object
'Oumuamua (1I) taken on 2017 October 29 from 04:28 to 08:40 UTC by the Apache
Point Observatory (APO) 3.5m telescope's ARCTIC camera. We find that 1I's
colors are $g-r=0.41\pm0.24$ and $r-i=0.23\pm0.25$, consistent with the visible
spectra of Masiero (2017), Ye et al. (2017) and Fitzsimmons et al. (2017), and
most comparable to the population of Solar System C/D asteroids, Trojans, or
comets. We find no evidence of any cometary activity at a heliocentric distance
of 1.46 au, approximately 1.5 months after 1I's closest approach distance to
the Sun. Significant brightness variability was seen in the $r$ observations,
with the object becoming notably brighter towards the end of the run. By
combining our APO photometric time series data with the Discovery Channel
Telescope (DCT) data of Knight et al. (2017), taken 20 h later on 2017 October
30, we construct an almost complete light curve with a most probable lightcurve
period of $P \simeq 4~{\rm h}$. Our results imply a double peaked rotation
period of 8.1 $\pm$ 0.02 h, with a peak-to-peak amplitude of 1.5 - 2.1 mags.
Assuming that 1I's shape can be approximated by an ellipsoid, the amplitude
constraint implies that 1I has an axial ratio of 3.5 to 10.3, which is
strikingly elongated. Assuming that 1I is rotating above its critical break up
limit, our results are compatible with 1I having having modest cohesive
strength and may have obtained its elongated shape during a tidal disruption
event before being ejected from its home system. Astrometry useful for
constraining 1I's orbit was also obtained and published in Weaver et al.
(2017).
",0,1,0,0,0,0
12918,Order-disorder transitions in lattice gases with annealed reactive constraints,"  We study equilibrium properties of catalytically-activated $A + A \to
\oslash$ reactions taking place on a lattice of adsorption sites. The particles
undergo continuous exchanges with a reservoir maintained at a constant chemical
potential $\mu$ and react when they appear at the neighbouring sites, provided
that some reactive conditions are fulfilled. We model the latter in two
different ways: In the Model I some fraction $p$ of the {\em bonds} connecting
neighbouring sites possesses special catalytic properties such that any two
$A$s appearing on the sites connected by such a bond instantaneously react and
desorb. In the Model II some fraction $p$ of the adsorption {\em sites}
possesses such properties and neighbouring particles react if at least one of
them resides on a catalytic site. For the case of \textit{annealed} disorder in
the distribution of the catalyst, which is tantamount to the situation when the
reaction may take place at any point on the lattice but happens with a finite
probability $p$, we provide an exact solution for both models for the interior
of an infinitely large Cayley tree - the so-called Bethe lattice. We show that
both models exhibit a rich critical behaviour: For the annealed Model I it is
characterised by a transition into an ordered state and a re-entrant transition
into a disordered phase, which both are continuous. For the annealed Model II,
which represents a rather exotic model of statistical mechanics in which
interactions of any particle with its environment have a peculiar Boolean form,
the transition to an ordered state is always continuous, while the re-entrant
transition into the disordered phase may be either continuous or discontinuous,
depending on the value of $p$.
",0,1,0,0,0,0
12059,Non interactive simulation of correlated distributions is decidable,"  A basic problem in information theory is the following: Let $\mathbf{P} =
(\mathbf{X}, \mathbf{Y})$ be an arbitrary distribution where the marginals
$\mathbf{X}$ and $\mathbf{Y}$ are (potentially) correlated. Let Alice and Bob
be two players where Alice gets samples $\{x_i\}_{i \ge 1}$ and Bob gets
samples $\{y_i\}_{i \ge 1}$ and for all $i$, $(x_i, y_i) \sim \mathbf{P}$. What
joint distributions $\mathbf{Q}$ can be simulated by Alice and Bob without any
interaction?
Classical works in information theory by G{á}cs-K{ö}rner and Wyner answer
this question when at least one of $\mathbf{P}$ or $\mathbf{Q}$ is the
distribution on $\{0,1\} \times \{0,1\}$ where each marginal is unbiased and
identical. However, other than this special case, the answer to this question
is understood in very few cases. Recently, Ghazi, Kamath and Sudan showed that
this problem is decidable for $\mathbf{Q}$ supported on $\{0,1\} \times
\{0,1\}$. We extend their result to $\mathbf{Q}$ supported on any finite
alphabet.
We rely on recent results in Gaussian geometry (by the authors) as well as a
new \emph{smoothing argument} inspired by the method of \emph{boosting} from
learning theory and potential function arguments from complexity theory and
additive combinatorics.
",1,0,1,0,0,0
10207,Adaptive Neural Networks for Efficient Inference,"  We present an approach to adaptively utilize deep neural networks in order to
reduce the evaluation time on new examples without loss of accuracy. Rather
than attempting to redesign or approximate existing networks, we propose two
schemes that adaptively utilize networks. We first pose an adaptive network
evaluation scheme, where we learn a system to adaptively choose the components
of a deep network to be evaluated for each example. By allowing examples
correctly classified using early layers of the system to exit, we avoid the
computational time associated with full evaluation of the network. We extend
this to learn a network selection system that adaptively selects the network to
be evaluated for each example. We show that computational time can be
dramatically reduced by exploiting the fact that many examples can be correctly
classified using relatively efficient networks and that complex,
computationally costly networks are only necessary for a small fraction of
examples. We pose a global objective for learning an adaptive early exit or
network selection policy and solve it by reducing the policy learning problem
to a layer-by-layer weighted binary classification problem. Empirically, these
approaches yield dramatic reductions in computational cost, with up to a 2.8x
speedup on state-of-the-art networks from the ImageNet image recognition
challenge with minimal (<1%) loss of top5 accuracy.
",1,0,0,1,0,0
16668,Ease.ml: Towards Multi-tenant Resource Sharing for Machine Learning Workloads,"  We present ease.ml, a declarative machine learning service platform we built
to support more than ten research groups outside the computer science
departments at ETH Zurich for their machine learning needs. With ease.ml, a
user defines the high-level schema of a machine learning application and
submits the task via a Web interface. The system automatically deals with the
rest, such as model selection and data movement. In this paper, we describe the
ease.ml architecture and focus on a novel technical problem introduced by
ease.ml regarding resource allocation. We ask, as a ""service provider"" that
manages a shared cluster of machines among all our users running machine
learning workloads, what is the resource allocation strategy that maximizes the
global satisfaction of all our users?
Resource allocation is a critical yet subtle issue in this multi-tenant
scenario, as we have to balance between efficiency and fairness. We first
formalize the problem that we call multi-tenant model selection, aiming for
minimizing the total regret of all users running automatic model selection
tasks. We then develop a novel algorithm that combines multi-armed bandits with
Bayesian optimization and prove a regret bound under the multi-tenant setting.
Finally, we report our evaluation of ease.ml on synthetic data and on one
service we are providing to our users, namely, image classification with deep
neural networks. Our experimental evaluation results show that our proposed
solution can be up to 9.8x faster in achieving the same global quality for all
users as the two popular heuristics used by our users before ease.ml.
",1,0,0,1,0,0
5113,Deformation estimation of an elastic object by partial observation using a neural network,"  Deformation estimation of elastic object assuming an internal organ is
important for the computer navigation of surgery. The aim of this study is to
estimate the deformation of an entire three-dimensional elastic object using
displacement information of very few observation points. A learning approach
with a neural network was introduced to estimate the entire deformation of an
object. We applied our method to two elastic objects; a rectangular
parallelepiped model, and a human liver model reconstructed from computed
tomography data. The average estimation error for the human liver model was
0.041 mm when the object was deformed up to 66.4 mm, from only around 3 %
observations. These results indicate that the deformation of an entire elastic
object can be estimated with an acceptable level of error from limited
observations by applying a trained neural network to a new deformation.
",1,0,0,1,0,0
6552,Predicting Financial Crime: Augmenting the Predictive Policing Arsenal,"  Financial crime is a rampant but hidden threat. In spite of this, predictive
policing systems disproportionately target ""street crime"" rather than white
collar crime. This paper presents the White Collar Crime Early Warning System
(WCCEWS), a white collar crime predictive model that uses random forest
classifiers to identify high risk zones for incidents of financial crime.
",1,0,0,0,0,0
7031,Approximate Optimal Designs for Multivariate Polynomial Regression,"  We introduce a new approach aiming at computing approximate optimal designs
for multivariate polynomial regressions on compact (semi-algebraic) design
spaces. We use the moment-sum-of-squares hierarchy of semidefinite programming
problems to solve numerically the approximate optimal design problem. The
geometry of the design is recovered via semidefinite programming duality
theory. This article shows that the hierarchy converges to the approximate
optimal design as the order of the hierarchy increases. Furthermore, we provide
a dual certificate ensuring finite convergence of the hierarchy and showing
that the approximate optimal design can be computed numerically with our
method. As a byproduct, we revisit the equivalence theorem of the experimental
design theory: it is linked to the Christoffel polynomial and it characterizes
finite convergence of the moment-sum-of-square hierarchies.
",0,0,1,1,0,0
14772,Beyond Worst-case: A Probabilistic Analysis of Affine Policies in Dynamic Optimization,"  Affine policies (or control) are widely used as a solution approach in
dynamic optimization where computing an optimal adjustable solution is usually
intractable. While the worst case performance of affine policies can be
significantly bad, the empirical performance is observed to be near-optimal for
a large class of problem instances. For instance, in the two-stage dynamic
robust optimization problem with linear covering constraints and uncertain
right hand side, the worst-case approximation bound for affine policies is
$O(\sqrt m)$ that is also tight (see Bertsimas and Goyal (2012)), whereas
observed empirical performance is near-optimal. In this paper, we aim to
address this stark-contrast between the worst-case and the empirical
performance of affine policies. In particular, we show that affine policies
give a good approximation for the two-stage adjustable robust optimization
problem with high probability on random instances where the constraint
coefficients are generated i.i.d. from a large class of distributions; thereby,
providing a theoretical justification of the observed empirical performance. On
the other hand, we also present a distribution such that the performance bound
for affine policies on instances generated according to that distribution is
$\Omega(\sqrt m)$ with high probability; however, the constraint coefficients
are not i.i.d.. This demonstrates that the empirical performance of affine
policies can depend on the generative model for instances.
",0,0,1,0,0,0
5922,HONE: Higher-Order Network Embeddings,"  This paper describes a general framework for learning Higher-Order Network
Embeddings (HONE) from graph data based on network motifs. The HONE framework
is highly expressive and flexible with many interchangeable components. The
experimental results demonstrate the effectiveness of learning higher-order
network representations. In all cases, HONE outperforms recent embedding
methods that are unable to capture higher-order structures with a mean relative
gain in AUC of $19\%$ (and up to $75\%$ gain) across a wide variety of networks
and embedding methods.
",1,0,0,1,0,0
3508,Photonic Loschmidt echo in binary waveguide lattices,"  Time reversal is one of the most intriguing yet elusive wave phenomenon of
major interest in different areas of classical and quantum physics. Time
reversal requires in principle to flip the sign of the Hamiltonian of the
system, leading to a revival of the initial state (Loschmidt echo). Here it is
shown that Loschmidt echo of photons can be observed in an optical setting
without resorting to reversal of the Hamiltonian. We consider photonic
propagation in a binary waveguide lattice and show that, by exchanging the two
sublattices after some propagation distance, a Loschmidt echo can be observed.
Examples of Loschmidt echoes for single photon and NOON states are given in
one- and two-dimensional waveguide lattices.
",0,1,0,0,0,0
10909,Modeling sepsis progression using hidden Markov models,"  Characterizing a patient's progression through stages of sepsis is critical
for enabling risk stratification and adaptive, personalized treatment. However,
commonly used sepsis diagnostic criteria fail to account for significant
underlying heterogeneity, both between patients as well as over time in a
single patient. We introduce a hidden Markov model of sepsis progression that
explicitly accounts for patient heterogeneity. Benchmarked against two sepsis
diagnostic criteria, the model provides a useful tool to uncover a patient's
latent sepsis trajectory and to identify high-risk patients in whom more
aggressive therapy may be indicated.
",0,0,0,1,1,0
8345,A multi-layered energy consumption model for smart wireless acoustic sensor networks,"  Smart sensing is expected to become a pervasive technology in smart cities
and environments of the near future. These services are improving their
capabilities due to integrated devices shrinking in size while maintaining
their computational power, which can run diverse Machine Learning algorithms
and achieve high performance in various data-processing tasks. One attractive
sensor modality to be used for smart sensing are acoustic sensors, which can
convey highly informative data while keeping a moderate energy consumption.
Unfortunately, the energy budget of current wireless sensor networks is usually
not enough to support the requirements of standard microphones. Therefore,
energy efficiency needs to be increased at all layers --- sensing, signal
processing and communication --- in order to bring wireless smart acoustic
sensors into the market. To help to attain this goal, this paper introduces
WASN-EM: an energy consumption model for wireless acoustic sensors networks
(WASN), whose aim is to aid in the development of novel techniques to increase
the energy-efficient of smart wireless acoustic sensors. This model provides a
first step of exploration prior to custom design of a smart wireless acoustic
sensor, and also can be used to compare the energy consumption of different
protocols.
",1,0,0,0,0,0
1902,A Simple Convex Layers Algorithm,"  Given a set of $n$ points $P$ in the plane, the first layer $L_1$ of $P$ is
formed by the points that appear on $P$'s convex hull. In general, a point
belongs to layer $L_i$, if it lies on the convex hull of the set $P \setminus
\bigcup_{j<i}\{L_j\}$. The \emph{convex layers problem} is to compute the
convex layers $L_i$. Existing algorithms for this problem either do not achieve
the optimal $\mathcal{O}\left(n\log n\right)$ runtime and linear space, or are
overly complex and difficult to apply in practice. We propose a new algorithm
that is both optimal and simple. The simplicity is achieved by independently
computing four sets of monotone convex chains in $\mathcal{O}\left(n\log
n\right)$ time and linear space. These are then merged in
$\mathcal{O}\left(n\log n\right)$ time.
",1,0,0,0,0,0
17696,Privacy Mining from IoT-based Smart Homes,"  Recently, a wide range of smart devices are deployed in a variety of
environments to improve the quality of human life. One of the important
IoT-based applications is smart homes for healthcare, especially for elders.
IoT-based smart homes enable elders' health to be properly monitored and taken
care of. However, elders' privacy might be disclosed from smart homes due to
non-fully protected network communication or other reasons. To demonstrate how
serious this issue is, we introduce in this paper a Privacy Mining Approach
(PMA) to mine privacy from smart homes by conducting a series of deductions and
analyses on sensor datasets generated by smart homes. The experimental results
demonstrate that PMA is able to deduce a global sensor topology for a smart
home and disclose elders' privacy in terms of their house layouts.
",0,0,0,1,0,0
9990,Improving on Q & A Recurrent Neural Networks Using Noun-Tagging,"  Often, more time is spent on finding a model that works well, rather than
tuning the model and working directly with the dataset. Our research began as
an attempt to improve upon a simple Recurrent Neural Network for answering
""simple"" first-order questions (QA-RNN), developed by Ferhan Ture and Oliver
Jojic, from Comcast Labs, using the SimpleQuestions dataset. Their baseline
model, a bidirectional, 2-layer LSTM RNN and a GRU RNN, have accuracies of 0.94
and 0.90, for entity detection and relation prediction, respectively. We fine
tuned these models by doing substantial hyper-parameter tuning, getting
resulting accuracies of 0.70 and 0.80, for entity detection and relation
prediction, respectively. An accuracy of 0.984 was obtained on entity detection
using a 1-layer LSTM, where preprocessing was done by removing all words not
part of a noun chunk from the question. 100% of the dataset was available for
relation prediction, but only 20% of the dataset, was available for entity
detection, which we believe to be much of the reason for our initial
difficulties in replicating their result, despite the fact we were able to
improve on their entity detection results.
",0,0,0,1,0,0
2843,Nilpotence order growth of recursion operators in characteristic p,"  We prove that the killing rate of certain degree-lowering ""recursion
operators"" on a polynomial algebra over a finite field grows slower than
linearly in the degree of the polynomial attacked. We also explain the
motivating application: obtaining a lower bound for the Krull dimension of a
local component of a big mod-p Hecke algebra in the genus-zero case. We sketch
the application for p=2 and p=3 in level one. The case p=2 was first
established in by Nicolas and Serre in 2012 using different methods.
",0,0,1,0,0,0
5818,The linear nature of pseudowords,"  Given a pseudoword over suitable pseudovarieties, we associate to it a
labeled linear order determined by the factorizations of the pseudoword. We
show that, in the case of the pseudovariety of aperiodic finite semigroups, the
pseudoword can be recovered from the labeled linear order.
",0,0,1,0,0,0
20508,Krylov methods for low-rank commuting generalized Sylvester equations,"  We consider generalizations of the Sylvester matrix equation, consisting of
the sum of a Sylvester operator and a linear operator $\Pi$ with a particular
structure. More precisely, the commutator of the matrix coefficients of the
operator $\Pi$ and the Sylvester operator coefficients are assumed to be
matrices with low rank. We show (under certain additional conditions) low-rank
approximability of this problem, i.e., the solution to this matrix equation can
be approximated with a low-rank matrix. Projection methods have successfully
been used to solve other matrix equations with low-rank approximability. We
propose a new projection method for this class of matrix equations. The choice
of subspace is a crucial ingredient for any projection method for matrix
equations. Our method is based on an adaption and extension of the extended
Krylov subspace method for Sylvester equations. A constructive choice of the
starting vector/block is derived from the low-rank commutators. We illustrate
the effectiveness of our method by solving large-scale matrix equations arising
from applications in control theory and the discretization of PDEs. The
advantages of our approach in comparison to other methods are also illustrated.
",0,0,1,0,0,0
13492,Linear-time approximation schemes for planar minimum three-edge connected and three-vertex connected spanning subgraphs,"  We present the first polynomial-time approximation schemes, i.e., (1 +
{\epsilon})-approximation algorithm for any constant {\epsilon} > 0, for the
minimum three-edge connected spanning subgraph problem and the minimum
three-vertex connected spanning subgraph problem in undirected planar graphs.
Both the approximation schemes run in linear time.
",1,0,0,0,0,0
6146,A simple neural network module for relational reasoning,"  Relational reasoning is a central component of generally intelligent
behavior, but has proven difficult for neural networks to learn. In this paper
we describe how to use Relation Networks (RNs) as a simple plug-and-play module
to solve problems that fundamentally hinge on relational reasoning. We tested
RN-augmented networks on three tasks: visual question answering using a
challenging dataset called CLEVR, on which we achieve state-of-the-art,
super-human performance; text-based question answering using the bAbI suite of
tasks; and complex reasoning about dynamic physical systems. Then, using a
curated dataset called Sort-of-CLEVR we show that powerful convolutional
networks do not have a general capacity to solve relational questions, but can
gain this capacity when augmented with RNs. Our work shows how a deep learning
architecture equipped with an RN module can implicitly discover and learn to
reason about entities and their relations.
",1,0,0,0,0,0
15003,Radial orbit instability in systems of highly eccentric orbits: Antonov problem reviewed,"  Stationary stellar systems with radially elongated orbits are subject to
radial orbit instability -- an important phenomenon that structures galaxies.
Antonov (1973) presented a formal proof of the instability for spherical
systems in the limit of purely radial orbits. However, such spheres have highly
inhomogeneous density distributions with singularity $\sim 1/r^2$, resulting in
an inconsistency in the proof. The proof can be refined, if one considers an
orbital distribution close to purely radial, but not entirely radial, which
allows to avoid the central singularity. For this purpose we employ
non-singular analogs of generalised polytropes elaborated recently in our work
in order to derive and solve new integral equations adopted for calculation of
unstable eigenmodes in systems with nearly radial orbits. In addition, we
establish a link between our and Antonov's approaches and uncover the meaning
of infinite entities in the purely radial case. Maximum growth rates tend to
infinity as the system becomes more and more radially anisotropic. The
instability takes place both for even and odd spherical harmonics, with all
unstable modes developing rapidly, i.e. having eigenfrequencies comparable to
or greater than typical orbital frequencies. This invalidates orbital
approximation in the case of systems with all orbits very close to purely
radial.
",0,1,0,0,0,0
10409,Learning to compress and search visual data in large-scale systems,"  The problem of high-dimensional and large-scale representation of visual data
is addressed from an unsupervised learning perspective. The emphasis is put on
discrete representations, where the description length can be measured in bits
and hence the model capacity can be controlled. The algorithmic infrastructure
is developed based on the synthesis and analysis prior models whose
rate-distortion properties, as well as capacity vs. sample complexity
trade-offs are carefully optimized. These models are then extended to
multi-layers, namely the RRQ and the ML-STC frameworks, where the latter is
further evolved as a powerful deep neural network architecture with fast and
sample-efficient training and discrete representations. For the developed
algorithms, three important applications are developed. First, the problem of
large-scale similarity search in retrieval systems is addressed, where a
double-stage solution is proposed leading to faster query times and shorter
database storage. Second, the problem of learned image compression is targeted,
where the proposed models can capture more redundancies from the training
images than the conventional compression codecs. Finally, the proposed
algorithms are used to solve ill-posed inverse problems. In particular, the
problems of image denoising and compressive sensing are addressed with
promising results.
",1,0,0,1,0,0
2519,Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data,"  One of the defining properties of deep learning is that models are chosen to
have many more parameters than available training data. In light of this
capacity for overfitting, it is remarkable that simple algorithms like SGD
reliably return solutions with low test error. One roadblock to explaining
these phenomena in terms of implicit regularization, structural properties of
the solution, and/or easiness of the data is that many learning bounds are
quantitatively vacuous when applied to networks learned by SGD in this ""deep
learning"" regime. Logically, in order to explain generalization, we need
nonvacuous bounds. We return to an idea by Langford and Caruana (2001), who
used PAC-Bayes bounds to compute nonvacuous numerical bounds on generalization
error for stochastic two-layer two-hidden-unit neural networks via a
sensitivity analysis. By optimizing the PAC-Bayes bound directly, we are able
to extend their approach and obtain nonvacuous generalization bounds for deep
stochastic neural network classifiers with millions of parameters trained on
only tens of thousands of examples. We connect our findings to recent and old
work on flat minima and MDL-based explanations of generalization.
",1,0,0,0,0,0
17090,Converging expansions for Lipschitz self-similar perforations of a plane sector,"  In contrast with the well-known methods of matching asymptotics and
multiscale (or compound) asymptotics, the "" functional analytic approach "" of
Lanza de Cristoforis (Analysis 28, 2008) allows to prove convergence of
expansions around interior small holes of size $\epsilon$ for solutions of
elliptic boundary value problems. Using the method of layer potentials, the
asymptotic behavior of the solution as $\epsilon$ tends to zero is described
not only by asymptotic series in powers of $\epsilon$, but by convergent power
series. Here we use this method to investigate the Dirichlet problem for the
Laplace operator where holes are collapsing at a polygonal corner of opening
$\omega$. Then in addition to the scale $\epsilon$ there appears the scale
$\eta = \epsilon^{\pi/\omega}$. We prove that when $\pi/\omega$ is irrational,
the solution of the Dirichlet problem is given by convergent series in powers
of these two small parameters. Due to interference of the two scales, this
convergence is obtained, in full generality, by grouping together integer
powers of the two scales that are very close to each other. Nevertheless, there
exists a dense subset of openings $\omega$ (characterized by Diophantine
approximation properties), for which real analyticity in the two variables
$\epsilon$ and $\eta$ holds and the power series converge unconditionally. When
$\pi/\omega$ is rational, the series are unconditionally convergent, but
contain terms in log $\epsilon$.
",0,0,1,0,0,0
2421,A note on species realizations and nondegeneracy of potentials,"  In this note we show that a mutation theory of species with potential can be
defined so that a certain class of skew-symmetrizable integer matrices have a
species realization admitting a non-degenerate potential. This gives a partial
affirmative answer to a question raised by Jan Geuenich and Daniel
Labardini-Fragoso. We also provide an example of a class of skew-symmetrizable
$4 \times 4$ integer matrices, which are not globally unfoldable nor strongly
primitive, and that have a species realization admitting a non-degenerate
potential.
",0,0,1,0,0,0
11630,Off The Beaten Lane: AI Challenges In MOBAs Beyond Player Control,"  MOBAs represent a huge segment of online gaming and are growing as both an
eSport and a casual genre. The natural starting point for AI researchers
interested in MOBAs is to develop an AI to play the game better than a human -
but MOBAs have many more challenges besides adversarial AI. In this paper we
introduce the reader to the wider context of MOBA culture, propose a range of
challenges faced by the community today, and posit concrete AI projects that
can be undertaken to begin solving them.
",1,0,0,0,0,0
13143,Extracting Hierarchies of Search Tasks & Subtasks via a Bayesian Nonparametric Approach,"  A significant amount of search queries originate from some real world
information need or tasks. In order to improve the search experience of the end
users, it is important to have accurate representations of tasks. As a result,
significant amount of research has been devoted to extracting proper
representations of tasks in order to enable search systems to help users
complete their tasks, as well as providing the end user with better query
suggestions, for better recommendations, for satisfaction prediction, and for
improved personalization in terms of tasks. Most existing task extraction
methodologies focus on representing tasks as flat structures. However, tasks
often tend to have multiple subtasks associated with them and a more
naturalistic representation of tasks would be in terms of a hierarchy, where
each task can be composed of multiple (sub)tasks. To this end, we propose an
efficient Bayesian nonparametric model for extracting hierarchies of such tasks
\& subtasks. We evaluate our method based on real world query log data both
through quantitative and crowdsourced experiments and highlight the importance
of considering task/subtask hierarchies.
",1,0,0,0,0,0
4668,Poisson--Gamma Dynamical Systems,"  We introduce a new dynamical system for sequentially observed multivariate
count data. This model is based on the gamma--Poisson construction---a natural
choice for count data---and relies on a novel Bayesian nonparametric prior that
ties and shrinks the model parameters, thus avoiding overfitting. We present an
efficient MCMC inference algorithm that advances recent work on augmentation
schemes for inference in negative binomial models. Finally, we demonstrate the
model's inductive bias using a variety of real-world data sets, showing that it
exhibits superior predictive performance over other models and infers highly
interpretable latent structure.
",1,0,0,1,0,0
7925,Maximal solutions for the Infinity-eigenvalue problem,"  In this article we prove that the first eigenvalue of the $\infty-$Laplacian
$$ \left\{ \begin{array}{rclcl}
\min\{ -\Delta_\infty v,\, |\nabla v|-\lambda_{1, \infty}(\Omega) v \} & = &
0 & \text{in} & \Omega v & = & 0 & \text{on} & \partial \Omega, \end{array}
\right. $$ has a unique (up to scalar multiplication) maximal solution. This
maximal solution can be obtained as the limit as $\ell \nearrow 1$ of concave
problems of the form $$ \left\{ \begin{array}{rclcl}
\min\{ -\Delta_\infty v_{\ell},\, |\nabla v_{\ell}|-\lambda_{1,
\infty}(\Omega) v_{\ell}^{\ell} \} & = & 0 & \text{in} & \Omega v_{\ell} & = &
0 & \text{on} & \partial \Omega. \end{array} \right. $$ In this way we obtain
that the maximal eigenfunction is the unique one that is the limit of the
concave problems as happens for the usual eigenvalue problem for the
$p-$Laplacian for a fixed $1<p<\infty$.
",0,0,1,0,0,0
4004,Spreading of an infectious disease between different locations,"  The endogenous adaptation of agents, that may adjust their local contact
network in response to the risk of being infected, can have the perverse effect
of increasing the overall systemic infectiveness of a disease. We study a
dynamical model over two geographically distinct but interacting locations, to
better understand theoretically the mechanism at play. Moreover, we provide
empirical motivation from the Italian National Bovine Database, for the period
2006-2013.
",0,0,0,0,0,1
3397,Temporal Convolution Networks for Real-Time Abdominal Fetal Aorta Analysis with Ultrasound,"  The automatic analysis of ultrasound sequences can substantially improve the
efficiency of clinical diagnosis. In this work we present our attempt to
automate the challenging task of measuring the vascular diameter of the fetal
abdominal aorta from ultrasound images. We propose a neural network
architecture consisting of three blocks: a convolutional layer for the
extraction of imaging features, a Convolution Gated Recurrent Unit (C-GRU) for
enforcing the temporal coherence across video frames and exploiting the
temporal redundancy of a signal, and a regularized loss function, called
\textit{CyclicLoss}, to impose our prior knowledge about the periodicity of the
observed signal. We present experimental evidence suggesting that the proposed
architecture can reach an accuracy substantially superior to previously
proposed methods, providing an average reduction of the mean squared error from
$0.31 mm^2$ (state-of-art) to $0.09 mm^2$, and a relative error reduction from
$8.1\%$ to $5.3\%$. The mean execution speed of the proposed approach of 289
frames per second makes it suitable for real time clinical use.
",0,0,0,1,0,0
17275,The Reinhardt Conjecture as an Optimal Control Problem,"  In 1934, Reinhardt conjectured that the shape of the centrally symmetric
convex body in the plane whose densest lattice packing has the smallest density
is a smoothed octagon. This conjecture is still open. We formulate the
Reinhardt Conjecture as a problem in optimal control theory. The smoothed
octagon is a Pontryagin extremal trajectory with bang-bang control. More
generally, the smoothed regular $6k+2$-gon is a Pontryagin extremal with
bang-bang control. The smoothed octagon is a strict (micro) local minimum to
the optimal control problem. The optimal solution to the Reinhardt problem is a
trajectory without singular arcs. The extremal trajectories that do not meet
the singular locus have bang-bang controls with finitely many switching times.
Finally, we reduce the Reinhardt problem to an optimization problem on a
five-dimensional manifold. (Each point on the manifold is an initial condition
for a potential Pontryagin extremal lifted trajectory.) We suggest that the
Reinhardt conjecture might eventually be fully resolved through optimal control
theory. Some proofs are computer-assisted using a computer algebra system.
",0,0,1,0,0,0
7338,The Mean and Median Criterion for Automatic Kernel Bandwidth Selection for Support Vector Data Description,"  Support vector data description (SVDD) is a popular technique for detecting
anomalies. The SVDD classifier partitions the whole space into an inlier
region, which consists of the region near the training data, and an outlier
region, which consists of points away from the training data. The computation
of the SVDD classifier requires a kernel function, and the Gaussian kernel is a
common choice for the kernel function. The Gaussian kernel has a bandwidth
parameter, whose value is important for good results. A small bandwidth leads
to overfitting, and the resulting SVDD classifier overestimates the number of
anomalies. A large bandwidth leads to underfitting, and the classifier fails to
detect many anomalies. In this paper we present a new automatic, unsupervised
method for selecting the Gaussian kernel bandwidth. The selected value can be
computed quickly, and it is competitive with existing bandwidth selection
methods.
",1,0,0,1,0,0
16684,Robust XVA,"  We introduce an arbitrage-free framework for robust valuation adjustments. An
investor trades a credit default swap portfolio with a risky counterparty, and
hedges credit risk by taking a position in the counterparty bond. The investor
does not know the expected rate of return of the counterparty bond, but he is
confident that it lies within an uncertainty interval. We derive both upper and
lower bounds for the XVA process of the portfolio, and show that these bounds
may be recovered as solutions of nonlinear ordinary differential equations. The
presence of collateralization and closeout payoffs leads to fundamental
differences with respect to classical credit risk valuation. The value of the
super-replicating portfolio cannot be directly obtained by plugging one of the
extremes of the uncertainty interval in the valuation equation, but rather
depends on the relation between the XVA replicating portfolio and the close-out
value throughout the life of the transaction.
",0,0,0,0,0,1
14534,Application of data science techniques to disentangle X-ray spectral variation of super-massive black holes,"  We apply three data science techniques, Nonnegative Matrix Factorization
(NMF), Principal Component Analysis (PCA) and Independent Component Analysis
(ICA), to simulated X-ray energy spectra of a particular class of super-massive
black holes. Two competing physical models, one whose variable components are
additive and the other whose variable components are multiplicative, are known
to successfully describe X-ray spectral variation of these super-massive black
holes, within accuracy of the contemporary observation. We hope to utilize
these techniques to compare the viability of the models by probing the
mathematical structure of the observed spectra, while comparing advantages and
disadvantages of each technique. We find that PCA is best to determine the
dimensionality of a dataset, while NMF is better suited for interpreting
spectral components and comparing them in terms of the physical models in
question. ICA is able to reconstruct the parameters responsible for spectral
variation. In addition, we find that the results of these techniques are
sufficiently different that applying them to observed data may be a useful test
in comparing the accuracy of the two spectral models.
",0,1,0,0,0,0
11744,Using Perturbed Underdamped Langevin Dynamics to Efficiently Sample from Probability Distributions,"  In this paper we introduce and analyse Langevin samplers that consist of
perturbations of the standard underdamped Langevin dynamics. The perturbed
dynamics is such that its invariant measure is the same as that of the
unperturbed dynamics. We show that appropriate choices of the perturbations can
lead to samplers that have improved properties, at least in terms of reducing
the asymptotic variance. We present a detailed analysis of the new Langevin
sampler for Gaussian target distributions. Our theoretical results are
supported by numerical experiments with non-Gaussian target measures.
",0,0,1,1,0,0
13116,"Logically Isolated, Actually Unpredictable? Measuring Hypervisor Performance in Multi-Tenant SDNs","  Ideally, by enabling multi-tenancy, network virtualization allows to improve
resource utilization, while providing performance isolation: although the
underlying resources are shared, the virtual network appears as a dedicated
network to the tenant. However, providing such an illusion is challenging in
practice, and over the last years, many expedient approaches have been proposed
to provide performance isolation in virtual networks, by enforcing bandwidth
reservations. We in this paper study another source for overheads and
unpredictable performance in virtual networks: the hypervisor.
The hypervisor is a critical component in multi-tenant environments, but its
overhead and influence on performance are hardly understood today. In
particular, we focus on OpenFlow-based virtualized Software Defined Networks
(vSDNs). Network virtualization is considered a killer application for SDNs: a
vSDN allows each tenant to flexibly manage its network from a logically
centralized perspective, via a simple API. For the purpose of our study, we
developed a new benchmarking tool for OpenFlow control and data planes,
enabling high and consistent OpenFlow message rates. Using our tool, we
identify and measure controllable and uncontrollable effects on performance and
overhead, including the hypervisor technology, the number of tenants as well as
the tenant type, as well as the type of OpenFlow messages.
",1,0,0,0,0,0
6651,The average sizes of two-torsion subgroups in quotients of class groups of cubic fields,"  We prove a generalization of a result of Bhargava regarding the average size
$\mathrm{Cl}(K)[2]$ as $K$ varies among cubic fields. For a fixed set of
rational primes $S$, we obtain a formula for the average size of
$\mathrm{Cl}(K)/\langle S \rangle[2]$ as $K$ varies among cubic fields with a
fixed signature, where $\langle S \rangle$ is the subgroup of $\mathrm{Cl}(K)$
generated by the classes of primes of $K$ above primes in $S$.
As a consequence, we are able to calculate the average sizes of
$K_{2n}(\mathcal{O}_K)[2]$ for $n > 0$ and for the relaxed Selmer group
$\mathrm{Sel}_2^S(K)$ as $K$ varies in these same families.
",0,0,1,0,0,0
12716,Continuity of the Green function in meromorphic families of polynomials,"  We prove that along any marked point the Green function of a meromorphic
family of polynomials parameterized by the punctured unit disk explodes
exponentially fast near the origin with a continuous error term.
",0,0,1,0,0,0
1536,Identification of Conduit Countries and Community Structures in the Withholding Tax Networks,"  Due to economic globalization, each country's economic law, including tax
laws and tax treaties, has been forced to work as a single network. However,
each jurisdiction (country or region) has not made its economic law under the
assumption that its law functions as an element of one network, so it has
brought unexpected results. We thought that the results are exactly
international tax avoidance. To contribute to the solution of international tax
avoidance, we tried to investigate which part of the network is vulnerable.
Specifically, focusing on treaty shopping, which is one of international tax
avoidance methods, we attempt to identified which jurisdiction are likely to be
used for treaty shopping from tax liabilities and the relationship between
jurisdictions which are likely to be used for treaty shopping and others. For
that purpose, based on withholding tax rates imposed on dividends, interest,
and royalties by jurisdictions, we produced weighted multiple directed graphs,
computed the centralities and detected the communities. As a result, we
clarified the jurisdictions that are likely to be used for treaty shopping and
pointed out that there are community structures. The results of this study
suggested that fewer jurisdictions need to introduce more regulations for
prevention of treaty abuse worldwide.
",0,0,0,0,0,1
13226,Urban Scene Segmentation with Laser-Constrained CRFs,"  Robots typically possess sensors of different modalities, such as colour
cameras, inertial measurement units, and 3D laser scanners. Often, solving a
particular problem becomes easier when more than one modality is used. However,
while there are undeniable benefits to combine sensors of different modalities
the process tends to be complicated. Segmenting scenes observed by the robot
into a discrete set of classes is a central requirement for autonomy as
understanding the scene is the first step to reason about future situations.
Scene segmentation is commonly performed using either image data or 3D point
cloud data. In computer vision many successful methods for scene segmentation
are based on conditional random fields (CRF) where the maximum a posteriori
(MAP) solution to the segmentation can be obtained by inference. In this paper
we devise a new CRF inference method for scene segmentation that incorporates
global constraints, enforcing the sets of nodes are assigned the same class
label. To do this efficiently, the CRF is formulated as a relaxed quadratic
program whose MAP solution is found using a gradient-based optimisation
approach. The proposed method is evaluated on images and 3D point cloud data
gathered in urban environments where image data provides the appearance
features needed by the CRF, while the 3D point cloud data provides global
spatial constraints over sets of nodes. Comparisons with belief propagation,
conventional quadratic programming relaxation, and higher order potential CRF
show the benefits of the proposed method.
",1,0,0,0,0,0
5993,"Weyl nodes in Andreev spectra of multiterminal Josephson junctions: Chern numbers, conductances and supercurrents","  We consider mesoscopic four-terminal Josephson junctions and study emergent
topological properties of the Andreev subgap bands. We use symmetry-constrained
analysis for Wigner-Dyson classes of scattering matrices to derive band
dispersions. When scattering matrix of the normal region connecting
superconducting leads is energy-independent, the determinant formula for
Andreev spectrum can be reduced to a palindromic equation that admits a
complete analytical solution. Band topology manifests with an appearance of the
Weyl nodes which serve as monopoles of finite Berry curvature. The
corresponding fluxes are quantified by Chern numbers that translate into a
quantized nonlocal conductance that we compute explicitly for the
time-reversal-symmetric scattering matrix. The topological regime can be also
identified by supercurrents as Josephson current-phase relationships exhibit
pronounced nonanalytic behavior and discontinuities near Weyl points that can
be controllably accessed in experiments.
",0,1,0,0,0,0
19777,Depth creates no more spurious local minima,"  We show that for any convex differentiable loss function, a deep linear
network has no spurious local minima as long as it is true for the two layer
case. When applied to the quadratic loss, our result immediately implies the
powerful result in [Kawaguchi 2016] that there is no spurious local minima in
deep linear networks. Further, with the recent work [Zhou and Liang 2018], we
can remove all the assumptions in [Kawaguchi 2016]. Our proof is short and
elementary. It builds on the recent work of [Laurent and von Brecht 2018] and
uses a new rank one perturbation argument.
",1,0,0,1,0,0
13682,Combinatorial properties of the G-degree,"  A strong interaction is known to exist between edge-colored graphs (which
encode PL pseudo-manifolds of arbitrary dimension) and random tensor models (as
a possible approach to the study of Quantum Gravity). The key tool is the {\it
G-degree} of the involved graphs, which drives the {\it $1/N$ expansion} in the
tensor models context. In the present paper - by making use of combinatorial
properties concerning Hamiltonian decompositions of the complete graph - we
prove that, in any even dimension $d\ge 4$, the G-degree of all bipartite
graphs, as well as of all (bipartite or non-bipartite) graphs representing
singular manifolds, is an integer multiple of $(d-1)!$. As a consequence, in
even dimension, the terms of the $1/N$ expansion corresponding to odd powers of
$1/N$ are null in the complex context, and do not involve colored graphs
representing singular manifolds in the real context.
In particular, in the 4-dimensional case, where the G-degree is shown to
depend only on the regular genera with respect to an arbitrary pair of
""associated"" cyclic permutations, several results are obtained, relating the
G-degree or the regular genus of 5-colored graphs and the Euler characteristic
of the associated PL 4-manifolds.
",0,0,1,0,0,0
7021,Propagation of regularity for the MHD system in optimal Sobolev space,"  We study the problem of propagation of regularity of solutions to the
incompressible viscous non-resistive magneto-hydrodynamics system. According to
scaling, the Sobolev space $H^{\frac n2-1}(\mathbb R^n)\times H^{\frac
n2}(\mathbb R^n)$ is critical for the system. We show that if a weak solution
$(u(t),b(t))$ is in $H^{s}(\mathbb R^n)\times H^{s+1}(\mathbb R^n)$ with
$s>\frac n2-1$ at a certain time $t_0$, then it will stay in the space for a
short time, provided the initial velocity $u(0)\in H^s(\mathbb R^n)$. In the
case that the uniqueness of weak solution in $H^{s}(\mathbb R^n)\times
H^{s+1}(\mathbb R^n)$ is known, the assumption of $u(0)\in H^s(\mathbb R^n)$ is
not necessary.
",0,1,1,0,0,0
7364,Robust Implicit Backpropagation,"  Arguably the biggest challenge in applying neural networks is tuning the
hyperparameters, in particular the learning rate. The sensitivity to the
learning rate is due to the reliance on backpropagation to train the network.
In this paper we present the first application of Implicit Stochastic Gradient
Descent (ISGD) to train neural networks, a method known in convex optimization
to be unconditionally stable and robust to the learning rate. Our key
contribution is a novel layer-wise approximation of ISGD which makes its
updates tractable for neural networks. Experiments show that our method is more
robust to high learning rates and generally outperforms standard
backpropagation on a variety of tasks.
",0,0,0,1,0,0
20030,Semiparametric spectral modeling of the Drosophila connectome,"  We present semiparametric spectral modeling of the complete larval Drosophila
mushroom body connectome. Motivated by a thorough exploratory data analysis of
the network via Gaussian mixture modeling (GMM) in the adjacency spectral
embedding (ASE) representation space, we introduce the latent structure model
(LSM) for network modeling and inference. LSM is a generalization of the
stochastic block model (SBM) and a special case of the random dot product graph
(RDPG) latent position model, and is amenable to semiparametric GMM in the ASE
representation space. The resulting connectome code derived via semiparametric
GMM composed with ASE captures latent connectome structure and elucidates
biologically relevant neuronal properties.
",0,0,0,1,0,0
3166,Deep learning enhanced mobile-phone microscopy,"  Mobile-phones have facilitated the creation of field-portable, cost-effective
imaging and sensing technologies that approach laboratory-grade instrument
performance. However, the optical imaging interfaces of mobile-phones are not
designed for microscopy and produce spatial and spectral distortions in imaging
microscopic specimens. Here, we report on the use of deep learning to correct
such distortions introduced by mobile-phone-based microscopes, facilitating the
production of high-resolution, denoised and colour-corrected images, matching
the performance of benchtop microscopes with high-end objective lenses, also
extending their limited depth-of-field. After training a convolutional neural
network, we successfully imaged various samples, including blood smears,
histopathology tissue sections, and parasites, where the recorded images were
highly compressed to ease storage and transmission for telemedicine
applications. This method is applicable to other low-cost, aberrated imaging
systems, and could offer alternatives for costly and bulky microscopes, while
also providing a framework for standardization of optical images for clinical
and biomedical applications.
",1,1,0,0,0,0
352,Neeman's characterization of K(R-Proj) via Bousfield localization,"  Let $R$ be an associative ring with unit and denote by $K({\rm R
\mbox{-}Proj})$ the homotopy category of complexes of projective left
$R$-modules. Neeman proved the theorem that $K({\rm R \mbox{-}Proj})$ is
$\aleph_1$-compactly generated, with the category $K^+ ({\rm R \mbox{-}proj})$
of left bounded complexes of finitely generated projective $R$-modules
providing an essentially small class of such generators. Another proof of
Neeman's theorem is explained, using recent ideas of Christensen and Holm, and
Emmanouil. The strategy of the proof is to show that every complex in $K({\rm R
\mbox{-}Proj})$ vanishes in the Bousfield localization $K({\rm R
\mbox{-}Flat})/\langle K^+ ({\rm R \mbox{-}proj}) \rangle.$
",0,0,1,0,0,0
10889,Estimating reducible stochastic differential equations by conversion to a least-squares problem,"  Stochastic differential equations (SDEs) are increasingly used in
longitudinal data analysis, compartmental models, growth modelling, and other
applications in a number of disciplines. Parameter estimation, however,
currently requires specialized software packages that can be difficult to use
and understand. This work develops and demonstrates an approach for estimating
reducible SDEs using standard nonlinear least squares or mixed-effects
software. Reducible SDEs are obtained through a change of variables in linear
SDEs, and are sufficiently flexible for modelling many situations. The approach
is based on extending a known technique that converts maximum likelihood
estimation for a Gaussian model with a nonlinear transformation of the
dependent variable into an equivalent least-squares problem. A similar idea can
be used for Bayesian maximum a posteriori estimation. It is shown how to obtain
parameter estimates for reducible SDEs containing both process and observation
noise, including hierarchical models with either fixed or random group
parameters. Code and examples in R are given. Univariate SDEs are discussed in
detail, with extensions to the multivariate case outlined more briefly. The use
of well tested and familiar standard software should make SDE modelling more
transparent and accessible. Keywords: stochastic processes; longitudinal data;
growth curves; compartmental models; mixed-effects; R
",0,0,0,1,0,0
13614,Extensions of interpolation between the arithmetic-geometric mean inequality for matrices,"  In this paper, we present some extensions of interpolation between the
arithmetic-geometric means inequality. Among other inequalities, it is shown
that if $A, B, X$ are $n\times n$ matrices, then \begin{align*}
\|AXB^*\|^2\leq\|f_1(A^*A)Xg_1(B^*B)\|\,\|f_2(A^*A)Xg_2(B^*B)\|, \end{align*}
where $f_1,f_2,g_1,g_2$ are non-negative continues functions such that
$f_1(t)f_2(t)=t$ and $g_1(t)g_2(t)=t\,\,(t\geq0)$. We also obtain the
inequality \begin{align*}
\left|\left|\left|AB^*\right|\right|\right|^2\nonumber&\leq
\left|\left|\left|p(A^*A)^{\frac{m}{p}}+
(1-p)(B^*B)^{\frac{s}{1-p}}\right|\right|\right|\,\left|\left|\left|(1-p)(A^*A)^{\frac{n}{1-p}}+
p(B^*B)^{\frac{t}{p}}\right|\right|\right|, \end{align*} in which $m,n,s,t$ are
real numbers such that $m+n=s+t=1$, $|||\cdot|||$ is an arbitrary unitarily
invariant norm and $p\in[0,1]$.
",0,0,1,0,0,0
16050,Particular type of gap in the spectrum of multiband superconductors,"  We show, that in contrast to the free electron model (standard BCS model), a
particular gap in the spectrum of multiband superconductors opens at some
distance from the Fermi energy, if conduction band is composed of hybridized
atomic orbitals of different symmetries. This gap has composite
superconducting-hybridization origin, because it exists only if both the
superconductivity and the hybridization between different kinds of orbitals are
present. So for many classes of superconductors with multiorbital structure
such spectrum changes should take place. These particular changes in the
spectrum at some distance from the Fermi level result in slow convergence of
the spectral weight of the optical conductivity even in quite conventional
superconductors with isotropic s-wave pairing mechanism.
",0,1,0,0,0,0
10959,Small-amplitude steady water waves with critical layers: non-symmetric waves,"  The problem for two-dimensional steady water waves with vorticity is
considered. Using methods of spatial dynamics, we reduce the problem to a
finite dimensional Hamiltonian system. As an application, we prove the
existence of non-symmetric steady water waves when the number of roots of the
dispersion equation is greater than 1.
",0,0,1,0,0,0
15873,Error estimates for Riemann sums of some singular functions,"  In this short note, we obtain error estimates for Riemann sums of some
singular functions.
",0,0,1,0,0,0
20717,A Benchmark on Reliability of Complex Discrete Systems: Emergency Power Supply of a Nuclear Power Plant,"  This paper contains two parts: the description of a real electrical system,
with many redundancies, reconfigurations and repairs, then the description of a
reliability model of this system, based on the BDMP (Boolean logic Driven
Markov Processes) formalism and partial results of a reliability and
availability calculation made from this model.
",1,0,0,0,0,0
10388,A Methodology for the Selection of Requirement Elicitation Techniques,"  In this paper, we present an approach to select a subset of requirement
elicitation technique for an optimum result in the requirement elicitation
process. Our approach consists of three steps. First, we identify various
attribute in three important dimensions namely project, people and the process
of software development that can influence the outcome of an elicitation
process. Second, we construct three p matrix (3PM) separately for each
dimension, that shows a relation between the elicitation techniques and three
dimensions of a software. Third, we provide a mapping criteria and use them in
the selection of a subset of elicitation techniques. We demonstrate the
applicability of the proposed approach using case studies to evaluate and
provide the contextual knowledge of selecting requirement elicitation
technique.
",1,0,0,0,0,0
2622,"Reconstruction from Periodic Nonlinearities, With Applications to HDR Imaging","  We consider the problem of reconstructing signals and images from periodic
nonlinearities. For such problems, we design a measurement scheme that supports
efficient reconstruction; moreover, our method can be adapted to extend to
compressive sensing-based signal and image acquisition systems. Our techniques
can be potentially useful for reducing the measurement complexity of high
dynamic range (HDR) imaging systems, with little loss in reconstruction
quality. Several numerical experiments on real data demonstrate the
effectiveness of our approach.
",0,0,0,1,0,0
18237,Gini-regularized Optimal Transport with an Application to Spatio-Temporal Forecasting,"  Rapidly growing product lines and services require a finer-granularity
forecast that considers geographic locales. However the open question remains,
how to assess the quality of a spatio-temporal forecast? In this manuscript we
introduce a metric to evaluate spatio-temporal forecasts. This metric is based
on an Opti- mal Transport (OT) problem. The metric we propose is a constrained
OT objec- tive function using the Gini impurity function as a regularizer. We
demonstrate through computer experiments both the qualitative and the
quantitative charac- teristics of the Gini regularized OT problem. Moreover, we
show that the Gini regularized OT problem converges to the classical OT
problem, when the Gini regularized problem is considered as a function of
{\lambda}, the regularization parame-ter. The convergence to the classical OT
solution is faster than the state-of-the-art Entropic-regularized OT[Cuturi,
2013] and results in a numerically more stable algorithm.
",1,0,0,1,0,0
6419,3k-4 theorem for ordered groups,"  Recently, G. A. Freiman, M. Herzog, P. Longobardi, M. Maj proved two
`structure theorems' for ordered groups \cite{FHLM}. We give elementary proof
of these two theorems.
",0,0,1,0,0,0
504,DeepSaucer: Unified Environment for Verifying Deep Neural Networks,"  In recent years, a number of methods for verifying DNNs have been developed.
Because the approaches of the methods differ and have their own limitations, we
think that a number of verification methods should be applied to a developed
DNN. To apply a number of methods to the DNN, it is necessary to translate
either the implementation of the DNN or the verification method so that one
runs in the same environment as the other. Since those translations are
time-consuming, a utility tool, named DeepSaucer, which helps to retain and
reuse implementations of DNNs, verification methods, and their environments, is
proposed. In DeepSaucer, code snippets of loading DNNs, running verification
methods, and creating their environments are retained and reused as software
assets in order to reduce cost of verifying DNNs. The feasibility of DeepSaucer
is confirmed by implementing it on the basis of Anaconda, which provides
virtual environment for loading a DNN and running a verification method. In
addition, the effectiveness of DeepSaucer is demonstrated by usecase examples.
",1,0,0,0,0,0
14818,Calabi-Yau hypersurfaces and SU-bordism,"  Batyrev constructed a family of Calabi-Yau hypersurfaces dual to the first
Chern class in toric Fano varieties. Using this construction, we introduce a
family of Calabi-Yau manifolds whose SU-bordism classes generate the special
unitary bordism ring
$\varOmega^{SU}\otimes\mathbb{Z}[\frac{1}{2}]\cong\mathbb{Z}[\frac{1}{2}][y_{i}\colon
i\ge 2]$. We also describe explicit Calabi-Yau representatives for
multiplicative generators of the SU-bordism ring in low dimensions.
",0,0,1,0,0,0
3740,A time series distance measure for efficient clustering of input output signals by their underlying dynamics,"  Starting from a dataset with input/output time series generated by multiple
deterministic linear dynamical systems, this paper tackles the problem of
automatically clustering these time series. We propose an extension to the
so-called Martin cepstral distance, that allows to efficiently cluster these
time series, and apply it to simulated electrical circuits data. Traditionally,
two ways of handling the problem are used. The first class of methods employs a
distance measure on time series (e.g. Euclidean, Dynamic Time Warping) and a
clustering technique (e.g. k-means, k-medoids, hierarchical clustering) to find
natural groups in the dataset. It is, however, often not clear whether these
distance measures effectively take into account the specific temporal
correlations in these time series. The second class of methods uses the
input/output data to identify a dynamic system using an identification scheme,
and then applies a model norm-based distance (e.g. H2, H-infinity) to find out
which systems are similar. This, however, can be very time consuming for large
amounts of long time series data. We show that the new distance measure
presented in this paper performs as good as when every input/output pair is
modelled explicitly, but remains computationally much less complex. The
complexity of calculating this distance between two time series of length N is
O(N logN).
",1,0,0,1,0,0
16643,The Value of Inferring the Internal State of Traffic Participants for Autonomous Freeway Driving,"  Safe interaction with human drivers is one of the primary challenges for
autonomous vehicles. In order to plan driving maneuvers effectively, the
vehicle's control system must infer and predict how humans will behave based on
their latent internal state (e.g., intentions and aggressiveness). This
research uses a simple model for human behavior with unknown parameters that
make up the internal states of the traffic participants and presents a method
for quantifying the value of estimating these states and planning with their
uncertainty explicitly modeled. An upper performance bound is established by an
omniscient Monte Carlo Tree Search (MCTS) planner that has perfect knowledge of
the internal states. A baseline lower bound is established by planning with
MCTS assuming that all drivers have the same internal state. MCTS variants are
then used to solve a partially observable Markov decision process (POMDP) that
models the internal state uncertainty to determine whether inferring the
internal state offers an advantage over the baseline. Applying this method to a
freeway lane changing scenario reveals that there is a significant performance
gap between the upper bound and baseline. POMDP planning techniques come close
to closing this gap, especially when important hidden model parameters are
correlated with measurable parameters.
",1,0,0,0,0,0
18929,Two-Step Disentanglement for Financial Data,"  In this work, we address the problem of disentanglement of factors that
generate a given data into those that are correlated with the labeling and
those that are not. Our solution is simpler than previous solutions and employs
adversarial training in a straightforward manner. We demonstrate the new method
on visual datasets as well as on financial data. In order to evaluate the
latter, we developed a hypothetical trading strategy whose performance is
affected by the performance of the disentanglement, namely, it trades better
when the factors are better separated.
",1,0,0,1,0,0
10585,Probabilistic interpretation of HJB equations by the representation theorem for generators of BSDEs,"  The purpose of this note is to propose a new approach for the probabilistic
interpretation of Hamilton-Jacobi-Bellman equations associated with stochastic
recursive optimal control problems, utilizing the representation theorem for
generators of backward stochastic differential equations. The key idea of our
approach for proving this interpretation consists of transmitting the signs
between the solution and generator via the identity given by representation
theorem. Compared with existing methods, our approach seems to be more
applicable for general settings. This can also be regarded as a new application
of such representation theorem.
",0,0,1,0,0,0
9533,Kepler red-clump stars in the field and in open clusters: constraints on core mixing,"  Convective mixing in Helium-core-burning (HeCB) stars is one of the
outstanding issues in stellar modelling. The precise asteroseismic measurements
of gravity-modes period spacing ($\Delta\Pi_1$) has opened the door to detailed
studies of the near-core structure of such stars, which had not been possible
before. Here we provide stringent tests of various core-mixing scenarios
against the largely unbiased population of red-clump stars belonging to the old
open clusters monitored by Kepler, and by coupling the updated precise
inference on $\Delta\Pi_1$ in thousands field stars with spectroscopic
constraints. We find that models with moderate overshooting successfully
reproduce the range observed of $\Delta\Pi_1$ in clusters. In particular we
show that there is no evidence for the need to extend the size of the
adiabatically stratified core, at least at the beginning of the HeCB phase.
This conclusion is based primarily on ensemble studies of $\Delta\Pi_1$ as a
function of mass and metallicity. While $\Delta\Pi_1$ shows no appreciable
dependence on the mass, we have found a clear dependence of $\Delta\Pi_1$ on
metallicity, which is also supported by predictions from models.
",0,1,0,0,0,0
16904,Framework for an Innovative Perceptive Mobile Network Using Joint Communication and Sensing,"  In this paper, we develop a framework for an innovative perceptive mobile
(i.e. cellular) network that integrates sensing with communication, and
supports new applications widely in transportation, surveillance and
environmental sensing. Three types of sensing methods implemented in the
base-stations are proposed, using either uplink or downlink multiuser
communication signals. The required changes to system hardware and major
technical challenges are briefly discussed. We also demonstrate the feasibility
of estimating sensing parameters via developing a compressive sensing based
scheme and providing simulation results to validate its effectiveness.
",1,0,0,0,0,0
16940,How to Quantize $n$ Outputs of a Binary Symmetric Channel to $n-1$ Bits?,"  Suppose that $Y^n$ is obtained by observing a uniform Bernoulli random vector
$X^n$ through a binary symmetric channel with crossover probability $\alpha$.
The ""most informative Boolean function"" conjecture postulates that the maximal
mutual information between $Y^n$ and any Boolean function $\mathrm{b}(X^n)$ is
attained by a dictator function. In this paper, we consider the ""complementary""
case in which the Boolean function is replaced by
$f:\left\{0,1\right\}^n\to\left\{0,1\right\}^{n-1}$, namely, an $n-1$ bit
quantizer, and show that $I(f(X^n);Y^n)\leq (n-1)\cdot\left(1-h(\alpha)\right)$
for any such $f$. Thus, in this case, the optimal function is of the form
$f(x^n)=(x_1,\ldots,x_{n-1})$.
",1,0,1,0,0,0
120,Improper posteriors are not improper,"  In 1933 Kolmogorov constructed a general theory that defines the modern
concept of conditional expectation. In 1955 Renyi fomulated a new axiomatic
theory for probability motivated by the need to include unbounded measures. We
introduce a general concept of conditional expectation in Renyi spaces. In this
theory improper priors are allowed, and the resulting posterior can also be
improper.
In 1965 Lindley published his classic text on Bayesian statistics using the
theory of Renyi, but retracted this idea in 1973 due to the appearance of
marginalization paradoxes presented by Dawid, Stone, and Zidek. The paradoxes
are investigated, and the seemingly conflicting results are explained. The
theory of Renyi can hence be used as an axiomatic basis for statistics that
allows use of unbounded priors.
Keywords: Haldane's prior; Poisson intensity; Marginalization paradox;
Measure theory; conditional probability space; axioms for statistics;
conditioning on a sigma field; improper prior
",0,0,1,1,0,0
11522,Extending holomorphic motions and monodromy,"  Let $E$ be a closed set in the Riemann sphere $\widehat{\mathbb{C}}$. We
consider a holomorphic motion $\phi$ of $E$ over a complex manifold $M$, that
is, a holomorphic family of injections on $E$ parametrized by $M$. It is known
that if $M$ is the unit disk $\Delta$ in the complex plane, then any
holomorphic motion of $E$ over $\Delta$ can be extended to a holomorphic motion
of the Riemann sphere over $\Delta$. In this paper, we consider conditions
under which a holomorphic motion of $E$ over a non-simply connected Riemann
surface $X$ can be extended to a holomorphic motion of $\widehat{\mathbb{C}}$
over $X$. Our main result shows that a topological condition, the triviality of
the monodromy, gives a necessary and sufficient condition for a holomorphic
motion of $E$ over $X$ to be extended to a holomorphic motion of
$\widehat{\mathbb{C}}$ over $X$. We give topological and geometric conditions
for a holomorphic motion over a Riemann surface to be extended. We also apply
our result to a lifting problem for holomorphic maps to Teichmüller spaces.
",0,0,1,0,0,0
5171,Solitons in Bose-Einstein Condensates with Helicoidal Spin-Orbit Coupling,"  We report on the existence and stability of freely moving solitons in a
spatially inhomogeneous Bose- Einstein condensate with helicoidal spin-orbit
(SO) coupling. In spite of the periodically varying parameters, the system
allows for the existence of stable propagating solitons. Such states are found
in the rotating frame, where the helicoidal SO coupling is reduced to a
homogeneous one. In the absence of the Zeeman splitting, the coupled
Gross-Pitaevskii equations describing localized states feature many properties
of the integrable systems. In particular, four-parametric families of solitons
can be obtained in the exact form. Such solitons interact elastically. Zeeman
splitting still allows for the existence of two families of moving solitons,
but makes collisions of solitons inelastic.
",0,1,0,0,0,0
3336,Exploiting routinely collected severe case data to monitor and predict influenza outbreaks,"  Influenza remains a significant burden on health systems. Effective responses
rely on the timely understanding of the magnitude and the evolution of an
outbreak. For monitoring purposes, data on severe cases of influenza in England
are reported weekly to Public Health England. These data are both readily
available and have the potential to provide valuable information to estimate
and predict the key transmission features of seasonal and pandemic influenza.
We propose an epidemic model that links the underlying unobserved influenza
transmission process to data on severe influenza cases. Within a Bayesian
framework, we infer retrospectively the parameters of the epidemic model for
each seasonal outbreak from 2012 to 2015, including: the effective reproduction
number; the initial susceptibility; the probability of admission to intensive
care given infection; and the effect of school closure on transmission. The
model is also implemented in real time to assess whether early forecasting of
the number of admission to intensive care is possible. Our model of admissions
data allows reconstruction of the underlying transmission dynamics revealing:
increased transmission during the season 2013/14 and a noticeable effect of
Christmas school holiday on disease spread during season 2012/13 and 2014/15.
When information on the initial immunity of the population is available,
forecasts of the number of admissions to intensive care can be substantially
improved. Readily available severe case data can be effectively used to
estimate epidemiological characteristics and to predict the evolution of an
epidemic, crucially allowing real-time monitoring of the transmission and
severity of the outbreak.
",0,1,0,1,0,0
6282,Wall modeling via function enrichment: extension to detached-eddy simulation,"  We extend the approach of wall modeling via function enrichment to
detached-eddy simulation. The wall model aims at using coarse cells in the
near-wall region by modeling the velocity profile in the viscous sublayer and
log-layer. However, unlike other wall models, the full Navier-Stokes equations
are still discretely fulfilled, including the pressure gradient and convective
term. This is achieved by enriching the elements of the high-order
discontinuous Galerkin method with the law-of-the-wall. As a result, the
Galerkin method can ""choose"" the optimal solution among the polynomial and
enrichment shape functions. The detached-eddy simulation methodology provides a
suitable turbulence model for the coarse near-wall cells. The approach is
applied to wall-modeled LES of turbulent channel flow in a wide range of
Reynolds numbers. Flow over periodic hills shows the superiority compared to an
equilibrium wall model under separated flow conditions.
",0,1,0,0,0,0
13317,Asymptotic Properties of the Maximum Likelihood Estimator in Regime Switching Econometric Models,"  Markov regime switching models have been widely used in numerous empirical
applications in economics and finance. However, the asymptotic distribution of
the maximum likelihood estimator (MLE) has not been proven for some empirically
popular Markov regime switching models. In particular, the asymptotic
distribution of the MLE has been unknown for models in which some elements of
the transition probability matrix have the value of zero, as is commonly
assumed in empirical applications with models with more than two regimes. This
also includes models in which the regime-specific density depends on both the
current and the lagged regimes such as the seminal model of Hamilton (1989) and
switching ARCH model of Hamilton and Susmel (1994). This paper shows the
asymptotic normality of the MLE and consistency of the asymptotic covariance
matrix estimate of these models.
",0,0,1,1,0,0
13244,Enemy At the Gateways: A Game Theoretic Approach to Proxy Distribution,"  A core technique used by popular proxy-based circumvention systems like Tor,
Psiphon, and Lantern is to secretly share the IP addresses of circumvention
proxies with the censored clients for them to be able to use such systems. For
instance, such secretly shared proxies are known as bridges in Tor. However, a
key challenge to this mechanism is the insider attack problem: censoring agents
can impersonate as benign censored clients in order to obtain (and then block)
such secretly shared circumvention proxies.
In this paper, we perform a fundamental study on the problem of insider
attack on proxy-based circumvention systems. We model the proxy distribution
problem using game theory, based on which we derive the optimal strategies of
the parties involved, i.e., the censors and circumvention system operators.
That is, we derive the optimal proxy distribution mechanism of a
circumvention system like Tor, against the censorship adversary who also takes
his optimal censorship strategies.
This is unlike previous works that design ad hoc mechanisms for proxy
distribution, against non-optimal censors.
We perform extensive simulations to evaluate our optimal proxy assignment
algorithm under various adversarial and network settings. Comparing with the
state-of-the-art prior work, we show that our optimal proxy assignment
algorithm has superior performance, i.e., better resistance to censorship even
against the strongest censorship adversary who takes her optimal actions. We
conclude with lessons and recommendation for the design of proxy-based
circumvention systems.
",1,0,0,0,0,0
11514,Semi-Supervised QA with Generative Domain-Adaptive Nets,"  We study the problem of semi-supervised question answering----utilizing
unlabeled text to boost the performance of question answering models. We
propose a novel training framework, the Generative Domain-Adaptive Nets. In
this framework, we train a generative model to generate questions based on the
unlabeled text, and combine model-generated questions with human-generated
questions for training question answering models. We develop novel domain
adaptation algorithms, based on reinforcement learning, to alleviate the
discrepancy between the model-generated data distribution and the
human-generated data distribution. Experiments show that our proposed framework
obtains substantial improvement from unlabeled text.
",1,0,0,0,0,0
17791,Subgraphs and motifs in a dynamic airline network,"  How does the small-scale topological structure of an airline network behave
as the network evolves? To address this question, we study the dynamic and
spatial properties of small undirected subgraphs using 15 years of data on
Southwest Airlines' domestic route service. We find that this real-world
network has much in common with random graphs, and describe a possible
power-law scaling between subgraph counts and the number of edges in the
network, that appears to be quite robust to changes in network density and
size. We use analytic formulae to identify statistically over- and
under-represented subgraphs, known as motifs and anti-motifs, and discover the
existence of substantial topology transitions. We propose a simple
subgraph-based node ranking measure, that is not always highly correlated with
standard node centrality, and can identify important nodes relative to specific
topologies, and investigate the spatial ""distribution"" of the triangle subgraph
using graphical tools. Our results have implications for the way in which
subgraphs can be used to analyze real-world networks.
",1,0,0,0,0,0
11754,OH Survey along Sightlines of Galactic Observations of Terahertz C+,"  We have obtained OH spectra of four transitions in the $^2\Pi_{3/2}$ ground
state, at 1612, 1665, 1667, and 1720 MHz, toward 51 sightlines that were
observed in the Herschel project Galactic Observations of Terahertz C+. The
observations cover the longitude range of (32$^\circ$, 64$^\circ$) and
(189$^\circ$, 207$^\circ$) in the northern Galactic plane. All of the diffuse
OH emissions conform to the so-called 'Sum Rule' of the four brightness
temperatures, indicating optically thin emission condition for OH from diffuse
clouds in the Galactic plane. The column densities of the HI `halos' N(HI)
surrounding molecular clouds increase monotonically with OH column density,
N(OH), until saturating when N(HI)=1.0 x 10$^{21}$ cm$^{-2}$ and N (OH) $\geq
4.5\times 10^{15}$ cm$^{-2}$, indicating the presence of molecular gas that
cannot be traced by HI. Such a linear correlation, albeit weak, is suggestive
of HI halos' contribution to the UV shielding required for molecular formation.
About 18% of OH clouds have no associated CO emission (CO-dark) at a
sensitivity of 0.07 K but are associated with C$^+$ emission. A weak
correlation exists between C$^+$ intensity and OH column density for CO-dark
molecular clouds. These results imply that OH seems to be a better tracer of
molecular gas than CO in diffuse molecular regions.
",0,1,0,0,0,0
5866,Global Robustness Evaluation of Deep Neural Networks with Provable Guarantees for the $L_0$ Norm,"  Deployment of deep neural networks (DNNs) in safety- or security-critical
systems requires provable guarantees on their correct behaviour. A common
requirement is robustness to adversarial perturbations in a neighbourhood
around an input. In this paper we focus on the $L_0$ norm and aim to compute,
for a trained DNN and an input, the maximal radius of a safe norm ball around
the input within which there are no adversarial examples. Then we define global
robustness as an expectation of the maximal safe radius over a test data set.
We first show that the problem is NP-hard, and then propose an approximate
approach to iteratively compute lower and upper bounds on the network's
robustness. The approach is \emph{anytime}, i.e., it returns intermediate
bounds and robustness estimates that are gradually, but strictly, improved as
the computation proceeds; \emph{tensor-based}, i.e., the computation is
conducted over a set of inputs simultaneously, instead of one by one, to enable
efficient GPU computation; and has \emph{provable guarantees}, i.e., both the
bounds and the robustness estimates can converge to their optimal values.
Finally, we demonstrate the utility of the proposed approach in practice to
compute tight bounds by applying and adapting the anytime algorithm to a set of
challenging problems, including global robustness evaluation, competitive $L_0$
attacks, test case generation for DNNs, and local robustness evaluation on
large-scale ImageNet DNNs. We release the code of all case studies via GitHub.
",0,0,0,1,0,0
4073,Fundamental solutions for second order parabolic systems with drift terms,"  We construct fundamental solutions of second-order parabolic systems of
divergence form with bounded and measurable leading coefficients and divergence
free first-order coefficients in the class of $BMO^{-1}_x$, under the
assumption that weak solutions of the system satisfy a certain local
boundedness estimate. We also establish Gaussian upper bound for such
fundamental solutions under the same conditions.
",0,0,1,0,0,0
721,Measurably entire functions and their growth,"  In 1997 B. Weiss introduced the notion of measurably entire functions and
proved that they exist on every arbitrary free C- action defined on standard
probability space. In the same paper he asked about the minimal possible growth
of measurably entire functions. In this work we show that for every arbitrary
free C- action defined on a standard probability space there exists a
measurably entire function whose growth does not exceed exp (exp[log^p |z|])
for any p > 3. This complements a recent result by Buhovski, Glücksam,
Logunov, and Sodin (arXiv:1703.08101) who showed that such functions cannot
grow slower than exp (exp[log^p |z|]) for any p < 2.
",0,0,1,0,0,0
17128,Controlling a population,"  We introduce a new setting where a population of agents, each modelled by a
finite-state system, are controlled uniformly: the controller applies the same
action to every agent. The framework is largely inspired by the control of a
biological system, namely a population of yeasts, where the controller may only
change the environment common to all cells. We study a synchronisation problem
for such populations: no matter how individual agents react to the actions of
the controller, the controller aims at driving all agents synchronously to a
target state. The agents are naturally represented by a non-deterministic
finite state automaton (NFA), the same for every agent, and the whole system is
encoded as a 2-player game. The first player (Controller) chooses actions, and
the second player (Agents) resolves non-determinism for each agent. The game
with m agents is called the m -population game. This gives rise to a
parameterized control problem (where control refers to 2 player games), namely
the population control problem: can Controller control the m-population game
for all m in N whatever Agents does?
",1,0,0,0,0,0
4016,Importance sampling the union of rare events with an application to power systems analysis,"  We consider importance sampling to estimate the probability $\mu$ of a union
of $J$ rare events $H_j$ defined by a random variable $\boldsymbol{x}$. The
sampler we study has been used in spatial statistics, genomics and
combinatorics going back at least to Karp and Luby (1983). It works by sampling
one event at random, then sampling $\boldsymbol{x}$ conditionally on that event
happening and it constructs an unbiased estimate of $\mu$ by multiplying an
inverse moment of the number of occuring events by the union bound. We prove
some variance bounds for this sampler. For a sample size of $n$, it has a
variance no larger than $\mu(\bar\mu-\mu)/n$ where $\bar\mu$ is the union
bound. It also has a coefficient of variation no larger than
$\sqrt{(J+J^{-1}-2)/(4n)}$ regardless of the overlap pattern among the $J$
events. Our motivating problem comes from power system reliability, where the
phase differences between connected nodes have a joint Gaussian distribution
and the $J$ rare events arise from unacceptably large phase differences. In the
grid reliability problems even some events defined by $5772$ constraints in
$326$ dimensions, with probability below $10^{-22}$, are estimated with a
coefficient of variation of about $0.0024$ with only $n=10{,}000$ sample
values.
",1,0,0,1,0,0
11634,Data-Driven Decentralized Optimal Power Flow,"  The implementation of optimal power flow (OPF) methods to perform voltage and
power flow regulation in electric networks is generally believed to require
communication. We consider distribution systems with multiple controllable
Distributed Energy Resources (DERs) and present a data-driven approach to learn
control policies for each DER to reconstruct and mimic the solution to a
centralized OPF problem from solely locally available information.
Collectively, all local controllers closely match the centralized OPF solution,
providing near-optimal performance and satisfaction of system constraints. A
rate distortion framework facilitates the analysis of how well the resulting
fully decentralized control policies are able to reconstruct the OPF solution.
Our methodology provides a natural extension to decide what buses a DER should
communicate with to improve the reconstruction of its individual policy. The
method is applied on both single- and three-phase test feeder networks using
data from real loads and distributed generators. It provides a framework for
Distribution System Operators to efficiently plan and operate the contributions
of DERs to active distribution networks.
",0,0,0,1,0,0
4049,Household poverty classification in data-scarce environments: a machine learning approach,"  We describe a method to identify poor households in data-scarce countries by
leveraging information contained in nationally representative household
surveys. It employs standard statistical learning techniques---cross-validation
and parameter regularization---which together reduce the extent to which the
model is over-fitted to match the idiosyncracies of observed survey data. The
automated framework satisfies three important constraints of this development
setting: i) The prediction model uses at most ten questions, which limits the
costs of data collection; ii) No computation beyond simple arithmetic is needed
to calculate the probability that a given household is poor, immediately after
data on the ten indicators is collected; and iii) One specification of the
model (i.e. one scorecard) is used to predict poverty throughout a country that
may be characterized by significant sub-national differences. Using survey data
from Zambia, the model's out-of-sample predictions distinguish poor households
from non-poor households using information contained in ten questions.
",0,0,0,1,0,0
4418,Multi-focus Attention Network for Efficient Deep Reinforcement Learning,"  Deep reinforcement learning (DRL) has shown incredible performance in
learning various tasks to the human level. However, unlike human perception,
current DRL models connect the entire low-level sensory input to the
state-action values rather than exploiting the relationship between and among
entities that constitute the sensory input. Because of this difference, DRL
needs vast amount of experience samples to learn. In this paper, we propose a
Multi-focus Attention Network (MANet) which mimics human ability to spatially
abstract the low-level sensory input into multiple entities and attend to them
simultaneously. The proposed method first divides the low-level input into
several segments which we refer to as partial states. After this segmentation,
parallel attention layers attend to the partial states relevant to solving the
task. Our model estimates state-action values using these attended partial
states. In our experiments, MANet attains highest scores with significantly
less experience samples. Additionally, the model shows higher performance
compared to the Deep Q-network and the single attention model as benchmarks.
Furthermore, we extend our model to attentive communication model for
performing multi-agent cooperative tasks. In multi-agent cooperative task
experiments, our model shows 20% faster learning than existing state-of-the-art
model.
",1,0,0,1,0,0
1372,Quantitative evaluation of an active Chemotaxis model in Discrete time,"  A system of $N$ particles in a chemical medium in $\mathbb{R}^{d}$ is studied
in a discrete time setting. Underlying interacting particle system in
continuous time can be expressed as \begin{eqnarray} dX_{i}(t)
&=&[-(I-A)X_{i}(t) + \bigtriangledown h(t,X_{i}(t))]dt + dW_{i}(t), \,\,
X_{i}(0)=x_{i}\in \mathbb{R}^{d}\,\,\forall i=1,\ldots,N\nonumber\\
\frac{\partial}{\partial t} h(t,x)&=&-\alpha h(t,x) + D\bigtriangleup h(t,x)
+\frac{\beta}{n} \sum_{i=1}^{N} g(X_{i}(t),x),\quad h(0,\cdot) =
h(\cdot).\label{main} \end{eqnarray} where $X_{i}(t)$ is the location of the
$i$th particle at time $t$ and $h(t,x)$ is the function measuring the
concentration of the medium at location $x$ with $h(0,x) = h(x)$. In this
article we describe a general discrete time non-linear formulation of the
aforementioned model and a strongly coupled particle system approximating it.
Similar models have been studied before (Budhiraja et al.(2011)) under a
restrictive compactness assumption on the domain of particles. In current work
the particles take values in $\R^{d}$ and consequently the stability analysis
is particularly challenging. We provide sufficient conditions for the existence
of a unique fixed point for the dynamical system governing the large $N$
asymptotics of the particle empirical measure. We also provide uniform in time
convergence rates for the particle empirical measure to the corresponding limit
measure under suitable conditions on the model.
",0,0,1,0,0,0
13177,Quasinonexpansive Iterations on the Affine Hull of Orbits: From Mann's Mean Value Algorithm to Inertial Methods,"  Fixed point iterations play a central role in the design and the analysis of
a large number of optimization algorithms. We study a new iterative scheme in
which the update is obtained by applying a composition of quasinonexpansive
operators to a point in the affine hull of the orbit generated up to the
current iterate. This investigation unifies several algorithmic constructs,
including Mann's mean value method, inertial methods, and multi-layer
memoryless methods. It also provides a framework for the development of new
algorithms, such as those we propose for solving monotone inclusion and
minimization problems.
",0,0,1,0,0,0
4326,Graded components of local cohomology modules,"  Let $A$ be a regular ring containing a field of characteristic zero and let
$R = A[X_1,\ldots, X_m]$. Consider $R$ as standard graded with $deg \ A = 0$
and $deg \ X_i = 1$ for all $i$. In this paper we present a comprehensive study
of graded components of local cohomology modules $H^i_I(R)$ where $I$ is an
\emph{arbitrary} homogeneous ideal in $R$. Our study seems to be the first in
this regard.
",0,0,1,0,0,0
12204,"On one nearly everywhere continuous and nowhere differentiable function, that defined by automaton with finite memory","  This paper is devoted to the investigation of the following function $$ f:
x=\Delta^{3}_{\alpha_{1}\alpha_{2}...\alpha_{n}...}{\rightarrow}
\Delta^{3}_{\varphi(\alpha_{1})\varphi(\alpha_{2})...\varphi(\alpha_{n})...}=f(x)=y,
$$ where $\varphi(i)=\frac{-3i^{2}+7i}{2}$, $ i \in N^{0}_{2}=\{0,1,2\}$, and
$\Delta^{3}_{\alpha_{1}\alpha_{2}...\alpha_{n}...}$ is the ternary
representation of $x \in [0;1]$. That is values of this function are obtained
from the ternary representation of the argument by the following change of
digits: 0 by 0, 1 by 2, and 2 by 1. This function preserves the ternary digit
$0$.
Main mapping properties and differential, integral, fractal properties of the
function are studied. Equivalent representations by additionally defined
auxiliary functions of this function are proved.
This paper is the paper translated from Ukrainian (the Ukrainian variant
available at this https URL). In 2012, the
Ukrainian variant of this paper was represented by the author in the
International Scientific Conference ""Asymptotic Methods in the Theory of
Differential Equations"" dedicated to 80th anniversary of M. I. Shkil (the
conference paper available at
this https URL). In 2013, the
investigations of the present article were generalized by the author in the
paper ""One one class of functions with complicated local structure""
(this https URL) and in the several conference papers
(available at: this https URL,
this https URL).
",0,0,1,0,0,0
11697,Chondrule Accretion with a Growing Protoplanet,"  Chondrules are primitive materials in the Solar System. They are formed in
the first about 3 Myr of the Solar System's history. This timescale is longer
than that of Mars formation, and it is conceivable that protoplanets,
planetesimals and chondrules might have existed simultaneously in the solar
nebula. Due to protoplanets perturbation on the planetesimal dynamics and
chondrule accretion on them, all the formed chondrules are unlikely to be
accreted by planetesimals. We investigate the amount of chondrules accreted by
planetesimals in such a condition. We assume that a protoplanet is in
oligarchic growth, and we perform analytical calculations of chondrule
accretion both by a protoplanet and by planetesimals. Through the oligarchic
growth stage, planetesimals accrete about half of the formed chondrules. The
smallest planetesimals get the largest amount of the chondrules, compared with
the amount accreted by more massive planetesimals. We perform a parameter study
and find that this fraction is not largely changed for a wide range of
parameter sets.
",0,1,0,0,0,0
9711,The Swift/BAT AGN Spectroscopic Survey (BASS) -- VI. The Gamma_X - L/L_Edd relation,"  We study the observed relation between accretion rate (in terms of L/L_Edd)
and shape of the hard X-ray spectral energy distribution (namely the photon
index Gamma_X) for a large sample of 228 hard X-ray selected, low-redshift
active galactic nuclei (AGN), drawn from the Swift/BAT AGN Spectroscopic Survey
(BASS). This includes 30 AGN for which black hole mass (and therefore L/L_Edd)
is measured directly through masers, spatially resolved gas or stellar
dynamics, or reverberation mapping. The high quality and broad energy coverage
of the data provided through BASS allow us to examine several alternative
determinations of both Gamma_X and L/L_Edd. For the BASS sample as a whole, we
find a statistically significant, albeit very weak correlation between Gamma_X
and L/L_Edd. The best-fitting relations we find, Gamma_X=0.15
log(L/L_Edd)+const., are considerably shallower than those reported in previous
studies. Moreover, we find no corresponding correlations among the subsets of
AGN with different M_BH determination methodology. In particular, we find no
robust evidence for a correlation when considering only those AGN with direct
or single-epoch M_BH estimates. This latter finding is in contrast to several
previous studies which focused on z>0.5 broad-line AGN. We discuss this tension
and conclude that it can be partially accounted for if one adopts a simplified,
power-law X-ray spectral model, combined with L/L_Edd estimates that are based
on the continuum emission and on single-epoch broad line spectroscopy in the
optical regime. We finally highlight the limitations on using Gamma_X as a
probe of supermassive black hole evolution in deep extragalactic X-ray surveys.
",0,1,0,0,0,0
16277,A New Backpressure Algorithm for Joint Rate Control and Routing with Vanishing Utility Optimality Gaps and Finite Queue Lengths,"  The backpressure algorithm has been widely used as a distributed solution to
the problem of joint rate control and routing in multi-hop data networks. By
controlling a parameter $V$ in the algorithm, the backpressure algorithm can
achieve an arbitrarily small utility optimality gap. However, this in turn
brings in a large queue length at each node and hence causes large network
delay. This phenomenon is known as the fundamental utility-delay tradeoff. The
best known utility-delay tradeoff for general networks is $[O(1/V), O(V)]$ and
is attained by a backpressure algorithm based on a drift-plus-penalty
technique. This may suggest that to achieve an arbitrarily small utility
optimality gap, the existing backpressure algorithms necessarily yield an
arbitrarily large queue length. However, this paper proposes a new backpressure
algorithm that has a vanishing utility optimality gap, so utility converges to
exact optimality as the algorithm keeps running, while queue lengths are
bounded throughout by a finite constant. The technique uses backpressure and
drift concepts with a new method for convex programming.
",1,0,1,0,0,0
17401,Fast Amortized Inference and Learning in Log-linear Models with Randomly Perturbed Nearest Neighbor Search,"  Inference in log-linear models scales linearly in the size of output space in
the worst-case. This is often a bottleneck in natural language processing and
computer vision tasks when the output space is feasibly enumerable but very
large. We propose a method to perform inference in log-linear models with
sublinear amortized cost. Our idea hinges on using Gumbel random variable
perturbations and a pre-computed Maximum Inner Product Search data structure to
access the most-likely elements in sublinear amortized time. Our method yields
provable runtime and accuracy guarantees. Further, we present empirical
experiments on ImageNet and Word Embeddings showing significant speedups for
sampling, inference, and learning in log-linear models.
",1,0,0,1,0,0
16512,Integral points on the complement of plane quartics,"  Let $Y$ be the complement of a plane quartic curve $D$ defined over a number
field. Our main theorem confirms the Lang-Vojta conjecture for $Y$ when $D$ is
a generic smooth quartic curve, by showing that its integral points are
confined in a curve except for a finite number of exceptions. The required
finiteness will be obtained by reducing it to the Shafarevich conjecture for K3
surfaces. Some variants of our method confirm the same conjecture when $D$ is a
reducible generic quartic curve which consists of four lines, two lines and a
conic, or two conics.
",0,0,1,0,0,0
14196,Targeted matrix completion,"  Matrix completion is a problem that arises in many data-analysis settings
where the input consists of a partially-observed matrix (e.g., recommender
systems, traffic matrix analysis etc.). Classical approaches to matrix
completion assume that the input partially-observed matrix is low rank. The
success of these methods depends on the number of observed entries and the rank
of the matrix; the larger the rank, the more entries need to be observed in
order to accurately complete the matrix. In this paper, we deal with matrices
that are not necessarily low rank themselves, but rather they contain low-rank
submatrices. We propose Targeted, which is a general framework for completing
such matrices. In this framework, we first extract the low-rank submatrices and
then apply a matrix-completion algorithm to these low-rank submatrices as well
as the remainder matrix separately. Although for the completion itself we use
state-of-the-art completion methods, our results demonstrate that Targeted
achieves significantly smaller reconstruction errors than other classical
matrix-completion methods. One of the key technical contributions of the paper
lies in the identification of the low-rank submatrices from the input
partially-observed matrices.
",1,0,0,1,0,0
9076,Masked Autoregressive Flow for Density Estimation,"  Autoregressive models are among the best performing neural density
estimators. We describe an approach for increasing the flexibility of an
autoregressive model, based on modelling the random numbers that the model uses
internally when generating data. By constructing a stack of autoregressive
models, each modelling the random numbers of the next model in the stack, we
obtain a type of normalizing flow suitable for density estimation, which we
call Masked Autoregressive Flow. This type of flow is closely related to
Inverse Autoregressive Flow and is a generalization of Real NVP. Masked
Autoregressive Flow achieves state-of-the-art performance in a range of
general-purpose density estimation tasks.
",1,0,0,1,0,0
18193,Temporal Logic Task Planning and Intermittent Connectivity Control of Mobile Robot Networks,"  In this paper, we develop a distributed intermittent communication and task
planning framework for mobile robot teams. The goal of the robots is to
accomplish complex tasks, captured by local Linear Temporal Logic formulas, and
share the collected information with all other robots and possibly also with a
user. Specifically, we consider situations where the robot communication
capabilities are not sufficient to form reliable and connected networks while
the robots move to accomplish their tasks. In this case, intermittent
communication protocols are necessary that allow the robots to temporarily
disconnect from the network in order to accomplish their tasks free of
communication constraints. We assume that the robots can only communicate with
each other when they meet at common locations in space. Our distributed control
framework jointly determines local plans that allow all robots fulfill their
assigned temporal tasks, sequences of communication events that guarantee
information exchange infinitely often, and optimal communication locations that
minimize a desired distance metric. Simulation results verify the efficacy of
the proposed controllers.
",1,0,0,0,0,0
209,Clamped seismic metamaterials: Ultra-low broad frequency stop-bands,"  The regularity of earthquakes, their destructive power, and the nuisance of
ground vibration in urban environments, all motivate designs of defence
structures to lessen the impact of seismic and ground vibration waves on
buildings. Low frequency waves, in the range $1$ to $10$ Hz for earthquakes and
up to a few tens of Hz for vibrations generated by human activities, cause a
large amount of damage, or inconvenience, depending on the geological
conditions they can travel considerable distances and may match the resonant
fundamental frequency of buildings. The ultimate aim of any seismic
metamaterial, or any other seismic shield, is to protect over this entire range
of frequencies, the long wavelengths involved, and low frequency, have meant
this has been unachievable to date.
Elastic flexural waves, applicable in the mechanical vibrations of thin
elastic plates, can be designed to have a broad zero-frequency stop-band using
a periodic array of very small clamped circles. Inspired by this experimental
and theoretical observation, all be it in a situation far removed from seismic
waves, we demonstrate that it is possible to achieve elastic surface (Rayleigh)
and body (pressure P and shear S) wave reflectors at very large wavelengths in
structured soils modelled as a fully elastic layer periodically clamped to
bedrock.
We identify zero frequency stop-bands that only exist in the limit of columns
of concrete clamped at their base to the bedrock. In a realistic configuration
of a sedimentary basin 15 meters deep we observe a zero frequency stop-band
covering a broad frequency range of $0$ to $30$ Hz.
",0,1,0,0,0,0
3249,A Random Sample Partition Data Model for Big Data Analysis,"  Big data sets must be carefully partitioned into statistically similar data
subsets that can be used as representative samples for big data analysis tasks.
In this paper, we propose the random sample partition (RSP) data model to
represent a big data set as a set of non-overlapping data subsets, called RSP
data blocks, where each RSP data block has a probability distribution similar
to the whole big data set. Under this data model, efficient block level
sampling is used to randomly select RSP data blocks, replacing expensive record
level sampling to select sample data from a big distributed data set on a
computing cluster. We show how RSP data blocks can be employed to estimate
statistics of a big data set and build models which are equivalent to those
built from the whole big data set. In this approach, analysis of a big data set
becomes analysis of few RSP data blocks which have been generated in advance on
the computing cluster. Therefore, the new method for data analysis based on RSP
data blocks is scalable to big data.
",1,0,0,1,0,0
4038,"Sparsity/Undersampling Tradeoffs in Anisotropic Undersampling, with Applications in MR Imaging/Spectroscopy","  We study anisotropic undersampling schemes like those used in
multi-dimensional NMR spectroscopy and MR imaging, which sample exhaustively in
certain time dimensions and randomly in others.
Our analysis shows that anisotropic undersampling schemes are equivalent to
certain block-diagonal measurement systems. We develop novel exact formulas for
the sparsity/undersampling tradeoffs in such measurement systems. Our formulas
predict finite-N phase transition behavior differing substantially from the
well known asymptotic phase transitions for classical Gaussian undersampling.
Extensive empirical work shows that our formulas accurately describe observed
finite-N behavior, while the usual formulas based on universality are
substantially inaccurate.
We also vary the anisotropy, keeping the total number of samples fixed, and
for each variation we determine the precise sparsity/undersampling tradeoff
(phase transition). We show that, other things being equal, the ability to
recover a sparse object decreases with an increasing number of
exhaustively-sampled dimensions.
",1,0,0,0,0,0
2999,Looping and Clustering model for the organization of protein-DNA complexes on the bacterial genome,"  The bacterial genome is organized in a structure called the nucleoid by a
variety of associated proteins. These proteins can form complexes on DNA that
play a central role in various biological processes, including chromosome
segregation. A prominent example is the large ParB-DNA complex, which forms an
essential component of the segregation machinery in many bacteria. ChIP-Seq
experiments show that ParB proteins localize around centromere-like parS sites
on the DNA to which ParB binds specifically, and spreads from there over large
sections of the chromosome. Recent theoretical and experimental studies suggest
that DNA-bound ParB proteins can interact with each other to condense into a
coherent 3D complex on the DNA. However, the structural organization of this
protein-DNA complex remains unclear, and a predictive quantitative theory for
the distribution of ParB proteins on DNA is lacking. Here, we propose the
Looping and Clustering (LC) model, which employs a statistical physics approach
to describe protein-DNA complexes. The LC model accounts for the extrusion of
DNA loops from a cluster of interacting DNA-bound proteins. Conceptually, the
structure of the protein-DNA complex is determined by a competition between
attractive protein interactions and the configurational and loop entropy of
this protein-DNA cluster. Indeed, we show that the protein interaction strength
determines the ""tightness"" of the loopy protein-DNA complex. With this approach
we consider the genomic organization of such a protein-DNA cluster around a
single high-affinity binding site. Thus, our model provides a theoretical
framework to quantitatively compute the binding profiles of ParB-like proteins
around a cognate (parS) binding site.
",0,1,0,0,0,0
7841,Guiding Chemical Synthesis: Computational Prediction of the Regioselectivity of CH Functionalization,"  We will develop a computational method (RegioSQM) for predicting the
regioselectivity of CH functionalization reactions that can be used by
synthetic chemists who are not experts in computational chemistry through a
simple web interface (regiosqm.org). CH functionalization, i.e. replacing the
hydrogen atom in a CH bond with another atom or molecule, is arguably single
most promising technique for increasing the efficiency of chemical synthesis,
but there are no generally applicable predictive tools that predict which CH
bond is most reactive. RegioSQM uses semiempirical quantum chemistry methods to
predict relative stabilities of reaction intermediates which correlates with
reaction rate and our goal is to determine which quantum method and
intermediate give the best result for each reaction type. Finally, we will
experimentally demonstrate how RegioSQM can be used to make the chemical
synthesis part of drug discovery more efficient.
",0,1,0,0,0,0
12358,Bounded game-theoretic semantics for modal mu-calculus,"  We introduce a new game-theoretic semantics (GTS) for the modal mu-calculus.
Our so-called bounded GTS replaces parity games with novel alternative
evaluation games where only finite paths arise. Infinite paths are not needed
even when the considered transition system is infinite.
",1,0,1,0,0,0
8014,Zonotope hit-and-run for efficient sampling from projection DPPs,"  Determinantal point processes (DPPs) are distributions over sets of items
that model diversity using kernels. Their applications in machine learning
include summary extraction and recommendation systems. Yet, the cost of
sampling from a DPP is prohibitive in large-scale applications, which has
triggered an effort towards efficient approximate samplers. We build a novel
MCMC sampler that combines ideas from combinatorial geometry, linear
programming, and Monte Carlo methods to sample from DPPs with a fixed sample
cardinality, also called projection DPPs. Our sampler leverages the ability of
the hit-and-run MCMC kernel to efficiently move across convex bodies. Previous
theoretical results yield a fast mixing time of our chain when targeting a
distribution that is close to a projection DPP, but not a DPP in general. Our
empirical results demonstrate that this extends to sampling projection DPPs,
i.e., our sampler is more sample-efficient than previous approaches which in
turn translates to faster convergence when dealing with costly-to-evaluate
functions, such as summary extraction in our experiments.
",1,0,0,1,0,0
12407,A bound for rational Thurston-Bennequin invariants,"  In this paper, we introduce a rational $\tau$ invariant for rationally
null-homologous knots in contact 3-manifolds with nontrivial
Ozsváth-Szabó contact invariants. Such an invariant is an upper bound
for the sum of rational Thurston-Bennequin invariant and the rational rotation
number of the Legendrian representatives of the knot. In the special case of
Floer simple knots in L-spaces, we can compute the rational $\tau$ invariants
by correction terms.
",0,0,1,0,0,0
17366,Deterministic and Probabilistic Conditions for Finite Completability of Low-rank Multi-View Data,"  We consider the multi-view data completion problem, i.e., to complete a
matrix $\mathbf{U}=[\mathbf{U}_1|\mathbf{U}_2]$ where the ranks of
$\mathbf{U},\mathbf{U}_1$, and $\mathbf{U}_2$ are given. In particular, we
investigate the fundamental conditions on the sampling pattern, i.e., locations
of the sampled entries for finite completability of such a multi-view data
given the corresponding rank constraints. In contrast with the existing
analysis on Grassmannian manifold for a single-view matrix, i.e., conventional
matrix completion, we propose a geometric analysis on the manifold structure
for multi-view data to incorporate more than one rank constraint. We provide a
deterministic necessary and sufficient condition on the sampling pattern for
finite completability. We also give a probabilistic condition in terms of the
number of samples per column that guarantees finite completability with high
probability. Finally, using the developed tools, we derive the deterministic
and probabilistic guarantees for unique completability.
",1,0,1,0,0,0
18845,Denoising Neural Machine Translation Training with Trusted Data and Online Data Selection,"  Measuring domain relevance of data and identifying or selecting well-fit
domain data for machine translation (MT) is a well-studied topic, but denoising
is not yet. Denoising is concerned with a different type of data quality and
tries to reduce the negative impact of data noise on MT training, in
particular, neural MT (NMT) training. This paper generalizes methods for
measuring and selecting data for domain MT and applies them to denoising NMT
training. The proposed approach uses trusted data and a denoising curriculum
realized by online data selection. Intrinsic and extrinsic evaluations of the
approach show its significant effectiveness for NMT to train on data with
severe noise.
",0,0,0,1,0,0
5459,Sparse Phase Retrieval via Sparse PCA Despite Model Misspecification: A Simplified and Extended Analysis,"  We consider the problem of high-dimensional misspecified phase retrieval.
This is where we have an $s$-sparse signal vector $\mathbf{x}_*$ in
$\mathbb{R}^n$, which we wish to recover using sampling vectors
$\textbf{a}_1,\ldots,\textbf{a}_m$, and measurements $y_1,\ldots,y_m$, which
are related by the equation $f(\left<\textbf{a}_i,\textbf{x}_*\right>) = y_i$.
Here, $f$ is an unknown link function satisfying a positive correlation with
the quadratic function. This problem was analyzed in a recent paper by Neykov,
Wang and Liu, who provided recovery guarantees for a two-stage algorithm with
sample complexity $m = O(s^2\log n)$. In this paper, we show that the first
stage of their algorithm suffices for signal recovery with the same sample
complexity, and extend the analysis to non-Gaussian measurements. Furthermore,
we show how the algorithm can be generalized to recover a signal vector
$\textbf{x}_*$ efficiently given geometric prior information other than
sparsity.
",1,0,1,0,0,0
11030,Integral and measure-turnpike properties for infinite-dimensional optimal control systems,"  We first derive a general integral-turnpike property around a set for
infinite-dimensional non-autonomous optimal control problems with any possible
terminal state constraints, under some appropriate assumptions. Roughly
speaking, the integral-turnpike property means that the time average of the
distance from any optimal trajectory to the turnpike set con- verges to zero,
as the time horizon tends to infinity. Then, we establish the measure-turnpike
property for strictly dissipative optimal control systems, with state and
control constraints. The measure-turnpike property, which is slightly stronger
than the integral-turnpike property, means that any optimal (state and control)
solution remains essentially, along the time frame, close to an optimal
solution of an associated static optimal control problem, except along a subset
of times that is of small relative Lebesgue measure as the time horizon is
large. Next, we prove that strict strong duality, which is a classical notion
in optimization, implies strict dissipativity, and measure-turnpike. Finally,
we conclude the paper with several comments and open problems.
",0,0,1,0,0,0
9614,Gaussian Process Regression for Arctic Coastal Erosion Forecasting,"  Arctic coastal morphology is governed by multiple factors, many of which are
affected by climatological changes. As the season length for shorefast ice
decreases and temperatures warm permafrost soils, coastlines are more
susceptible to erosion from storm waves. Such coastal erosion is a concern,
since the majority of the population centers and infrastructure in the Arctic
are located near the coasts. Stakeholders and decision makers increasingly need
models capable of scenario-based predictions to assess and mitigate the effects
of coastal morphology on infrastructure and land use. Our research uses
Gaussian process models to forecast Arctic coastal erosion along the Beaufort
Sea near Drew Point, AK. Gaussian process regression is a data-driven modeling
methodology capable of extracting patterns and trends from data-sparse
environments such as remote Arctic coastlines. To train our model, we use
annual coastline positions and near-shore summer temperature averages from
existing datasets and extend these data by extracting additional coastlines
from satellite imagery. We combine our calibrated models with future climate
models to generate a range of plausible future erosion scenarios. Our results
show that the Gaussian process methodology substantially improves yearly
predictions compared to linear and nonlinear least squares methods, and is
capable of generating detailed forecasts suitable for use by decision makers.
",0,1,0,1,0,0
19884,Topology-optimized Dual-Polarization Dirac Cones,"  We apply a large-scale computational technique, known as topology
optimization, to the inverse design of photonic Dirac cones. In particular, we
report on a variety of photonic crystal geometries, realizable in simple
isotropic dielectric materials, which exhibit dual-polarization and
dual-wavelength Dirac cones. We demonstrate the flexibility of this technique
by designing photonic crystals of different symmetry types, such as ones with
four-fold and six-fold rotational symmetry, which possess Dirac cones at
different points within the Brillouin zone. The demonstrated and related
optimization techniques could open new avenues to band-structure engineering
and manipulating the propagation of light in periodic media, with possible
applications in exotic optical phenomena such as effective zero-index media and
topological photonics.
",0,1,0,0,0,0
6217,The Impedance of Flat Metallic Plates with Small Corrugations,"  Summarizes recent work on the wakefields and impedances of flat, metallic
plates with small corrugations
",0,1,0,0,0,0
9677,Processes accompanying stimulated recombination of atoms,"  The phenomenon of polarization of nuclei in the process of stimulated
recombination of atoms in the field of circularly polarized laser radiation is
considered. This effect is considered for the case of the proton-electron beams
used in the method of electron cooling. An estimate is obtained for the maximum
degree of polarization of the protons on components of the hyperfine structure
of the 2s state of the hydrogen atom.
",0,1,0,0,0,0
16216,On the representation of integers by binary quadratic forms,"  In this note we show that for a given irreducible binary quadratic form
$f(x,y)$ with integer coefficients, whenever we have $f(x,y) = f(u,v)$ for
integers $x,y,u,v$, there exists a rational automorphism of $f$ which sends
$(x,y)$ to $(u,v)$.
",0,0,1,0,0,0
4111,An Information Matrix Approach for State Secrecy,"  This paper studies the problem of remote state estimation in the presence of
a passive eavesdropper. A sensor measures a linear plant's state and transmits
it to an authorized user over a packet-dropping channel, which is susceptible
to eavesdropping. Our goal is to design a coding scheme such that the
eavesdropper cannot infer the plant's current state, while the user
successfully decodes the sent messages. We employ a novel class of codes,
termed State-Secrecy Codes, which are fast and efficient for dynamical systems.
They apply linear time-varying transformations to the current and past states
received by the user. In this way, they force the eavesdropper's information
matrix to decrease with asymptotically the same rate as in the open-loop
prediction case, i.e. when the eavesdropper misses all messages. As a result,
the eavesdropper's minimum mean square error (mmse) for the unstable states
grows unbounded, while the respective error for the stable states converges to
the open-loop prediction one. These secrecy guarantees are achieved under
minimal conditions, which require that, at least once, the user receives the
corresponding packet while the eavesdropper fails to intercept it. Meanwhile,
the user's estimation performance remains optimal. The theoretical results are
illustrated in simulations.
",1,0,0,0,0,0
8188,Soliton solutions for the elastic metric on spaces of curves,"  In this article we investigate a first order reparametrization-invariant
Sobolev metric on the space of immersed curves. Motivated by applications in
shape analysis where discretizations of this infinite-dimensional space are
needed, we extend this metric to the space of Lipschitz curves, establish the
wellposedness of the geodesic equation thereon, and show that the space of
piecewise linear curves is a totally geodesic submanifold. Thus, piecewise
linear curves are natural finite elements for the discretization of the
geodesic equation. Interestingly, geodesics in this space can be seen as
soliton solutions of the geodesic equation, which were not known to exist for
reparametrization-invariant Sobolev metrics on spaces of curves.
",0,0,1,0,0,0
4129,A recognition algorithm for simple-triangle graphs,"  A simple-triangle graph is the intersection graph of triangles that are
defined by a point on a horizontal line and an interval on another horizontal
line. The time complexity of the recognition problem for simple-triangle graphs
was a longstanding open problem, which was recently settled. This paper
provides a new recognition algorithm for simple-triangle graphs to improve the
time bound from $O(n^2 \overline{m})$ to $O(nm)$, where $n$, $m$, and
$\overline{m}$ are the number of vertices, edges, and non-edges of the graph,
respectively. The algorithm uses the vertex ordering characterization that a
graph is a simple-triangle graph if and only if there is a linear ordering of
the vertices containing both an alternating orientation of the graph and a
transitive orientation of the complement of the graph. We also show, as a
byproduct, that an alternating orientation can be obtained in $O(nm)$ time for
cocomparability graphs, and it is NP-complete to decide whether a graph has an
orientation that is alternating and acyclic.
",1,0,0,0,0,0
2704,A new class of ferromagnetic semiconductors with high Curie temperatures,"  Ferromagnetic semiconductors (FMSs), which have the properties and
functionalities of both semiconductors and ferromagnets, provide fascinating
opportunities for basic research in condensed matter physics and device
applications. Over the past two decades, however, intensive studies on various
FMS materials, inspired by the influential mean-field Zener (MFZ) model have
failed to realise reliable FMSs that have a high Curie temperature (Tc > 300
K), good compatibility with semiconductor electronics, and characteristics
superior to those of their non-magnetic host semiconductors. Here, we
demonstrate a new n type Fe-doped narrow-gap III-V FMS, (In,Fe)Sb, in which
ferromagnetic order is induced by electron carriers, and its Tc is unexpectedly
high, reaching ~335 K at a modest Fe concentration of 16%. Furthermore, we show
that by utilizing the large anomalous Hall effect of (In,Fe)Sb at room
temperature, it is possible to obtain a Hall sensor with a very high
sensitivity that surpasses that of the best commercially available InSb Hall
sensor devices. Our results reveal a new design rule of FMSs that is not
expected from the conventional MFZ model. (This work was presented at the JSAP
Spring meeting, presentation No. E15a-501-2:
this https URL)
",0,1,0,0,0,0
4292,Predicting shim gaps in aircraft assembly with machine learning and sparse sensing,"  A modern aircraft may require on the order of thousands of custom shims to
fill gaps between structural components in the airframe that arise due to
manufacturing tolerances adding up across large structures. These shims are
necessary to eliminate gaps, maintain structural performance, and minimize
pull-down forces required to bring the aircraft into engineering nominal
configuration for peak aerodynamic efficiency. Gap filling is a time-consuming
process, involving either expensive by-hand inspection or computations on vast
quantities of measurement data from increasingly sophisticated metrology
equipment. Either case amounts to significant delays in production, with much
of the time spent in the critical path of aircraft assembly. This work presents
an alternative strategy for predictive shimming, based on machine learning and
sparse sensing to first learn gap distributions from historical data, and then
design optimized sparse sensing strategies to streamline data collection and
processing. This new approach is based on the assumption that patterns exist in
shim distributions across aircraft, which may be mined and used to reduce the
burden of data collection and processing in future aircraft. Specifically,
robust principal component analysis is used to extract low-dimensional patterns
in the gap measurements while rejecting outliers. Next, optimized sparse
sensors are obtained that are most informative about the dimensions of a new
aircraft in these low-dimensional principal components. We demonstrate the
success of the proposed approach, called PIXel Identification Despite
Uncertainty in Sensor Technology (PIXI-DUST), on historical production data
from 54 representative Boeing commercial aircraft. Our algorithm successfully
predicts $99\%$ of shim gaps within the desired measurement tolerance using
$3\%$ of the laser scan points typically required; all results are
cross-validated.
",0,0,0,1,0,0
2580,Semi-extraspecial groups with an abelian subgroup of maximal possible order,"  Let $p$ be a prime. A $p$-group $G$ is defined to be semi-extraspecial if for
every maximal subgroup $N$ in $Z(G)$ the quotient $G/N$ is a an extraspecial
group. In addition, we say that $G$ is ultraspecial if $G$ is semi-extraspecial
and $|G:G'| = |G'|^2$. In this paper, we prove that every $p$-group of
nilpotence class $2$ is isomorphic to a subgroup of some ultraspecial group.
Given a prime $p$ and a positive integer $n$, we provide a framework to
construct of all the ultraspecial groups order $p^{3n}$ that contain an abelian
subgroup of order $p^{2n}$. In the literature, it has been proved that every
ultraspecial group $G$ order $p^{3n}$ with at least two abelian subgroups of
order $p^{2n}$ can be associated to a semifield. We provide a generalization of
semifield, and then we show that every semi-extraspecial group $G$ that is the
product of two abelian subgroups can be associated with this generalization of
semifield.
",0,0,1,0,0,0
11108,The Role of Gender in Social Network Organization,"  The digital traces we leave behind when engaging with the modern world offer
an interesting lens through which we study behavioral patterns as expression of
gender. Although gender differentiation has been observed in a number of
settings, the majority of studies focus on a single data stream in isolation.
Here we use a dataset of high resolution data collected using mobile phones, as
well as detailed questionnaires, to study gender differences in a large cohort.
We consider mobility behavior and individual personality traits among a group
of more than $800$ university students. We also investigate interactions among
them expressed via person-to-person contacts, interactions on online social
networks, and telecommunication. Thus, we are able to study the differences
between male and female behavior captured through a multitude of channels for a
single cohort. We find that while the two genders are similar in a number of
aspects, there are robust deviations that include multiple facets of social
interactions, suggesting the existence of inherent behavioral differences.
Finally, we quantify how aspects of an individual's characteristics and social
behavior reveals their gender by posing it as a classification problem. We ask:
How well can we distinguish between male and female study participants based on
behavior alone? Which behavioral features are most predictive?
",1,0,0,0,0,0
2021,Multiplex Network Regression: How do relations drive interactions?,"  We introduce a statistical method to investigate the impact of dyadic
relations on complex networks generated from repeated interactions. It is based
on generalised hypergeometric ensembles, a class of statistical network
ensembles developed recently. We represent different types of known relations
between system elements by weighted graphs, separated in the different layers
of a multiplex network. With our method we can regress the influence of each
relational layer, the independent variables, on the interaction counts, the
dependent variables. Moreover, we can test the statistical significance of the
relations as explanatory variables for the observed interactions. To
demonstrate the power of our approach and its broad applicability, we will
present examples based on synthetic and empirical data.
",1,1,0,1,0,0
20668,Linear Quadratic Optimal Control Problems with Fixed Terminal States and Integral Quadratic Constraints,"  This paper is concerned with a linear quadratic (LQ, for short) optimal
control problem with fixed terminal states and integral quadratic constraints.
A Riccati equation with infinite terminal value is introduced, which is
uniquely solvable and whose solution can be approximated by the solution for a
suitable unconstrained LQ problem with penalized terminal state. Using results
from duality theory, the optimal control is explicitly derived by solving the
Riccati equation together with an optimal parameter selection problem. It turns
out that the optimal control is not only a feedback of the current state, but
also a feedback of the target (terminal state). Some examples are presented to
illustrate the theory developed.
",0,0,1,0,0,0
17747,Fairness in representation: quantifying stereotyping as a representational harm,"  While harms of allocation have been increasingly studied as part of the
subfield of algorithmic fairness, harms of representation have received
considerably less attention. In this paper, we formalize two notions of
stereotyping and show how they manifest in later allocative harms within the
machine learning pipeline. We also propose mitigation strategies and
demonstrate their effectiveness on synthetic datasets.
",1,0,0,1,0,0
1428,A proof of the Flaherty-Keller formula on the effective property of densely packed elastic composites,"  We prove in a mathematically rigorous way the asymptotic formula of Flaherty
and Keller on the effective property of densely packed periodic elastic
composites with hard inclusions. The proof is based on the primal-dual
variational principle, where the upper bound is derived by using the
Keller-type test functions and the lower bound by singular functions made of
nuclei of strain. Singular functions are solutions of the Lamé system and
capture precisely singular behavior of the stress in the narrow region between
two adjacent hard inclusions.
",0,0,1,0,0,0
7697,Evidence for coherent spicule oscillations by correcting Hinode/SOT Ca II H in the southeast limb of the Sun,"  Wave theories of heating the chromosphere, corona, and solar wind due to
photospheric fluctuations are strengthened by the existence of observed wave
coherency up to the transition region (TR). The coherency of solar spicules'
intensity oscillations was explored using the Solar Optical Telescope (SOT) on
the Hinode spacecraft with a height increase above the solar limb in active
region (AR). We used time sequences near the southeast region from the
Hinode/SOT in Ca II H line obtained on April 3, 2015 and applied the
de-convolution procedure to the spicule in order to illustrate how effectively
our restoration method works on fine structures such as spicules. Moreover, the
intensity oscillations at different heights above the solar limb were analysed
through wavelet transforms. Afterwards, the phase difference was measured among
oscillations at two certain heights in search of evidence for coherent
oscillations. The results of wavelet transformations revealed dominant period
peaks in 2, 4, 5.5, and 6.5 min at four separate heights. The dominant
frequencies for coherency level higher than 75 percent was found to be around
5.5 and 8.5 mHz. Mean phase speeds of 155-360 km s-1 were measured. We found
that the mean phase speeds increased with height. The results suggest that the
energy flux carried by coherent waves into the corona and heliosphere may be
several times larger than previous estimates that were based solely on constant
velocities. We provide compelling evidence for the existence of upwardly
propagating coherent waves.
",0,1,0,0,0,0
9211,Optimization Based Methods for Partially Observed Chaotic Systems,"  In this paper we consider filtering and smoothing of partially observed
chaotic dynamical systems that are discretely observed, with an additive
Gaussian noise in the observation. These models are found in a wide variety of
real applications and include the Lorenz 96' model. In the context of a fixed
observation interval $T$, observation time step $h$ and Gaussian observation
variance $\sigma_Z^2$, we show under assumptions that the filter and smoother
are well approximated by a Gaussian with high probability when $h$ and
$\sigma^2_Z h$ are sufficiently small. Based on this result we show that the
Maximum-a-posteriori (MAP) estimators are asymptotically optimal in mean square
error as $\sigma^2_Z h$ tends to $0$. Given these results, we provide a batch
algorithm for the smoother and filter, based on Newton's method, to obtain the
MAP. In particular, we show that if the initial point is close enough to the
MAP, then Newton's method converges to it at a fast rate. We also provide a
method for computing such an initial point. These results contribute to the
theoretical understanding of widely used 4D-Var data assimilation method. Our
approach is illustrated numerically on the Lorenz 96' model with state vector
up to 1 million dimensions, with code running in the order of minutes. To our
knowledge the results in this paper are the first of their type for this class
of models.
",0,0,1,1,0,0
10979,Urban Swarms: A new approach for autonomous waste management,"  Modern cities are growing ecosystems that face new challenges due to the
increasing population demands. One of the many problems they face nowadays is
waste management, which has become a pressing issue requiring new solutions.
Swarm robotics systems have been attracting an increasing amount of attention
in the past years and they are expected to become one of the main driving
factors for innovation in the field of robotics. The research presented in this
paper explores the feasibility of a swarm robotics system in an urban
environment. By using bio-inspired foraging methods such as multi-place
foraging and stigmergy-based navigation, a swarm of robots is able to improve
the efficiency and autonomy of the urban waste management system in a realistic
scenario. To achieve this, a diverse set of simulation experiments was
conducted using real-world GIS data and implementing different garbage
collection scenarios driven by robot swarms. Results presented in this research
show that the proposed system outperforms current approaches. Moreover, results
not only show the efficiency of our solution, but also give insights about how
to design and customize these systems.
",1,0,0,0,0,0
15163,Hausdorff dimension of limsup sets of random rectangles in products of regular spaces,"  The almost sure Hausdorff dimension of the limsup set of randomly distributed
rectangles in a product of Ahlfors regular metric spaces is computed in terms
of the singular value function of the rectangles.
",0,0,1,0,0,0
18901,Fermi bubbles: high latitude X-ray supersonic shell,"  The nature of the bipolar, $\gamma$-ray Fermi bubbles (FB) is still unclear,
in part because their faint, high-latitude X-ray counterpart has until now
eluded a clear detection. We stack ROSAT data at varying distances from the FB
edges, thus boosting the signal and identifying an expanding shell behind the
southwest, southeast, and northwest edges, albeit not in the dusty northeast
sector near Loop I. A Primakoff-like model for the underlying flow is invoked
to show that the signals are consistent with halo gas heated by a strong,
forward shock to $\sim$keV temperatures. Assuming ion--electron thermal
equilibrium then implies a $\sim10^{56}$ erg event near the Galactic centre
$\sim7$ Myr ago. However, the reported high absorption-line velocities suggest
a preferential shock-heating of ions, and thus more energetic ($\sim 10^{57}$
erg), younger ($\lesssim 3$ Myr) FBs.
",0,1,0,0,0,0
18313,AdaComp : Adaptive Residual Gradient Compression for Data-Parallel Distributed Training,"  Highly distributed training of Deep Neural Networks (DNNs) on future compute
platforms (offering 100 of TeraOps/s of computational capacity) is expected to
be severely communication constrained. To overcome this limitation, new
gradient compression techniques are needed that are computationally friendly,
applicable to a wide variety of layers seen in Deep Neural Networks and
adaptable to variations in network architectures as well as their
hyper-parameters. In this paper we introduce a novel technique - the Adaptive
Residual Gradient Compression (AdaComp) scheme. AdaComp is based on localized
selection of gradient residues and automatically tunes the compression rate
depending on local activity. We show excellent results on a wide spectrum of
state of the art Deep Learning models in multiple domains (vision, speech,
language), datasets (MNIST, CIFAR10, ImageNet, BN50, Shakespeare), optimizers
(SGD with momentum, Adam) and network parameters (number of learners,
minibatch-size etc.). Exploiting both sparsity and quantization, we demonstrate
end-to-end compression rates of ~200X for fully-connected and recurrent layers,
and ~40X for convolutional layers, without any noticeable degradation in model
accuracies.
",1,0,0,1,0,0
16118,Absolute spectroscopy near 7.8 μm with a comb-locked extended-cavity quantum-cascade-laser,"  We report the first experimental demonstration of frequency-locking of an
extended-cavity quantum-cascade-laser (EC-QCL) to a near-infrared frequency
comb. The locking scheme is applied to carry out absolute spectroscopy of N2O
lines near 7.87 {\mu}m with an accuracy of ~60 kHz. Thanks to a single mode
operation over more than 100 cm^{-1}, the comb-locked EC-QCL shows great
potential for the accurate retrieval of line center frequencies in a spectral
region that is currently outside the reach of broadly tunable cw sources,
either based on difference frequency generation or optical parametric
oscillation. The approach described here can be straightforwardly extended up
to 12 {\mu}m, which is the current wavelength limit for commercial cw EC-QCLs.
",0,1,0,0,0,0
4681,How to centralize and normalize quandle extensions,"  We show that quandle coverings in the sense of Eisermann form a (regular
epi)-reflective subcategory of the category of surjective quandle
homomorphisms, both by using arguments coming from categorical Galois theory
and by constructing concretely a centralization congruence. Moreover, we show
that a similar result holds for normal quandle extensions.
",0,0,1,0,0,0
4464,A combinatorial model for the free loop fibration,"  We introduce the abstract notion of a closed necklical set in order to
describe a functorial combinatorial model of the free loop fibration $\Omega
Y\rightarrow \Lambda Y\rightarrow Y$ over the geometric realization $Y=|X|$ of
a path connected simplicial set $X.$ In particular, to any path connected
simplicial set $X$ we associate a closed necklical set
$\widehat{\mathbf{\Lambda}}X$ such that its geometric realization
$|\widehat{\mathbf{\Lambda}}X|$, a space built out of gluing ""freehedrical"" and
""cubical"" cells, is homotopy equivalent to the free loop space $\Lambda Y$ and
the differential graded module of chains $C_*(\widehat{\mathbf{\Lambda}}X)$
generalizes the coHochschild chain complex of the chain coalgebra $C_\ast(X).$
",0,0,1,0,0,0
195,On nonlinear profile decompositions and scattering for a NLS-ODE model,"  In this paper, we consider a Hamiltonian system combining a nonlinear Schr\""
odinger equation (NLS) and an ordinary differential equation (ODE). This system
is a simplified model of the NLS around soliton solutions. Following Nakanishi
\cite{NakanishiJMSJ}, we show scattering of $L^2$ small $H^1$ radial solutions.
The proof is based on Nakanishi's framework and Fermi Golden Rule estimates on
$L^4$ in time norms.
",0,0,1,0,0,0
18669,$\ell_1$-minimization method for link flow correction,"  A computational method, based on $\ell_1$-minimization, is proposed for the
problem of link flow correction, when the available traffic flow data on many
links in a road network are inconsistent with respect to the flow conservation
law. Without extra information, the problem is generally ill-posed when a large
portion of the link sensors are unhealthy. It is possible, however, to correct
the corrupted link flows \textit{accurately} with the proposed method under a
recoverability condition if there are only a few bad sensors which are located
at certain links. We analytically identify the links that are robust to
miscounts and relate them to the geometric structure of the traffic network by
introducing the recoverability concept and an algorithm for computing it. The
recoverability condition for corrupted links is simply the associated
recoverability being greater than 1. In a more realistic setting, besides the
unhealthy link sensors, small measurement noises may be present at the other
sensors. Under the same recoverability condition, our method guarantees to give
an estimated traffic flow fairly close to the ground-truth data and leads to a
bound for the correction error. Both synthetic and real-world examples are
provided to demonstrate the effectiveness of the proposed method.
",1,1,0,0,0,0
14995,Efficient Lightweight Encryption Algorithm for Smart Video Applications,"  The future generation networks: Internet of things (IoT), in combination with
the advanced computer vision techniques poses new challenges for securing
videos for end-users. The visual devices generally have constrained resources
in respects to their low computation power, small memory with limited power
supply. Therefore, to facilitate the video security in smart environment,
lightweight security schemes are required instead of inefficient existing
traditional cryptography algorithms. This research paper provides the solution
to overcome such problems. A novel lightweight cipher algorithm is proposed
here which targets multimedia in IoT with an in-house name EXPer i.e. Extended
permutation with eXclusive OR (XOR). EXPer is a symmetric stream cipher that
consists of simple XOR and left shift operations with three keys of 128 bits.
The proposed cipher algorithm has been tested on various sample videos.
Comparison of proposed algorithm has been made with the traditional cipher
algorithms XOR and Advanced Encryption Standard (AES). Visual results confirm
that EXPer provides security level equivalent to the AES algorithm with less
computational cost than AES. Therefore, it can easily be perceived that the
EXPer is a better replacement of AES for securing real-time video applications
in IoT.
",1,0,0,0,0,0
5546,Complexity of the Regularized Newton Method,"  Newton's method for finding an unconstrained minimizer for strictly convex
functions, generally speaking, does not converge from any starting point.
We introduce and study the damped regularized Newton's method (DRNM). It
converges globally for any strictly convex function, which has a minimizer in
$R^n$.
Locally DRNM converges with a quadratic rate. We characterize the
neighborhood of the minimizer, where the quadratic rate occurs. Based on it we
estimate the number of DRNM's steps required for finding an $\varepsilon$-
approximation for the minimizer.
",0,0,1,0,0,0
19569,On the Origin of Deep Learning,"  This paper is a review of the evolutionary history of deep learning models.
It covers from the genesis of neural networks when associationism modeling of
the brain is studied, to the models that dominate the last decade of research
in deep learning like convolutional neural networks, deep belief networks, and
recurrent neural networks. In addition to a review of these models, this paper
primarily focuses on the precedents of the models above, examining how the
initial ideas are assembled to construct the early models and how these
preliminary models are developed into their current forms. Many of these
evolutionary paths last more than half a century and have a diversity of
directions. For example, CNN is built on prior knowledge of biological vision
system; DBN is evolved from a trade-off of modeling power and computation
complexity of graphical models and many nowadays models are neural counterparts
of ancient linear models. This paper reviews these evolutionary paths and
offers a concise thought flow of how these models are developed, and aims to
provide a thorough background for deep learning. More importantly, along with
the path, this paper summarizes the gist behind these milestones and proposes
many directions to guide the future research of deep learning.
",1,0,0,1,0,0
18064,Correlations between primes in short intervals on curves over finite fields,"  We prove an analogue of the Hardy-Littlewood conjecture on the asymptotic
distribution of prime constellations in the setting of short intervals in
function fields of smooth projective curves over finite fields.
",0,0,1,0,0,0
12808,The Linearity of the Mitchell Order,"  We show from a weak comparison principle (the Ultrapower Axiom) that the
Mitchell order is linear on certain kinds of ultrafilters: normal ultrafilters,
Dodd solid ultrafilters, and assuming GCH, generalized normal ultrafilters. In
the process we prove a generalization of Solovay's lemma to singular cardinals.
",0,0,1,0,0,0
16517,HATS-36b and 24 other transiting/eclipsing systems from the HATSouth - K2 Campaign 7 program,"  We report on the result of a campaign to monitor 25 HATSouth candidates using
the K2 space telescope during Campaign 7 of the K2 mission. We discover
HATS-36b (EPIC 215969174b), a hot Jupiter with a mass of 2.79$\pm$0.40 M$_J$
and a radius of 1.263$\pm$0.045 R$_J$ which transits a solar-type G0V star
(V=14.386) in a 4.1752d period. We also refine the properties of three
previously discovered HATSouth transiting planets (HATS-9b, HATS-11b, and
HATS-12b) and search the K2 data for TTVs and additional transiting planets in
these systems. In addition we also report on a further three systems that
remain as Jupiter-radius transiting exoplanet candidates. These candidates do
not have determined masses, however pass all of our other vetting observations.
Finally we report on the 18 candidates which we are now able to classify as
eclipsing binary or blended eclipsing binary systems based on a combination of
the HATSouth data, the K2 data, and follow-up ground-based photometry and
spectroscopy. These range in periods from 0.7 days to 16.7 days, and down to
1.5 mmag in eclipse depths. Our results show the power of combining
ground-based imaging and spectroscopy with higher precision space-based
photometry, and serve as an illustration as to what will be possible when
combining ground-based observations with TESS data.
",0,1,0,0,0,0
11533,Transport in a disordered $ν=2/3$ fractional quantum Hall junction,"  Electric and thermal transport properties of a $\nu=2/3$ fractional quantum
Hall junction are analyzed. We investigate the evolution of the electric and
thermal two-terminal conductances, $G$ and $G^Q$, with system size $L$ and
temperature $T$. This is done both for the case of strong interaction between
the 1 and 1/ 3 modes (when the low-temperature physics of the interacting
segment of the device is controlled by the vicinity of the strong-disorder
Kane-Fisher-Polchinski fixed point) and for relatively weak interaction, for
which the disorder is irrelevant at $T=0$ in the renormalization-group sense.
The transport properties in both cases are similar in several respects. In
particular, $G(L)$ is close to 4/3 (in units of $e^2/h$) and $G^Q$ to 2 (in
units of $\pi T / 6 \hbar$) for small $L$, independently of the interaction
strength. For large $L$ the system is in an incoherent regime, with $G$ given
by 2/3 and $G^Q$ showing the Ohmic scaling, $G^Q\propto 1/L$, again for any
interaction strength. The hallmark of the strong-disorder fixed point is the
emergence of an intermediate range of $L$, in which the electric conductance
shows strong mesoscopic fluctuations and the thermal conductance is $G^Q=1$.
The analysis is extended also to a device with floating 1/3 mode, as studied in
a recent experiment [A. Grivnin et al, Phys. Rev. Lett. 113, 266803 (2014)].
",0,1,0,0,0,0
14471,Novel solid state vacuum quartz encapsulated growth of p-Terphenyl: the parent High Tc Oraganic Superconductor (HTOS),"  We report an easy and versatile route for the synthesis of the parent phase
of newest superconducting wonder material i.e. p-Terphenyl. Doped p-terphenyl
has recently shown superconductivity with transition temperature as high as
120K. For crystal growth, the commercially available p-Terphenyl powder is
pelletized, encapsulated in evacuated (10-4 Torr) quartz tube and subjected to
high temperature (260C) melt followed by slow cooling at 5C/hour. Simple
temperature controlled heating furnace is used during the process. The obtained
crystal is one piece, shiny and plate like. Single crystal surface XRD (X-ray
Diffraction) showed unidirectional (00l) lines, indicating that the crystal is
grown along c-direction. Powder XRD of the specimen showed that as grown
p-Terphenyl is crystallized in monoclinic structure with space group P21/a
space group, having lattice parameters a = 8.08(2) A, b = 5.62(5) A and c=
13.58(3) A. Scanning electron microscopy (SEM) pictures of the crystal showed
clear layered slab like growth without any visible contamination from oxygen.
Characteristic reported Raman active modes related to C-C-C bending, C-H
bending, C-C stretching and C-H stretching vibrations are seen clearly for the
studied p-Terphenyl crystal. The physical properties of crystal are yet
underway. The short letter reports an easy and versatile crystal growth method
for obtaining quality p-terphenyl. The same growth method may probably be
applied to doped p-terphenyl and to subsequently achieve superconductivity to
the tune of as high 120K for the newest superconductivity wonder i.e., High Tc
Oraganic Superconductor (HTOS).
",0,1,0,0,0,0
15816,Multi-particle instability in a spin-imbalanced Fermi gas,"  Weak attractive interactions in a spin-imbalanced Fermi gas induce a
multi-particle instability, binding multiple fermions together. The maximum
binding energy per particle is achieved when the ratio of the number of up- and
down-spin particles in the instability is equal to the ratio of the up- and
down-spin densities of states in momentum at the Fermi surfaces, to utilize the
variational freedom of all available momentum states. We derive this result
using an analytical approach, and verify it using exact diagonalization. The
multi-particle instability extends the Cooper pairing instability of balanced
Fermi gases to the imbalanced case, and could form the basis of a many-body
state, analogously to the construction of the Bardeen-Cooper-Schrieffer theory
of superconductivity out of Cooper pairs.
",0,1,0,0,0,0
9537,"Unexpected 3+ valence of iron in FeO$_2$, a geologically important material lying ""in between"" oxides and peroxides","  Recent discovery of pyrite FeO$_2$, which can be an important ingredient of
the Earth's lower mantle and which in particular may serve as an extra source
of water in the Earth's interior, opens new perspectives for geophysics and
geochemistry, but this is also an extremely interesting material from physical
point of view. We found that in contrast to naive expectations Fe is nearly 3+
in this material, which strongly affects its magnetic properties and makes it
qualitatively different from well known sulfide analogue - FeS$_2$. Doping,
which is most likely to occur in the Earth's mantle, makes FeO$_2$ much more
magnetic. In addition we show that unique electronic structure places FeO$_2$
""in between"" the usual dioxides and peroxides making this system interesting
both for physics and solid state chemistry.
",0,1,0,0,0,0
9815,Reconstructing a $f(R)$ theory from the $α$-Attractors,"  We show an analogy at high curvature between a $f(R) = R + aR^{n - 1} + bR^2$
theory and the $\alpha$-Attractors. We calculate the expressions of the
parameters $a$, $b$ and $n$ as functions of $\alpha$ and the predictions of the
model $f(R) = R + aR^{n - 1} + bR^2$ on the scalar spectral index $n_{\rm s}$
and the tensor-to-scalar ratio $r$. We find that the power law correction $R^{n
- 1}$ allows for a production of gravitational waves enhanced with respect to
the one in the Starobinsky model, while maintaining a viable prediction on
$n_{\rm s}$. We numerically reconstruct the full $\alpha$-Attractors class of
models testing the goodness of our high-energy approximation $f(R) = R + aR^{n
- 1} + bR^2$. Moreover, we also investigate the case of a single power law
$f(R) = \gamma R^{2 - \delta}$ theory, with $\gamma$ and $\delta$ free
parameters. We calculate analytically the predictions of this model on the
scalar spectral index $n_{\rm s}$ and the tensor-to-scalar ratio $r$ and the
values of $\delta$ which are allowed from the current observational results. We
find that $-0.015 < \delta < 0.016$, confirming once again the excellent
agreement between the Starobinsky model and observation.
",0,1,0,0,0,0
11593,Database Engines: Evolution of Greenness,"  Context: Information Technology consumes up to 10\% of the world's
electricity generation, contributing to CO2 emissions and high energy costs.
Data centers, particularly databases, use up to 23% of this energy. Therefore,
building an energy-efficient (green) database engine could reduce energy
consumption and CO2 emissions.
Goal: To understand the factors driving databases' energy consumption and
execution time throughout their evolution.
Method: We conducted an empirical case study of energy consumption by two
MySQL database engines, InnoDB and MyISAM, across 40 releases. We examined the
relationships of four software metrics to energy consumption and execution time
to determine which metrics reflect the greenness and performance of a database.
Results: Our analysis shows that database engines' energy consumption and
execution time increase as databases evolve. Moreover, the Lines of Code metric
is correlated moderately to strongly with energy consumption and execution time
in 88% of cases.
Conclusions: Our findings provide insights to both practitioners and
researchers. Database administrators may use them to select a fast, green
release of the MySQL database engine. MySQL database-engine developers may use
the software metric to assess products' greenness and performance. Researchers
may use our findings to further develop new hypotheses or build models to
predict greenness and performance of databases.
",1,0,0,0,0,0
16186,"Rare-earth/transition-metal magnetic interactions in pristine and (Ni,Fe)-doped YCo5 and GdCo5","  We present an investigation into the intrinsic magnetic properties of the
compounds YCo5 and GdCo5, members of the RETM5 class of permanent magnets (RE =
rare earth, TM = transition metal). Focusing on Y and Gd provides direct
insight into both the TM magnetization and RE-TM interactions without the
complication of strong crystal field effects. We synthesize single crystals of
YCo5 and GdCo5 using the optical floating zone technique and measure the
magnetization from liquid helium temperatures up to 800 K. These measurements
are interpreted through calculations based on a Green's function formulation of
density-functional theory, treating the thermal disorder of the local magnetic
moments within the coherent potential approximation. The rise in the
magnetization of GdCo5 with temperature is shown to arise from a faster
disordering of the Gd magnetic moments compared to the antiferromagnetically
aligned Co sublattice. We use the calculations to analyze the different Curie
temperatures of the compounds and also compare the molecular (Weiss) fields at
the RE site with previously published neutron scattering experiments. To gain
further insight into the RE-TM interactions, we perform substitutional doping
on the TM site, studying the compounds RECo4.5Ni0.5, RECo4Ni, and RECo4.5Fe0.5.
Both our calculations and experiments on powdered samples find an
increased/decreased magnetization with Fe/Ni doping, respectively. The
calculations further reveal a pronounced dependence on the location of the
dopant atoms of both the Curie temperatures and the Weiss field at the RE site.
",0,1,0,0,0,0
1394,The relation between galaxy morphology and colour in the EAGLE simulation,"  We investigate the relation between kinematic morphology, intrinsic colour
and stellar mass of galaxies in the EAGLE cosmological hydrodynamical
simulation. We calculate the intrinsic u-r colours and measure the fraction of
kinetic energy invested in ordered corotation of 3562 galaxies at z=0 with
stellar masses larger than $10^{10}M_{\odot}$. We perform a visual inspection
of gri-composite images and find that our kinematic morphology correlates
strongly with visual morphology. EAGLE produces a galaxy population for which
morphology is tightly correlated with the location in the colour- mass diagram,
with the red sequence mostly populated by elliptical galaxies and the blue
cloud by disc galaxies. Satellite galaxies are more likely to be on the red
sequence than centrals, and for satellites the red sequence is morphologically
more diverse. These results show that the connection between mass, intrinsic
colour and morphology arises from galaxy formation models that reproduce the
observed galaxy mass function and sizes.
",0,1,0,0,0,0
2947,Fixed-Gain Augmented-State Tracking-Filters,"  A procedure for the design of fixed-gain tracking filters, using an
augmented-state observer with signal and interference subspaces, is proposed.
The signal subspace incorporates an integrating Newtonian model and a
second-order maneuver model that is matched to a sustained constant-g turn; the
deterministic interference model creates a Nyquist null for smoother track
estimates. The selected models provide a simple means of shaping and analyzing
the (transient and steady-state) response of tracking-filters of elevated
order.
",1,0,0,0,0,0
4098,"Hierarchy of Information Scrambling, Thermalization, and Hydrodynamic Flow in Graphene","  We determine the information scrambling rate $\lambda_{L}$ due to
electron-electron Coulomb interaction in graphene. $\lambda_{L}$ characterizes
the growth of chaos and has been argued to give information about the
thermalization and hydrodynamic transport coefficients of a many-body system.
We demonstrate that $\lambda_{L}$ behaves for strong coupling similar to
transport and energy relaxation rates. A weak coupling analysis, however,
reveals that scrambling is related to dephasing or single particle relaxation.
Furthermore, $\lambda_{L}$ is found to be parametrically larger than the
collision rate relevant for hydrodynamic processes, such as electrical
conduction or viscous flow, and the rate of energy relaxation, relevant for
thermalization. Thus, while scrambling is obviously necessary for
thermalization and quantum transport, it does generically not set the time
scale for these processes. In addition we derive a quantum kinetic theory for
information scrambling that resembles the celebrated Boltzmann equation and
offers a physically transparent insight into quantum chaos in many-body
systems.
",0,1,0,0,0,0
16856,Low Rank Matrix Recovery with Simultaneous Presence of Outliers and Sparse Corruption,"  We study a data model in which the data matrix D can be expressed as D = L +
S + C, where L is a low rank matrix, S an element-wise sparse matrix and C a
matrix whose non-zero columns are outlying data points. To date, robust PCA
algorithms have solely considered models with either S or C, but not both. As
such, existing algorithms cannot account for simultaneous element-wise and
column-wise corruptions. In this paper, a new robust PCA algorithm that is
robust to simultaneous types of corruption is proposed. Our approach hinges on
the sparse approximation of a sparsely corrupted column so that the sparse
expansion of a column with respect to the other data points is used to
distinguish a sparsely corrupted inlier column from an outlying data point. We
also develop a randomized design which provides a scalable implementation of
the proposed approach. The core idea of sparse approximation is analyzed
analytically where we show that the underlying ell_1-norm minimization can
obtain the representation of an inlier in presence of sparse corruptions.
",1,0,0,1,0,0
3848,The set of quantum correlations is not closed,"  We construct a linear system non-local game which can be played perfectly
using a limit of finite-dimensional quantum strategies, but which cannot be
played perfectly on any finite-dimensional Hilbert space, or even with any
tensor-product strategy. In particular, this shows that the set of
(tensor-product) quantum correlations is not closed. The constructed non-local
game provides another counterexample to the ""middle"" Tsirelson problem, with a
shorter proof than our previous paper (though at the loss of the universal
embedding theorem). We also show that it is undecidable to determine if a
linear system game can be played perfectly with a finite-dimensional strategy,
or a limit of finite-dimensional quantum strategies.
",0,0,1,0,0,0
4448,GoT-WAVE: Temporal network alignment using graphlet-orbit transitions,"  Global pairwise network alignment (GPNA) aims to find a one-to-one node
mapping between two networks that identifies conserved network regions. GPNA
algorithms optimize node conservation (NC) and edge conservation (EC). NC
quantifies topological similarity between nodes. Graphlet-based degree vectors
(GDVs) are a state-of-the-art topological NC measure. Dynamic GDVs (DGDVs) were
used as a dynamic NC measure within the first-ever algorithms for GPNA of
temporal networks: DynaMAGNA++ and DynaWAVE. The latter is superior for larger
networks. We recently developed a different graphlet-based measure of temporal
node similarity, graphlet-orbit transitions (GoTs). Here, we use GoTs instead
of DGDVs as a new dynamic NC measure within DynaWAVE, resulting in a new
approach, GoT-WAVE.
On synthetic networks, GoT-WAVE improves DynaWAVE's accuracy by 25% and speed
by 64%. On real networks, when optimizing only dynamic NC, each method is
superior ~50% of the time. While DynaWAVE benefits more from also optimizing
dynamic EC, only GoT-WAVE can support directed edges. Hence, GoT-WAVE is a
promising new temporal GPNA algorithm, which efficiently optimizes dynamic NC.
Future work on better incorporating dynamic EC may yield further improvements.
",1,0,0,1,0,0
4611,Knowledge distillation using unlabeled mismatched images,"  Current approaches for Knowledge Distillation (KD) either directly use
training data or sample from the training data distribution. In this paper, we
demonstrate effectiveness of 'mismatched' unlabeled stimulus to perform KD for
image classification networks. For illustration, we consider scenarios where
this is a complete absence of training data, or mismatched stimulus has to be
used for augmenting a small amount of training data. We demonstrate that
stimulus complexity is a key factor for distillation's good performance. Our
examples include use of various datasets for stimulating MNIST and CIFAR
teachers.
",1,0,0,1,0,0
8659,Active Anomaly Detection via Ensembles,"  In critical applications of anomaly detection including computer security and
fraud prevention, the anomaly detector must be configurable by the analyst to
minimize the effort on false positives. One important way to configure the
anomaly detector is by providing true labels for a few instances. We study the
problem of label-efficient active learning to automatically tune anomaly
detection ensembles and make four main contributions. First, we present an
important insight into how anomaly detector ensembles are naturally suited for
active learning. This insight allows us to relate the greedy querying strategy
to uncertainty sampling, with implications for label-efficiency. Second, we
present a novel formalism called compact description to describe the discovered
anomalies and show that it can also be employed to improve the diversity of the
instances presented to the analyst without loss in the anomaly discovery rate.
Third, we present a novel data drift detection algorithm that not only detects
the drift robustly, but also allows us to take corrective actions to adapt the
detector in a principled manner. Fourth, we present extensive experiments to
evaluate our insights and algorithms in both batch and streaming settings. Our
results show that in addition to discovering significantly more anomalies than
state-of-the-art unsupervised baselines, our active learning algorithms under
the streaming-data setup are competitive with the batch setup.
",0,0,0,1,0,0
11218,Ramp Reversal Memory and Phase-Boundary Scarring in Transition Metal Oxides,"  Transition metal oxides (TMOs) are complex electronic systems which exhibit a
multitude of collective phenomena. Two archetypal examples are VO2 and NdNiO3,
which undergo a metal-insulator phase-transition (MIT), the origin of which is
still under debate. Here we report the discovery of a memory effect in both
systems, manifest through an increase of resistance at a specific temperature,
which is set by reversing the temperature-ramp from heating to cooling during
the MIT. The characteristics of this ramp-reversal memory effect do not
coincide with any previously reported history or memory effects in manganites,
electron-glass or magnetic systems. From a broad range of experimental
features, supported by theoretical modelling, we find that the main ingredients
for the effect to arise are the spatial phase-separation of metallic and
insulating regions during the MIT and the coupling of lattice strain to the
local critical temperature of the phase transition. We conclude that the
emergent memory effect originates from phase boundaries at the
reversal-temperature leaving `scars` in the underlying lattice structure,
giving rise to a local increase in the transition temperature. The universality
and robustness of the effect shed new light on the MIT in complex oxides.
",0,1,0,0,0,0
10429,Performance Evaluation of Spectrum Mobility in Multi-homed Mobile IPv6 Cognitive Radio Cellular Networks,"  Technological developments alongside VLSI achievements enable mobile devices
to be equipped with multiple radio interfaces which is known as multihoming. On
the other hand, the combination of various wireless access technologies, known
as Next Generation Wireless Networks (NGWNs) has been introduced to provide
continuous connection to mobile devices in any time and location. Cognitive
radio networks as a part of NGWNs aroused to overcome spectrum inefficiency and
spectrum scarcity issues. In order to provide seamless and ubiquitous
connection across heterogeneous wireless access networks in the context of
cognitive radio networks, utilizing Mobile IPv6 is beneficial. In this paper, a
mobile device equipped with two radio interfaces is considered in order to
evaluate performance of spectrum handover in terms of handover latency. The
analytical results show that the proposed model can achieve better performance
compared to other related mobility management protocols mainly in terms of
handover latency.
",1,0,0,0,0,0
16554,Bobtail: A Proof-of-Work Target that Minimizes Blockchain Mining Variance (Draft),"  Blockchain systems are designed to produce blocks at a constant average rate.
The most popular systems currently employ a Proof of Work (PoW) algorithm as a
means of creating these blocks. Bitcoin produces, on average, one block every
10 minutes. An unfortunate limitation of all deployed PoW blockchain systems is
that the time between blocks has high variance. For example, 5% of the time,
Bitcoin's inter-block time is at least 40 minutes. This variance impedes the
consistent flow of validated transactions through the system. We propose an
alternative process for PoW-based block discovery that results in an
inter-block time with significantly lower variance. Our algorithm, called
Bobtail, generalizes the current algorithm by comparing the mean of the k
lowest order statistics to a target. We show that the variance of inter-block
times decreases as k increases. If our approach were applied to Bitcoin, about
80% of blocks would be found within 7 to 12 minutes, and nearly every block
would be found within 5 to 18 minutes; the average inter-block time would
remain at 10 minutes. Further, we show that low-variance mining significantly
thwarts doublespend and selfish mining attacks. For Bitcoin and Ethereum
currently (k=1), an attacker with 40% of the mining power will succeed with 30%
probability when the merchant sets up an embargo of 8 blocks; however, when
k>=20, the probability of success falls to less than 1%. Similarly, for Bitcoin
and Ethereum currently, a selfish miner with 40% of the mining power will claim
about 66% of blocks; however, when k>=5, the same miner will find that selfish
mining is less successful than honest mining. The cost of our approach is a
larger block header.
",1,0,0,0,0,0
13625,The bubble algebras at roots of unity,"  We introduce multi-colour partition algebras $P_{n,m}(\delta_0, ...,
\delta_{m-1})$, which are generalization of both bubble algebras and partition
algebras, then define the bubble algebra $T_{n,m}(\delta_0, ..., \delta_{m-1})$
as a sub-algebra of the algebra $P_{n,m}(\delta_0, ..., \delta_{m-1})$. We
present general techniques to determine the structure of the bubble algebra
over the complex field in the non-semisimple case.
",0,0,1,0,0,0
20943,Contextual Outlier Interpretation,"  Outlier detection plays an essential role in many data-driven applications to
identify isolated instances that are different from the majority. While many
statistical learning and data mining techniques have been used for developing
more effective outlier detection algorithms, the interpretation of detected
outliers does not receive much attention. Interpretation is becoming
increasingly important to help people trust and evaluate the developed models
through providing intrinsic reasons why the certain outliers are chosen. It is
difficult, if not impossible, to simply apply feature selection for explaining
outliers due to the distinct characteristics of various detection models,
complicated structures of data in certain applications, and imbalanced
distribution of outliers and normal instances. In addition, the role of
contrastive contexts where outliers locate, as well as the relation between
outliers and contexts, are usually overlooked in interpretation. To tackle the
issues above, in this paper, we propose a novel Contextual Outlier
INterpretation (COIN) method to explain the abnormality of existing outliers
spotted by detectors. The interpretability for an outlier is achieved from
three aspects: outlierness score, attributes that contribute to the
abnormality, and contextual description of its neighborhoods. Experimental
results on various types of datasets demonstrate the flexibility and
effectiveness of the proposed framework compared with existing interpretation
approaches.
",1,0,0,1,0,0
14406,Object Region Mining with Adversarial Erasing: A Simple Classification to Semantic Segmentation Approach,"  We investigate a principle way to progressively mine discriminative object
regions using classification networks to address the weakly-supervised semantic
segmentation problems. Classification networks are only responsive to small and
sparse discriminative regions from the object of interest, which deviates from
the requirement of the segmentation task that needs to localize dense, interior
and integral regions for pixel-wise inference. To mitigate this gap, we propose
a new adversarial erasing approach for localizing and expanding object regions
progressively. Starting with a single small object region, our proposed
approach drives the classification network to sequentially discover new and
complement object regions by erasing the current mined regions in an
adversarial manner. These localized regions eventually constitute a dense and
complete object region for learning semantic segmentation. To further enhance
the quality of the discovered regions by adversarial erasing, an online
prohibitive segmentation learning approach is developed to collaborate with
adversarial erasing by providing auxiliary segmentation supervision modulated
by the more reliable classification scores. Despite its apparent simplicity,
the proposed approach achieves 55.0% and 55.7% mean Intersection-over-Union
(mIoU) scores on PASCAL VOC 2012 val and test sets, which are the new
state-of-the-arts.
",1,0,0,0,0,0
324,Bayesian Optimization for Probabilistic Programs,"  We present the first general purpose framework for marginal maximum a
posteriori estimation of probabilistic program variables. By using a series of
code transformations, the evidence of any probabilistic program, and therefore
of any graphical model, can be optimized with respect to an arbitrary subset of
its sampled variables. To carry out this optimization, we develop the first
Bayesian optimization package to directly exploit the source code of its
target, leading to innovations in problem-independent hyperpriors, unbounded
optimization, and implicit constraint satisfaction; delivering significant
performance improvements over prominent existing packages. We present
applications of our method to a number of tasks including engineering design
and parameter optimization.
",1,0,0,1,0,0
17672,Direct Experimental Observation of the Gas Filamentation Effect using a Two-bunch X-ray FEL Beam,"  We report the experimental observation of the filamentation effect in gas
devices designed for X-ray Free-electron Lasers. The measurements were carried
out at the Linac Coherent Light Source on the X-ray Correlation Spectroscopy
(XCS) instrument using a Two-bunch FEL beam at 6.5 keV with 122.5 ns separation
passing through an Argon gas cell. The relative intensities of the two pulses
of the Two-bunch beam were measured, after and before the gas cell, from the
X-ray scattering off thin targets by using fast diodes with sufficient temporal
resolution. It was found that the after-to-before ratio of the intensities of
the second pulse was consistently higher than that of the first pulse,
revealing lower effective attenuation of the gas cell due to the heating and
subsequent gas density reduction in the beam path by the first pulse. This
measurement is important in guiding the design and/or mitigating the adverse
effect in gas devices for high repetition-rate FELs such as the LCLS-II and the
European XFEL or other future high repetition-rate upgrade to existing FEL
facilities
",0,1,0,0,0,0
17066,Exoplanet Radius Gap Dependence on Host Star Type,"  Exoplanets smaller than Neptune are numerous, but the nature of the planet
populations in the 1-4 Earth radii range remains a mystery. The complete Kepler
sample of Q1-Q17 exoplanet candidates shows a radius gap at ~ 2 Earth radii, as
reported by us in January 2017 in LPSC conference abstract #1576 (Zeng et al.
2017). A careful analysis of Kepler host stars spectroscopy by the CKS survey
allowed Fulton et al. (2017) in March 2017 to unambiguously show this radius
gap. The cause of this gap is still under discussion (Ginzburg et al. 2017;
Lehmer & Catling 2017; Owen & Wu 2017). Here we add to our original analysis
the dependence of the radius gap on host star type.
",0,1,0,0,0,0
9900,General tête-à-tête graphs and Seifert manifolds,"  Tête-à-tête graphs and relative tête-à-tête graphs were
introduced by N. A'Campo in 2010 to model monodromies of isolated plane curves.
By recent workof Fdez de Bobadilla, Pe Pereira and the author, they provide a
way of modeling the periodic mapping classes that leave some boundary component
invariant. In this work we introduce the notion of general tête-à-tête
graph and prove that they model all periodic mapping classes. We also describe
algorithms that take a Seifert manifold and a horizontal surface and return a
tête-à-tête graph and vice versa.
",0,0,1,0,0,0
424,Distributive Aronszajn trees,"  Ben-David and Shelah proved that if $\lambda$ is a singular strong-limit
cardinal and $2^\lambda=\lambda^+$, then $\square^*_\lambda$ entails the
existence of a normal $\lambda$-distributive $\lambda^+$-Aronszajn tree. Here,
it is proved that the same conclusion remains valid after replacing the
hypothesis $\square^*_\lambda$ by $\square(\lambda^+,{<}\lambda)$.
As $\square(\lambda^+,{<}\lambda)$ does not impose a bound on the order-type
of the witnessing clubs, our construction is necessarily different from that of
Ben-David and Shelah, and instead uses walks on ordinals augmented with club
guessing.
A major component of this work is the study of postprocessing functions and
their effect on square sequences. A byproduct of this study is the finding that
for $\kappa$ regular uncountable, $\square(\kappa)$ entails the existence of a
partition of $\kappa$ into $\kappa$ many fat sets. When contrasted with a
classic model of Magidor, this shows that it is equiconsistent with the
existence of a weakly compact cardinal that $\omega_2$ cannot be split into two
fat sets.
",0,0,1,0,0,0
12561,Maximum Margin Interval Trees,"  Learning a regression function using censored or interval-valued output data
is an important problem in fields such as genomics and medicine. The goal is to
learn a real-valued prediction function, and the training output labels
indicate an interval of possible values. Whereas most existing algorithms for
this task are linear models, in this paper we investigate learning nonlinear
tree models. We propose to learn a tree by minimizing a margin-based
discriminative objective function, and we provide a dynamic programming
algorithm for computing the optimal solution in log-linear time. We show
empirically that this algorithm achieves state-of-the-art speed and prediction
accuracy in a benchmark of several data sets.
",1,0,0,1,0,0
3223,A Novel Model of Cancer-Induced Peripheral Neuropathy and the Role of TRPA1 in Pain Transduction,"  Background. Models of cancer-induced neuropathy are designed by injecting
cancer cells near the peripheral nerves. The interference of tissue-resident
immune cells does not allow a direct contact with nerve fibres which affects
the tumor microenvironment and the invasion process. Methods. Anaplastic
tumor-1 (AT-1) cells were inoculated within the sciatic nerves (SNs) of male
Copenhagen rats. Lumbar dorsal root ganglia (DRGs) and the SNs were collected
on days 3, 7, 14, and 21. SN tissues were examined for morphological changes
and DRG tissues for immunofluorescence, electrophoretic tendency, and mRNA
quantification. Hypersensitivities to cold, mechanical, and thermal stimuli
were determined. HC-030031, a selective TRPA1 antagonist, was used to treat
cold allodynia. Results. Nociception thresholds were identified on day 6.
Immunofluorescent micrographs showed overexpression of TRPA1 on days 7 and 14
and of CGRP on day 14 until day 21. Both TRPA1 and CGRP were coexpressed on the
same cells. Immunoblots exhibited an increase in TRPA1 expression on day 14.
TRPA1 mRNA underwent an increase on day 7 (normalized to 18S). Injection of
HC-030031 transiently reversed the cold allodynia. Conclusion. A novel and a
promising model of cancer-induced neuropathy was established, and the role of
TRPA1 and CGRP in pain transduction was examined.
",0,0,0,0,1,0
12182,CTCModel: a Keras Model for Connectionist Temporal Classification,"  We report an extension of a Keras Model, called CTCModel, to perform the
Connectionist Temporal Classification (CTC) in a transparent way. Combined with
Recurrent Neural Networks, the Connectionist Temporal Classification is the
reference method for dealing with unsegmented input sequences, i.e. with data
that are a couple of observation and label sequences where each label is
related to a subset of observation frames. CTCModel makes use of the CTC
implementation in the Tensorflow backend for training and predicting sequences
of labels using Keras. It consists of three branches made of Keras models: one
for training, computing the CTC loss function; one for predicting, providing
sequences of labels; and one for evaluating that returns standard metrics for
analyzing sequences of predictions.
",1,0,0,1,0,0
12215,6.2-GHz modulated terahertz light detection using fast terahertz quantum well photodetectors,"  The fast detection of terahertz radiation is of great importance for various
applications such as fast imaging, high speed communications, and spectroscopy.
Most commercial products capable of sensitively responding the terahertz
radiation are thermal detectors, i.e., pyroelectric sensors and bolometers.
This class of terahertz detectors is normally characterized by low modulation
frequency (dozens or hundreds of Hz). Here we demonstrate the first fast
semiconductor-based terahertz quantum well photodetectors by carefully
designing the device structure and microwave transmission line for high
frequency signal extraction. Modulation response bandwidth of gigahertz level
is obtained. As an example, the 6.2-GHz modulated terahertz light emitted from
a Fabry-Pérot terahertz quantum cascade laser is successfully detected
using the fast terahertz quantum well photodetector. In addition to the fast
terahertz detection, the technique presented in this work can also facilitate
the frequency stability or phase noise characterizations for terahertz quantum
cascade lasers.
",0,1,0,0,0,0
2203,Detecting Galaxy-Filament Alignments in the Sloan Digital Sky Survey III,"  Previous studies have shown the filamentary structures in the cosmic web
influence the alignments of nearby galaxies. We study this effect in the LOWZ
sample of the Sloan Digital Sky Survey using the ""Cosmic Web Reconstruction""
filament catalogue of Chen et al. (2016). We find that LOWZ galaxies exhibit a
small but statistically significant alignment in the direction parallel to the
orientation of nearby filaments. This effect is detectable even in the absence
of nearby galaxy clusters, which suggests it is an effect from the matter
distribution in the filament. A nonparametric regression model suggests that
the alignment effect with filaments extends over separations of $30-40$ Mpc. We
find that galaxies that are bright and early-forming align more strongly with
the directions of nearby filaments than those that are faint and late-forming;
however, trends with stellar mass are less statistically significant, within
the narrow range of stellar mass of this sample.
",0,0,0,1,0,0
1202,Adversarial Pseudo Healthy Synthesis Needs Pathology Factorization,"  Pseudo healthy synthesis, i.e. the creation of a subject-specific `healthy'
image from a pathological one, could be helpful in tasks such as anomaly
detection, understanding changes induced by pathology and disease or even as
data augmentation. We treat this task as a factor decomposition problem: we aim
to separate what appears to be healthy and where disease is (as a map). The two
factors are then recombined (by a network) to reconstruct the input disease
image. We train our models in an adversarial way using either paired or
unpaired settings, where we pair disease images and maps (as segmentation
masks) when available. We quantitatively evaluate the quality of pseudo healthy
images. We show in a series of experiments, performed in ISLES and BraTS
datasets, that our method is better than conditional GAN and CycleGAN,
highlighting challenges in using adversarial methods in the image translation
task of pseudo healthy image generation.
",1,0,0,1,0,0
3044,A multilayer multiconfiguration time-dependent Hartree study of the nonequilibrium Anderson impurity model at zero temperature,"  Quantum transport is studied for the nonequilibrium Anderson impurity model
at zero temperature employing the multilayer multiconfiguration time-dependent
Hartree theory within the second quantization representation (ML-MCTDH-SQR) of
Fock space. To adress both linear and nonlinear conductance in the Kondo
regime, two new techniques of the ML-MCTDH-SQR simulation methodology are
introduced: (i) the use of correlated initial states, which is achieved by
imaginary time propagation of the overall Hamiltonian at zero voltage and (ii)
the adoption of the logarithmic discretization of the electronic continuum.
Employing the improved methodology, the signature of the Kondo effect is
analyzed.
",0,1,0,0,0,0
19028,Doping-induced quantum cross-over in Er$_2$Ti$_{2-x}$Sn$_x$O$_7$,"  We present the results of the investigation of magnetic properties of the
Er$_2$Ti$_{2-x}$Sn$_x$O$_7$ series. For small doping values the ordering
temperature decreases linearly with $x$ while the moment configuration remains
the same as in the $x = 0$ parent compound. Around $x = 1.7$ doping level we
observe a change in the behavior, where the ordering temperature starts to
increase and new magnetic Bragg peaks appear. For the first time we present
evidence of a long-range order (LRO) in Er$_2$Sn$_2$O$_7$ ($x = 2.0$) below
$T_N = 130$ mK. It is revealed that the moment configuration corresponds to a
Palmer-Chalker type with a value of the magnetic moment significantly
renormalized compared to $x = 0$. We discuss our results in the framework of a
possible quantum phase transition occurring close to $x = 1.7$.
",0,1,0,0,0,0
4157,How To Extract Fashion Trends From Social Media? A Robust Object Detector With Support For Unsupervised Learning,"  With the proliferation of social media, fashion inspired from celebrities,
reputed designers as well as fashion influencers has shortened the cycle of
fashion design and manufacturing. However, with the explosion of fashion
related content and large number of user generated fashion photos, it is an
arduous task for fashion designers to wade through social media photos and
create a digest of trending fashion. This necessitates deep parsing of fashion
photos on social media to localize and classify multiple fashion items from a
given fashion photo. While object detection competitions such as MSCOCO have
thousands of samples for each of the object categories, it is quite difficult
to get large labeled datasets for fast fashion items. Moreover,
state-of-the-art object detectors do not have any functionality to ingest large
amount of unlabeled data available on social media in order to fine tune object
detectors with labeled datasets. In this work, we show application of a generic
object detector, that can be pretrained in an unsupervised manner, on 24
categories from recently released Open Images V4 dataset. We first train the
base architecture of the object detector using unsupervisd learning on 60K
unlabeled photos from 24 categories gathered from social media, and then
subsequently fine tune it on 8.2K labeled photos from Open Images V4 dataset.
On 300 X 300 image inputs, we achieve 72.7% mAP on a test dataset of 2.4K
photos while performing 11% to 17% better as compared to the state-of-the-art
object detectors. We show that this improvement is due to our choice of
architecture that lets us do unsupervised learning and that performs
significantly better in identifying small objects.
",0,0,0,1,0,0
15758,Orthogonal foliations on riemannian manifolds,"  In this work, we find an equation that relates the Ricci curvature of a
riemannian manifold $M$ and the second fundamental forms of two orthogonal
foliations of complementary dimensions, $\mathcal{F}$ and $\mathcal{F}^{\bot}$,
defined on $M$. Using this equation, we show a sufficient condition for the
manifold M to be locally a riemannian product of the leaves of $\mathcal{F}$
and $\mathcal{F}^{\bot}$, if one of the foliations is totally umbilical. We
also prove an integral formula for such foliations.
",0,0,1,0,0,0
20968,Contemporary machine learning: a guide for practitioners in the physical sciences,"  Machine learning is finding increasingly broad application in the physical
sciences. This most often involves building a model relationship between a
dependent, measurable output and an associated set of controllable, but
complicated, independent inputs. We present a tutorial on current techniques in
machine learning -- a jumping-off point for interested researchers to advance
their work. We focus on deep neural networks with an emphasis on demystifying
deep learning. We begin with background ideas in machine learning and some
example applications from current research in plasma physics. We discuss
supervised learning techniques for modeling complicated functions, beginning
with familiar regression schemes, then advancing to more sophisticated deep
learning methods. We also address unsupervised learning and techniques for
reducing the dimensionality of input spaces. Along the way, we describe methods
for practitioners to help ensure that their models generalize from their
training data to as-yet-unseen test data. We describe classes of tasks --
predicting scalars, handling images, fitting time-series -- and prepare the
reader to choose an appropriate technique. We finally point out some
limitations to modern machine learning and speculate on some ways that
practitioners from the physical sciences may be particularly suited to help.
",1,1,0,0,0,0
7776,The HoTT reals coincide with the Escardó-Simpson reals,"  Escardó and Simpson defined a notion of interval object by a universal
property in any category with binary products. The Homotopy Type Theory book
defines a higher-inductive notion of reals, and suggests that the interval may
satisfy this universal property. We show that this is indeed the case in the
category of sets of any universe. We also show that the type of HoTT reals is
the least Cauchy complete subset of the Dedekind reals containing the
rationals.
",1,0,1,0,0,0
4950,Second-grade fluids in curved pipes,"  This paper is concerned with the application of finite element methods to
obtain solutions for steady fully developed second-grade flows in a curved pipe
of circular cross-section and arbitrary curvature ratio, under a given axial
pressure gradient. The qualitative and quantitative behavior of the secondary
flows is analyzed with respect to inertia and viscoelasticity.
",0,1,1,0,0,0
8479,The Physics of Eccentric Binary Black Hole Mergers. A Numerical Relativity Perspective,"  Gravitational wave observations of eccentric binary black hole mergers will
provide unequivocal evidence for the formation of these systems through
dynamical assembly in dense stellar environments. The study of these
astrophysically motivated sources is timely in view of electromagnetic
observations, consistent with the existence of stellar mass black holes in the
globular cluster M22 and in the Galactic center, and the proven detection
capabilities of ground-based gravitational wave detectors. In order to get
insights into the physics of these objects in the dynamical, strong-field
gravity regime, we present a catalog of 89 numerical relativity waveforms that
describe binary systems of non-spinning black holes with mass-ratios $1\leq q
\leq 10$, and initial eccentricities as high as $e_0=0.18$ fifteen cycles
before merger. We use this catalog to provide landmark results regarding the
loss of energy through gravitational radiation, both for quadrupole and
higher-order waveform multipoles, and the astrophysical properties, final mass
and spin, of the post-merger black hole as a function of eccentricity and
mass-ratio. We discuss the implications of these results for gravitational wave
source modeling, and the design of algorithms to search for and identify the
complex signatures of these events in realistic detection scenarios.
",1,0,0,0,0,0
3849,Correction to the article: Floer homology and splicing knot complements,"  This note corrects the mistakes in the splicing formulas of the paper ""Floer
homology and splicing knot complements"". The mistakes are the result of the
incorrect assumption that for a knot $K$ inside a homology sphere $Y$, the
involution on the knot Floer homology of $K$ which corresponds to moving the
basepoints by one full twist around $K$ is trivial. The correction implicitly
involves considering the contribution from this (possibly non-trivial)
involution in a number of places.
",0,0,1,0,0,0
7302,La leggenda del quanto centenario,"  Around year 2000 the centenary of Planck's thermal radiation formula awakened
interest in the origins of quantum theory, traditionally traced back to the
Planck's conference on 14 December 1900 at the Berlin Academy of Sciences. A
lot of more accurate historical reconstructions, conducted under the stimulus
of that recurrence, placed the birth date of quantum theory in March 1905 when
Einstein advanced his light quantum hypothesis. Both interpretations are yet
controversial, but science historians agree on one point: the emergence of
quantum theory from a presumed ""crisis"" of classical physics is a myth with
scarce adherence to the historical truth. This article, written in Italian
language, was originally presented in connection with the celebration of the
World Year of Phyics 2005 with the aim of bringing these scholarly theses to a
wider audience.
---
Tradizionalmente la nascita della teoria quantistica viene fatta risalire al
14 dicembre 1900, quando Planck presentò all'Accademia delle Scienze di
Berlino la dimostrazione della formula della radiazione termica. Numerose
ricostruzioni storiche più accurate, effettuate nel periodo intorno al 2000
sotto lo stimolo dell'interesse per il centenario di quell'avvenimento,
collocano invece la nascita della teoria quantistica nel marzo del 1905, quando
Einstein avanzò l'ipotesi dei quanti di luce. Entrambe le interpretazioni
sono tuttora controverse, ma gli storici della scienza concordano su un punto:
l'emergere della teoria quantistica da una presunta ""crisi"" della fisica
classica è un mito con scarsa aderenza alla verità storica. Con questo
articolo in italiano, presentato originariamente in occasione delle
celebrazioni per il World Year of Phyics 2005, si è inteso portare a un più
largo pubblico queste tesi già ben note agli specialisti.
",0,1,0,0,0,0
17866,Tensor Networks in a Nutshell,"  Tensor network methods are taking a central role in modern quantum physics
and beyond. They can provide an efficient approximation to certain classes of
quantum states, and the associated graphical language makes it easy to describe
and pictorially reason about quantum circuits, channels, protocols, open
systems and more. Our goal is to explain tensor networks and some associated
methods as quickly and as painlessly as possible. Beginning with the key
definitions, the graphical tensor network language is presented through
examples. We then provide an introduction to matrix product states. We conclude
the tutorial with tensor contractions evaluating combinatorial counting
problems. The first one counts the number of solutions for Boolean formulae,
whereas the second is Penrose's tensor contraction algorithm, returning the
number of $3$-edge-colorings of $3$-regular planar graphs.
",0,1,0,0,0,0
8728,Detecting Policy Preferences and Dynamics in the UN General Debate with Neural Word Embeddings,"  Foreign policy analysis has been struggling to find ways to measure policy
preferences and paradigm shifts in international political systems. This paper
presents a novel, potential solution to this challenge, through the application
of a neural word embedding (Word2vec) model on a dataset featuring speeches by
heads of state or government in the United Nations General Debate. The paper
provides three key contributions based on the output of the Word2vec model.
First, it presents a set of policy attention indices, synthesizing the semantic
proximity of political speeches to specific policy themes. Second, it
introduces country-specific semantic centrality indices, based on topological
analyses of countries' semantic positions with respect to each other. Third, it
tests the hypothesis that there exists a statistical relation between the
semantic content of political speeches and UN voting behavior, falsifying it
and suggesting that political speeches contain information of different nature
then the one behind voting outcomes. The paper concludes with a discussion of
the practical use of its results and consequences for foreign policy analysis,
public accountability, and transparency.
",1,0,0,1,0,0
13580,Embedding dimension and codimension of tensor products of algebras over a field,"  Let k be a field. This paper investigates the embedding dimension and
codimension of Noetherian local rings arising as localizations of tensor
products of k-algebras. We use results and techniques from prime spectra and
dimension theory to establish an analogue of the ""special chain theorem"" for
the embedding dimension of tensor products, with effective consequence on the
transfer or defect of regularity as exhibited by the (embedding) codimension.
",0,0,1,0,0,0
18572,Numerical studies of Thompson's group F and related groups,"  We have developed polynomial-time algorithms to generate terms of the
cogrowth series for groups $\mathbb{Z}\wr \mathbb{Z},$ the lamplighter group,
$(\mathbb{Z}\wr \mathbb{Z})\wr \mathbb{Z}$ and the Navas-Brin group $B.$ We
have also given an improved algorithm for the coefficients of Thompson's group
$F,$ giving 32 terms of the cogrowth series. We develop numerical techniques to
extract the asymptotics of these various cogrowth series. We present improved
rigorous lower bounds on the growth-rate of the cogrowth series for Thompson's
group $F$ using the method from \cite{HHR15} applied to our extended series. We
also generalise their method by showing that it applies to loops on any locally
finite graph. Unfortunately, lower bounds less than 16 do not help in
determining amenability.
Again for Thompson's group $F$ we prove that, if the group is amenable, there
cannot be a sub-dominant stretched exponential term in the
asymptotics\footnote{ }. Yet the numerical data provides compelling evidence
for the presence of such a term. This observation suggests a potential path to
a proof of non-amenability: If the universality class of the cogrowth sequence
can be determined rigorously, it will likely prove non-amenability.
We estimate the asymptotics of the cogrowth coefficients of $F$ to be $$ c_n
\sim c \cdot \mu^n \cdot \kappa^{n^\sigma \log^\delta{n}} \cdot n^g,$$ where
$\mu \approx 15,$ $\kappa \approx 1/e,$ $\sigma \approx 1/2,$ $\delta \approx
1/2,$ and $g \approx -1.$ The growth constant $\mu$ must be 16 for amenability.
These two approaches, plus a third based on extrapolating lower bounds, support
the conjecture \cite{ERvR15, HHR15} that the group is not amenable.
",0,0,1,0,0,0
5196,Near-IR period-luminosity relations for pulsating stars in $ω$ Centauri (NGC 5139),"  $\omega$ Centauri (NGC 5139) hosts hundreds of pulsating variable stars of
different types, thus representing a treasure trove for studies of their
corresponding period-luminosity (PL) relations. Our goal in this study is to
obtain the PL relations for RR Lyrae, and SX Phoenicis stars in the field of
the cluster, based on high-quality, well-sampled light curves in the
near-infrared (IR). $\omega$ Centauri was observed using VIRCAM mounted on
VISTA. A total of 42 epochs in $J$ and 100 epochs in $K_{\rm S}$ were obtained,
spanning 352 days. Point-spread function photometry was performed using DoPhot
and DAOPHOT in the outer and inner regions of the cluster, respectively. Based
on the comprehensive catalogue of near-IR light curves thus secured, PL
relations were obtained for the different types of pulsators in the cluster,
both in the $J$ and $K_{\rm S}$ bands. This includes the first PL relations in
the near-IR for fundamental-mode SX Phoenicis stars. The near-IR magnitudes and
periods of Type II Cepheids and RR Lyrae stars were used to derive an updated
true distance modulus to the cluster, with a resulting value of $(m-M)_0 =
13.708 \pm 0.035 \pm 0.10$ mag, where the error bars correspond to the adopted
statistical and systematic errors, respectively. Adding the errors in
quadrature, this is equivalent to a heliocentric distance of $5.52\pm 0.27$
kpc.
",0,1,0,0,0,0
12797,Novel polystyrene-based nanocomposites by phosphorene dispersion,"  Polystyrene-based phosphorene nanocomposites were prepared by a solvent
blending procedure allowing the embedding of black phosphorus (BP) nanoflakes
in the polymer matrix. Raman spectroscopy, X Ray Diffraction and TEM microscopy
were employed to characterize the structural and the morphological
characteristics of the achieved hybrids, with the aim to evaluate the
dispersion level of black phosphorus layers. TGA, DSC analysis as well as
thermal oxidation and photo-degradation techniques were employed to investigate
the thermal- and the photo-stability of the samples. The collected results
evidenced better thermal and photostability of both polymer matrix and
dispersed layered phosphorus, suggesting really interesting polymer-nanofiller
synergic effects ascribable to the presence and the good dispersion of the
2D-nanomaterial.
",0,1,0,0,0,0
12778,The growth rates of automaton groups generated by reset automata,"  We give sufficient conditions for when groups generated by automata in a
class $\mathcal{C}$ of transducers, which contains the class of reset automata
transducers, have infinite order. As a consequence we also demonstrate that if
a group generated by an automata in $\mathcal{C}$ is infinite, then it contains
a free semigroup of rank at least 2. This gives a new proof, in the context of
groups generated by automaton in $\mathcal{C}$, of a result of Chou showing
that finitely generated elementary amenable groups either have polynomial
growth or contain a free semigroup of rank at least 2.
We also study what we call the `core growth rate' of elements of
$\mathcal{C}$. This turns out to be equivalent to the growth rate of certain
initial transducers. We give examples of transducers with exponential core
growth rate, and conjecture that all infinite order transducers in the class
$\mathcal{C}$ have exponential core growth rate.
",0,0,1,0,0,0
17787,On the possibility of developing quasi-cw high-power high-pressure laser on 4p-4s transition of ArI with electron beam - optical pumping: quenching of 4s (3P2) lower laser level,"  A new electron beam-optical procedure is proposed for quasi-cw pumping of
high-pressure large-volume He-Ar laser on 4p[1/2]1 - 4s[3/2]2 argon atom
transition at the wavelength of 912.5 nm. It consists of creation and
maintenance of a necessary density of 4s[3/2]2 metastable state in the gain
medium by a fast electron beam and subsequent optically pumping of the upper
laser level via the classical three-level scheme using a laser diode.
Absorption probing is used to study collisional quenching of Ar* metastable in
electron-beam-excited high-pressure He-Ar mixtures with a low content of argon.
The rate constants for plasma-chemical reactions Ar*+He+Ar-Ar2*+He (3.6 +-
0.4)x10-33 cm6/s, Ar+2He-HeAr*+He (4.4 +- 0.9)x10-36 cm6/s and
Ar*+He-Products+He (2.4 +- 0.3)x10-15 cm3/s are for the first time measured.
",0,1,0,0,0,0
2200,A New Perspective on Robust $M$-Estimation: Finite Sample Theory and Applications to Dependence-Adjusted Multiple Testing,"  Heavy-tailed errors impair the accuracy of the least squares estimate, which
can be spoiled by a single grossly outlying observation. As argued in the
seminal work of Peter Huber in 1973 [{\it Ann. Statist.} {\bf 1} (1973)
799--821], robust alternatives to the method of least squares are sorely
needed. To achieve robustness against heavy-tailed sampling distributions, we
revisit the Huber estimator from a new perspective by letting the tuning
parameter involved diverge with the sample size. In this paper, we develop
nonasymptotic concentration results for such an adaptive Huber estimator,
namely, the Huber estimator with the tuning parameter adapted to sample size,
dimension, and the variance of the noise. Specifically, we obtain a
sub-Gaussian-type deviation inequality and a nonasymptotic Bahadur
representation when noise variables only have finite second moments. The
nonasymptotic results further yield two conventional normal approximation
results that are of independent interest, the Berry-Esseen inequality and
Cramér-type moderate deviation. As an important application to large-scale
simultaneous inference, we apply these robust normal approximation results to
analyze a dependence-adjusted multiple testing procedure for moderately
heavy-tailed data. It is shown that the robust dependence-adjusted procedure
asymptotically controls the overall false discovery proportion at the nominal
level under mild moment conditions. Thorough numerical results on both
simulated and real datasets are also provided to back up our theory.
",0,0,1,1,0,0
9518,"Calidad en repositorios digitales en Argentina, estudio comparativo y cualitativo","  Numerous institutions and organizations need not only to preserve the
material and publications they produce, but also have as their task (although
it would be desirable it was an obligation) to publish, disseminate and make
publicly available all the results of the research and any other
scientific/academic material. The Open Archives Initiative (OAI) and the
introduction of Open Archives Initiative Protocol for Metadata Harvesting
(OAI-PMH), make this task much easier. The main objective of this work is to
make a comparative and qualitative study of the data -metadata specifically-
contained in the whole set of Argentine repositories listed in the ROAR portal,
focusing on the functional perspective of the quality of this metadata. Another
objective is to offer an overview of the status of these repositories, in an
attempt to detect common failures and errors institutions incur when storing
the metadata of the resources contained in these repositories, and thus be able
to suggest measures to be able to improve the load and further retrieval
processes. It was found that the eight most used Dublin Core fields are:
identifier, type, title, date, subject, creator, language and description. Not
all repositories fill all the fields, and the lack of normalization, or the
excessive use of fields like language, type, format and subject is somewhat
striking, and in some cases even alarming
",1,0,0,0,0,0
16484,End-to-End Learning for the Deep Multivariate Probit Model,"  The multivariate probit model (MVP) is a popular classic model for studying
binary responses of multiple entities. Nevertheless, the computational
challenge of learning the MVP model, given that its likelihood involves
integrating over a multidimensional constrained space of latent variables,
significantly limits its application in practice. We propose a flexible deep
generalization of the classic MVP, the Deep Multivariate Probit Model (DMVP),
which is an end-to-end learning scheme that uses an efficient parallel sampling
process of the multivariate probit model to exploit GPU-boosted deep neural
networks. We present both theoretical and empirical analysis of the convergence
behavior of DMVP's sampling process with respect to the resolution of the
correlation structure. We provide convergence guarantees for DMVP and our
empirical analysis demonstrates the advantages of DMVP's sampling compared with
standard MCMC-based methods. We also show that when applied to multi-entity
modelling problems, which are natural DMVP applications, DMVP trains faster
than classical MVP, by at least an order of magnitude, captures rich
correlations among entities, and further improves the joint likelihood of
entities compared with several competitive models.
",0,0,0,1,0,0
9505,Visual-Based Analysis of Classification Measures with Applications to Imbalanced Data,"  With a plethora of available classification performance measures, choosing
the right metric for the right task requires careful thought. To make this
decision in an informed manner, one should study and compare general properties
of candidate measures. However, analysing measures with respect to complete
ranges of their domain values is a difficult and challenging task. In this
study, we attempt to support such analyses with a specialized visualization
technique, which operates in a barycentric coordinate system using a 3D
tetrahedron. Additionally, we adapt this technique to the context of imbalanced
data and put forward a set of properties which should be taken into account
when selecting a classification performance measure. As a result, we compare 22
popular measures and show important differences in their behaviour. Moreover,
for parametric measures such as the F$_{\beta}$ and IBA$_\alpha$(G-mean), we
analytically derive parameter thresholds that change measure properties.
Finally, we provide an online visualization tool that can aid the analysis of
complete domain ranges of performance measures.
",1,0,0,0,0,0
19407,Radiomics strategies for risk assessment of tumour failure in head-and-neck cancer,"  Quantitative extraction of high-dimensional mineable data from medical images
is a process known as radiomics. Radiomics is foreseen as an essential
prognostic tool for cancer risk assessment and the quantification of
intratumoural heterogeneity. In this work, 1615 radiomic features (quantifying
tumour image intensity, shape, texture) extracted from pre-treatment FDG-PET
and CT images of 300 patients from four different cohorts were analyzed for the
risk assessment of locoregional recurrences (LR) and distant metastases (DM) in
head-and-neck cancer. Prediction models combining radiomic and clinical
variables were constructed via random forests and imbalance-adjustment
strategies using two of the four cohorts. Independent validation of the
prediction and prognostic performance of the models was carried out on the
other two cohorts (LR: AUC = 0.69 and CI = 0.67; DM: AUC = 0.86 and CI = 0.88).
Furthermore, the results obtained via Kaplan-Meier analysis demonstrated the
potential of radiomics for assessing the risk of specific tumour outcomes using
multiple stratification groups. This could have important clinical impact,
notably by allowing for a better personalization of chemo-radiation treatments
for head-and-neck cancer patients from different risk groups.
",1,0,0,0,0,0
5771,Learning to Invert: Signal Recovery via Deep Convolutional Networks,"  The promise of compressive sensing (CS) has been offset by two significant
challenges. First, real-world data is not exactly sparse in a fixed basis.
Second, current high-performance recovery algorithms are slow to converge,
which limits CS to either non-real-time applications or scenarios where massive
back-end computing is available. In this paper, we attack both of these
challenges head-on by developing a new signal recovery framework we call {\em
DeepInverse} that learns the inverse transformation from measurement vectors to
signals using a {\em deep convolutional network}. When trained on a set of
representative images, the network learns both a representation for the signals
(addressing challenge one) and an inverse map approximating a greedy or convex
recovery algorithm (addressing challenge two). Our experiments indicate that
the DeepInverse network closely approximates the solution produced by
state-of-the-art CS recovery algorithms yet is hundreds of times faster in run
time. The tradeoff for the ultrafast run time is a computationally intensive,
off-line training procedure typical to deep networks. However, the training
needs to be completed only once, which makes the approach attractive for a host
of sparse recovery problems.
",1,0,0,1,0,0
11064,The Mira-Titan Universe II: Matter Power Spectrum Emulation,"  We introduce a new cosmic emulator for the matter power spectrum covering
eight cosmological parameters. Targeted at optical surveys, the emulator
provides accurate predictions out to a wavenumber k~5/Mpc and redshift z<=2.
Besides covering the standard set of LCDM parameters, massive neutrinos and a
dynamical dark energy of state are included. The emulator is built on a sample
set of 36 cosmological models, carefully chosen to provide accurate predictions
over the wide and large parameter space. For each model, we have performed a
high-resolution simulation, augmented with sixteen medium-resolution
simulations and TimeRG perturbation theory results to provide accurate coverage
of a wide k-range; the dataset generated as part of this project is more than
1.2Pbyte. With the current set of simulated models, we achieve an accuracy of
approximately 4%. Because the sampling approach used here has established
convergence and error-control properties, follow-on results with more than a
hundred cosmological models will soon achieve ~1% accuracy. We compare our
approach with other prediction schemes that are based on halo model ideas and
remapping approaches. The new emulator code is publicly available.
",0,1,0,0,0,0
16720,Predicting Opioid Relapse Using Social Media Data,"  Opioid addiction is a severe public health threat in the U.S, causing massive
deaths and many social problems. Accurate relapse prediction is of practical
importance for recovering patients since relapse prediction promotes timely
relapse preventions that help patients stay clean. In this paper, we introduce
a Generative Adversarial Networks (GAN) model to predict the addiction relapses
based on sentiment images and social influences. Experimental results on real
social media data from Reddit.com demonstrate that the GAN model delivers a
better performance than comparable alternative techniques. The sentiment images
generated by the model show that relapse is closely connected with two emotions
`joy' and `negative'. This work is one of the first attempts to predict
relapses using massive social media data and generative adversarial nets. The
proposed method, combined with knowledge of social media mining, has the
potential to revolutionize the practice of opioid addiction prevention and
treatment.
",1,0,0,0,0,0
14108,MOROCO: The Moldavian and Romanian Dialectal Corpus,"  In this work, we introduce the MOldavian and ROmanian Dialectal COrpus
(MOROCO), which is freely available for download at
this https URL. The corpus contains 33564 samples of
text (with over 10 million tokens) collected from the news domain. The samples
belong to one of the following six topics: culture, finance, politics, science,
sports and tech. The data set is divided into 21719 samples for training, 5921
samples for validation and another 5924 samples for testing. For each sample,
we provide corresponding dialectal and category labels. This allows us to
perform empirical studies on several classification tasks such as (i) binary
discrimination of Moldavian versus Romanian text samples, (ii) intra-dialect
multi-class categorization by topic and (iii) cross-dialect multi-class
categorization by topic. We perform experiments using a shallow approach based
on string kernels, as well as a novel deep approach based on character-level
convolutional neural networks containing Squeeze-and-Excitation blocks. We also
present and analyze the most discriminative features of our best performing
model, before and after named entity removal.
",1,0,0,0,0,0
10032,Separation-Free Super-Resolution from Compressed Measurements is Possible: an Orthonormal Atomic Norm Minimization Approach,"  We consider the problem of recovering the superposition of $R$ distinct
complex exponential functions from compressed non-uniform time-domain samples.
Total Variation (TV) minimization or atomic norm minimization was proposed in
the literature to recover the $R$ frequencies or the missing data. However, it
is known that in order for TV minimization and atomic norm minimization to
recover the missing data or the frequencies, the underlying $R$ frequencies are
required to be well-separated, even when the measurements are noiseless. This
paper shows that the Hankel matrix recovery approach can super-resolve the $R$
complex exponentials and their frequencies from compressed non-uniform
measurements, regardless of how close their frequencies are to each other. We
propose a new concept of orthonormal atomic norm minimization (OANM), and
demonstrate that the success of Hankel matrix recovery in separation-free
super-resolution comes from the fact that the nuclear norm of a Hankel matrix
is an orthonormal atomic norm. More specifically, we show that, in traditional
atomic norm minimization, the underlying parameter values $\textbf{must}$ be
well separated to achieve successful signal recovery, if the atoms are changing
continuously with respect to the continuously-valued parameter. In contrast,
for the OANM, it is possible the OANM is successful even though the original
atoms can be arbitrarily close.
As a byproduct of this research, we provide one matrix-theoretic inequality
of nuclear norm, and give its proof from the theory of compressed sensing.
",1,0,0,0,0,0
15222,Vertex algebras associated with hypertoric varieties,"  We construct a family of vertex algebras associated with a family of
symplectic singularity/resolution, called hypertoric varieties. While the
hypertoric varieties are constructed by a certain Hamiltonian reduction
associated with a torus action, our vertex algebras are constructed by
(semi-infinite) BRST reduction. The construction works algebro-geometrically
and we construct sheaves of $\hbar$-adic vertex algebras over hypertoric
varieties which localize the vertex algebras. We show when the vertex algebras
are vertex operator algebras by giving explicit conformal vectors. We also show
that the Zhu algebras of the vertex algebras, associative algebras associated
with non-negatively graded vertex algebras, gives a certain family of filtered
quantizations of the coordinate rings of the hypertoric varieties.
",0,0,1,0,0,0
14103,Unified Spectral Clustering with Optimal Graph,"  Spectral clustering has found extensive use in many areas. Most traditional
spectral clustering algorithms work in three separate steps: similarity graph
construction; continuous labels learning; discretizing the learned labels by
k-means clustering. Such common practice has two potential flaws, which may
lead to severe information loss and performance degradation. First, predefined
similarity graph might not be optimal for subsequent clustering. It is
well-accepted that similarity graph highly affects the clustering results. To
this end, we propose to automatically learn similarity information from data
and simultaneously consider the constraint that the similarity matrix has exact
c connected components if there are c clusters. Second, the discrete solution
may deviate from the spectral solution since k-means method is well-known as
sensitive to the initialization of cluster centers. In this work, we transform
the candidate solution into a new one that better approximates the discrete
one. Finally, those three subtasks are integrated into a unified framework,
with each subtask iteratively boosted by using the results of the others
towards an overall optimal solution. It is known that the performance of a
kernel method is largely determined by the choice of kernels. To tackle this
practical problem of how to select the most suitable kernel for a particular
data set, we further extend our model to incorporate multiple kernel learning
ability. Extensive experiments demonstrate the superiority of our proposed
method as compared to existing clustering approaches.
",1,0,0,1,0,0
5961,Monte Carlo Tree Search for Asymmetric Trees,"  We present an extension of Monte Carlo Tree Search (MCTS) that strongly
increases its efficiency for trees with asymmetry and/or loops. Asymmetric
termination of search trees introduces a type of uncertainty for which the
standard upper confidence bound (UCB) formula does not account. Our first
algorithm (MCTS-T), which assumes a non-stochastic environment, backs-up tree
structure uncertainty and leverages it for exploration in a modified UCB
formula. Results show vastly improved efficiency in a well-known asymmetric
domain in which MCTS performs arbitrarily bad. Next, we connect the ideas about
asymmetric termination to the presence of loops in the tree, where the same
state appears multiple times in a single trace. An extension to our algorithm
(MCTS-T+), which in addition to non-stochasticity assumes full state
observability, further increases search efficiency for domains with loops as
well. Benchmark testing on a set of OpenAI Gym and Atari 2600 games indicates
that our algorithms always perform better than or at least equivalent to
standard MCTS, and could be first-choice tree search algorithms for
non-stochastic, fully-observable environments.
",0,0,0,1,0,0
3283,Soliton-potential interactions for nonlinear Schrödinger equation in $\mathbb{R}^3$,"  In this work we mainly consider the dynamics and scattering of a narrow
soliton of NLS equation with a potential in $\mathbb{R}^3$, where the
asymptotic state of the system can be far from the initial state in parameter
space. Specifically, if we let a narrow soliton state with initial velocity
$\upsilon_{0}$ to interact with an extra potential $V(x)$, then the velocity
$\upsilon_{+}$ of outgoing solitary wave in infinite time will in general be
very different from $\upsilon_{0}$. In contrast to our present work, previous
works proved that the soliton is asymptotically stable under the assumption
that $\upsilon_{+}$ stays close to $\upsilon_{0}$ in a certain manner.
",0,0,1,0,0,0
11762,"A Spectral Approach for the Design of Experiments: Design, Analysis and Algorithms","  This paper proposes a new approach to construct high quality space-filling
sample designs. First, we propose a novel technique to quantify the
space-filling property and optimally trade-off uniformity and randomness in
sample designs in arbitrary dimensions. Second, we connect the proposed metric
(defined in the spatial domain) to the objective measure of the design
performance (defined in the spectral domain). This connection serves as an
analytic framework for evaluating the qualitative properties of space-filling
designs in general. Using the theoretical insights provided by this
spatial-spectral analysis, we derive the notion of optimal space-filling
designs, which we refer to as space-filling spectral designs. Third, we propose
an efficient estimator to evaluate the space-filling properties of sample
designs in arbitrary dimensions and use it to develop an optimization framework
to generate high quality space-filling designs. Finally, we carry out a
detailed performance comparison on two different applications in 2 to 6
dimensions: a) image reconstruction and b) surrogate modeling on several
benchmark optimization functions and an inertial confinement fusion (ICF)
simulation code. We demonstrate that the propose spectral designs significantly
outperform existing approaches especially in high dimensions.
",1,0,0,1,0,0
6642,"Period polynomials, derivatives of $L$-functions, and zeros of polynomials","  Period polynomials have long been fruitful tools for the study of values of
$L$-functions in the context of major outstanding conjectures. In this paper,
we survey some facets of this study from the perspective of Eichler cohomology.
We discuss ways to incorporate non-cuspidal modular forms and values of
derivatives of $L$-functions into the same framework. We further review
investigations of the location of zeros of the period polynomial as well as of
its analogue for $L$-derivatives.
",0,0,1,0,0,0
20380,From dynamical systems with time-varying delay to circle maps and Koopmanism,"  In the present paper we investigate the influence of the retarded access by a
time-varying delay on the dynamics of delay systems. We show that there are two
universality classes of delays, which lead to fundamental differences in
dynamical quantities such as the Lyapunov spectrum. Therefore we introduce an
operator theoretic framework, where the solution operator of the delay system
is decomposed into the Koopman operator describing the delay access and an
operator similar to the solution operator known from systems with constant
delay. The Koopman operator corresponds to an iterated map, called access map,
which is defined by the iteration of the delayed argument of the delay
equation. The dynamics of this one-dimensional iterated map determines the
universality classes of the infinite-dimensional state dynamics governed by the
delay differential equation. In this way, we connect the theory of time-delay
systems with the theory of circle maps and the framework of the Koopman
operator. In the present paper we extend our previous work [Otto, Müller, and
Radons, Phys. Rev. Lett. 118, 044104 (2017)], by elaborating the mathematical
details and presenting further results also on the Lyapunov vectors.
",0,1,1,0,0,0
9771,Complex Networks Unveiling Spatial Patterns in Turbulence,"  Numerical and experimental turbulence simulations are nowadays reaching the
size of the so-called big data, thus requiring refined investigative tools for
appropriate statistical analyses and data mining. We present a new approach
based on the complex network theory, offering a powerful framework to explore
complex systems with a huge number of interacting elements. Although interest
on complex networks has been increasing in the last years, few recent studies
have been applied to turbulence. We propose an investigation starting from a
two-point correlation for the kinetic energy of a forced isotropic field
numerically solved. Among all the metrics analyzed, the degree centrality is
the most significant, suggesting the formation of spatial patterns which
coherently move with similar vorticity over the large eddy turnover time scale.
Pattern size can be quantified through a newly-introduced parameter (i.e.,
average physical distance) and varies from small to intermediate scales. The
network analysis allows a systematic identification of different spatial
regions, providing new insights into the spatial characterization of turbulent
flows. Based on present findings, the application to highly inhomogeneous flows
seems promising and deserves additional future investigation.
",0,1,0,0,0,0
1298,Threshold Constraints with Guarantees for Parity Objectives in Markov Decision Processes,"  The beyond worst-case synthesis problem was introduced recently by Bruyère
et al. [BFRR14]: it aims at building system controllers that provide strict
worst-case performance guarantees against an antagonistic environment while
ensuring higher expected performance against a stochastic model of the
environment. Our work extends the framework of [BFRR14] and follow-up papers,
which focused on quantitative objectives, by addressing the case of
$\omega$-regular conditions encoded as parity objectives, a natural way to
represent functional requirements of systems.
We build strategies that satisfy a main parity objective on all plays, while
ensuring a secondary one with sufficient probability. This setting raises new
challenges in comparison to quantitative objectives, as one cannot easily mix
different strategies without endangering the functional properties of the
system. We establish that, for all variants of this problem, deciding the
existence of a strategy lies in ${\sf NP} \cap {\sf coNP}$, the same complexity
class as classical parity games. Hence, our framework provides additional
modeling power while staying in the same complexity class.
[BFRR14] Véronique Bruyère, Emmanuel Filiot, Mickael Randour, and
Jean-François Raskin. Meet your expectations with guarantees: Beyond
worst-case synthesis in quantitative games. In Ernst W. Mayr and Natacha
Portier, editors, 31st International Symposium on Theoretical Aspects of
Computer Science, STACS 2014, March 5-8, 2014, Lyon, France, volume 25 of
LIPIcs, pages 199-213. Schloss Dagstuhl - Leibniz - Zentrum fuer Informatik,
2014.
",1,0,1,0,0,0
2178,Minimax Rényi Redundancy,"  The redundancy for universal lossless compression of discrete memoryless
sources in Campbell's setting is characterized as a minimax Rényi divergence,
which is shown to be equal to the maximal $\alpha$-mutual information via a
generalized redundancy-capacity theorem. Special attention is placed on the
analysis of the asymptotics of minimax Rényi divergence, which is determined
up to a term vanishing in blocklength.
",1,0,1,0,0,0
2481,Free fermions on a piecewise linear four-manifold. II: Pachner moves,"  This is the second in a series of papers where we construct an invariant of a
four-dimensional piecewise linear manifold $M$ with a given middle cohomology
class $h\in H^2(M,\mathbb C)$. This invariant is the square root of the torsion
of unusual chain complex introduced in Part I (arXiv:1605.06498) of our work,
multiplied by a correcting factor. Here we find this factor by studying the
behavior of our construction under all four-dimensional Pachner moves, and show
that it can be represented in a multiplicative form: a product of same-type
multipliers over all 2-faces, multiplied by a product of same-type multipliers
over all pentachora.
",0,0,1,0,0,0
9368,Braid group action and root vectors for the $q$-Onsager algebra,"  We define two algebra automorphisms $T_0$ and $T_1$ of the $q$-Onsager
algebra $B_c$, which provide an analog of G. Lusztig's braid group action for
quantum groups. These automorphisms are used to define root vectors which give
rise to a PBW basis for $B_c$. We show that the root vectors satisfy
$q$-analogs of Onsager's original commutation relations. The paper is much
inspired by I. Damiani's construction and investigation of root vectors for the
quantized enveloping algebra of $\widehat{\mathfrak{sl}}_2$.
",0,0,1,0,0,0
2009,Core structure of two-dimensional Fermi gas vortices in the BEC-BCS crossover region,"  We report $T=0$ diffusion Monte Carlo results for the ground-state and vortex
excitation of unpolarized spin-1/2 fermions in a two-dimensional disk. We
investigate how vortex core structure properties behave over the BEC-BCS
crossover. We calculate the vortex excitation energy, density profiles, and
vortex core properties related to the current. We find a density suppression at
the vortex core on the BCS side of the crossover, and a depleted core on the
BEC limit. Size-effect dependencies in the disk geometry were carefully
studied.
",0,1,0,0,0,0
20495,Merging fragments of classical logic,"  We investigate the possibility of extending the non-functionally complete
logic of a collection of Boolean connectives by the addition of further Boolean
connectives that make the resulting set of connectives functionally complete.
More precisely, we will be interested in checking whether an axiomatization for
Classical Propositional Logic may be produced by merging Hilbert-style calculi
for two disjoint incomplete fragments of it. We will prove that the answer to
that problem is a negative one, unless one of the components includes only
top-like connectives.
",1,0,1,0,0,0
12835,Mixed measurements and the detection of phase synchronization in networks,"  Multivariate singular spectrum analysis (M-SSA), with a varimax rotation of
eigenvectors, was recently proposed to provide detailed information about phase
synchronization in networks of nonlinear oscillators without any a priori need
for phase estimation. The discriminatory power of M-SSA is often enhanced by
using only the time series of the variable that provides the best observability
of the node dynamics. In practice, however, diverse factors could prevent one
to have access to this variable in some nodes and other variables should be
used, resulting in a mixed set of variables. In the present work, the impact of
this mixed measurement approach on the M-SSA is numerically investigated in
networks of Rössler systems and cord oscillators. The results are threefold.
First, a node measured by a poor variable, in terms of observability, becomes
virtually invisible to the technique. Second, a side effect of using a poor
variable is that the characterization of phase synchronization clustering of
the {\it other}\, nodes is hindered by a small amount. This suggests that,
given a network, synchronization analysis with M-SSA could be more reliable by
not measuring those nodes that are accessible only through poor variables.
Third, global phase synchronization could be detected even using only poor
variables, given enough of them are measured. These insights could be useful in
defining measurement strategies for both experimental design and real world
applications for use with M-SSA.
",0,1,0,0,0,0
16798,Scalable Magnetic Field SLAM in 3D Using Gaussian Process Maps,"  We present a method for scalable and fully 3D magnetic field simultaneous
localisation and mapping (SLAM) using local anomalies in the magnetic field as
a source of position information. These anomalies are due to the presence of
ferromagnetic material in the structure of buildings and in objects such as
furniture. We represent the magnetic field map using a Gaussian process model
and take well-known physical properties of the magnetic field into account. We
build local maps using three-dimensional hexagonal block tiling. To make our
approach computationally tractable we use reduced-rank Gaussian process
regression in combination with a Rao-Blackwellised particle filter. We show
that it is possible to obtain accurate position and orientation estimates using
measurements from a smartphone, and that our approach provides a scalable
magnetic field SLAM algorithm in terms of both computational complexity and map
storage.
",1,0,0,1,0,0
10664,On Fundamental Limits of Robust Learning,"  We consider the problems of robust PAC learning from distributed and
streaming data, which may contain malicious errors and outliers, and analyze
their fundamental complexity questions. In particular, we establish lower
bounds on the communication complexity for distributed robust learning
performed on multiple machines, and on the space complexity for robust learning
from streaming data on a single machine. These results demonstrate that gaining
robustness of learning algorithms is usually at the expense of increased
complexities. As far as we know, this work gives the first complexity results
for distributed and online robust PAC learning.
",1,0,0,1,0,0
9940,"Resonant Drag Instabilities in protoplanetary disks: the streaming instability and new, faster-growing instabilities","  We identify and study a number of new, rapidly growing instabilities of dust
grains in protoplanetary disks, which may be important for planetesimal
formation. The study is based on the recognition that dust-gas mixtures are
generically unstable to a Resonant Drag Instability (RDI), whenever the gas,
absent dust, supports undamped linear modes. We show that the ""streaming
instability"" is an RDI associated with epicyclic oscillations; this provides
simple interpretations for its mechanisms and accurate analytic expressions for
its growth rates and fastest-growing wavelengths. We extend this analysis to
more general dust streaming motions and other waves, including buoyancy and
magnetohydrodynamic oscillations, finding various new instabilities. Most
importantly, we identify the disk ""settling instability,"" which occurs as dust
settles vertically into the midplane of a rotating disk. For small grains, this
instability grows many orders of magnitude faster than the standard streaming
instability, with a growth rate that is independent of grain size. Growth
timescales for realistic dust-to-gas ratios are comparable to the disk orbital
period, and the characteristic wavelengths are more than an order of magnitude
larger than the streaming instability (allowing the instability to concentrate
larger masses). This suggests that in the process of settling, dust will band
into rings then filaments or clumps, potentially seeding dust traps,
high-metallicity regions that in turn seed the streaming instability, or even
overdensities that coagulate or directly collapse to planetesimals.
",0,1,0,0,0,0
3534,Spatio-temporal canards in neural field equations,"  Canards are special solutions to ordinary differential equations that follow
invariant repelling slow manifolds for long time intervals. In realistic
biophysical single cell models, canards are responsible for several complex
neural rhythms observed experimentally, but their existence and role in
spatially-extended systems is largely unexplored. We describe a novel type of
coherent structure in which a spatial pattern displays temporal canard
behaviour. Using interfacial dynamics and geometric singular perturbation
theory, we classify spatio-temporal canards and give conditions for the
existence of folded-saddle and folded-node canards. We find that
spatio-temporal canards are robust to changes in the synaptic connectivity and
firing rate. The theory correctly predicts the existence of spatio-temporal
canards with octahedral symmetries in a neural field model posed on the unit
sphere.
",0,1,1,0,0,0
20617,Methods to locate Saddle Points in Complex Landscapes,"  We present a class of simple algorithms that allows to find the reaction path
in systems with a complex potential energy landscape. The approach does not
need any knowledge on the product state and does not require the calculation of
any second derivatives. The underlying idea is to use two nearby points in
configuration space to locate the path of slowest ascent. By introducing a weak
noise term, the algorithm is able to find even low-lying saddle points that are
not reachable by means of a slowest ascent path. Since the algorithm makes only
use of the value of the potential and its gradient, the computational effort to
find saddles is linear in the number of degrees of freedom, if the potential is
short-ranged. We test the performance of the algorithm for two potential energy
landscapes. For the Müller-Brown surface we find that the algorithm always
finds the correct saddle point. For the modified Müller-Brown surface, which
has a saddle point that is not reachable by means of a slowest ascent path, the
algorithm is still able to find this saddle point with high probability.
",0,1,0,0,0,0
844,Stochastic and Chance-Constrained Conic Distribution System Expansion Planning Using Bilinear Benders Decomposition,"  Second order conic programming (SOCP) has been used to model various
applications in power systems, such as operation and expansion planning. In
this paper, we present a two-stage stochastic mixed integer SOCP (MISOCP) model
for the distribution system expansion planning problem that considers
uncertainty and also captures the nonlinear AC power flow. To avoid costly
investment plans due to some extreme scenarios, we further present a
chance-constrained variant that could lead to cost-effective solutions. To
address the computational challenge, we extend the basic Benders decomposition
method and develop a bilinear variant to compute stochastic and
chance-constrained MISOCP formulations. A set of numerical experiments is
performed to illustrate the performance of our models and computational
methods. In particular, results show that our Benders decomposition algorithms
drastically outperform a professional MISOCP solver in handling stochastic
scenarios by orders of magnitude.
",0,0,1,0,0,0
16852,Online Scheduling of Spark Workloads with Mesos using Different Fair Allocation Algorithms,"  In the following, we present example illustrative and experimental results
comparing fair schedulers allocating resources from multiple servers to
distributed application frameworks. Resources are allocated so that at least
one resource is exhausted in every server. Schedulers considered include DRF
(DRFH) and Best-Fit DRF (BF-DRF), TSF, and PS-DSF. We also consider server
selection under Randomized Round Robin (RRR) and based on their residual
(unreserved) resources. In the following, we consider cases with frameworks of
equal priority and without server-preference constraints. We first give typical
results of a illustrative numerical study and then give typical results of a
study involving Spark workloads on Mesos which we have modified and
open-sourced to prototype different schedulers.
",1,0,0,0,0,0
18368,A one-dimensional mathematical model of collecting lymphatics coupled with an electro-fluid-mechanical contraction model and valve dynamics,"  We propose a one-dimensional model for collecting lymphatics coupled with a
novel Electro-Fluid-Mechanical Contraction (EFMC) model for dynamical
contractions, based on a modified FitzHugh-Nagumo model for action potentials.
The one-dimensional model for a compliant lymphatic vessel is a set of
hyperbolic Partial Differential Equations (PDEs). The EFMC model combines the
electrical activity of lymphangions (action potentials) with fluid-mechanical
feedback (stretch of the lymphatic wall and wall shear stress) and the
mechanical variation of the lymphatic wall properties (contractions). The EFMC
model is governed by four Ordinary Differential Equations (ODEs) and
phenomenologically relies on: (1) environmental calcium influx, (2)
stretch-activated calcium influx, and (3) contraction inhibitions induced by
wall shear stresses. We carried out a complete mathematical analysis of the
stability of the stationary state of the EFMC model. Overall, the EFMC model
allows imitating the influence of pressure and wall shear stress on the
frequency of contractions observed experimentally. Lymphatic valves are
modelled using a well-established lumped-parameter model which allows
simulating stenotic and regurgitant valves. We analysed several lymphodynamical
indexes of a single lymphangion for a wide range of upstream and downstream
pressure combinations. Stenotic and regurgitant valves were modelled, and their
effects are here quantified. Results for stenotic valves showed in the
downstream lymphangion that for low frequencies of contractions the Calculated
Pump Flow (CPF) index remained almost unaltered, while for high frequencies the
CPF dramatically decreased depending on the severity of the stenosis (up to 93%
for a severe stenosis). Results for incompetent valves showed that the net flow
during a lymphatic cycle tends to zero as the degree of incompetence increases.
",0,1,1,0,0,0
35,Deep Neural Network Optimized to Resistive Memory with Nonlinear Current-Voltage Characteristics,"  Artificial Neural Network computation relies on intensive vector-matrix
multiplications. Recently, the emerging nonvolatile memory (NVM) crossbar array
showed a feasibility of implementing such operations with high energy
efficiency, thus there are many works on efficiently utilizing emerging NVM
crossbar array as analog vector-matrix multiplier. However, its nonlinear I-V
characteristics restrain critical design parameters, such as the read voltage
and weight range, resulting in substantial accuracy loss. In this paper,
instead of optimizing hardware parameters to a given neural network, we propose
a methodology of reconstructing a neural network itself optimized to resistive
memory crossbar arrays. To verify the validity of the proposed method, we
simulated various neural network with MNIST and CIFAR-10 dataset using two
different specific Resistive Random Access Memory (RRAM) model. Simulation
results show that our proposed neural network produces significantly higher
inference accuracies than conventional neural network when the synapse devices
have nonlinear I-V characteristics.
",1,0,0,0,0,0
12031,Constraints on the Intergalactic Magnetic Field from Bow Ties in the Gamma-ray Sky,"  Pair creation on the cosmic infrared background and subsequent
inverse-Compton scattering on the CMB potentially reprocesses the TeV emission
of blazars into faint GeV halos with structures sensitive to intergalactic
magnetic fields (IGMF). We attempt to detect such halos exploiting their highly
anisotropic shape. Their persistent nondetection excludes at greater than
$3.9\sigma$ an IGMF with correlation lengths >100 Mpc and current-day strengths
in the range $10^{-16}$ to $10^{-15}$ G, and at 2 sigma from $10^{-17}$ to
$10^{-14}$ G, covering the range implied by gamma-ray spectra of nearby TeV
emitters. Alternatively, plasma processes could pre-empt the inverse-Compton
cascade.
",0,1,0,0,0,0
5287,Threshold Selection for Multivariate Heavy-Tailed Data,"  Regular variation is often used as the starting point for modeling
multivariate heavy-tailed data. A random vector is regularly varying if and
only if its radial part $R$ is regularly varying and is asymptotically
independent of the angular part $\Theta$ as $R$ goes to infinity. The
conditional limiting distribution of $\Theta$ given $R$ is large characterizes
the tail dependence of the random vector and hence its estimation is the
primary goal of applications. A typical strategy is to look at the angular
components of the data for which the radial parts exceed some threshold. While
a large class of methods has been proposed to model the angular distribution
from these exceedances, the choice of threshold has been scarcely discussed in
the literature. In this paper, we describe a procedure for choosing the
threshold by formally testing the independence of $R$ and $\Theta$ using a
measure of dependence called distance covariance. We generalize the limit
theorem for distance covariance to our unique setting and propose an algorithm
which selects the threshold for $R$. This algorithm incorporates a subsampling
scheme that is also applicable to weakly dependent data. Moreover, it avoids
the heavy computation in the calculation of the distance covariance, a typical
limitation for this measure. The performance of our method is illustrated on
both simulated and real data.
",0,0,1,1,0,0
12481,Accelerated Stochastic Quasi-Newton Optimization on Riemann Manifolds,"  We propose an L-BFGS optimization algorithm on Riemannian manifolds using
minibatched stochastic variance reduction techniques for fast convergence with
constant step sizes, without resorting to linesearch methods designed to
satisfy Wolfe conditions. We provide a new convergence proof for strongly
convex functions without using curvature conditions on the manifold, as well as
a convergence discussion for nonconvex functions. We discuss a couple of ways
to obtain the correction pairs used to calculate the product of the gradient
with the inverse Hessian, and empirically demonstrate their use in synthetic
experiments on computation of Karcher means for symmetric positive definite
matrices and leading eigenvalues of large scale data matrices. We compare our
method to VR-PCA for the latter experiment, along with Riemannian SVRG for both
cases, and show strong convergence results for a range of datasets.
",0,0,1,1,0,0
6165,Binary Ensemble Neural Network: More Bits per Network or More Networks per Bit?,"  Binary neural networks (BNN) have been studied extensively since they run
dramatically faster at lower memory and power consumption than floating-point
networks, thanks to the efficiency of bit operations. However, contemporary
BNNs whose weights and activations are both single bits suffer from severe
accuracy degradation. To understand why, we investigate the representation
ability, speed and bias/variance of BNNs through extensive experiments. We
conclude that the error of BNNs is predominantly caused by the intrinsic
instability (training time) and non-robustness (train & test time). Inspired by
this investigation, we propose the Binary Ensemble Neural Network (BENN) which
leverages ensemble methods to improve the performance of BNNs with limited
efficiency cost. While ensemble techniques have been broadly believed to be
only marginally helpful for strong classifiers such as deep neural networks,
our analyses and experiments show that they are naturally a perfect fit to
boost BNNs. We find that our BENN, which is faster and much more robust than
state-of-the-art binary networks, can even surpass the accuracy of the
full-precision floating number network with the same architecture.
",0,0,0,1,0,0
16571,On the Log Partition Function of Ising Model on Stochastic Block Model,"  A sparse stochastic block model (SBM) with two communities is defined by the
community probability $\pi_0,\pi_1$, and the connection probability between
communities $a,b\in\{0,1\}$, namely $q_{ab} = \frac{\alpha_{ab}}{n}$. When
$q_{ab}$ is constant in $a,b$, the random graph is simply the
Erdős-Rény random graph. We evaluate the log partition function of the
Ising model on sparse SBM with two communities.
As an application, we give consistent parameter estimation of the sparse SBM
with two communities in a special case. More specifically, let $d_0,d_1$ be the
average degree of the two communities, i.e.,
$d_0\overset{def}{=}\pi_0\alpha_{00}+\pi_1\alpha_{01},d_1\overset{def}{=}\pi_0\alpha_{10}+\pi_1\alpha_{11}$.
We focus on the regime $d_0=d_1$ (the regime $d_0\ne d_1$ is trivial). In this
regime, there exists $d,\lambda$ and $r\geq 0$ with $\pi_0=\frac{1}{1+r},
\pi_1=\frac{r}{1+r}$, $\alpha_{00}=d(1+r\lambda), \alpha_{01}=\alpha_{10} =
d(1-\lambda), \alpha_{11} = d(1+\frac{\lambda}{r})$. We give a consistent
estimator of $r$ when $\lambda<0$. The estimator of $\lambda$ given by
\citep{mossel2015reconstruction} is valid in the general situation. We also
provide a random clustering algorithm which does not require knowledge of
parameters and which is positively correlated with the true community label
when $\lambda<0$.
",0,0,0,1,0,0
7851,Self-Supervised Generalisation with Meta Auxiliary Learning,"  Learning with auxiliary tasks has been shown to improve the generalisation of
a primary task. However, this comes at the cost of manually-labelling
additional tasks which may, or may not, be useful for the primary task. We
propose a new method which automatically learns labels for an auxiliary task,
such that any supervised learning task can be improved without requiring access
to additional data. The approach is to train two neural networks: a
label-generation network to predict the auxiliary labels, and a multi-task
network to train the primary task alongside the auxiliary task. The loss for
the label-generation network incorporates the multi-task network's performance,
and so this interaction between the two networks can be seen as a form of meta
learning. We show that our proposed method, Meta AuXiliary Learning (MAXL),
outperforms single-task learning on 7 image datasets by a significant margin,
without requiring additional auxiliary labels. We also show that MAXL
outperforms several other baselines for generating auxiliary labels, and is
even competitive when compared with human-defined auxiliary labels. The
self-supervised nature of our method leads to a promising new direction towards
automated generalisation. The source code is available at
\url{this https URL}.
",1,0,0,1,0,0
7199,A Utility-Driven Multi-Queue Admission Control Solution for Network Slicing,"  The combination of recent emerging technologies such as network function
virtualization (NFV) and network programmability (SDN) gave birth to the
Network Slicing revolution. 5G networks consist of multi-tenant infrastructures
capable of offering leased network ""slices"" to new customers (e.g., vertical
industries) enabling a new telecom business model: Slice-as-aService (SlaaS).
In this paper, we aim i ) to study the slicing admission control problem by
means of a multi-queuing system for heterogeneous tenant requests, ii ) to
derive its statistical behavior model, and iii ) to provide a utility-based
admission control optimization. Our results analyze the capability of the
proposed SlaaS system to be approximately Markovian and evaluate its
performance as compared to legacy solutions.
",1,0,0,0,0,0
13593,Precision of Evaluation Methods in White Light Interferometry: Correlogram Correlation Method,"  In this paper we promote a method for the evaluation of a surface topography
which we call the correlogram correlation method. Employing a theoretical
analysis as well as numerical simulations the method is proven to be the most
accurate among available evaluation algorithms in the common case of
uncorrelated noise. Examples illustrate the superiority of the correlogram
correlation method over the common envelope and phase methods.
",0,1,0,0,0,0
9578,The maximum number of cycles in a graph with fixed number of edges,"  The main topic considered is maximizing the number of cycles in a graph with
given number of edges. In 2009, Király conjectured that there is constant $c$
such that any graph with $m$ edges has at most $(1.4)^m$ cycles. In this paper,
it is shown that for sufficiently large $m$, a graph with $m$ edges has at most
$(1.443)^m$ cycles. For sufficiently large $m$, examples of a graph with $m$
edges and $(1.37)^m$ cycles are presented. For a graph with given number of
vertices and edges an upper bound on the maximal number of cycles is given.
Also, exponentially tight bounds are proved for the maximum number of cycles in
a multigraph with given number of edges, as well as in a multigraph with given
number of vertices and edges.
",0,0,1,0,0,0
18949,Predicting Foreground Object Ambiguity and Efficiently Crowdsourcing the Segmentation(s),"  We propose the ambiguity problem for the foreground object segmentation task
and motivate the importance of estimating and accounting for this ambiguity
when designing vision systems. Specifically, we distinguish between images
which lead multiple annotators to segment different foreground objects
(ambiguous) versus minor inter-annotator differences of the same object. Taking
images from eight widely used datasets, we crowdsource labeling the images as
""ambiguous"" or ""not ambiguous"" to segment in order to construct a new dataset
we call STATIC. Using STATIC, we develop a system that automatically predicts
which images are ambiguous. Experiments demonstrate the advantage of our
prediction system over existing saliency-based methods on images from vision
benchmarks and images taken by blind people who are trying to recognize objects
in their environment. Finally, we introduce a crowdsourcing system to achieve
cost savings for collecting the diversity of all valid ""ground truth""
foreground object segmentations by collecting extra segmentations only when
ambiguity is expected. Experiments show our system eliminates up to 47% of
human effort compared to existing crowdsourcing methods with no loss in
capturing the diversity of ground truths.
",1,0,0,0,0,0
20835,"Evaluation complexity bounds for smooth constrained nonlinear optimisation using scaled KKT conditions, high-order models and the criticality measure $χ$","  Evaluation complexity for convexly constrained optimization is considered and
it is shown first that the complexity bound of $O(\epsilon^{-3/2})$ proved by
Cartis, Gould and Toint (IMAJNA 32(4) 2012, pp.1662-1695) for computing an
$\epsilon$-approximate first-order critical point can be obtained under
significantly weaker assumptions. Moreover, the result is generalized to the
case where high-order derivatives are used, resulting in a bound of
$O(\epsilon^{-(p+1)/p})$ evaluations whenever derivatives of order $p$ are
available. It is also shown that the bound of
$O(\epsilon_P^{-1/2}\epsilon_D^{-3/2})$ evaluations ($\epsilon_P$ and
$\epsilon_D$ being primal and dual accuracy thresholds) suggested by Cartis,
Gould and Toint (SINUM, 2015) for the general nonconvex case involving both
equality and inequality constraints can be generalized to a bound of
$O(\epsilon_P^{-1/p}\epsilon_D^{-(p+1)/p})$ evaluations under similarly
weakened assumptions. This paper is variant of a companion report (NTR-11-2015,
University of Namur, Belgium) which uses a different first-order criticality
measure to obtain the same complexity bounds.
",1,0,1,0,0,0
19855,Entendendo o Pensamento Computacional,"  The goal of this article is to clarify the meaning of Computational Thinking.
We differentiate logical from computational reasoning and discuss the
importance of Computational Thinking in solving problems. The three pillars of
Computational Thinking - Abstraction, Automation and Analysis - are outlined,
highlighting the role of each one in developing the skills needed for the
problem-solving process.
-----
O objetivo deste artigo é esclarecer o significado de Pensamento
Computacional. Diferencia-se o raciocínio lógico do computacional e
discute-se a importância do Pensamento Computacional na resolução de
problemas. Os três pilares do Pensamento Computacional - Abstração,
Automação e Análise - são delineados, destacando-se o papel de cada
um deles no desenvolvimento das habilidades necessárias para o processo de
solução de problemas.
",1,0,0,0,0,0
19378,Gapped paramagnetic state in a frustrated spin-$\frac{1}{2}$ Heisenberg antiferromagnet on the cross-striped square lattice,"  We implement the coupled cluster method to very high orders of approximation
to study the spin-$\frac{1}{2}$ $J_{1}$--$J_{2}$ Heisenberg model on a
cross-striped square lattice. Every nearest-neighbour pair of sites on the
square lattice has an isotropic antiferromagnetic exchange bond of strength
$J_{1}>0$, while the basic square plaquettes in alternate columns have either
both or neither next-nearest-neighbour (diagonal) pairs of sites connected by
an equivalent frustrating bond of strength $J_{2} \equiv \alpha J_{1} > 0$. By
studying the magnetic order parameter (i.e., the average local on-site
magnetization) in the range $0 \leq \alpha \leq 1$ of the frustration parameter
we find that the quasiclassical antiferromagnetic Néel and (so-called)
double Néel states form the stable ground-state phases in the respective
regions $\alpha < \alpha_{1a}^{c} = 0.46(1)$ and $\alpha > \alpha_{1b}^{c} =
0.615(5)$. The double Néel state has Néel
($\cdots\uparrow\downarrow\uparrow\downarrow\cdots$) ordering along the
(column) direction parallel to the stripes of squares with both or no $J_{2}$
bonds, and spins alternating in a pairwise
($\cdots\uparrow\uparrow\downarrow\downarrow\uparrow\uparrow\downarrow\downarrow\cdots$)
fashion along the perpendicular (row) direction, so that the parallel pairs
occur on squares with both $J_{2}$ bonds present. Further explicit calculations
of both the triplet spin gap and the zero-field uniform transverse magnetic
susceptibility provide compelling evidence that the ground-state phase over all
or most of the intermediate regime $\alpha_{1a}^{c} < \alpha < \alpha_{1b}^{c}$
is a gapped state with no discernible long-range magnetic order.
",0,1,0,0,0,0
2827,Robust Task Clustering for Deep Many-Task Learning,"  We investigate task clustering for deep-learning based multi-task and
few-shot learning in a many-task setting. We propose a new method to measure
task similarities with cross-task transfer performance matrix for the deep
learning scenario. Although this matrix provides us critical information
regarding similarity between tasks, its asymmetric property and unreliable
performance scores can affect conventional clustering methods adversely.
Additionally, the uncertain task-pairs, i.e., the ones with extremely
asymmetric transfer scores, may collectively mislead clustering algorithms to
output an inaccurate task-partition. To overcome these limitations, we propose
a novel task-clustering algorithm by using the matrix completion technique. The
proposed algorithm constructs a partially-observed similarity matrix based on
the certainty of cluster membership of the task-pairs. We then use a matrix
completion algorithm to complete the similarity matrix. Our theoretical
analysis shows that under mild constraints, the proposed algorithm will
perfectly recover the underlying ""true"" similarity matrix with a high
probability. Our results show that the new task clustering method can discover
task clusters for training flexible and superior neural network models in a
multi-task learning setup for sentiment classification and dialog intent
classification tasks. Our task clustering approach also extends metric-based
few-shot learning methods to adapt multiple metrics, which demonstrates
empirical advantages when the tasks are diverse.
",1,0,0,1,0,0
7785,Annihilating wild kernels,"  Let $L/K$ be a finite Galois extension of number fields with Galois group
$G$. Let $p$ be an odd prime and $r>1$ be an integer. Assuming a conjecture of
Schneider, we formulate a conjecture that relates special values of equivariant
Artin $L$-series at $s=r$ to the compact support cohomology of the étale
$p$-adic sheaf $\mathbb Z_p(r)$. We show that our conjecture is essentially
equivalent to the $p$-part of the equivariant Tamagawa number conjecture for
the pair $(h^0(\mathrm{Spec}(L))(r), \mathbb Z[G])$. We derive from this
explicit constraints on the Galois module structure of Banaszak's $p$-adic wild
kernels.
",0,0,1,0,0,0
12106,Designing diagnostic platforms for analysis of disease patterns and probing disease emergence,"  The emerging era of personalized medicine relies on medical decisions,
practices, and products being tailored to the individual patient. Point-of-care
systems, at the heart of this model, play two important roles. First, they are
required for identifying subjects for optimal therapies based on their genetic
make-up and epigenetic profile. Second, they will be used for assessing the
progression of such therapies. Central to this vision is designing systems
that, with minimal user-intervention, can transduce complex signals from
biosystems in complement with clinical information to inform medical decision
within point-of-care settings. To reach our ultimate goal of developing
point-of-care systems and realizing personalized medicine, we are taking a
multistep systems-level approach towards understanding cellular processes and
biomolecular profiles, to quantify disease states and external interventions.
",0,0,0,0,1,0
11213,Polymorphism and the obstinate circularity of second order logic: a victims' tale,"  The investigations on higher-order type theories and on the related notion of
parametric polymorphism constitute the technical counterpart of the old
foundational problem of the circularity (or impredicativity) of second and
higher order logic. However, the epistemological significance of such
investigations, and of their often non trivial results, has not received much
attention in the contemporary foundational debate. The results recalled in this
paper suggest that the question of the circularity of second order logic cannot
be reduced to the simple assessment of a vicious circle. Through a comparison
between the faulty consistency arguments given by Frege and Martin-Löf,
respectively for the logical system of the Grundgesetze (shown inconsistent by
Russell's paradox) and for the intuitionistic type theory with a type of all
types (shown inconsistent by Girard's paradox), and the normalization argument
for second order type theory (or System F), we indicate a bunch of subtle
mathematical problems and logical concepts hidden behind the hazardous idea of
impredicative quantification, constituting a vast (and largely unexplored)
domain for foundational research.
",1,0,1,0,0,0
20471,Individual position diversity in dependence socioeconomic networks increases economic output,"  The availability of big data recorded from massively multiplayer online
role-playing games (MMORPGs) allows us to gain a deeper understanding of the
potential connection between individuals' network positions and their economic
outputs. We use a statistical filtering method to construct dependence networks
from weighted friendship networks of individuals. We investigate the 30
distinct motif positions in the 13 directed triadic motifs which represent
microscopic dependences among individuals. Based on the structural similarity
of motif positions, we further classify individuals into different groups. The
node position diversity of individuals is found to be positively correlated
with their economic outputs. We also find that the economic outputs of leaf
nodes are significantly lower than that of the other nodes in the same motif.
Our findings shed light on understanding the influence of network structure on
economic activities and outputs in socioeconomic system.
",1,1,0,0,0,0
2335,Classical Spacetime Structure,"  I discuss several issues related to ""classical"" spacetime structure. I review
Galilean, Newtonian, and Leibnizian spacetimes, and briefly describe more
recent developments. The target audience is undergraduates and early graduate
students in philosophy; the presentation avoids mathematical formalism as much
as possible.
",0,1,0,0,0,0
15378,Assessing Excited State Energy Gaps with Time-Dependent Density Functional Theory on Ru(II) Complexes,"  A set of density functionals coming from different rungs on Jacob's ladder
are employed to evaluate the electronic excited states of three Ru(II)
complexes. While most studies on the performance of density functionals compare
the vertical excitation energies, in this work we focus on the energy gaps
between the electronic excited states, of the same and different multiplicity.
Excited state energy gaps are important for example to determine radiationless
transition probabilities. Besides energies, a functional should deliver the
correct state character and state ordering. Therefore, wavefunction overlaps
are introduced to systematically evaluate the effect of different functionals
on the character of the excited states. As a reference, the energies and state
characters from multi-state second-order perturbation theory complete active
space (MS-CASPT2) are used. In comparison to MS-CASPT2, it is found that while
hybrid functionals provide better vertical excitation energies, pure
functionals typically give more accurate excited state energy gaps. Pure
functionals are also found to reproduce the state character and ordering in
closer agreement to MS-CASPT2 than the hybrid functionals.
",0,1,0,0,0,0
2642,Observable dictionary learning for high-dimensional statistical inference,"  This paper introduces a method for efficiently inferring a high-dimensional
distributed quantity from a few observations. The quantity of interest (QoI) is
approximated in a basis (dictionary) learned from a training set. The
coefficients associated with the approximation of the QoI in the basis are
determined by minimizing the misfit with the observations. To obtain a
probabilistic estimate of the quantity of interest, a Bayesian approach is
employed. The QoI is treated as a random field endowed with a hierarchical
prior distribution so that closed-form expressions can be obtained for the
posterior distribution. The main contribution of the present work lies in the
derivation of \emph{a representation basis consistent with the observation
chain} used to infer the associated coefficients. The resulting dictionary is
then tailored to be both observable by the sensors and accurate in
approximating the posterior mean. An algorithm for deriving such an observable
dictionary is presented. The method is illustrated with the estimation of the
velocity field of an open cavity flow from a handful of wall-mounted point
sensors. Comparison with standard estimation approaches relying on Principal
Component Analysis and K-SVD dictionaries is provided and illustrates the
superior performance of the present approach.
",0,0,0,1,0,0
8874,Relativistic Astronomy,"  The ""Breakthrough Starshot"" aims at sending near-speed-of-light cameras to
nearby stellar systems in the future. Due to the relativistic effects, a
trans-relativistic camera naturally serves as a spectrograph, a lens, and a
wide-field camera. We demonstrate this through a simulation of the optical-band
image of the nearby galaxy M51 in the rest frame of the trans-relativistic
camera. We suggest that observing celestial objects using a trans-relativistic
camera may allow one to study the astronomical objects in a special way, and to
perform unique tests on the principles of special relativity. We outline
several examples that trans-relativistic cameras may make important
contributions to astrophysics and suggest that the Breakthrough Starshot
cameras may be launched in any direction to serve as a unique astronomical
observatory.
",0,1,0,0,0,0
12969,Staging superstructures in high-$T_c$ Sr/O co-doped La$_{2-x}$Sr$_x$CuO$_{4+y}$,"  We present high energy X-ray diffraction studies on the structural phases of
an optimal high-$T_c$ superconductor La$_{2-x}$Sr$_x$CuO$_{4+y}$ tailored by
co-hole-doping. This is specifically done by varying the content of two very
different chemical species, Sr and O, respectively, in order to study the
influence of each. A superstructure known as staging is observed in all
samples, with the staging number $n$ increasing for higher Sr dopings $x$. We
find that the staging phases emerge abruptly with temperature, and can be
described as a second order phase transition with transition temperatures
slightly depending on the Sr doping. The Sr appears to correlate the
interstitial oxygen in a way that stabilises the reproducibility of the staging
phase both in terms of staging period and volume fraction in a specific sample.
The structural details as investigated in this letter appear to have no direct
bearing on the electronic phase separation previously observed in the same
samples. This provides new evidence that the electronic phase separation is
determined by the overall hole concentration rather than specific Sr/O content
and concommittant structural details.
",0,1,0,0,0,0
18282,Scene Graph Generation by Iterative Message Passing,"  Understanding a visual scene goes beyond recognizing individual objects in
isolation. Relationships between objects also constitute rich semantic
information about the scene. In this work, we explicitly model the objects and
their relationships using scene graphs, a visually-grounded graphical structure
of an image. We propose a novel end-to-end model that generates such structured
scene representation from an input image. The model solves the scene graph
inference problem using standard RNNs and learns to iteratively improves its
predictions via message passing. Our joint inference model can take advantage
of contextual cues to make better predictions on objects and their
relationships. The experiments show that our model significantly outperforms
previous methods for generating scene graphs using Visual Genome dataset and
inferring support relations with NYU Depth v2 dataset.
",1,0,0,0,0,0
1731,The nature of the progenitor of the M31 North-western stream: globular clusters as milestones of its orbit,"  We examine the nature, possible orbits and physical properties of the
progenitor of the North-western stellar stream (NWS) in the halo of the
Andromeda galaxy (M31). The progenitor is assumed to be an accreting dwarf
galaxy with globular clusters (GCs). It is, in general, difficult to determine
the progenitor's orbit precisely because of many necessary parameters.
Recently, Veljanoski et al. 2014 reported five GCs whose positions and radial
velocities suggest an association with the stream. We use this data to
constrain the orbital motions of the progenitor using test-particle
simulations. Our simulations split the orbit solutions into two branches
according to whether the stream ends up in the foreground or in the background
of M31. Upcoming observations that will determine the distance to the NWS will
be able to reject one of the two branches. In either case, the solutions
require that the pericentric radius of any possible orbit be over 2 kpc. We
estimate the efficiency of the tidal disruption and confirm the consistency
with the assumption for the progenitor being a dwarf galaxy. The progenitor
requires the mass $\ga 2\times10^6 M_{\sun}$ and half-light radius $\ga 30$ pc.
In addition, $N$-body simulations successfully reproduce the basic observed
features of the NWS and the GCs' line-of-sight velocities.
",0,1,0,0,0,0
20826,Coupled Self-Organized Hydrodynamics and Stokes models for suspensions of active particles,"  We derive macroscopic dynamics for self-propelled particles in a fluid. The
starting point is a coupled Vicsek-Stokes system. The Vicsek model describes
self-propelled agents interacting through alignment. It provides a
phenomenological description of hydrodynamic interactions between agents at
high density. Stokes equations describe a low Reynolds number fluid. These two
dynamics are coupled by the interaction between the agents and the fluid. The
fluid contributes to rotating the particles through Jeffery's equation.
Particle self-propulsion induces a force dipole on the fluid. After
coarse-graining we obtain a coupled Self-Organised Hydrodynamics (SOH)-Stokes
system. We perform a linear stability analysis for this system which shows that
both pullers and pushers have unstable modes. We conclude by providing
extensions of the Vicsek-Stokes model including short-distance repulsion,
finite particle inertia and finite Reynolds number fluid regime.
",0,1,1,0,0,0
6418,Computing Tropical Prevarieties in Parallel,"  The computation of the tropical prevariety is the first step in the
application of polyhedral methods to compute positive dimensional solution sets
of polynomial systems. In particular, pretropisms are candidate leading
exponents for the power series developments of the solutions. The computation
of the power series may start as soon as one pretropism is available, so our
parallel computation of the tropical prevariety has an application in a
pipelined solver.
We present a parallel implementation of dynamic enumeration. Our first
distributed memory implementation with forked processes achieved good speedups,
but quite often resulted in large variations in the execution times of the
processes. The shared memory multithreaded version applies work stealing to
reduce the variability of the run time. Our implementation applies the thread
safe Parma Polyhedral Library (PPL), in exact arithmetic with the GNU
Multiprecision Arithmetic Library (GMP), aided by the fast memory allocations
of TCMalloc.
Our parallel implementation is capable of computing the tropical prevariety
of the cyclic 16-roots problem. We also report on computational experiments on
the $n$-body and $n$-vortex problems; our computational results compare
favorably with Gfan.
",1,0,1,0,0,0
7248,The relationships between PM2.5 and meteorological factors in China: Seasonal and regional variations,"  The interactions between PM2.5 and meteorological factors play a crucial role
in air pollution analysis. However, previous studies that have researched the
relationships between PM2.5 concentration and meteorological conditions have
been mainly confined to a certain city or district, and the correlation over
the whole of China remains unclear. Whether or not spatial and seasonal
variations exit deserves further research. In this study, the relationships
between PM2.5 concentration and meteorological factors were investigated in 74
major cities in China for a continuous period of 22 months from February 2013
to November 2014, at season, year, city, and regional scales, and the spatial
and seasonal variations were analyzed. The meteorological factors were relative
humidity (RH), temperature (TEM), wind speed (WS), and surface pressure (PS).
We found that spatial and seasonal variations of their relationships with PM2.5
do exist. Spatially, RH is positively correlated with PM2.5 concentration in
North China and Urumqi, but the relationship turns to negative in other areas
of China. WS is negatively correlated with PM2.5 everywhere expect for Hainan
Island. PS has a strong positive relationship with PM2.5 concentration in
Northeast China and Mid-south China, and in other areas the correlation is
weak. Seasonally, the positive correlation between PM2.5 concentration and RH
is stronger in winter and spring. TEM has a negative relationship with PM2.5 in
autumn and the opposite in winter. PS is more positively correlated with PM2.5
in autumn than in other seasons. Our study investigated the relationships
between PM2.5 and meteorological factors in terms of spatial and seasonal
variations, and the conclusions about the relationships between PM2.5 and
meteorological factors are more comprehensive and precise than before.
",0,1,0,0,0,0
1816,Row-Centric Lossless Compression of Markov Images,"  Motivated by the question of whether the recently introduced Reduced Cutset
Coding (RCC) offers rate-complexity performance benefits over conventional
context-based conditional coding for sources with two-dimensional Markov
structure, this paper compares several row-centric coding strategies that vary
in the amount of conditioning as well as whether a model or an empirical table
is used in the encoding of blocks of rows. The conclusion is that, at least for
sources exhibiting low-order correlations, 1-sided model-based conditional
coding is superior to the method of RCC for a given constraint on complexity,
and conventional context-based conditional coding is nearly as good as the
1-sided model-based coding.
",1,0,0,0,0,0
607,Boosting the Actor with Dual Critic,"  This paper proposes a new actor-critic-style algorithm called Dual
Actor-Critic or Dual-AC. It is derived in a principled way from the Lagrangian
dual form of the Bellman optimality equation, which can be viewed as a
two-player game between the actor and a critic-like function, which is named as
dual critic. Compared to its actor-critic relatives, Dual-AC has the desired
property that the actor and dual critic are updated cooperatively to optimize
the same objective function, providing a more transparent way for learning the
critic that is directly related to the objective function of the actor. We then
provide a concrete algorithm that can effectively solve the minimax
optimization problem, using techniques of multi-step bootstrapping, path
regularization, and stochastic dual ascent algorithm. We demonstrate that the
proposed algorithm achieves the state-of-the-art performances across several
benchmarks.
",1,0,0,0,0,0
1504,New Methods of Enhancing Prediction Accuracy in Linear Models with Missing Data,"  In this paper, prediction for linear systems with missing information is
investigated. New methods are introduced to improve the Mean Squared Error
(MSE) on the test set in comparison to state-of-the-art methods, through
appropriate tuning of Bias-Variance trade-off. First, the use of proposed Soft
Weighted Prediction (SWP) algorithm and its efficacy are depicted and compared
to previous works for non-missing scenarios. The algorithm is then modified and
optimized for missing scenarios. It is shown that controlled over-fitting by
suggested algorithms will improve prediction accuracy in various cases.
Simulation results approve our heuristics in enhancing the prediction accuracy.
",1,0,0,1,0,0
5372,The connection between zero chromaticity and long in-plane polarization lifetime in a magnetic storage ring,"  In this paper, we demonstrate the connection between a magnetic storage ring
with additional sextupole fields set so that the x and y chromaticities vanish
and the maximizing of the lifetime of in-plane polarization (IPP) for a
0.97-GeV/c deuteron beam. The IPP magnitude was measured by continuously
monitoring the down-up scattering asymmetry (sensitive to sideways
polarization) in an in-beam, carbon-target polarimeter and unfolding the
precession of the IPP due to the magnetic anomaly of the deuteron. The optimum
operating conditions for a long IPP lifetime were made by scanning the field of
the storage ring sextupole magnet families while observing the rate of IPP loss
during storage of the beam. The beam was bunched and electron cooled. The IPP
losses appear to arise from the change of the orbit circumference, and
consequently the particle speed and spin tune, due to the transverse betatron
oscillations of individual particles in the beam. The effects of these changes
are canceled by an appropriate sextupole field setting.
",0,1,0,0,0,0
6068,Observational signatures of linear warps in circumbinary discs,"  In recent years an increasing number of observational studies have hinted at
the presence of warps in protoplanetary discs, however a general comprehensive
description of observational diagnostics of warped discs was missing. We
performed a series of 3D SPH hydrodynamic simulations and combined them with 3D
radiative transfer calculations to study the observability of warps in
circumbinary discs, whose plane is misaligned with respect to the orbital plane
of the central binary. Our numerical hydrodynamic simulations confirm previous
analytical results on the dependence of the warp structure on the viscosity and
the initial misalignment between the binary and the disc. To study the
observational signatures of warps we calculate images in the continuum at
near-infrared and sub-millimetre wavelengths and in the pure rotational
transition of CO in the sub-millimetre. Warped circumbinary discs show surface
brightness asymmetry in near-infrared scattered light images as well as in
optically thick gas lines at sub-millimetre wavelengths. The asymmetry is
caused by self-shadowing of the disc by the inner warped regions, thus the
strength of the asymmetry depends on the strength of the warp. The projected
velocity field, derived from line observations, shows characteristic
deviations, twists and a change in the slope of the rotation curve, from that
of an unperturbed disc. In extreme cases even the direction of rotation appears
to change in the disc inwards of a characteristic radius. The strength of the
kinematical signatures of warps decreases with increasing inclination. The
strength of all warp signatures decreases with decreasing viscosity.
",0,1,0,0,0,0
18835,Competition evolution of Rayleigh-Taylor bubbles,"  Material mixing induced by a Rayleigh-Taylor instability occurs ubiquitously
in either nature or engineering when a light fluid pushes against a heavy
fluid, accompanying with the formation and evolution of chaotic bubbles. Its
general evolution involves two mechanisms: bubble-merge and bubble-competition.
The former obeys a universa1 evolution law and has been well-studied, while the
latter depends on many factors and has not been well-recognized. In this paper,
we establish a theory for the latter to clarify and quantify the longstanding
open question: the dependence of bubbles evolution on the dominant factors of
arbitrary density ratio, broadband initial perturbations and various material
properties (e.g., viscosity, miscibility, surface tensor). Evolution of the
most important characteristic quantities, i.e., the diameter of dominant bubble
$D$ and the height of bubble zone $h$, is derived: (i) the $D$ expands
self-similarly with steady aspect ratio $\beta \equiv D/h \thickapprox (1{\rm{
+ }}A)/4$, depending only on dimensionless density ratio $A$, and (ii) the $h$
grows quadratically with constant growth coefficient $\alpha \equiv h/(Ag{t^2})
\thickapprox [2\phi/{\ln}(2{\eta _{\rm{0}}})]^2$, depending on both
dimensionless initial perturbation amplitude ${\eta _{\rm{0}}}$ and
material-property-associated linear growth rate ratio
$\phi\equiv\Gamma_{actual}/\Gamma_{ideal}\leqslant1$. The theory successfully
explains the continued puzzle about the widely varying $\alpha\in (0.02,0.12)$
in experiments and simulations, conducted at all value of $A \in (0,1)$ and
widely varying value of ${\eta _{\rm{0}}} \in [{10^{ - 7}},{10^{ - 2}}]$ with
different materials. The good agreement between theory and experiments implies
that majority of actual mixing depends on initial perturbations and material
properties, to which more attention should be paid in either natural or
engineering problems.
",0,1,0,0,0,0
485,Learning to Drive in a Day,"  We demonstrate the first application of deep reinforcement learning to
autonomous driving. From randomly initialised parameters, our model is able to
learn a policy for lane following in a handful of training episodes using a
single monocular image as input. We provide a general and easy to obtain
reward: the distance travelled by the vehicle without the safety driver taking
control. We use a continuous, model-free deep reinforcement learning algorithm,
with all exploration and optimisation performed on-vehicle. This demonstrates a
new framework for autonomous driving which moves away from reliance on defined
logical rules, mapping, and direct supervision. We discuss the challenges and
opportunities to scale this approach to a broader range of autonomous driving
tasks.
",1,0,0,1,0,0
8825,Cellular function given parametric variation: excitability in the Hodgkin-Huxley model,"  How is reliable physiological function maintained in cells despite
considerable variability in the values of key parameters of multiple
interacting processes that govern that function? Here we use the classic
Hodgkin-Huxley formulation of the squid giant axon action potential to propose
a possible approach to this problem. Although the full Hodgkin-Huxley model is
very sensitive to fluctuations that independently occur in its many parameters,
the outcome is in fact determined by simple combinations of these parameters
along two physiological dimensions: Structural and Kinetic (denoted $S$ and
$K$). Structural parameters describe the properties of the cell, including its
capacitance and the densities of its ion channels. Kinetic parameters are those
that describe the opening and closing of the voltage-dependent conductances.
The impacts of parametric fluctuations on the dynamics of the system, seemingly
complex in the high dimensional representation of the Hodgkin-Huxley model, are
tractable when examined within the $S-K$ plane. We demonstrate that slow
inactivation, a ubiquitous activity-dependent feature of ionic channels, is a
powerful local homeostatic control mechanism that stabilizes excitability amid
changes in structural and kinetic parameters.
",0,0,0,0,1,0
14072,Observation of pseudogap in MgB2,"  Pseudogap phase in superconductors continues to be an outstanding puzzle that
differentiates unconventional superconductors from the conventional ones
(BCS-superconductors). Employing high resolution photoemission spectroscopy on
a highly dense conventional superconductor, MgB2, we discover an interesting
scenario. While the spectral evolution close to the Fermi energy is
commensurate to BCS descriptions as expected, the spectra in the wider energy
range reveal emergence of a pseudogap much above the superconducting transition
temperature indicating apparent departure from the BCS scenario. The energy
scale of the pseudogap is comparable to the energy of E2g phonon mode
responsible for superconductivity in MgB2 and the pseudogap can be attributed
to the effect of electron-phonon coupling on the electronic structure. These
results reveal a scenario of the emergence of the superconducting gap within an
electron-phonon coupling induced pseudogap.
",0,1,0,0,0,0
219,"Intersections of $ω$ classes in $\overline{\mathcal{M}}_{g,n}$","  We provide a graph formula which describes an arbitrary monomial in {\omega}
classes (also referred to as stable {\psi} classes) in terms of a simple family
of dual graphs (pinwheel graphs) with edges decorated by rational functions in
{\psi} classes. We deduce some numerical consequences and in particular a
combinatorial formula expressing top intersections of \k{appa} classes on Mg in
terms of top intersections of {\psi} classes.
",0,0,1,0,0,0
10039,Memory-efficient Kernel PCA via Partial Matrix Sampling and Nonconvex Optimization: a Model-free Analysis of Local Minima,"  Kernel PCA is a widely used nonlinear dimension reduction technique in
machine learning, but storing the kernel matrix is notoriously challenging when
the sample size is large. Inspired by Yi et al. [2016], where the idea of
partial matrix sampling followed by nonconvex optimization is proposed for
matrix completion and robust PCA, we apply a similar approach to
memory-efficient Kernel PCA. In theory, with no assumptions on the kernel
matrix in terms of eigenvalues or eigenvectors, we established a model-free
theory for the low-rank approximation based on any local minimum of the
proposed objective function. As interesting byproducts, when the underlying
positive semidefinite matrix is assumed to be low-rank and highly structured,
corollaries of our main theorem improve the state-of-the-art results of Ge et
al. [2016, 2017] for nonconvex matrix completion with no spurious local minima.
Numerical experiments also show that our approach is competitive in terms of
approximation accuracy compared to the well-known Nyström algorithm for
Kernel PCA.
",1,0,0,1,0,0
11430,Least Square Variational Bayesian Autoencoder with Regularization,"  In recent years Variation Autoencoders have become one of the most popular
unsupervised learning of complicated distributions.Variational Autoencoder
(VAE) provides more efficient reconstructive performance over a traditional
autoencoder. Variational auto enocders make better approximaiton than MCMC. The
VAE defines a generative process in terms of ancestral sampling through a
cascade of hidden stochastic layers. They are a directed graphic models.
Variational autoencoder is trained to maximise the variational lower bound.
Here we are trying maximise the likelihood and also at the same time we are
trying to make a good approximation of the data. Its basically trading of the
data log-likelihood and the KL divergence from the true posterior. This paper
describes the scenario in which we wish to find a point-estimate to the
parameters $\theta$ of some parametric model in which we generate each
observations by first sampling a local latent variable and then sampling the
associated observation. Here we use least square loss function with
regularization in the the reconstruction of the image, the least square loss
function was found to give better reconstructed images and had a faster
training time.
",1,0,0,1,0,0
12661,FORM version 4.2,"  We introduce FORM 4.2, a new minor release of the symbolic manipulation
toolkit. We demonstrate several new features, such as a new pattern matching
option, new output optimization, and automatic expansion of rational functions.
",1,0,0,0,0,0
11598,The Inner 25 AU Debris Distribution in the epsilon Eri System,"  Debris disk morphology is wavelength dependent due to the wide range of
particle sizes and size-dependent dynamics influenced by various forces.
Resolved images of nearby debris disks reveal complex disk structures that are
difficult to distinguish from their spectral energy distributions. Therefore,
multi-wavelength resolved images of nearby debris systems provide an essential
foundation to understand the intricate interplay between collisional,
gravitational, and radiative forces that govern debris disk structures. We
present the SOFIA 35 um resolved disk image of epsilon Eri, the closest debris
disk around a star similar to the early Sun. Combining with the Spitzer
resolved image at 24 um and 15-38 um excess spectrum, we examine two proposed
origins of the inner debris in epsilon Eri: (1) in-situ planetesimal belt(s)
and (2) dragged-in grains from the cold outer belt. We find that the presence
of in-situ dust-producing planetesmial belt(s) is the most likely source of the
excess emission in the inner 25 au region. Although a small amount of
dragged-in grains from the cold belt could contribute to the excess emission in
the inner region, the resolution of the SOFIA data is high enough to rule out
the possibility that the entire inner warm excess results from dragged-in
grains, but not enough to distinguish one broad inner disk from two narrow
belts.
",0,1,0,0,0,0
9691,Subconvex bounds for Hecke-Maass forms on compact arithmetic quotients of semisimple Lie groups,"  Let $H$ be a semisimple algebraic group, $K$ a maximal compact subgroup of
$G:=H(\mathbb{R})$, and $\Gamma\subset H(\mathbb{Q})$ a congruence arithmetic
subgroup. In this paper, we generalize existing subconvex bounds for
Hecke-Maass forms on the locally symmetric space $\Gamma \backslash G/K$ to
corresponding bounds on the arithmetic quotient $\Gamma \backslash G$ for
cocompact lattices using the spectral function of an elliptic operator. The
bounds obtained extend known subconvex bounds for automorphic forms to
non-trivial $K$-types, yielding subconvex bounds for new classes of automorphic
representations, and constitute subconvex bounds for eigenfunctions on compact
manifolds with both positive and negative sectional curvature. We also obtain
new subconvex bounds for holomorphic modular forms in the weight aspect.
",0,0,1,0,0,0
19080,Isometric immersions into manifolds with metallic structures,"  We consider submanifolds into Riemannian manifold with metallic structures.
We obtain some new results for hypersurfaces in these spaces and we express the
fundamental theorem of submanifolds into products spaces in terms of metallic
structures. Moreover, we define new structures called complex metallic
structures. We show that these structures are linked with complex structures.
Then, we consider submanifolds into Riemannian manifold with such structures
with a focus on invariant submanifolds and hypersurfaces. We also express in
particular the fundamental theorem of submanifolds of complex space form in
terms of complex metallic structures.
",0,0,1,0,0,0
15291,A Frame Tracking Model for Memory-Enhanced Dialogue Systems,"  Recently, resources and tasks were proposed to go beyond state tracking in
dialogue systems. An example is the frame tracking task, which requires
recording multiple frames, one for each user goal set during the dialogue. This
allows a user, for instance, to compare items corresponding to different goals.
This paper proposes a model which takes as input the list of frames created so
far during the dialogue, the current user utterance as well as the dialogue
acts, slot types, and slot values associated with this utterance. The model
then outputs the frame being referenced by each triple of dialogue act, slot
type, and slot value. We show that on the recently published Frames dataset,
this model significantly outperforms a previously proposed rule-based baseline.
In addition, we propose an extensive analysis of the frame tracking task by
dividing it into sub-tasks and assessing their difficulty with respect to our
model.
",1,0,0,0,0,0
12383,Seasonal Variation of the Underground Cosmic Muon Flux Observed at Daya Bay,"  The Daya Bay Experiment consists of eight identically designed detectors
located in three underground experimental halls named as EH1, EH2, EH3, with
250, 265 and 860 meters of water equivalent vertical overburden, respectively.
Cosmic muon events have been recorded over a two-year period. The underground
muon rate is observed to be positively correlated with the effective
atmospheric temperature and to follow a seasonal modulation pattern. The
correlation coefficient $\alpha$, describing how a variation in the muon rate
relates to a variation in the effective atmospheric temperature, is found to be
$\alpha_{\text{EH1}} = 0.362\pm0.031$, $\alpha_{\text{EH2}} = 0.433\pm0.038$
and $\alpha_{\text{EH3}} = 0.641\pm0.057$ for each experimental hall.
",0,1,0,0,0,0
17560,Enumeration of Tree-like Maps with Arbitrary Number of Vertices,"  This paper provides the generating series for the embedding of tree-like
graphs of arbitrary number of vertices, accourding to their genus. It applies
and extends the techniques of Chan, where it was used to give an alternate
proof of the Goulden and Slofstra formula. Furthermore, this greatly
generalizes the famous Harer-Zagier formula, which computes the Euler
characteristic of the moduli space of curves, and is equivalent to the
computation of one vertex maps.
",0,0,1,0,0,0
15798,A Comparison of Parallel Graph Processing Implementations,"  The rapidly growing number of large network analysis problems has led to the
emergence of many parallel and distributed graph processing systems---one
survey in 2014 identified over 80. Since then, the landscape has evolved; some
packages have become inactive while more are being developed. Determining the
best approach for a given problem is infeasible for most developers. To enable
easy, rigorous, and repeatable comparison of the capabilities of such systems,
we present an approach and associated software for analyzing the performance
and scalability of parallel, open-source graph libraries. We demonstrate our
approach on five graph processing packages: GraphMat, the Graph500, the Graph
Algorithm Platform Benchmark Suite, GraphBIG, and PowerGraph using synthetic
and real-world datasets. We examine previously overlooked aspects of parallel
graph processing performance such as phases of execution and energy usage for
three algorithms: breadth first search, single source shortest paths, and
PageRank and compare our results to Graphalytics.
",1,0,0,0,0,0
3190,Deep Over-sampling Framework for Classifying Imbalanced Data,"  Class imbalance is a challenging issue in practical classification problems
for deep learning models as well as traditional models. Traditionally
successful countermeasures such as synthetic over-sampling have had limited
success with complex, structured data handled by deep learning models. In this
paper, we propose Deep Over-sampling (DOS), a framework for extending the
synthetic over-sampling method to exploit the deep feature space acquired by a
convolutional neural network (CNN). Its key feature is an explicit, supervised
representation learning, for which the training data presents each raw input
sample with a synthetic embedding target in the deep feature space, which is
sampled from the linear subspace of in-class neighbors. We implement an
iterative process of training the CNN and updating the targets, which induces
smaller in-class variance among the embeddings, to increase the discriminative
power of the deep representation. We present an empirical study using public
benchmarks, which shows that the DOS framework not only counteracts class
imbalance better than the existing method, but also improves the performance of
the CNN in the standard, balanced settings.
",1,0,0,1,0,0
829,A Theoretical Perspective of Solving Phaseless Compressed Sensing via Its Nonconvex Relaxation,"  As a natural extension of compressive sensing and the requirement of some
practical problems, Phaseless Compressed Sensing (PCS) has been introduced and
studied recently. Many theoretical results have been obtained for PCS with the
aid of its convex relaxation. Motivated by successful applications of nonconvex
relaxed methods for solving compressive sensing, in this paper, we try to
investigate PCS via its nonconvex relaxation. Specifically, we relax PCS in the
real context by the corresponding $\ell_p$-minimization with $p\in (0,1)$. We
show that there exists a constant $p^\ast\in (0,1]$ such that for any fixed
$p\in(0, p^\ast)$, every optimal solution to the $\ell_p$-minimization also
solves the concerned problem; and derive an expression of such a constant
$p^\ast$ by making use of the known data and the sparsity level of the
concerned problem. These provide a theoretical basis for solving this class of
problems via the corresponding $\ell_p$-minimization.
",0,0,1,0,0,0
14365,Really should we pruning after model be totally trained? Pruning based on a small amount of training,"  Pre-training of models in pruning algorithms plays an important role in
pruning decision-making. We find that excessive pre-training is not necessary
for pruning algorithms. According to this idea, we propose a pruning
algorithm---Incremental pruning based on less training (IPLT). Compared with
the traditional pruning algorithm based on a large number of pre-training, IPLT
has competitive compression effect than the traditional pruning algorithm under
the same simple pruning strategy. On the premise of ensuring accuracy, IPLT can
achieve 8x-9x compression for VGG-19 on CIFAR-10 and only needs to pre-train
few epochs. For VGG-19 on CIFAR-10, we can not only achieve 10 times test
acceleration, but also about 10 times training acceleration. At present, the
research mainly focuses on the compression and acceleration in the application
stage of the model, while the compression and acceleration in the training
stage are few. We newly proposed a pruning algorithm that can compress and
accelerate in the training stage. It is novel to consider the amount of
pre-training required by pruning algorithm. Our results have implications: Too
much pre-training may be not necessary for pruning algorithms.
",1,0,0,0,0,0
10364,Outlier Cluster Formation in Spectral Clustering,"  Outlier detection and cluster number estimation is an important issue for
clustering real data. This paper focuses on spectral clustering, a time-tested
clustering method, and reveals its important properties related to outliers.
The highlights of this paper are the following two mathematical observations:
first, spectral clustering's intrinsic property of an outlier cluster
formation, and second, the singularity of an outlier cluster with a valid
cluster number. Based on these observations, we designed a function that
evaluates clustering and outlier detection results. In experiments, we prepared
two scenarios, face clustering in photo album and person re-identification in a
camera network. We confirmed that the proposed method detects outliers and
estimates the number of clusters properly in both problems. Our method
outperforms state-of-the-art methods in both the 128-dimensional sparse space
for face clustering and the 4,096-dimensional non-sparse space for person
re-identification.
",1,0,0,0,0,0
10400,Beyond Log-concavity: Provable Guarantees for Sampling Multi-modal Distributions using Simulated Tempering Langevin Monte Carlo,"  A key task in Bayesian statistics is sampling from distributions that are
only specified up to a partition function (i.e., constant of proportionality).
However, without any assumptions, sampling (even approximately) can be #P-hard,
and few works have provided ""beyond worst-case"" guarantees for such settings.
For log-concave distributions, classical results going back to Bakry and
Émery (1985) show that natural continuous-time Markov chains called Langevin
diffusions mix in polynomial time. The most salient feature of log-concavity
violated in practice is uni-modality: commonly, the distributions we wish to
sample from are multi-modal. In the presence of multiple deep and
well-separated modes, Langevin diffusion suffers from torpid mixing.
We address this problem by combining Langevin diffusion with simulated
tempering. The result is a Markov chain that mixes more rapidly by
transitioning between different temperatures of the distribution. We analyze
this Markov chain for the canonical multi-modal distribution: a mixture of
gaussians (of equal variance). The algorithm based on our Markov chain provably
samples from distributions that are close to mixtures of gaussians, given
access to the gradient of the log-pdf. For the analysis, we use a spectral
decomposition theorem for graphs (Gharan and Trevisan, 2014) and a Markov chain
decomposition technique (Madras and Randall, 2002).
",1,0,0,1,0,0
16869,Modeling and optimal control of HIV/AIDS prevention through PrEP,"  Pre-exposure prophylaxis (PrEP) consists in the use of an antiretroviral
medication to prevent the acquisition of HIV infection by uninfected
individuals and has recently demonstrated to be highly efficacious for HIV
prevention. We propose a new epidemiological model for HIV/AIDS transmission
including PrEP. Existence, uniqueness and global stability of the disease free
and endemic equilibriums are proved. The model with no PrEP is calibrated with
the cumulative cases of infection by HIV and AIDS reported in Cape Verde from
1987 to 2014, showing that it predicts well such reality. An optimal control
problem with a mixed state control constraint is then proposed and analyzed,
where the control function represents the PrEP strategy and the mixed
constraint models the fact that, due to PrEP costs, epidemic context and
program coverage, the number of individuals under PrEP is limited at each
instant of time. The objective is to determine the PrEP strategy that satisfies
the mixed state control constraint and minimizes the number of individuals with
pre-AIDS HIV-infection as well as the costs associated with PrEP. The optimal
control problem is studied analytically. Through numerical simulations, we
demonstrate that PrEP reduces HIV transmission significantly.
",0,0,1,0,0,0
6092,On the radius of spatial analyticity for the quartic generalized KdV equation,"  Lower bound on the rate of decrease in time of the uniform radius of spatial
analyticity of solutions to the quartic generalized KdV equation is derived,
which improves an earlier result by Bona, Grujić and Kalisch.
",0,0,1,0,0,0
4881,Adaptive Similar Triangles Method: a Stable Alternative to Sinkhorn's Algorithm for Regularized Optimal Transport,"  In this paper, we are motivated by two important applications:
entropy-regularized optimal transport problem and road or IP traffic demand
matrix estimation by entropy model. Both of them include solving a special type
of optimization problem with linear equality constraints and objective given as
a sum of an entropy regularizer and a linear function. It is known that the
state-of-the-art solvers for this problem, which are based on Sinkhorn's method
(also known as RSA or balancing method), can fail to work, when the
entropy-regularization parameter is small. We consider the above optimization
problem as a particular instance of a general strongly convex optimization
problem with linear constraints. We propose a new algorithm to solve this
general class of problems. Our approach is based on the transition to the dual
problem. First, we introduce a new accelerated gradient method with adaptive
choice of gradient's Lipschitz constant. Then, we apply this method to the dual
problem and show, how to reconstruct an approximate solution to the primal
problem with provable convergence rate. We prove the rate $O(1/k^2)$, $k$ being
the iteration counter, both for the absolute value of the primal objective
residual and constraints infeasibility. Our method has similar to Sinkhorn's
method complexity of each iteration, but is faster and more stable numerically,
when the regularization parameter is small. We illustrate the advantage of our
method by numerical experiments for the two mentioned applications. We show
that there exists a threshold, such that, when the regularization parameter is
smaller than this threshold, our method outperforms the Sinkhorn's method in
terms of computation time.
",0,0,1,0,0,0
5815,Promising Accurate Prefix Boosting for sequence-to-sequence ASR,"  In this paper, we present promising accurate prefix boosting (PAPB), a
discriminative training technique for attention based sequence-to-sequence
(seq2seq) ASR. PAPB is devised to unify the training and testing scheme in an
effective manner. The training procedure involves maximizing the score of each
partial correct sequence obtained during beam search compared to other
hypotheses. The training objective also includes minimization of token
(character) error rate. PAPB shows its efficacy by achieving 10.8\% and 3.8\%
WER with and without RNNLM respectively on Wall Street Journal dataset.
",1,0,0,0,0,0
15246,Entropic Spectral Learning in Large Scale Networks,"  We present a novel algorithm for learning the spectral density of large scale
networks using stochastic trace estimation and the method of maximum entropy.
The complexity of the algorithm is linear in the number of non-zero elements of
the matrix, offering a computational advantage over other algorithms. We apply
our algorithm to the problem of community detection in large networks. We show
state-of-the-art performance on both synthetic and real datasets.
",0,0,0,1,0,0
8576,"Discrete Integrable Systems, Supersymmetric Quantum Mechanics, and Framed BPS States - I","  It is possible to understand whether a given BPS spectrum is generated by a
relevant deformation of a 4D N=2 SCFT or of an asymptotically free theory from
the periodicity properties of the corresponding quantum monodromy. With the aim
of giving a better understanding of the above conjecture, in this paper we
revisit the description of framed BPS states of four-dimensional relativistic
quantum field theories with eight conserved supercharges in terms of
supersymmetric quantum mechanics. We unveil aspects of the deep
interrelationship in between the Seiberg-dualities of the latter, the discrete
symmetries of the theory in the bulk, and quantum discrete integrable systems.
",0,1,1,0,0,0
15779,Soft-proton exchange on Magnesium-oxide-doped substrates a route toward efficient and power-resistant nonlinear converters,"  Despite its attractive features, Congruent-melted Lithium Niobate (CLN)
suffers from Photo-Refractive Damage (PRD). This light-induced refractive-index
change hampers the use of CLN when high-power densities are in play, a typical
regime in integrated optics. The resistance to PRD can be largely improved by
doping the lithium-niobate substrates with magnesium oxide. However, the
fabrication of waveguides on MgO-doped substrates is not as effective as for
CLN: either the resistance to PRD is strongly reduced by the waveguide
fabrication process (as it happens in Ti-indiffused waveguides) or the
nonlinear conversion efficiency is lowered (as it occurs in annealed-proton
exchange). Here we fabricate, for the first time, waveguides starting from
MgO-doped substrates using the Soft-Proton Exchange (SPE) technique and we show
that this third way represents a promising alternative. We demonstrate that SPE
allows to produce refractive-index profiles almost identical to those produced
on CLN without reducing the nonlinearity in the substrate. We also prove that
the SPE does not affect substantially the resistance to PRD. Since the
fabrication recipe is identical between CLN and MgO-doped substrates, we
believe that SPE might outperform standard techniques to fabricate robust and
efficient waveguides for high-intensity-beam confinement.
",0,1,0,0,0,0
10467,Time-dependent probability density functions and information geometry in stochastic logistic and Gompertz models,"  A probabilistic description is essential for understanding growth processes
far from equilibrium. In this paper, we compute time-dependent Probability
Density Functions (PDFs) in order to investigate stochastic logistic and
Gompertz models, which are two of the most popular growth models. We consider
different types of short-correlated internal (multiplicative) and external
(additive) stochastic noises and compare the time-dependent PDFs in the two
models, elucidating the effects of the additive and multiplicative noises on
the form of PDFs. We demonstrate an interesting transition from a unimodal to a
bimodal PDF as the multiplicative noise increases for a fixed value of the
additive noise. A much weaker (leaky) attractor in the Gompertz model leads to
a significant (singular) growth of the population of a very small size. We
point out the limitation of using stationary PDFs, mean value and variance in
understanding statistical properties of the growth far from equilibrium,
highlighting the importance of time-dependent PDFs. We further compare these
two models from the perspective of information change that occurs during the
growth process. Specifically, we define an infinitesimal distance at any time
by comparing two PDFs at times infinitesimally apart and sum these distances in
time. The total distance along the trajectory quantifies the total number of
different states that the system undergoes in time, and is called the
information length. We show that the time-evolution of the two models become
more similar when measured in units of the information length and point out the
merit of using the information length in unifying and understanding the dynamic
evolution of different growth processes.
",0,1,0,0,0,0
17781,Functoriality and uniformity in Hrushovski's groupoid-cover correspondence,"  The correspondence between definable connected groupoids in a theory $T$ and
internal generalised imaginary sorts of $T$, established by Hrushovski in
[""Groupoids, imaginaries and internal covers,"" Turkish Journal of Mathematics,
2012], is here extended in two ways: First, it is shown that the correspondence
is in fact an equivalence of categories, with respect to appropriate notions of
morphism. Secondly, the equivalence of categories is shown to vary uniformly in
definable families, with respect to an appropriate relativisation of these
categories. Some elaboration on Hrushovki's original constructions are also
included.
",0,0,1,0,0,0
3483,Deep Learning for Accelerated Reliability Analysis of Infrastructure Networks,"  Natural disasters can have catastrophic impacts on the functionality of
infrastructure systems and cause severe physical and socio-economic losses.
Given budget constraints, it is crucial to optimize decisions regarding
mitigation, preparedness, response, and recovery practices for these systems.
This requires accurate and efficient means to evaluate the infrastructure
system reliability. While numerous research efforts have addressed and
quantified the impact of natural disasters on infrastructure systems, typically
using the Monte Carlo approach, they still suffer from high computational cost
and, thus, are of limited applicability to large systems. This paper presents a
deep learning framework for accelerating infrastructure system reliability
analysis. In particular, two distinct deep neural network surrogates are
constructed and studied: (1) A classifier surrogate which speeds up the
connectivity determination of networks, and (2) An end-to-end surrogate that
replaces a number of components such as roadway status realization,
connectivity determination, and connectivity averaging. The proposed approach
is applied to a simulation-based study of the two-terminal connectivity of a
California transportation network subject to extreme probabilistic earthquake
events. Numerical results highlight the effectiveness of the proposed approach
in accelerating the transportation system two-terminal reliability analysis
with extremely high prediction accuracy.
",1,0,0,1,0,0
14375,Neural Semantic Parsing over Multiple Knowledge-bases,"  A fundamental challenge in developing semantic parsers is the paucity of
strong supervision in the form of language utterances annotated with logical
form. In this paper, we propose to exploit structural regularities in language
in different domains, and train semantic parsers over multiple knowledge-bases
(KBs), while sharing information across datasets. We find that we can
substantially improve parsing accuracy by training a single
sequence-to-sequence model over multiple KBs, when providing an encoding of the
domain at decoding time. Our model achieves state-of-the-art performance on the
Overnight dataset (containing eight domains), improves performance over a
single KB baseline from 75.6% to 79.6%, while obtaining a 7x reduction in the
number of model parameters.
",1,0,0,0,0,0
13036,Indirect observation of molecular disassociation in solid benzene at low temperatures,"  The molecular dynamics of solid benzene are extremely complex; especially
below 77 K, its inner mechanics remain mostly unexplored. Benzene is also a
prototypical molecular crystal that becomes energetically frustrated at low
temperatures and usually unusual phenomena accompanies such scenarios. We
performed dielectric constant measurements on solid benzene down to 5 K and
observed a previously unidentified minimum in the imaginary part of the
dielectric constant at Tm=17.9 K. Results obtained on deuterated solid benzene
(C6D6, where D is deuterium) show an isotope effect in the form of a shift of
the critical temperature to Tm'=18.9 K. Our findings indicate that at Tm, only
the protons without the carbon atoms continue again to undergo rotational
tunneling about the hexad axes. The deuterons appear to do the same accounting
for an indirect observation of a continued and sustained 12-body tunneling
event. We discuss how similar experiments performed on hydrogen-based molecular
crystals can be exploited to help us obtain more insight on the quantum
mechanics of many-body tunneling.
",0,1,0,0,0,0
3913,Poisson-Fermi Formulation of Nonlocal Electrostatics in Electrolyte Solutions,"  We present a nonlocal electrostatic formulation of nonuniform ions and water
molecules with interstitial voids that uses a Fermi-like distribution to
account for steric and correlation effects in electrolyte solutions. The
formulation is based on the volume exclusion of hard spheres leading to a
steric potential and Maxwell's displacement field with Yukawa-type interactions
resulting in a nonlocal electric potential. The classical Poisson-Boltzmann
model fails to describe steric and correlation effects important in a variety
of chemical and biological systems, especially in high field or large
concentration conditions found in and near binding sites, ion channels, and
electrodes. Steric effects and correlations are apparent when we compare
nonlocal Poisson-Fermi results to Poisson-Boltzmann calculations in electric
double layer and to experimental measurements on the selectivity of potassium
channels for K+ over Na+. The present theory links atomic scale descriptions of
the crystallized KcsA channel with macroscopic bulk conditions. Atomic
structures and macroscopic conditions determine complex functions of great
importance in biology, nanotechnology, and electrochemistry.
",0,1,0,0,0,0
9852,New estimates for some functions defined over primes,"  In this paper we first establish new explicit estimates for Chebyshev's
$\vartheta$-function. Applying these new estimates, we derive new upper and
lower bounds for some functions defined over the prime numbers, for instance
the prime counting function $\pi(x)$, which improve the currently best ones.
Furthermore, we use the obtained estimates for the prime counting function to
give two new results concerning the existence of prime numbers in short
intervals.
",0,0,1,0,0,0
439,It Takes Two to Tango: Towards Theory of AI's Mind,"  Theory of Mind is the ability to attribute mental states (beliefs, intents,
knowledge, perspectives, etc.) to others and recognize that these mental states
may differ from one's own. Theory of Mind is critical to effective
communication and to teams demonstrating higher collective performance. To
effectively leverage the progress in Artificial Intelligence (AI) to make our
lives more productive, it is important for humans and AI to work well together
in a team. Traditionally, there has been much emphasis on research to make AI
more accurate, and (to a lesser extent) on having it better understand human
intentions, tendencies, beliefs, and contexts. The latter involves making AI
more human-like and having it develop a theory of our minds. In this work, we
argue that for human-AI teams to be effective, humans must also develop a
theory of AI's mind (ToAIM) - get to know its strengths, weaknesses, beliefs,
and quirks. We instantiate these ideas within the domain of Visual Question
Answering (VQA). We find that using just a few examples (50), lay people can be
trained to better predict responses and oncoming failures of a complex VQA
model. We further evaluate the role existing explanation (or interpretability)
modalities play in helping humans build ToAIM. Explainable AI has received
considerable scientific and popular attention in recent times. Surprisingly, we
find that having access to the model's internal states - its confidence in its
top-k predictions, explicit or implicit attention maps which highlight regions
in the image (and words in the question) the model is looking at (and listening
to) while answering a question about an image - do not help people better
predict its behavior.
",1,0,0,0,0,0
7555,Room temperature line lists for CO\2 symmetric isotopologues with \textit{ab initio} computed intensities,"  Remote sensing experiments require high-accuracy, preferably sub-percent,
line intensities and in response to this need we present computed room
temperature line lists for six symmetric isotopologues of carbon dioxide:
$^{13}$C$^{16}$O$_2$, $^{14}$C$^{16}$O$_2$, $^{12}$C$^{17}$O$_2$,
$^{12}$C$^{18}$O$_2$, $^{13}$C$^{17}$O$_2$ and $^{13}$C$^{18}$O$_2$, covering
the range 0-8000 \cm. Our calculation scheme is based on variational nuclear
motion calculations and on a reliability analysis of the generated line
intensities. Rotation-vibration wavefunctions and energy levels are computed
using the DVR3D software suite and a high quality semi-empirical potential
energy surface (PES), followed by computation of intensities using an
\abinitio\ dipole moment surface (DMS). Four line lists are computed for each
isotopologue to quantify sensitivity to minor distortions of the PES/DMS.
Reliable lines are benchmarked against recent state-of-the-art measurements and
against the HITRAN2012 database, supporting the claim that the majority of line
intensities for strong bands are predicted with sub-percent accuracy. Accurate
line positions are generated using an effective Hamiltonian. We recommend the
use of these line lists for future remote sensing studies and their inclusion
in databases.
",0,1,0,0,0,0
3046,Semi-Lagrangian one-step methods for two classes of time-dependent partial differential systems,"  Semi-Lagrangian methods are numerical methods designed to find approximate
solutions to particular time-dependent partial differential equations (PDEs)
that describe the advection process. We propose semi-Lagrangian one-step
methods for numerically solving initial value problems for two general systems
of partial differential equations. Along the characteristic lines of the PDEs,
we use ordinary differential equation (ODE) numerical methods to solve the
PDEs. The main benefit of our methods is the efficient achievement of high
order local truncation error through the use of Runge-Kutta methods along the
characteristics. In addition, we investigate the numerical analysis of
semi-Lagrangian methods applied to systems of PDEs: stability, convergence, and
maximum error bounds.
",0,0,1,0,0,0
9363,Pseudo-Recursal: Solving the Catastrophic Forgetting Problem in Deep Neural Networks,"  In general, neural networks are not currently capable of learning tasks in a
sequential fashion. When a novel, unrelated task is learnt by a neural network,
it substantially forgets how to solve previously learnt tasks. One of the
original solutions to this problem is pseudo-rehearsal, which involves learning
the new task while rehearsing generated items representative of the previous
task/s. This is very effective for simple tasks. However, pseudo-rehearsal has
not yet been successfully applied to very complex tasks because in these tasks
it is difficult to generate representative items. We accomplish
pseudo-rehearsal by using a Generative Adversarial Network to generate items so
that our deep network can learn to sequentially classify the CIFAR-10, SVHN and
MNIST datasets. After training on all tasks, our network loses only 1.67%
absolute accuracy on CIFAR-10 and gains 0.24% absolute accuracy on SVHN. Our
model's performance is a substantial improvement compared to the current state
of the art solution.
",0,0,0,1,0,0
20347,"Sesqui-arrays, a generalisation of triple arrays","  A triple array is a rectangular array containing letters, each letter
occurring equally often with no repeats in rows or columns, such that the
number of letters common to two rows, two columns, or a row and a column are
(possibly different) non-zero constants. Deleting the condition on the letters
common to a row and a column gives a double array. We propose the term
\emph{sesqui-array} for such an array when only the condition on pairs of
columns is deleted. Thus all triple arrays are sesqui-arrays.
In this paper we give three constructions for sesqui-arrays. The first gives
$(n+1)\times n^2$ arrays on $n(n+1)$ letters for $n\geq 2$. (Such an array for
$n=2$ was found by Bagchi.) This construction uses Latin squares. The second
uses the \emph{Sylvester graph}, a subgraph of the Hoffman--Singleton graph, to
build a good block design for $36$ treatments in $42$ blocks of size~$6$, and
then uses this in a $7\times 36$ sesqui-array for $42$ letters.
We also give a construction for $K\times(K-1)(K-2)/2$ sesqui-arrays on
$K(K-1)/2$ letters. This construction uses biplanes. It starts with a block of
a biplane and produces an array which satisfies the requirements for a
sesqui-array except possibly that of having no repeated letters in a row or
column. We show that this condition holds if and only if the \emph{Hussain
chains} for the selected block contain no $4$-cycles. A sufficient condition
for the construction to give a triple array is that each Hussain chain is a
union of $3$-cycles; but this condition is not necessary, and we give a few
further examples.
We also discuss the question of which of these arrays provide good designs
for experiments.
",0,0,1,1,0,0
9521,SHINE: Signed Heterogeneous Information Network Embedding for Sentiment Link Prediction,"  In online social networks people often express attitudes towards others,
which forms massive sentiment links among users. Predicting the sign of
sentiment links is a fundamental task in many areas such as personal
advertising and public opinion analysis. Previous works mainly focus on textual
sentiment classification, however, text information can only disclose the ""tip
of the iceberg"" about users' true opinions, of which the most are unobserved
but implied by other sources of information such as social relation and users'
profile. To address this problem, in this paper we investigate how to predict
possibly existing sentiment links in the presence of heterogeneous information.
First, due to the lack of explicit sentiment links in mainstream social
networks, we establish a labeled heterogeneous sentiment dataset which consists
of users' sentiment relation, social relation and profile knowledge by
entity-level sentiment extraction method. Then we propose a novel and flexible
end-to-end Signed Heterogeneous Information Network Embedding (SHINE) framework
to extract users' latent representations from heterogeneous networks and
predict the sign of unobserved sentiment links. SHINE utilizes multiple deep
autoencoders to map each user into a low-dimension feature space while
preserving the network structure. We demonstrate the superiority of SHINE over
state-of-the-art baselines on link prediction and node recommendation in two
real-world datasets. The experimental results also prove the efficacy of SHINE
in cold start scenario.
",1,0,0,1,0,0
2930,Rovibrational optical cooling of a molecular beam,"  Cooling the rotation and the vibration of molecules by broadband light
sources was possible for trapped molecular ions or ultracold molecules. Because
of a low power spectral density, the cooling timescale has never fell below
than a few milliseconds. Here we report on rotational and vibrational cooling
of a supersonic beam of barium monofluoride molecules in less than 440 $\mu$s.
Vibrational cooling was optimized by enhancing the spectral power density of a
semiconductor light source at the underlying molecular transitions allowing us
to transfer all the populations of $v''=1-3$ into the vibrational ground state
($v''=0$). Rotational cooling, that requires an efficient vibrational pumping,
was then achieved. According to a Boltzmann fit, the rotation temperature was
reduced by almost a factor of 10. In this fashion, the population of the lowest
rotational levels increased by more than one order of magnitude.
",0,1,0,0,0,0
11273,Language Modeling with Generative Adversarial Networks,"  Generative Adversarial Networks (GANs) have been promising in the field of
image generation, however, they have been hard to train for language
generation. GANs were originally designed to output differentiable values, so
discrete language generation is challenging for them which causes high levels
of instability in training GANs. Consequently, past work has resorted to
pre-training with maximum-likelihood or training GANs without pre-training with
a WGAN objective with a gradient penalty. In this study, we present a
comparison of those approaches. Furthermore, we present the results of some
experiments that indicate better training and convergence of Wasserstein GANs
(WGANs) when a weaker regularization term is enforcing the Lipschitz
constraint.
",0,0,0,1,0,0
20290,"Test them all, is it worth it? Assessing configuration sampling on the JHipster Web development stack","  Many approaches for testing configurable software systems start from the same
assumption: it is impossible to test all configurations. This motivated the
definition of variability-aware abstractions and sampling techniques to cope
with large configuration spaces. Yet, there is no theoretical barrier that
prevents the exhaustive testing of all configurations by simply enumerating
them, if the effort required to do so remains acceptable. Not only this: we
believe there is lots to be learned by systematically and exhaustively testing
a configurable system. In this case study, we report on the first ever
endeavour to test all possible configurations of an industry-strength, open
source configurable software system, JHipster, a popular code generator for web
applications. We built a testing scaffold for the 26,000+ configurations of
JHipster using a cluster of 80 machines during 4 nights for a total of 4,376
hours (182 days) CPU time. We find that 35.70% configurations fail and we
identify the feature interactions that cause the errors. We show that sampling
strategies (like dissimilarity and 2-wise): (1) are more effective to find
faults than the 12 default configurations used in the JHipster continuous
integration; (2) can be too costly and exceed the available testing budget. We
cross this quantitative analysis with the qualitative assessment of JHipster's
lead developers.
",1,0,0,0,0,0
11632,Improved torque formula for low and intermediate mass planetary migration,"  The migration of planets on nearly circular, non-inclined orbits in
protoplanetary discs is entirely described by the disc's torque. This torque is
a complex function of the disc parameters, and essentially amounts to the sum
of two components: the Lindblad torque and the corotation torque. Known torque
formulae do not reproduce accurately the torque actually experienced in
numerical simulations by low- and intermediate- mass planets in radiative
discs. One of the main reasons for this inaccuracy is that these formulae have
been worked out in two-dimensional analyses. Here we revisit the torque formula
and update many of its dimensionless coefficients by means of tailored, three-
dimensional numerical simulations. In particular, we derive the dependence of
the Lindblad torque on the temperature gradient, the dependence of the
corotation torque on the radial entropy gradient (and work out a suitable
expression of this gradient in a three-dimensional disc). We also work out the
dependence of the corotation torque on the radial temperature gradient,
overlooked so far. Corotation torques are known to scale very steeply with the
width of the horseshoe region. We extend the expression of this width to the
domain of intermediate mass planets, so that our updated torque formula remains
valid for planets up to typically several tens of Earth masses, provided these
relatively massive planets do not significantly deplete their coorbital region.
Our torque expression can be applied to low- and intermediate-mass planets in
optically thick protoplanetary discs, as well as protomoons embedded in
circumplanetary discs.
",0,1,0,0,0,0
9640,"Point distributions in compact metric spaces, II","  We consider finite point subsets (distributions) in compact metric spaces. In
the case of general rectifiable metric spaces, non-trivial bounds for sums of
distances between points of distributions and for discrepancies of
distributions in metric balls are given (Theorem 1.1). We generalize
Stolarsky's invariance principle to distance-invariant spaces (Theorem 2.1).
For arbitrary metric spaces, we prove a probabilistic invariance principle
(Theorem 3.1). Furthermore, we construct equal-measure partitions of general
rectifiable compact metric spaces into parts of small average diameter (Theorem
4.1). This version of the paper will be published in Mathematika
",0,0,1,0,0,0
16092,KeyXtract Twitter Model - An Essential Keywords Extraction Model for Twitter Designed using NLP Tools,"  Since a tweet is limited to 140 characters, it is ambiguous and difficult for
traditional Natural Language Processing (NLP) tools to analyse. This research
presents KeyXtract which enhances the machine learning based Stanford CoreNLP
Part-of-Speech (POS) tagger with the Twitter model to extract essential
keywords from a tweet. The system was developed using rule-based parsers and
two corpora. The data for the research was obtained from a Twitter profile of a
telecommunication company. The system development consisted of two stages. At
the initial stage, a domain specific corpus was compiled after analysing the
tweets. The POS tagger extracted the Noun Phrases and Verb Phrases while the
parsers removed noise and extracted any other keywords missed by the POS
tagger. The system was evaluated using the Turing Test. After it was tested and
compared against Stanford CoreNLP, the second stage of the system was developed
addressing the shortcomings of the first stage. It was enhanced using Named
Entity Recognition and Lemmatization. The second stage was also tested using
the Turing test and its pass rate increased from 50.00% to 83.33%. The
performance of the final system output was measured using the F1 score.
Stanford CoreNLP with the Twitter model had an average F1 of 0.69 while the
improved system had a F1 of 0.77. The accuracy of the system could be improved
by using a complete domain specific corpus. Since the system used linguistic
features of a sentence, it could be applied to other NLP tools.
",1,0,0,0,0,0
15415,Model Averaging and its Use in Economics,"  The method of model averaging has become an important tool to deal with model
uncertainty, for example in situations where a large amount of different
theories exist, as are common in economics. Model averaging is a natural and
formal response to model uncertainty in a Bayesian framework, and most of the
paper deals with Bayesian model averaging. The important role of the prior
assumptions in these Bayesian procedures is highlighted. In addition,
frequentist model averaging methods are also discussed. Numerical methods to
implement these methods are explained, and I point the reader to some freely
available computational resources. The main focus is on uncertainty regarding
the choice of covariates in normal linear regression models, but the paper also
covers other, more challenging, settings, with particular emphasis on sampling
models commonly used in economics. Applications of model averaging in economics
are reviewed and discussed in a wide range of areas, among which growth
economics, production modelling, finance and forecasting macroeconomic
quantities.
",0,0,0,1,0,0
13207,Mosquito Detection with Neural Networks: The Buzz of Deep Learning,"  Many real-world time-series analysis problems are characterised by scarce
data. Solutions typically rely on hand-crafted features extracted from the time
or frequency domain allied with classification or regression engines which
condition on this (often low-dimensional) feature vector. The huge advances
enjoyed by many application domains in recent years have been fuelled by the
use of deep learning architectures trained on large data sets. This paper
presents an application of deep learning for acoustic event detection in a
challenging, data-scarce, real-world problem. Our candidate challenge is to
accurately detect the presence of a mosquito from its acoustic signature. We
develop convolutional neural networks (CNNs) operating on wavelet
transformations of audio recordings. Furthermore, we interrogate the network's
predictive power by visualising statistics of network-excitatory samples. These
visualisations offer a deep insight into the relative informativeness of
components in the detection problem. We include comparisons with conventional
classifiers, conditioned on both hand-tuned and generic features, to stress the
strength of automatic deep feature learning. Detection is achieved with
performance metrics significantly surpassing those of existing algorithmic
methods, as well as marginally exceeding those attained by individual human
experts.
",1,0,0,1,0,0
14987,Bilipschitz Equivalence of Trees and Hyperbolic Fillings,"  We combine conditions found in [Wh] with results from [MPR] to show that
quasi-isometries between uniformly discrete bounded geometry spaces that
satisfy linear isoperimetric inequalities are within bounded distance to
bilipschitz equivalences. We apply this result to regularly branching trees and
hyperbolic fillings of metric spaces.
",0,0,1,0,0,0
13989,New integrable semi-discretizations of the coupled nonlinear Schrodinger equations,"  We have undertaken an algorithmic search for new integrable
semi-discretizations of physically relevant nonlinear partial differential
equations. The search is performed by using a compatibility condition for the
discrete Lax operators and symbolic computations. We have discovered a new
integrable system of coupled nonlinear Schrodinger equations which combines
elements of the Ablowitz-Ladik lattice and the triangular-lattice ribbon
studied by Vakhnenko. We show that the continuum limit of the new integrable
system is given by uncoupled complex modified Korteweg-de Vries equations and
uncoupled nonlinear Schrodinger equations.
",0,1,1,0,0,0
11887,Exploiting OxRAM Resistive Switching for Dynamic Range Improvement of CMOS Image Sensors,"  We present a unique application of OxRAM devices in CMOS Image Sensors (CIS)
for dynamic range (DR) improvement. We propose a modified 3T-APS (Active Pixel
Sensor) circuit that incorporates OxRAM in 1T-1R configuration. DR improvement
is achieved by resistive compression of the pixel output signal through
autonomous programming of OxRAM device resistance during exposure. We show that
by carefully preconditioning the OxRAM resistance, pixel DR can be enhanced.
Detailed impact of OxRAM SET-to-RESET and RESET-to-SET transitions on pixel DR
is discussed. For experimental validation with specific OxRAM preprogrammed
states, a 4 Kb 10 nm thick HfOx (1T-1R) matrix was fabricated and
characterized. Best case, relative pixel DR improvement of ~ 50 dB was obtained
for our design.
",1,0,0,0,0,0
11199,Turbulent gas accretion between supermassive black holes and star-forming rings in the circumnuclear disk,"  While supermassive black holes are known to co-evolve with their host galaxy,
the precise nature and origin of this co-evolution is not clear. We here
explore the possible connection between star formation and black hole growth in
the circumnuclear disk (CND) to probe this connection in the vicinity close to
the black hole. We adopt here the circumnuclear disk model developed by
Kawakatu & Wada (2008) and Wutschik et al. (2013), and explore both the
dependence on the star formation recipe as well as the role of the
gravitational field, which can be dominated by the central black hole, the CND
itself or the host galaxy. A specific emphasis is put on the turbulence
regulated star formation model by Krumholz et al. (2005) to explore the impact
of a realistic star formation recipe. It is shown that this model helps to
introduce realistic fluctuations in the black hole and star formation rate,
without overestimating them. Consistent with previous works, we show that the
final black hole masses are rather insensitive to the masses of the initial
seeds, even for seed masses of up to 10^6 M_sol. In addition, we apply our
model to the formation of high-redshift quasars, as well as to the nearby
system NGC 6951, where a tentative comparison is made in spite of the presence
of a bar in the galaxy. We show that our model can reproduce the high black
hole masses of the high-redshift quasars within a sufficiently short time,
provided a high mass supply rate from the host galaxy. In addition, it
reproduces several of the properties observed in NGC 6951. With respect to the
latter system, our analysis suggests that supernova feedback may be important
to create the observed fluctuations in the star formation history as a result
of negative feedback effects.
",0,1,0,0,0,0
10337,"Constraining Effective Temperature, Mass and Radius of Hot White Dwarfs","  By introducing a simplified transport model of outer layers of white dwarfs
we derive an analytical semi-empirical relation which constrains effective
temperature-mass-radius for white dwarfs. This relation is used to classify
recent data of white dwarfs according to their time evolution in non-accretion
process of cooling. This formula permit us to study the population map of white
dwarfs in the central temperature and mass plane, and discuss the relation with
the ignition temperature for C-O material. Our effective
temperature-mass-radius relation provide a quick method to estimate the mass of
newly observed white dwarfs from their spectral measurements of effective
temperature and superficial gravity.
",0,1,0,0,0,0
10247,Upper-limit on the Advanced Virgo output mode cleaner cavity length noise,"  The Advanced Virgo detector uses two monolithic optical cavities at its
output port to suppress higher order modes and radio frequency sidebands from
the carrier light used for gravitational wave detection. These two cavities in
series form the output mode cleaner. We present a measured upper limit on the
length noise of these cavities that is consistent with the thermo-refractive
noise prediction of $8 \times 10^{-16}\,\textrm{m/Hz}^{1/2}$ at 15 Hz. The
cavity length is controlled using Peltier cells and piezo-electric actuators to
maintain resonance on the incoming light. A length lock precision of $3.5
\times 10^{-13}\,\textrm{m}$ is achieved. These two results are combined to
demonstrate that the broadband length noise of the output mode cleaner in the
10-60 Hz band is at least a factor 10 below other expected noise sources in the
Advanced Virgo detector design configuration.
",0,1,0,0,0,0
16404,Option Pricing in Illiquid Markets with Jumps,"  The classical linear Black--Scholes model for pricing derivative securities
is a popular model in financial industry. It relies on several restrictive
assumptions such as completeness, and frictionless of the market as well as the
assumption on the underlying asset price dynamics following a geometric
Brownian motion. The main purpose of this paper is to generalize the classical
Black--Scholes model for pricing derivative securities by taking into account
feedback effects due to an influence of a large trader on the underlying asset
price dynamics exhibiting random jumps. The assumption that an investor can
trade large amounts of assets without affecting the underlying asset price
itself is usually not satisfied, especially in illiquid markets. We generalize
the Frey--Stremme nonlinear option pricing model for the case the underlying
asset follows a Levy stochastic process with jumps. We derive and analyze a
fully nonlinear parabolic partial-integro differential equation for the price
of the option contract. We propose a semi-implicit numerical discretization
scheme and perform various numerical experiments showing influence of a large
trader and intensity of jumps on the option price.
",0,0,0,0,0,1
18754,Evaluating and Modelling Hanabi-Playing Agents,"  Agent modelling involves considering how other agents will behave, in order
to influence your own actions. In this paper, we explore the use of agent
modelling in the hidden-information, collaborative card game Hanabi. We
implement a number of rule-based agents, both from the literature and of our
own devising, in addition to an Information Set Monte Carlo Tree Search
(IS-MCTS) agent. We observe poor results from IS-MCTS, so construct a new,
predictor version that uses a model of the agents with which it is paired. We
observe a significant improvement in game-playing strength from this agent in
comparison to IS-MCTS, resulting from its consideration of what the other
agents in a game would do. In addition, we create a flawed rule-based agent to
highlight the predictor's capabilities with such an agent.
",1,0,0,0,0,0
15479,Counting $G$-Extensions by Discriminant,"  The problem of analyzing the number of number field extensions $L/K$ with
bounded (relative) discriminant has been the subject of renewed interest in
recent years, with significant advances made by Schmidt, Ellenberg-Venkatesh,
Bhargava, Bhargava-Shankar-Wang, and others. In this paper, we use the geometry
of numbers and invariant theory of finite groups, in a manner similar to
Ellenberg and Venkatesh, to give an upper bound on the number of extensions
$L/K$ with fixed degree, bounded relative discriminant, and specified Galois
closure.
",0,0,1,0,0,0
16089,Social Clustering in Epidemic Spread on Coevolving Networks,"  Even though transitivity is a central structural feature of social networks,
its influence on epidemic spread on coevolving networks has remained relatively
unexplored. Here we introduce and study an adaptive SIS epidemic model wherein
the infection and network coevolve with non-trivial probability to close
triangles during edge rewiring, leading to substantial reinforcement of network
transitivity. This new model provides a unique opportunity to study the role of
transitivity in altering the SIS dynamics on a coevolving network. Using
numerical simulations and Approximate Master Equations (AME), we identify and
examine a rich set of dynamical features in the new model. In many cases, the
AME including transitivity reinforcement provide accurate predictions of
stationary-state disease prevalence and network degree distributions.
Furthermore, for some parameter settings, the AME accurately trace the temporal
evolution of the system. We show that higher transitivity reinforcement in the
model leads to lower levels of infective individuals in the population; when
closing a triangle is the only rewiring mechanism. These methods and results
may be useful in developing ideas and modeling strategies for controlling SIS
type epidemics.
",1,0,0,0,0,0
17455,"A Counterexample to the Vector Generalization of Costa's EPI, and Partial Resolution","  We give a counterexample to the vector generalization of Costa's entropy
power inequality (EPI) due to Liu, Liu, Poor and Shamai. In particular, the
claimed inequality can fail if the matix-valued parameter in the convex
combination does not commute with the covariance of the additive Gaussian
noise. Conversely, the inequality holds if these two matrices commute.
",1,0,0,0,0,0
4843,DeepPermNet: Visual Permutation Learning,"  We present a principled approach to uncover the structure of visual data by
solving a novel deep learning task coined visual permutation learning. The goal
of this task is to find the permutation that recovers the structure of data
from shuffled versions of it. In the case of natural images, this task boils
down to recovering the original image from patches shuffled by an unknown
permutation matrix. Unfortunately, permutation matrices are discrete, thereby
posing difficulties for gradient-based methods. To this end, we resort to a
continuous approximation of these matrices using doubly-stochastic matrices
which we generate from standard CNN predictions using Sinkhorn iterations.
Unrolling these iterations in a Sinkhorn network layer, we propose DeepPermNet,
an end-to-end CNN model for this task. The utility of DeepPermNet is
demonstrated on two challenging computer vision problems, namely, (i) relative
attributes learning and (ii) self-supervised representation learning. Our
results show state-of-the-art performance on the Public Figures and OSR
benchmarks for (i) and on the classification and segmentation tasks on the
PASCAL VOC dataset for (ii).
",1,0,0,0,0,0
15173,Design Patterns for Fusion-Based Object Retrieval,"  We address the task of ranking objects (such as people, blogs, or verticals)
that, unlike documents, do not have direct term-based representations. To be
able to match them against keyword queries, evidence needs to be amassed from
documents that are associated with the given object. We present two design
patterns, i.e., general reusable retrieval strategies, which are able to
encompass most existing approaches from the past. One strategy combines
evidence on the term level (early fusion), while the other does it on the
document level (late fusion). We demonstrate the generality of these patterns
by applying them to three different object retrieval tasks: expert finding,
blog distillation, and vertical ranking.
",1,0,0,0,0,0
697,Clarifying the Hubble constant tension with a Bayesian hierarchical model of the local distance ladder,"  Estimates of the Hubble constant, $H_0$, from the distance ladder and the
cosmic microwave background (CMB) differ at the $\sim$3-$\sigma$ level,
indicating a potential issue with the standard $\Lambda$CDM cosmology.
Interpreting this tension correctly requires a model comparison calculation
depending on not only the traditional `$n$-$\sigma$' mismatch but also the
tails of the likelihoods. Determining the form of the tails of the local $H_0$
likelihood is impossible with the standard Gaussian least-squares
approximation, as it requires using non-Gaussian distributions to faithfully
represent anchor likelihoods and model outliers in the Cepheid and supernova
(SN) populations, and simultaneous fitting of the full distance-ladder dataset
to correctly propagate uncertainties. We have developed a Bayesian hierarchical
model that describes the full distance ladder, from nearby geometric anchors
through Cepheids to Hubble-Flow SNe. This model does not rely on any
distributions being Gaussian, allowing outliers to be modeled and obviating the
need for arbitrary data cuts. Sampling from the $\sim$3000-parameter joint
posterior using Hamiltonian Monte Carlo, we find $H_0$ = (72.72 $\pm$ 1.67)
${\rm km\,s^{-1}\,Mpc^{-1}}$ when applied to the outlier-cleaned Riess et al.
(2016) data, and ($73.15 \pm 1.78$) ${\rm km\,s^{-1}\,Mpc^{-1}}$ with SN
outliers reintroduced. Our high-fidelity sampling of the low-$H_0$ tail of the
distance-ladder likelihood allows us to apply Bayesian model comparison to
assess the evidence for deviation from $\Lambda$CDM. We set up this comparison
to yield a lower limit on the odds of the underlying model being $\Lambda$CDM
given the distance-ladder and Planck XIII (2016) CMB data. The odds against
$\Lambda$CDM are at worst 10:1 or 7:1, depending on whether the SNe outliers
are cut or modeled, or 60:1 if an approximation to the Planck Int. XLVI (2016)
likelihood is used.
",0,1,0,0,0,0
521,Towards a Service-oriented Platform for Intelligent Apps in Intermediate Cities,"  Smart cities are a growing trend in many cities in Argentina. In particular,
the so-called intermediate cities present a context and requirements different
from those of large cities with respect to smart cities. One aspect of
relevance is to encourage the development of applications (generally for mobile
devices) that enable citizens to take advantage of data and services normally
associated with the city, for example, in the urban mobility domain. In this
work, a platform is proposed for intermediate cities that provide ""high level""
services and that allow the construction of software applications that consume
those services. Our platform-centric strategy focused aims to integrate systems
and heterogeneous data sources, and provide ""intelligent"" services to different
applications. Examples of these services include: construction of user
profiles, recommending local events, and collaborative sensing based on data
mining techniques, among others. In this work, the design of this platform
(currently in progress) is described, and experiences of applications for urban
mobility are discussed, which are being migrated in the form of reusable
services provided by the platform
",1,0,0,0,0,0
20851,Acylindrical actions on projection complexes,"  We simplify the construction of projection complexes due to
Bestvina-Bromberg-Fujiwara. To do so, we introduce a sharper version of the
Behrstock inequality, and show that it can always be enforced. Furthermore, we
use the new setup to prove acylindricity results for the action on the
projection complexes. We also treat quasi-trees of metric spaces associated to
projection complexes, and prove an acylindricity criterion in that context as
well.
",0,0,1,0,0,0
8179,Deep Reinforcement Learning for Swarm Systems,"  Recently, deep reinforcement learning (RL) methods have been applied
successfully to multi-agent scenarios. Typically, these methods rely on a
concatenation of agent states to represent the information content required for
decentralized decision making. However, concatenation scales poorly to swarm
systems with a large number of homogeneous agents as it does not exploit the
fundamental properties inherent to these systems: (i) the agents in the swarm
are interchangeable and (ii) the exact number of agents in the swarm is
irrelevant. Therefore, we propose a new state representation for deep
multi-agent RL based on mean embeddings of distributions. We treat the agents
as samples of a distribution and use the empirical mean embedding as input for
a decentralized policy. We define different feature spaces of the mean
embedding using histograms, radial basis functions and a neural network learned
end-to-end. We evaluate the representation on two well known problems from the
swarm literature (rendezvous and pursuit evasion), in a globally and locally
observable setup. For the local setup we furthermore introduce simple
communication protocols. Of all approaches, the mean embedding representation
using neural network features enables the richest information exchange between
neighboring agents facilitating the development of more complex collective
strategies.
",1,0,0,1,0,0
963,Dynamics of a Camphoric Acid boat at the air-water interface,"  We report experiments on an agarose gel tablet loaded with camphoric acid
(c-boat) set into self-motion by interfacial tension gradients at the air-water
interface. We observe three distinct modes of c-boat motion: harmonic mode
where the c-boat speed oscillates sinusoidally in time, a steady mode where the
c-boat maintains constant speed, and a relaxation oscillation mode where the
c-boat maintains near-zero speed between sudden jumps in speed and position at
regular time intervals. Whereas all three modes have been separately reported
before in different systems, we show they belong to a common description.
Through control of the air-water surface tension with Sodium Dodecyl Sulfate
(SDS), we experimentally deduce the three self-propulsive modes result from
surface tension difference between Camphoric Acid (CA) and the ambient
surroundings.
",0,1,0,0,0,0
18823,Distributed Functional Observers for LTI Systems,"  We study the problem of designing distributed functional observers for LTI
systems. Specifically, we consider a setting consisting of a state vector that
evolves over time according to a dynamical process. A set of nodes distributed
over a communication network wish to collaboratively estimate certain functions
of the state. We first show that classical existence conditions for the design
of centralized functional observers do not directly translate to the
distributed setting, due to the coupling that exists between the dynamics of
the functions of interest and the diverse measurements at the various nodes.
Accordingly, we design transformations that reveal such couplings and identify
portions of the corresponding dynamics that are locally detectable at each
sensor node. We provide sufficient conditions on the network, along with state
estimate update and exchange rules for each node, that guarantee asymptotic
reconstruction of the functions at each sensor node.
",0,0,1,0,0,0
10000,The Signs in Elliptic Nets,"  We give a generalization of a theorem of Silverman and Stephens regarding the
signs in an elliptic divisibility sequence to the case of an elliptic net. We
also describe applications of this theorem in the study of the distribution of
the signs in elliptic nets and generating elliptic nets using the denominators
of the linear combination of points on elliptic curves.
",0,0,1,0,0,0
2141,Human-Level Intelligence or Animal-Like Abilities?,"  The vision systems of the eagle and the snake outperform everything that we
can make in the laboratory, but snakes and eagles cannot build an eyeglass or a
telescope or a microscope. (Judea Pearl)
",1,0,0,1,0,0
6226,A quantum dynamic belief model to explain the interference effects of categorization on decision making,"  Categorization is necessary for many decision making tasks. However, the
categorization process may interfere the decision making result and the law of
total probability can be violated in some situations. To predict the
interference effect of categorization, some model based on quantum probability
has been proposed. In this paper, a new quantum dynamic belief (QDB) model is
proposed. Considering the precise decision may not be made during the process,
the concept of uncertainty is introduced in our model to simulate real human
thinking process. Then the interference effect categorization can be predicted
by handling the uncertain information. The proposed model is applied to a
categorization decision-making experiment to explain the interference effect of
categorization. Compared with other models, our model is relatively more
succinct and the result shows the correctness and effectiveness of our model.
",1,0,0,0,0,0
2951,Collective Sedimentation of Squirmers under Gravity,"  Active particles, which interact hydrodynamically, display a remarkable
variety of emergent collective phenomena. We use squirmers to model spherical
microswimmers and explore the collective behavior of thousands of them under
the influence of strong gravity using the method of multi-particle collision
dynamics for simulating fluid flow. The sedimentation profile depends on the
ratio of swimming to sedimentation velocity as well as on the squirmer type. It
shows close packed squirmer layers at the bottom and a highly dynamic region
with exponential density dependence towards the top. The mean vertical
orientation of the squirmers strongly depends on height. For swimming
velocities larger than the sedimentation velocity, squirmers show strong
convection in the exponential region. We quantify the strength of convection
and the extent of convection cells by the vertical current density and its
current dipole, which are large for neutral squirmers as well as for weak
pushers and pullers.
",0,1,0,0,0,0
2302,Highly Efficient Human Action Recognition with Quantum Genetic Algorithm Optimized Support Vector Machine,"  In this paper we propose the use of quantum genetic algorithm to optimize the
support vector machine (SVM) for human action recognition. The Microsoft Kinect
sensor can be used for skeleton tracking, which provides the joints' position
data. However, how to extract the motion features for representing the dynamics
of a human skeleton is still a challenge due to the complexity of human motion.
We present a highly efficient features extraction method for action
classification, that is, using the joint angles to represent a human skeleton
and calculating the variance of each angle during an action time window. Using
the proposed representation, we compared the human action classification
accuracy of two approaches, including the optimized SVM based on quantum
genetic algorithm and the conventional SVM with grid search. Experimental
results on the MSR-12 dataset show that the conventional SVM achieved an
accuracy of $ 93.85\% $. The proposed approach outperforms the conventional
method with an accuracy of $ 96.15\% $.
",1,0,0,1,0,0
13203,New Algorithms for Unordered Tree Inclusion,"  The tree inclusion problem is, given two node-labeled trees $P$ and $T$ (the
""pattern tree"" and the ""text tree""), to locate every minimal subtree in $T$ (if
any) that can be obtained by applying a sequence of node insertion operations
to $P$. The ordered tree inclusion problem is known to be solvable in
polynomial time while the unordered tree inclusion problem is NP-hard. The
currently fastest algorithm for the latter is from 1995 and runs in
$O(poly(m,n) \cdot 2^{2d}) = O^{\ast}(4^{d})$ time, where $m$ and $n$ are the
sizes of the pattern and text trees, respectively, and $d$ is the degree of the
pattern tree. Here, we develop a new algorithm that improves the exponent $2d$
to $d$ by considering a particular type of ancestor-descendant relationships
and applying dynamic programming, thus reducing the time complexity to
$O^{\ast}(2^{d})$. We then study restricted variants of the unordered tree
inclusion problem where the number of occurrences of different node labels
and/or the input trees' heights are bounded and show that although the problem
remains NP-hard in many such cases, if the leaves of $P$ are distinctly labeled
and each label occurs at most $c$ times in $T$ then it can be solved in
polynomial time for $c = 2$ and in $O^{\ast}(1.8^d)$ time for $c = 3$.
",1,0,0,0,0,0
9513,Constrained Least Squares for Extended Complex Factor Analysis,"  For subspace estimation with an unknown colored noise, Factor Analysis (FA)
is a good candidate for replacing the popular eigenvalue decomposition (EVD).
Finding the unknowns in factor analysis can be done by solving a non-linear
least square problem. For this type of optimization problems, the Gauss-Newton
(GN) algorithm is a powerful and simple method. The most expensive part of the
GN algorithm is finding the direction of descent by solving a system of
equations at each iteration. In this paper we show that for FA, the matrices
involved in solving these systems of equations can be diagonalized in a closed
form fashion and the solution can be found in a computationally efficient way.
We show how the unknown parameters can be updated without actually constructing
these matrices. The convergence performance of the algorithm is studied via
numerical simulations.
",0,0,0,1,0,0
12115,Efficient Simulation of Temperature Evolution of Overhead Transmission Lines Based on Analytical Solution and NWP,"  Transmission lines are vital components in power systems. Tripping of
transmission lines caused by over-temperature is a major threat to the security
of system operations, so it is necessary to efficiently simulate line
temperature under both normal operation conditions and foreseen fault
conditions. Existing methods based on thermal-steady-state analyses cannot
reflect transient temperature evolution, and thus cannot provide timing
information needed for taking remedial actions. Moreover, conventional
numerical method requires huge computational efforts and barricades system-wide
analysis. In this regard, this paper derives an approximate analytical solution
of transmission-line temperature evolution enabling efficient analysis on
multiple operation states. Considering the uncertainties in environmental
parameters, the region of over-temperature is constructed in the environmental
parameter space to realize the over-temperature risk assessment in both the
planning stage and real-time operations. A test on a typical conductor model
verifies the accuracy of the approximate analytical solution. Based on the
analytical solution and numerical weather prediction (NWP) data, an efficient
simulation method for temperature evolution of transmission systems under
multiple operation states is proposed. As demonstrated on an NPCC 140-bus
system, it achieves over 1000 times of efficiency enhancement, verifying its
potentials in online risk assessment and decision support.
",1,0,0,0,0,0
14946,Counting Motifs with Graph Sampling,"  Applied researchers often construct a network from a random sample of nodes
in order to infer properties of the parent network. Two of the most widely used
sampling schemes are subgraph sampling, where we sample each vertex
independently with probability $p$ and observe the subgraph induced by the
sampled vertices, and neighborhood sampling, where we additionally observe the
edges between the sampled vertices and their neighbors.
In this paper, we study the problem of estimating the number of motifs as
induced subgraphs under both models from a statistical perspective. We show
that: for any connected $h$ on $k$ vertices, to estimate $s=\mathsf{s}(h,G)$,
the number of copies of $h$ in the parent graph $G$ of maximum degree $d$, with
a multiplicative error of $\epsilon$, (a) For subgraph sampling, the optimal
sampling ratio $p$ is $\Theta_{k}(\max\{ (s\epsilon^2)^{-\frac{1}{k}}, \;
\frac{d^{k-1}}{s\epsilon^{2}} \})$, achieved by Horvitz-Thompson type of
estimators. (b) For neighborhood sampling, we propose a family of estimators,
encompassing and outperforming the Horvitz-Thompson estimator and achieving the
sampling ratio $O_{k}(\min\{ (\frac{d}{s\epsilon^2})^{\frac{1}{k-1}}, \;
\sqrt{\frac{d^{k-2}}{s\epsilon^2}}\})$. This is shown to be optimal for all
motifs with at most $4$ vertices and cliques of all sizes.
The matching minimax lower bounds are established using certain algebraic
properties of subgraph counts. These results quantify how much more informative
neighborhood sampling is than subgraph sampling, as empirically verified by
experiments on both synthetic and real-world data. We also address the issue of
adaptation to the unknown maximum degree, and study specific problems for
parent graphs with additional structures, e.g., trees or planar graphs.
",0,0,0,1,0,0
5555,Topical homophily in online social systems,"  Understanding the dynamics of social interactions is crucial to comprehend
human behavior. The emergence of online social media has enabled access to data
regarding people relationships at a large scale. Twitter, specifically, is an
information oriented network, with users sharing and consuming information. In
this work, we study whether users tend to be in contact with people interested
in similar topics, i.e., topical homophily. To do so, we propose an approach
based on the use of hashtags to extract information topics from Twitter
messages and model users' interests. Our results show that, on average, users
are connected with other users similar to them and stronger relationships are
due to a higher topical similarity. Furthermore, we show that topical homophily
provides interesting information that can eventually allow inferring users'
connectivity. Our work, besides providing a way to assess the topical
similarity of users, quantifies topical homophily among individuals,
contributing to a better understanding of how complex social systems are
structured.
",1,0,0,0,0,0
2468,Development and Characterisation of a Gas System and its Associated Slow-Control System for an ATLAS Small-Strip Thin Gap Chamber Testing Facility,"  A quality assurance and performance qualification laboratory was built at
McGill University for the Canadian-made small-strip Thin Gap Chamber (sTGC)
muon detectors produced for the 2019-2020 ATLAS experiment muon spectrometer
upgrade. The facility uses cosmic rays as a muon source to ionise the quenching
gas mixture of pentane and carbon dioxide flowing through the sTGC detector. A
gas system was developed and characterised for this purpose, with a simple and
efficient gas condenser design utilizing a Peltier thermoelectric cooler (TEC).
The gas system was tested to provide the desired 45 vol% pentane concentration.
For continuous operations, a state-machine system was implemented with alerting
and remote monitoring features to run all cosmic-ray data-acquisition
associated slow-control systems, such as high/low voltage, gas system and
environmental monitoring, in a safe and continuous mode, even in the absence of
an operator.
",0,1,0,0,0,0
9214,Autoregressive Point-Processes as Latent State-Space Models: a Moment-Closure Approach to Fluctuations and Autocorrelations,"  Modeling and interpreting spike train data is a task of central importance in
computational neuroscience, with significant translational implications. Two
popular classes of data-driven models for this task are autoregressive Point
Process Generalized Linear models (PPGLM) and latent State-Space models (SSM)
with point-process observations. In this letter, we derive a mathematical
connection between these two classes of models. By introducing an auxiliary
history process, we represent exactly a PPGLM in terms of a latent, infinite
dimensional dynamical system, which can then be mapped onto an SSM by basis
function projections and moment closure. This representation provides a new
perspective on widely used methods for modeling spike data, and also suggests
novel algorithmic approaches to fitting such models. We illustrate our results
on a phasic bursting neuron model, showing that our proposed approach provides
an accurate and efficient way to capture neural dynamics.
",0,0,0,0,1,0
13977,Bubble size statistics during reionization from 21-cm tomography,"  The upcoming SKA1-Low radio interferometer will be sensitive enough to
produce tomographic imaging data of the redshifted 21-cm signal from the Epoch
of Reionization. Due to the non-Gaussian distribution of the signal, a power
spectrum analysis alone will not provide a complete description of its
properties. Here, we consider an additional metric which could be derived from
tomographic imaging data, namely the bubble size distribution of ionized
regions. We study three methods that have previously been used to characterize
bubble size distributions in simulation data for the hydrogen ionization
fraction - the spherical-average, mean-free-path and friends-of-friends methods
- and apply them to simulated 21-cm data cubes. Our simulated data cubes have
the (sensitivity-dictated) resolution expected for the SKA1-Low reionization
experiment and we study the impact of both the light-cone and redshift space
distortion effects. To identify ionized regions in the 21-cm data we introduce
a new, self-adjusting thresholding approach based on the K-Means algorithm. We
find that the fraction of ionized cells identified in this way consistently
falls below the mean volume-averaged ionized fraction. From a comparison of the
three bubble size methods, we conclude that all three methods are useful, but
that the mean-free-path method performs best in terms of tracking the progress
of reionization and separating different reionization scenarios. The light-cone
effect is found to affect data spanning more than about 10~MHz in frequency
($\Delta z\sim0.5$). We find that redshift space distortions only marginally
affect the bubble size distributions.
",0,1,0,0,0,0
11283,Reduction and specialization of hyperelliptic continued fractions,"  For a monic polynomial $D(X)$ of even degree, express $\sqrt D$ as a Laurent
series in $X^{-1}$; this yields a continued fraction expansion (similar to
continued fractions of real numbers): \[\sqrt
D=a_0+\dfrac{1}{a_1+\dfrac{1}{a_2+\dfrac{1}{\ddots}}},\quad a_i\text{
polynomials in }X.\] Such continued fractions were first considered by Abel in
1826, and later by Chebyshev. It turns out they are rarely periodic unless $D$
is defined over a finite field.
Around 2001 van der Poorten studied non-periodic continued fractions of
$\sqrt D$, with $D$ defined over the rationals, and simultaneously the
continued fraction of $\sqrt D$ modulo a suitable prime $p$; the latter
continued fraction is automatically periodic. He found that one recovers all
the convergents (rational function approximations to $\sqrt D$ obtained by
cutting off the continued fraction) of $\sqrt D \mod{p}$ by appropriately
normalising and then reducing the convergents of $\sqrt D$.
By developing a general specialization theory for continued fractions of
Laurent series, I produced a rigorous proof of this result stated by van der
Poorten and further was able to show the following:
If $D$ is defined over the rationals and the continued fraction of $\sqrt D$
is non-periodic, then for all but finitely many primes $p \in \mathbb Z$, this
prime $p$ occurs in the denominator of the leading coefficient of infinitely
many $a_i$.
For $\mathrm{deg}\,D = 4$, I can even give a description of the orders in
which the prime appears, and the $p$-adic Gauss norms of the $a_i$ and the
convergents. These results also generalise to number fields.
Moreover, I derive optimised formulae for computing quadratic continued
fractions, along with several example expansions. I discuss a few known results
on the heights of the convergents, and explain some relations with the
reduction of hyperelliptic curves and Jacobians.
",0,0,1,0,0,0
4853,Exponentiated Generalized Pareto Distribution: Properties and applications towards Extreme Value Theory,"  The Generalized Pareto Distribution (GPD) plays a central role in modelling
heavy tail phenomena in many applications. Applying the GPD to actual datasets
however is a non-trivial task. One common way suggested in the literature to
investigate the tail behaviour is to take logarithm to the original dataset in
order to reduce the sample variability. Inspired by this, we propose and study
the Exponentiated Generalized Pareto Distribution (exGPD), which is created via
log-transform of the GPD variable. After introducing the exGPD we derive
various distributional quantities, including the moment generating function,
tail risk measures. As an application we also develop a plot as an alternative
to the Hill plot to identify the tail index of heavy tailed datasets, based on
the moment matching for the exGPD. Various numerical analyses with both
simulated and actual datasets show that the proposed plot works well.
",0,0,1,1,0,0
20920,Symmetries and synchronization in multilayer random networks,"  In the light of the recently proposed scenario of asymmetry-induced
synchronization (AISync), in which dynamical uniformity and consensus in a
distributed system would demand certain asymmetries in the underlying network,
we investigate here the influence of some regularities in the interlayer
connection patterns on the synchronization properties of multilayer random
networks. More specifically, by considering a Stuart-Landau model of complex
oscillators with random frequencies, we report for multilayer networks a
dynamical behavior that could be also classified as a manifestation of AISync.
We show, namely, that the presence of certain symmetries in the interlayer
connection pattern tends to diminish the synchronization capability of the
whole network or, in other words, asymmetries in the interlayer connections
would enhance synchronization in such structured networks. Our results might
help the understanding not only of the AISync mechanism itself, but also its
possible role in the determination of the interlayer connection pattern of
multilayer and other structured networks with optimal synchronization
properties.
",0,1,0,0,0,0
3143,Investigation of Using VAE for i-Vector Speaker Verification,"  New system for i-vector speaker recognition based on variational autoencoder
(VAE) is investigated. VAE is a promising approach for developing accurate deep
nonlinear generative models of complex data. Experiments show that VAE provides
speaker embedding and can be effectively trained in an unsupervised manner. LLR
estimate for VAE is developed. Experiments on NIST SRE 2010 data demonstrate
its correctness. Additionally, we show that the performance of VAE-based system
in the i-vectors space is close to that of the diagonal PLDA. Several
interesting results are also observed in the experiments with $\beta$-VAE. In
particular, we found that for $\beta\ll 1$, VAE can be trained to capture the
features of complex input data distributions in an effective way, which is hard
to obtain in the standard VAE ($\beta=1$).
",1,0,0,1,0,0
14668,"Valued fields, Metastable groups","  We introduce a class of theories called metastable, including the theory of
algebraically closed valued fields (ACVF) as a motivating example. The key
local notion is that of definable types dominated by their stable part. A
theory is metastable (over a sort $\Gamma$) if every type over a sufficiently
rich base structure can be viewed as part of a $\Gamma$-parametrized family of
stably dominated types. We initiate a study of definable groups in metastable
theories of finite rank. Groups with a stably dominated generic type are shown
to have a canonical stable quotient. Abelian groups are shown to be
decomposable into a part coming from $\Gamma$, and a definable direct limit
system of groups with stably dominated generic. In the case of ACVF, among
definable subgroups of affine algebraic groups, we characterize the groups with
stably dominated generics in terms of group schemes over the valuation ring.
Finally, we classify all fields definable in ACVF.
",0,0,1,0,0,0
14554,Spatial risk measures induced by powers of max-stable random fields,"  A meticulous assessment of the risk of extreme environmental events is of
great necessity for populations, civil authorities as well as the
insurance/reinsurance industry. Koch (2017, 2018) introduced a concept of
spatial risk measure and a related set of axioms which are well-suited to
analyse and quantify the risk due to events having a spatial extent, precisely
such as natural disasters. In this paper, we first carry out a detailed study
of the correlation (and covariance) structure of powers of the Smith and
Brown-Resnick max-stable random fields. Then, using the latter results, we
thoroughly investigate spatial risk measures associated with variance and
induced by powers of max-stable random fields. In addition, we show that
spatial risk measures associated with several classical risk measures and
induced by such cost fields satisfy (at least) part of the previously mentioned
axioms under appropriate conditions on the max-stable fields. Considering such
cost fields is particularly relevant when studying the impact of extreme wind
speeds on buildings and infrastructure.
",0,0,0,0,0,1
3951,Improved Speech Reconstruction from Silent Video,"  Speechreading is the task of inferring phonetic information from visually
observed articulatory facial movements, and is a notoriously difficult task for
humans to perform. In this paper we present an end-to-end model based on a
convolutional neural network (CNN) for generating an intelligible and
natural-sounding acoustic speech signal from silent video frames of a speaking
person. We train our model on speakers from the GRID and TCD-TIMIT datasets,
and evaluate the quality and intelligibility of reconstructed speech using
common objective measurements. We show that speech predictions from the
proposed model attain scores which indicate significantly improved quality over
existing models. In addition, we show promising results towards reconstructing
speech from an unconstrained dictionary.
",1,0,0,0,0,0
1291,Accelerating Discrete Wavelet Transforms on GPUs,"  The two-dimensional discrete wavelet transform has a huge number of
applications in image-processing techniques. Until now, several papers compared
the performance of such transform on graphics processing units (GPUs). However,
all of them only dealt with lifting and convolution computation schemes. In
this paper, we show that corresponding horizontal and vertical lifting parts of
the lifting scheme can be merged into non-separable lifting units, which halves
the number of steps. We also discuss an optimization strategy leading to a
reduction in the number of arithmetic operations. The schemes were assessed
using the OpenCL and pixel shaders. The proposed non-separable lifting scheme
outperforms the existing schemes in many cases, irrespective of its higher
complexity.
",1,0,0,0,0,0
4984,Measuring Cognitive Conflict in Virtual Reality with Feedback-Related Negativity,"  As virtual reality (VR) emerges as a mainstream platform, designers have
started to experiment new interaction techniques to enhance the user
experience. This is a challenging task because designers not only strive to
provide designs with good performance but also carefully ensure not to disrupt
users' immersive experience. There is a dire need for a new evaluation tool
that extends beyond traditional quantitative measurements to assist designers
in the design process. We propose an EEG-based experiment framework that
evaluates interaction techniques in VR by measuring intentionally elicited
cognitive conflict. Through the analysis of the feedback-related negativity
(FRN) as well as other quantitative measurements, this framework allows
designers to evaluate the effect of the variables of interest. We studied the
framework by applying it to the fundamental task of 3D object selection using
direct 3D input, i.e. tracked hand in VR. The cognitive conflict is
intentionally elicited by manipulating the selection radius of the target
object. Our first behavior experiment validated the framework in line with the
findings of conflict-induced behavior adjustments like those reported in other
classical psychology experiment paradigms. Our second EEG-based experiment
examines the effect of the appearance of virtual hands. We found that the
amplitude of FRN correlates with the level of realism of the virtual hands,
which concurs with the Uncanny Valley theory.
",1,0,0,0,0,0
7718,Rethinking probabilistic prediction in the wake of the 2016 U.S. presidential election,"  To many statisticians and citizens, the outcome of the most recent U.S.
presidential election represents a failure of data-driven methods on the
grandest scale. This impression has led to much debate and discussion about how
the election predictions went awry -- Were the polls inaccurate? Were the
models wrong? Did we misinterpret the probabilities? -- and how they went right
-- Perhaps the analyses were correct even though the predictions were wrong,
that's just the nature of probabilistic forecasting. With this in mind, we
analyze the election outcome with respect to a core set of effectiveness
principles. Regardless of whether and how the election predictions were right
or wrong, we argue that they were ineffective in conveying the extent to which
the data was informative of the outcome and the level of uncertainty in making
these assessments. Among other things, our analysis sheds light on the
shortcomings of the classical interpretations of probability and its
communication to consumers in the form of predictions. We present here an
alternative approach, based on a notion of validity, which offers two immediate
insights for predictive inference. First, the predictions are more
conservative, arguably more realistic, and come with certain guarantees on the
probability of an erroneous prediction. Second, our approach easily and
naturally reflects the (possibly substantial) uncertainty about the model by
outputting plausibilities instead of probabilities. Had these simple steps been
taken by the popular prediction outlets, the election outcome may not have been
so shocking.
",0,0,1,1,0,0
3459,Controlling light in complex media beyond the acoustic diffraction-limit using the acousto-optic transmission matrix,"  Studying the internal structure of complex samples with light is an important
task, but a difficult challenge due to light scattering. While the complex
optical distortions induced by multiple scattering can be effectively undone
with the knowledge of the medium's scattering-matrix, this matrix is generally
unknown, and cannot be measured with high resolution without the presence of
fluorescent or absorbing probes at all points of interest. To overcome these
limitations, we introduce here the concept of the acousto-optic transmission
matrix (AOTM). Taking advantage of the near scattering-free propagation of
ultrasound in complex samples, we noninvasively measure an
ultrasonically-encoded, spatially-resolved, optical scattering-matrix. We
demonstrate that a singular value decomposition analysis of the AOTM, acquired
using a single or multiple ultrasonic beams, allows controlled optical focusing
beyond the acoustic diffraction limit in scattering media. Our approach
provides a generalized framework for analyzing acousto-optical experiments, and
for noninvasive, high-resolution study of complex media.
",0,1,0,0,0,0
5571,Rational Solutions of the Painlevé-II Equation Revisited,"  The rational solutions of the Painlevé-II equation appear in several
applications and are known to have many remarkable algebraic and analytic
properties. They also have several different representations, useful in
different ways for establishing these properties. In particular,
Riemann-Hilbert representations have proven to be useful for extracting the
asymptotic behavior of the rational solutions in the limit of large degree
(equivalently the large-parameter limit). We review the elementary properties
of the rational Painlevé-II functions, and then we describe three different
Riemann-Hilbert representations of them that have appeared in the literature: a
representation by means of the isomonodromy theory of the Flaschka-Newell Lax
pair, a second representation by means of the isomonodromy theory of the
Jimbo-Miwa Lax pair, and a third representation found by Bertola and Bothner
related to pseudo-orthogonal polynomials. We prove that the Flaschka-Newell and
Bertola-Bothner Riemann-Hilbert representations of the rational Painlevé-II
functions are explicitly connected to each other. Finally, we review recent
results describing the asymptotic behavior of the rational Painlevé-II
functions obtained from these Riemann-Hilbert representations by means of the
steepest descent method.
",0,1,1,0,0,0
12706,Modeling Temporally Evolving and Spatially Globally Dependent Data,"  The last decades have seen an unprecedented increase in the availability of
data sets that are inherently global and temporally evolving, from remotely
sensed networks to climate model ensembles. This paper provides a view of
statistical modeling techniques for space-time processes, where space is the
sphere representing our planet. In particular, we make a distintion between (a)
second order-based, and (b) practical approaches to model temporally evolving
global processes. The former are based on the specification of a class of
space-time covariance functions, with space being the two-dimensional sphere.
The latter are based on explicit description of the dynamics of the space-time
process, i.e., by specifying its evolution as a function of its past history
with added spatially dependent noise.
We especially focus on approach (a), where the literature has been sparse. We
provide new models of space-time covariance functions for random fields defined
on spheres cross time. Practical approaches, (b), are also discussed, with
special emphasis on models built directly on the sphere, without projecting the
spherical coordinate on the plane.
We present a case study focused on the analysis of air pollution from the
2015 wildfires in Equatorial Asia, an event which was classified as the year's
worst environmental disaster. The paper finishes with a list of the main
theoretical and applied research problems in the area, where we expect the
statistical community to engage over the next decade.
",0,0,1,1,0,0
4982,Nonparametric regression using deep neural networks with ReLU activation function,"  Consider the multivariate nonparametric regression model. It is shown that
estimators based on sparsely connected deep neural networks with ReLU
activation function and properly chosen network architecture achieve the
minimax rates of convergence (up to log n-factors) under a general composition
assumption on the regression function. The framework includes many well-studied
structural constraints such as (generalized) additive models. While there is a
lot of flexibility in the network architecture, the tuning parameter is the
sparsity of the network. Specifically, we consider large networks with number
of potential network parameters exceeding the sample size. The analysis gives
some insights why multilayer feedforward neural networks perform well in
practice. Interestingly, the depth (number of layers) of the neural network
architectures plays an important role and our theory suggests that for
nonparametric regression scaling the network depth with the logarithm of the
sample size is natural. It is also shown that under the composition assumption
wavelet estimators can only achieve suboptimal rates.
",1,0,1,1,0,0
13166,Realization of functions on the symmetrized bidisc,"  We prove a realization formula and a model formula for analytic functions
with modulus bounded by $1$ on the symmetrized bidisc \[ G\stackrel{\rm def}{=}
\{(z+w,zw): |z|<1, \, |w| < 1\}. \] As an application we prove a Pick-type
theorem giving a criterion for the existence of such a function satisfying a
finite set of interpolation conditions.
",0,0,1,0,0,0
12913,Contextual Stochastic Block Models,"  We provide the first information theoretic tight analysis for inference of
latent community structure given a sparse graph along with high dimensional
node covariates, correlated with the same latent communities. Our work bridges
recent theoretical breakthroughs in the detection of latent community structure
without nodes covariates and a large body of empirical work using diverse
heuristics for combining node covariates with graphs for inference. The
tightness of our analysis implies in particular, the information theoretical
necessity of combining the different sources of information. Our analysis holds
for networks of large degrees as well as for a Gaussian version of the model.
",1,0,0,1,0,0
6468,The discrete moment problem with nonconvex shape constraints,"  The discrete moment problem is a foundational problem in distribution-free
robust optimization, where the goal is to find a worst-case distribution that
satisfies a given set of moments. This paper studies the discrete moment
problems with additional ""shape constraints"" that guarantee the worst case
distribution is either log-concave or has an increasing failure rate. These
classes of shape constraints have not previously been studied in the
literature, in part due to their inherent nonconvexities. Nonetheless, these
classes of distributions are useful in practice. We characterize the structure
of optimal extreme point distributions by developing new results in reverse
convex optimization, a lesser-known tool previously employed in designing
global optimization algorithms. We are able to show, for example, that an
optimal extreme point solution to a moment problem with $m$ moments and
log-concave shape constraints is piecewise geometric with at most $m$ pieces.
Moreover, this structure allows us to design an exact algorithm for computing
optimal solutions in a low-dimensional space of parameters. Moreover, We
describe a computational approach to solving these low-dimensional problems,
including numerical results for a representative set of instances.
",0,0,1,1,0,0
6099,Probabilistic Sensor Fusion for Ambient Assisted Living,"  There is a widely-accepted need to revise current forms of health-care
provision, with particular interest in sensing systems in the home. Given a
multiple-modality sensor platform with heterogeneous network connectivity, as
is under development in the Sensor Platform for HEalthcare in Residential
Environment (SPHERE) Interdisciplinary Research Collaboration (IRC), we face
specific challenges relating to the fusion of the heterogeneous sensor
modalities.
We introduce Bayesian models for sensor fusion, which aims to address the
challenges of fusion of heterogeneous sensor modalities. Using this approach we
are able to identify the modalities that have most utility for each particular
activity, and simultaneously identify which features within that activity are
most relevant for a given activity.
We further show how the two separate tasks of location prediction and
activity recognition can be fused into a single model, which allows for
simultaneous learning an prediction for both tasks.
We analyse the performance of this model on data collected in the SPHERE
house, and show its utility. We also compare against some benchmark models
which do not have the full structure,and show how the proposed model compares
favourably to these methods
",1,0,0,1,0,0
17645,Physics-Based Modeling of TID Induced Global Static Leakage in Different CMOS Circuits,"  Compact modeling of inter-device radiation-induced leakage underneath the
gateless thick STI oxide is presented and validated taking into account CMOS
technology and hardness parameters, dose-rate and annealing effects, and
dependence on electric modes under irradiation. It was shown that proposed
approach can be applied for description of dose dependent static leakage
currents in complex FPGA circuits.
",0,1,0,0,0,0
8969,Supervised Hashing based on Energy Minimization,"  Recently, supervised hashing methods have attracted much attention since they
can optimize retrieval speed and storage cost while preserving semantic
information. Because hashing codes learning is NP-hard, many methods resort to
some form of relaxation technique. But the performance of these methods can
easily deteriorate due to the relaxation. Luckily, many supervised hashing
formulations can be viewed as energy functions, hence solving hashing codes is
equivalent to learning marginals in the corresponding conditional random field
(CRF). By minimizing the KL divergence between a fully factorized distribution
and the Gibbs distribution of this CRF, a set of consistency equations can be
obtained, but updating them in parallel may not yield a local optimum since the
variational lower bound is not guaranteed to increase. In this paper, we use a
linear approximation of the sigmoid function to convert these consistency
equations to linear systems, which have a closed-form solution. By applying
this novel technique to two classical hashing formulations KSH and SPLH, we
obtain two new methods called EM (energy minimizing based)-KSH and EM-SPLH.
Experimental results on three datasets show the superiority of our methods.
",1,0,0,1,0,0
18057,Asai cube L-functions and the local Langlands conjecture,"  Let $F$ be a non-archimedean locally compact field. We study a class of
Langlands-Shahidi pairs $({\bf H},{\bf L})$, consisting of a quasi-split
connected reductive group $\bf H$ over $F$ and a Levi subgroup $\bf L$ which is
closely related to a product of restriction of scalars of ${\rm GL}_1$'s or
${\rm GL}_2$'s. We prove the compatibility of the resulting local factors with
the Langlands correspondence. In particular, let $E$ be a cubic separable
extension of $F$. We consider a simply connected quasi-split semisimple group
$\bf H$ over $F$ of type $D_4$, with triality corresponding to $E$, and let
$\bf L$ be its Levi subgroup with derived group ${\rm Res}_{E/F} {\rm SL}_2$.
In this way we obtain Asai cube local factors attached to irreducible smooth
representations of ${\rm GL}_2(E)$; we prove that they are Weil-Deligne factors
obtained via the local Langlands correspondence for ${\rm GL}_2(E)$ and tensor
induction from $E$ to $F$. A consequence is that Asai cube $\gamma$- and
$\varepsilon$-factors become stable under twists by highly ramified characters.
",0,0,1,0,0,0
20893,The COM-negative binomial distribution: modeling overdispersion and ultrahigh zero-inflated count data,"  In this paper, we focus on the COM-type negative binomial distribution with
three parameters, which belongs to COM-type $(a,b,0)$ class distributions and
family of equilibrium distributions of arbitrary birth-death process. Besides,
we show abundant distributional properties such as overdispersion and
underdispersion, log-concavity, log-convexity (infinite divisibility), pseudo
compound Poisson, stochastic ordering and asymptotic approximation. Some
characterizations including sum of equicorrelated geometrically distributed
random variables, conditional distribution, limit distribution of COM-negative
hypergeometric distribution, and Stein's identity are given for theoretical
properties. COM-negative binomial distribution was applied to overdispersion
and ultrahigh zero-inflated data sets. With the aid of ratio regression, we
employ maximum likelihood method to estimate the parameters and the
goodness-of-fit are evaluated by the discrete Kolmogorov-Smirnov test.
",0,0,1,1,0,0
11765,On The Asymptotic Efficiency of Selection Procedures for Independent Gaussian Populations,"  The field of discrete event simulation and optimization techniques motivates
researchers to adjust classic ranking and selection (R&S) procedures to the
settings where the number of populations is large. We use insights from extreme
value theory in order to reveal the asymptotic properties of R&S procedures.
Namely, we generalize the asymptotic result of Robbins and Siegmund regarding
selection from independent Gaussian populations with known constant variance by
their means to the case of selecting a subset of varying size out of a given
set of populations. In addition, we revisit the problem of selecting the
population with the highest mean among independent Gaussian populations with
unknown and possibly different variances. Particularly, we derive the relative
asymptotic efficiency of Dudewicz and Dalal's and Rinott's procedures, showing
that the former can be asymptotically superior by a multiplicative factor which
is larger than one, but this factor may be reduced by proper choice of
parameters. We also use our asymptotic results to suggest that the sample size
in the first stage of the two procedures should be logarithmic in the number of
populations.
",0,0,1,1,0,0
8467,Phase locking the spin precession in a storage ring,"  This letter reports the successful use of feedback from a spin polarization
measurement to the revolution frequency of a 0.97 GeV/$c$ bunched and polarized
deuteron beam in the Cooler Synchrotron (COSY) storage ring in order to control
both the precession rate ($\approx 121$ kHz) and the phase of the horizontal
polarization component. Real time synchronization with a radio frequency (rf)
solenoid made possible the rotation of the polarization out of the horizontal
plane, yielding a demonstration of the feedback method to manipulate the
polarization. In particular, the rotation rate shows a sinusoidal function of
the horizontal polarization phase (relative to the rf solenoid), which was
controlled to within a one standard deviation range of $\sigma = 0.21$ rad. The
minimum possible adjustment was 3.7 mHz out of a revolution frequency of 753
kHz, which changes the precession rate by 26 mrad/s. Such a capability meets a
requirement for the use of storage rings to look for an intrinsic electric
dipole moment of charged particles.
",0,1,0,0,0,0
1693,Freeness and The Partial Transposes of Wishart Random Matrices,"  We show that the partial transposes of complex Wishart random matrices are
asymptotically free. We also investigate regimes where the number of blocks is
fixed but the size of the blocks increases. This gives a example where the
partial transpose produces freeness at the operator level. Finally we
investigate the case of real Wishart matrices.
",0,0,1,0,0,0
11224,"TRINITY: Coordinated Performance, Energy and Temperature Management in 3D Processor-Memory Stacks","  The consistent demand for better performance has lead to innovations at
hardware and microarchitectural levels. 3D stacking of memory and logic dies
delivers an order of magnitude improvement in available memory bandwidth. The
price paid however is, tight thermal constraints.
In this paper, we study the complex multiphysics interactions between
performance, energy and temperature. Using a cache coherent multicore processor
cycle level simulator coupled with power and thermal estimation tools, we
investigate the interactions between (a) thermal behaviors (b) compute and
memory microarchitecture and (c) application workloads. The key insights from
this exploration reveal the need to manage performance, energy and temperature
in a coordinated fashion. Furthermore, we identify the concept of ""effective
heat capacity"" i.e. the heat generated beyond which no further gains in
performance is observed with increases in voltage-frequency of the compute
logic. Subsequently, a real-time, numerical optimization based, application
agnostic controller (TRINITY) is developed which intelligently manages the
three parameters of interest. We observe up to $30\%$ improvement in Energy
Delay$^2$ Product and up to $8$ Kelvin lower core temperatures as compared to
fixed frequencies. Compared to the \texttt{ondemand} Linux CPU DVFS governor,
for similar energy efficiency, TRINITY keeps the cores cooler by $6$ Kelvin
which increases the lifetime reliability by up to 59\%.
",1,0,0,0,0,0
12089,Optimal transport and integer partitions,"  We link the theory of optimal transportation to the theory of integer
partitions. Let $\mathscr P(n)$ denote the set of integer partitions of $n \in
\mathbb N$ and write partitions $\pi \in \mathscr P(n)$ as $(n_1, \dots,
n_{k(\pi)})$. Using terminology from optimal transport, we characterize certain
classes of partitions like symmetric partitions and those in Euler's identity
$|\{ \pi \in \mathscr P(n) |$ all $ n_i $ distinct $ \} | = | \{ \pi \in
\mathscr P(n) | $ all $ n_i $ odd $ \}|$.
Then we sketch how optimal transport might help to understand higher
dimensional partitions.
",0,0,1,0,0,0
19433,On dimension-free variational inequalities for averaging operators in $\mathbb R^d$,"  We study dimension-free $L^p$ inequalities for $r$-variations of the
Hardy--Littlewood averaging operators defined over symmetric convex bodies in
$\mathbb R^d$.
",0,0,1,0,0,0
3942,Central Moment Discrepancy (CMD) for Domain-Invariant Representation Learning,"  The learning of domain-invariant representations in the context of domain
adaptation with neural networks is considered. We propose a new regularization
method that minimizes the discrepancy between domain-specific latent feature
representations directly in the hidden activation space. Although some standard
distribution matching approaches exist that can be interpreted as the matching
of weighted sums of moments, e.g. Maximum Mean Discrepancy (MMD), an explicit
order-wise matching of higher order moments has not been considered before. We
propose to match the higher order central moments of probability distributions
by means of order-wise moment differences. Our model does not require
computationally expensive distance and kernel matrix computations. We utilize
the equivalent representation of probability distributions by moment sequences
to define a new distance function, called Central Moment Discrepancy (CMD). We
prove that CMD is a metric on the set of probability distributions on a compact
interval. We further prove that convergence of probability distributions on
compact intervals w.r.t. the new metric implies convergence in distribution of
the respective random variables. We test our approach on two different
benchmark data sets for object recognition (Office) and sentiment analysis of
product reviews (Amazon reviews). CMD achieves a new state-of-the-art
performance on most domain adaptation tasks of Office and outperforms networks
trained with MMD, Variational Fair Autoencoders and Domain Adversarial Neural
Networks on Amazon reviews. In addition, a post-hoc parameter sensitivity
analysis shows that the new approach is stable w.r.t. parameter changes in a
certain interval. The source code of the experiments is publicly available.
",0,0,0,1,0,0
20815,"Finding structure in the dark: coupled dark energy, weak lensing, and the mildly nonlinear regime","  We reexamine interactions between the dark sectors of cosmology, with a focus
on robust constraints that can be obtained using only mildly nonlinear scales.
While it is well known that couplings between dark matter and dark energy can
be constrained to the percent level when including the full range of scales
probed by future optical surveys, calibrating matter power spectrum emulators
to all possible choices of potentials and couplings requires many
computationally expensive n-body simulations. Here we show that lensing and
clustering of galaxies in combination with the Cosmic Microwave Background
(CMB) is capable of probing the dark sector coupling to the few percent level
for a given class of models, using only linear and quasi-linear Fourier modes.
These scales can, in principle, be described by semi-analytical techniques such
as the effective field theory of large-scale structure.
",0,1,0,0,0,0
7553,Robust Inference under the Beta Regression Model with Application to Health Care Studies,"  Data on rates, percentages or proportions arise frequently in many different
applied disciplines like medical biology, health care, psychology and several
others. In this paper, we develop a robust inference procedure for the beta
regression model which is used to describe such response variables taking
values in $(0, 1)$ through some related explanatory variables. In relation to
the beta regression model, the issue of robustness has been largely ignored in
the literature so far. The existing maximum likelihood based inference has
serious lack of robustness against outliers in data and generate drastically
different (erroneous) inference in presence of data contamination. Here, we
develop the robust minimum density power divergence estimator and a class of
robust Wald-type tests for the beta regression model along with several
applications. We derive their asymptotic properties and describe their
robustness theoretically through the influence function analyses. Finite sample
performances of the proposed estimators and tests are examined through suitable
simulation studies and real data applications in the context of health care and
psychology. Although we primarily focus on the beta regression models with a
fixed dispersion parameter, some indications are also provided for extension to
the variable dispersion beta regression models with an application.
",0,0,0,1,0,0
15413,Quantum Multicriticality near the Dirac-Semimetal to Band-Insulator Critical Point in Two Dimensions: A Controlled Ascent from One Dimension,"  We compute the effects of generic short-range interactions on gapless
electrons residing at the quantum critical point separating a two-dimensional
Dirac semimetal (DSM) and a symmetry-preserving band insulator (BI). The
electronic dispersion at this critical point is anisotropic ($E_{\mathbf k}=\pm
\sqrt{v^2 k^2_x + b^2 k^{2n}_y}$ with $n=2$), which results in unconventional
scaling of physical observables. Due to the vanishing density of states
($\varrho(E) \sim |E|^{1/n}$), this anisotropic semimetal (ASM) is stable
against weak short-range interactions. However, for stronger interactions the
direct DSM-BI transition can either $(i)$ become a first-order transition, or
$(ii)$ get avoided by an intervening broken-symmetry phase (BSP). We perform a
renormalization group analysis by perturbing away from the one-dimensional
limit with the small parameter $\epsilon = 1/n$, augmented with a $1/n$
expansion (parametrically suppressing quantum fluctuations in higher
dimension). We identify charge density wave (CDW), antiferromagnet (AFM) and
singlet s-wave superconductor as the three dominant candidates for the BSP. The
onset of any such order at strong coupling $(\sim \epsilon)$ takes place
through a continuous quantum phase transition across multicritical point. We
also present the phase diagram of an extended Hubbard model for the ASM,
obtained via the controlled deformation of its counterpart in one dimension.
The latter displays spin-charge separation and instabilities to CDW, spin
density wave, and Luther-Emery liquid phases at arbitrarily weak coupling. The
spin density wave and Luther-Emery liquid phases deform into pseudospin
SU(2)-symmetric quantum critical points separating the ASM from the AFM and
superconducting orders, respectively. Our results can be germane for a
uniaxially strained honeycomb lattice or organic compound
$\alpha$-(BEDT-TTF)$_2\text{I}_3$.
",0,1,0,0,0,0
11569,A lower bound of the hyperbolic dimension for meromorphic functions having a logarithmic Hölder tract,"  We improve existing lower bounds of the hyperbolic dimension for meromophic
functions that have a logarithmic tract {\Omega} which is a Hölder domain.
These bounds are given in terms of the fractal behavior, measured with integral
means, of the boundary of {\Omega} at infinity.
",0,0,1,0,0,0
12526,Temporal Graph Offset Reconstruction: Towards Temporally Robust Graph Representation Learning,"  Graphs are a commonly used construct for representing relationships between
elements in complex high dimensional datasets. Many real-world phenomenon are
dynamic in nature, meaning that any graph used to represent them is inherently
temporal. However, many of the machine learning models designed to capture
knowledge about the structure of these graphs ignore this rich temporal
information when creating representations of the graph. This results in models
which do not perform well when used to make predictions about the future state
of the graph -- especially when the delta between time stamps is not small. In
this work, we explore a novel training procedure and an associated unsupervised
model which creates graph representations optimised to predict the future state
of the graph. We make use of graph convolutional neural networks to encode the
graph into a latent representation, which we then use to train our temporal
offset reconstruction method, inspired by auto-encoders, to predict a later
time point -- multiple time steps into the future. Using our method, we
demonstrate superior performance for the task of future link prediction
compared with none-temporal state-of-the-art baselines. We show our approach to
be capable of outperforming non-temporal baselines by 38% on a real world
dataset.
",1,0,0,0,0,0
2267,Satellite altimetry reveals spatial patterns of variations in the Baltic Sea wave climate,"  The main properties of the climate of waves in the seasonally ice-covered
Baltic Sea and its decadal changes since 1990 are estimated from satellite
altimetry data. The data set of significant wave heights (SWH) from all
existing nine satellites, cleaned and cross-validated against in situ
measurements, shows overall a very consistent picture. A comparison with visual
observations shows a good correspondence with correlation coefficients of
0.6-0.8. The annual mean SWH reveals a tentative increase of 0.005 m yr-1, but
higher quantiles behave in a cyclic manner with a timescale of 10-15 yr.
Changes in the basin-wide average SWH have a strong meridional pattern: an
increase in the central and western parts of the sea and decrease in the east.
This pattern is likely caused by a rotation of wind directions rather than by
an increase in the wind speed.
",0,1,0,0,0,0
11898,Generalized Theta Functions. I,"  Generalizations of classical theta functions are proposed that include any
even number of analytic parameters for which conditions of quasi-periodicity
are fulfilled and that are representations of extended Heisenberg group.
Differential equations for generalized theta functions and finite non-unitary
representations of extended Heisenberg group are presented so as other
properties and possible applications are pointed out such as a projective
embedding of tori by means of generalized theta functions.
",0,1,0,0,0,0
2268,Estimation of Covariance Matrices for Portfolio Optimization using Gaussian Processes,"  Estimating covariances between financial assets plays an important role in
risk management and optimal portfolio allocation. In practice, when the sample
size is small compared to the number of variables, i.e. when considering a wide
universe of assets over just a few years, this poses considerable challenges
and the empirical estimate is known to be very unstable.
Here, we propose a novel covariance estimator based on the Gaussian Process
Latent Variable Model (GP-LVM). Our estimator can be considered as a non-linear
extension of standard factor models with readily interpretable parameters
reminiscent of market betas. Furthermore, our Bayesian treatment naturally
shrinks the sample covariance matrix towards a more structured matrix given by
the prior and thereby systematically reduces estimation errors.
",0,0,0,0,0,1
16502,Linear Regression with Sparsely Permuted Data,"  In regression analysis of multivariate data, it is tacitly assumed that
response and predictor variables in each observed response-predictor pair
correspond to the same entity or unit. In this paper, we consider the situation
of ""permuted data"" in which this basic correspondence has been lost. Several
recent papers have considered this situation without further assumptions on the
underlying permutation. In applications, the latter is often to known to have
additional structure that can be leveraged. Specifically, we herein consider
the common scenario of ""sparsely permuted data"" in which only a small fraction
of the data is affected by a mismatch between response and predictors. However,
an adverse effect already observed for sparsely permuted data is that the least
squares estimator as well as other estimators not accounting for such partial
mismatch are inconsistent. One approach studied in detail herein is to treat
permuted data as outliers which motivates the use of robust regression
formulations to estimate the regression parameter. The resulting estimate can
subsequently be used to recover the permutation. A notable benefit of the
proposed approach is its computational simplicity given the general lack of
procedures for the above problem that are both statistically sound and
computationally appealing.
",0,0,1,1,0,0
314,Human perception in computer vision,"  Computer vision has made remarkable progress in recent years. Deep neural
network (DNN) models optimized to identify objects in images exhibit
unprecedented task-trained accuracy and, remarkably, some generalization
ability: new visual problems can now be solved more easily based on previous
learning. Biological vision (learned in life and through evolution) is also
accurate and general-purpose. Is it possible that these different learning
regimes converge to similar problem-dependent optimal computations? We
therefore asked whether the human system-level computation of visual perception
has DNN correlates and considered several anecdotal test cases. We found that
perceptual sensitivity to image changes has DNN mid-computation correlates,
while sensitivity to segmentation, crowding and shape has DNN end-computation
correlates. Our results quantify the applicability of using DNN computation to
estimate perceptual loss, and are consistent with the fascinating theoretical
view that properties of human perception are a consequence of
architecture-independent visual learning.
",1,0,0,0,0,0
4310,Effects of parametric uncertainties in cascaded open quantum harmonic oscillators and robust generation of Gaussian invariant states,"  This paper is concerned with the generation of Gaussian invariant states in
cascades of open quantum harmonic oscillators governed by linear quantum
stochastic differential equations. We carry out infinitesimal perturbation
analysis of the covariance matrix for the invariant Gaussian state of such a
system and the related purity functional subject to inaccuracies in the energy
and coupling matrices of the subsystems. This leads to the problem of balancing
the state-space realizations of the component oscillators through symplectic
similarity transformations in order to minimize the mean square sensitivity of
the purity functional to small random perturbations of the parameters. This
results in a quadratic optimization problem with an effective solution in the
case of cascaded one-mode oscillators, which is demonstrated by a numerical
example. We also discuss a connection of the sensitivity index with classical
statistical distances and outline infinitesimal perturbation analysis for
translation invariant cascades of identical oscillators. The findings of the
paper are applicable to robust state generation in quantum stochastic networks.
",1,0,1,0,0,0
11960,Multi-agent Economics and the Emergence of Critical Markets,"  The dual crises of the sub-prime mortgage crisis and the global financial
crisis has prompted a call for explanations of non-equilibrium market dynamics.
Recently a promising approach has been the use of agent based models (ABMs) to
simulate aggregate market dynamics. A key aspect of these models is the
endogenous emergence of critical transitions between equilibria, i.e. market
collapses, caused by multiple equilibria and changing market parameters.
Several research themes have developed microeconomic based models that include
multiple equilibria: social decision theory (Brock and Durlauf), quantal
response models (McKelvey and Palfrey), and strategic complementarities
(Goldstein). A gap that needs to be filled in the literature is a unified
analysis of the relationship between these models and how aggregate criticality
emerges from the individual agent level. This article reviews the agent-based
foundations of markets starting with the individual agent perspective of
McFadden and the aggregate perspective of catastrophe theory emphasising
connections between the different approaches. It is shown that changes in the
uncertainty agents have in the value of their interactions with one another,
even if these changes are one-sided, plays a central role in systemic market
risks such as market instability and the twin crises effect. These interactions
can endogenously cause crises that are an emergent phenomena of markets.
",0,0,0,0,0,1
7049,"Chaotic dynamics of movements stochastic instability and the hypothesis of N.A. Bernstein about ""repetition without repetition""","  The registration of tremor was performed in two groups of subjects (15 people
in each group) with different physical fitness at rest and at a static loads of
3N. Each subject has been tested 15 series (number of series N=15) in both
states (with and without physical loads) and each series contained 15 samples
(n=15) of tremorogramm measurements (500 elements in each sample, registered
coordinates x1(t) of the finger position relative to eddy current sensor) of
the finger. Using non-parametric Wilcoxon test of each series of experiment a
pairwise comparison was made forming 15 tables in which the results of
calculation of pairwise comparison was presented as a matrix (15x15) for
tremorogramms are presented. The average number of hits random pairs of samples
(<k>) and standard deviation {\sigma} were calculated for all 15 matrices
without load and under the impact of physical load (3N), which showed an
increase almost in twice in the number k of pairs of matching samples of
tremorogramms at conditions of a static load. For all these samples it was
calculated special quasi-attractor (this square was presented the distinguishes
between physical load and without it. All samples present the stochastic
unstable state.
",0,0,0,0,1,0
15321,Big Data Technology Accelerate Genomics Precision Medicine,"  During genomics life science research, the data volume of whole genomics and
life science algorithm is going bigger and bigger, which is calculated as TB,
PB or EB etc. The key problem will be how to store and analyze the data with
optimized way. This paper demonstrates how Intel Big Data Technology and
Architecture help to facilitate and accelerate the genomics life science
research in data store and utilization. Intel defines high performance
GenomicsDB for variant call data query and Lustre filesystem with Hierarchal
Storage Management for genomics data store. Based on these great technology,
Intel defines genomics knowledge share and exchange architecture, which is
landed and validated in BGI China and Shanghai Children Hospital with very
positive feedback. And these big data technology can definitely be scaled to
much more genomics life science partners in the world.
",1,0,0,0,0,0
252,Linear time-periodic dynamical systems: An H2 analysis and a model reduction framework,"  Linear time-periodic (LTP) dynamical systems frequently appear in the
modeling of phenomena related to fluid dynamics, electronic circuits, and
structural mechanics via linearization centered around known periodic orbits of
nonlinear models. Such LTP systems can reach orders that make repeated
simulation or other necessary analysis prohibitive, motivating the need for
model reduction.
We develop here an algorithmic framework for constructing reduced models that
retains the linear time-periodic structure of the original LTP system. Our
approach generalizes optimal approaches that have been established previously
for linear time-invariant (LTI) model reduction problems. We employ an
extension of the usual H2 Hardy space defined for the LTI setting to
time-periodic systems and within this broader framework develop an a posteriori
error bound expressible in terms of related LTI systems. Optimization of this
bound motivates our algorithm. We illustrate the success of our method on two
numerical examples.
",1,0,0,0,0,0
12265,Smart Contract SLAs for Dense Small-Cell-as-a-Service,"  The disruptive power of blockchain technologies represents a great
opportunity to re-imagine standard practices of telecommunication networks and
to identify critical areas that can benefit from brand new approaches. As a
starting point for this debate, we look at the current limits of infrastructure
sharing, and specifically at the Small-Cell-as-a-Service trend, asking
ourselves how we could push it to its natural extreme: a scenario in which any
individual home or business user can become a service provider for mobile
network operators, freed from all the scalability and legal constraints that
are inherent to the current modus operandi. We propose the adoption of smart
contracts to implement simple but effective Service Level Agreements (SLAs)
between small cell providers and mobile operators, and present an example
contract template based on the Ethereum blockchain.
",1,0,0,0,0,0
13592,Machine Assisted Analysis of Vowel Length Contrasts in Wolof,"  Growing digital archives and improving algorithms for automatic analysis of
text and speech create new research opportunities for fundamental research in
phonetics. Such empirical approaches allow statistical evaluation of a much
larger set of hypothesis about phonetic variation and its conditioning factors
(among them geographical / dialectal variants). This paper illustrates this
vision and proposes to challenge automatic methods for the analysis of a not
easily observable phenomenon: vowel length contrast. We focus on Wolof, an
under-resourced language from Sub-Saharan Africa. In particular, we propose
multiple features to make a fine evaluation of the degree of length contrast
under different factors such as: read vs semi spontaneous speech ; standard vs
dialectal Wolof. Our measures made fully automatically on more than 20k vowel
tokens show that our proposed features can highlight different degrees of
contrast for each vowel considered. We notably show that contrast is weaker in
semi-spontaneous speech and in a non standard semi-spontaneous dialect.
",1,0,0,0,0,0
15279,Categorical Structures on Bundle Gerbes and Higher Geometric Prequantisation,"  We present a construction of a 2-Hilbert space of sections of a bundle gerbe,
a suitable candidate for a prequantum 2-Hilbert space in higher geometric
quantisation. We introduce a direct sum on the morphism categories in the
2-category of bundle gerbes and show that these categories are cartesian
monoidal and abelian. Endomorphisms of the trivial bundle gerbe, or higher
functions, carry the structure of a rig-category, which acts on generic
morphism categories of bundle gerbes. We continue by presenting a
categorification of the hermitean metric on a hermitean line bundle. This is
achieved by introducing a functorial dual that extends the dual of vector
bundles to morphisms of bundle gerbes, and constructing a two-variable
adjunction for the aforementioned rig-module category structure on morphism
categories. Its right internal hom is the module action, composed by taking the
dual of higher functions, while the left internal hom is interpreted as a
bundle gerbe metric. Sections of bundle gerbes are defined as morphisms from
the trivial bundle gerbe to a given bundle gerbe. The resulting categories of
sections carry a rig-module structure over the category of finite-dimensional
Hilbert spaces. A suitable definition of 2-Hilbert spaces is given, modifying
previous definitions by the use of two-variable adjunctions. We prove that the
category of sections of a bundle gerbe fits into this framework, thus obtaining
a 2-Hilbert space of sections. In particular, this can be constructed for
prequantum bundle gerbes in problems of higher geometric quantisation. We
define a dimensional reduction functor and show that the categorical structures
introduced on bundle gerbes naturally reduce to their counterparts on hermitean
line bundles with connections. In several places in this thesis, we provide
examples, making 2-Hilbert spaces of sections and dimensional reduction very
explicit.
",0,0,1,0,0,0
14704,Distributed Representation of Subgraphs,"  Network embeddings have become very popular in learning effective feature
representations of networks. Motivated by the recent successes of embeddings in
natural language processing, researchers have tried to find network embeddings
in order to exploit machine learning algorithms for mining tasks like node
classification and edge prediction. However, most of the work focuses on
finding distributed representations of nodes, which are inherently ill-suited
to tasks such as community detection which are intuitively dependent on
subgraphs.
Here, we propose sub2vec, an unsupervised scalable algorithm to learn feature
representations of arbitrary subgraphs. We provide means to characterize
similarties between subgraphs and provide theoretical analysis of sub2vec and
demonstrate that it preserves the so-called local proximity. We also highlight
the usability of sub2vec by leveraging it for network mining tasks, like
community detection. We show that sub2vec gets significant gains over
state-of-the-art methods and node-embedding methods. In particular, sub2vec
offers an approach to generate a richer vocabulary of features of subgraphs to
support representation and reasoning.
",1,0,0,1,0,0
17424,Asymmetric Variational Autoencoders,"  Variational inference for latent variable models is prevalent in various
machine learning problems, typically solved by maximizing the Evidence Lower
Bound (ELBO) of the true data likelihood with respect to a variational
distribution. However, freely enriching the family of variational distribution
is challenging since the ELBO requires variational likelihood evaluations of
the latent variables. In this paper, we propose a novel framework to enrich the
variational family by incorporating auxiliary variables to the variational
family. The resulting inference network doesn't require density evaluations for
the auxiliary variables and thus complex implicit densities over the auxiliary
variables can be constructed by neural networks. It can be shown that the
actual variational posterior of the proposed approach is essentially modeling a
rich probabilistic mixture of simple variational posterior indexed by auxiliary
variables, thus a flexible inference model can be built. Empirical evaluations
on several density estimation tasks demonstrates the effectiveness of the
proposed method.
",1,0,0,1,0,0
17477,Spurious Vanishing Problem in Approximate Vanishing Ideal,"  Approximate vanishing ideal, which is a new concept from computer algebra, is
a set of polynomials that almost takes a zero value for a set of given data
points. The introduction of approximation to exact vanishing ideal has played a
critical role in capturing the nonlinear structures of noisy data by computing
the approximate vanishing polynomials. However, approximate vanishing has a
theoretical problem, which is giving rise to the spurious vanishing problem
that any polynomial turns into an approximate vanishing polynomial by
coefficient scaling. In the present paper, we propose a general method that
enables many basis construction methods to overcome this problem. Furthermore,
a coefficient truncation method is proposed that balances the theoretical
soundness and computational cost. The experiments show that the proposed method
overcomes the spurious vanishing problem and significantly increases the
accuracy of classification.
",1,0,0,1,0,0
1711,HoloScope: Topology-and-Spike Aware Fraud Detection,"  As online fraudsters invest more resources, including purchasing large pools
of fake user accounts and dedicated IPs, fraudulent attacks become less obvious
and their detection becomes increasingly challenging. Existing approaches such
as average degree maximization suffer from the bias of including more nodes
than necessary, resulting in lower accuracy and increased need for manual
verification. Hence, we propose HoloScope, which uses information from graph
topology and temporal spikes to more accurately detect groups of fraudulent
users. In terms of graph topology, we introduce ""contrast suspiciousness,"" a
dynamic weighting approach, which allows us to more accurately detect
fraudulent blocks, particularly low-density blocks. In terms of temporal
spikes, HoloScope takes into account the sudden bursts and drops of fraudsters'
attacking patterns. In addition, we provide theoretical bounds for how much
this increases the time cost needed for fraudsters to conduct adversarial
attacks. Additionally, from the perspective of ratings, HoloScope incorporates
the deviation of rating scores in order to catch fraudsters more accurately.
Moreover, HoloScope has a concise framework and sub-quadratic time complexity,
making the algorithm reproducible and scalable. Extensive experiments showed
that HoloScope achieved significant accuracy improvements on synthetic and real
data, compared with state-of-the-art fraud detection methods.
",1,0,0,0,0,0
9850,Application of the Bead Perturbation Technique to a Study of a Tunable 5 GHz Annular Cavity,"  Microwave cavities for a Sikivie-type axion search are subject to several
constraints. In the fabrication and operation of such cavities, often used at
frequencies where the resonator is highly overmoded, it is important to be able
to reliably identify several properties of the cavity. Those include
identifying the symmetry of the mode of interest, confirming its form factor,
and determining the frequency ranges where mode crossings with intruder levels
cause unacceptable admixture, thus leading to the loss of purity of the mode of
interest. A simple and powerful diagnostic for mapping out the electric field
of a cavity is the bead perturbation technique. While a standard tool in
accelerator physics, we have, for the first time, applied this technique to
cavities used in the axion search. We report initial results from an extensive
study for the initial cavity used in the HAYSTAC experiment. Two effects have
been investigated: the role of rod misalignment in mode localization, and
mode-mixing at avoided crossings of TM/TE modes. Future work will extend these
results by incorporating precision metrology and high-fidelity simulations.
",0,1,0,0,0,0
18985,Cost Functions for Robot Motion Style,"  We focus on autonomously generating robot motion for day to day physical
tasks that is expressive of a certain style or emotion. Because we seek
generalization across task instances and task types, we propose to capture
style via cost functions that the robot can use to augment its nominal task
cost and task constraints in a trajectory optimization process. We compare two
approaches to representing such cost functions: a weighted linear combination
of hand-designed features, and a neural network parameterization operating on
raw trajectory input. For each cost type, we learn weights for each style from
user feedback. We contrast these approaches to a nominal motion across
different tasks and for different styles in a user study, and find that they
both perform on par with each other, and significantly outperform the baseline.
Each approach has its advantages: featurized costs require learning fewer
parameters and can perform better on some styles, but neural network
representations do not require expert knowledge to design features and could
even learn more complex, nuanced costs than an expert can easily design.
",1,0,0,0,0,0
13221,Whipping of electrified visco-capillary jets in airflows,"  An electrified visco-capillary jet shows different dynamic behavior, such as
cone forming, breakage into droplets, whipping and coiling, depending on the
considered parameter regime. The whipping instability that is of fundamental
importance for electrospinning has been approached by means of stability
analysis in previous papers. In this work we alternatively propose a model
framework in which the instability can be computed straightforwardly as the
stable stationary solution of an asymptotic Cosserat rod description. For this
purpose, we adopt a procedure by Ribe (Proc. Roy. Soc. Lond. A, 2004)
describing the jet dynamics with respect to a frame rotating with the a priori
unknown whipping frequency that itself becomes part of the solution. The rod
model allows for stretching, bending and torsion, taking into account inertia,
viscosity, surface tension, electric field and air drag. For the resulting
parametric boundary value problem of ordinary differential equations we present
a continuation-collocation method. On top of an implicit Runge-Kutta scheme of
fifth order, our developed continuation procedure makes the efficient and
robust simulation and navigation through a high-dimensional parameter space
possible. Despite the simplicity of the employed electric force model the
numerical results are convincing, the whipping effect is qualitatively well
characterized.
",0,1,1,0,0,0
7478,"Advances in Detection and Error Correction for Coherent Optical Communications: Regular, Irregular, and Spatially Coupled LDPC Code Designs","  In this chapter, we show how the use of differential coding and the presence
of phase slips in the transmission channel affect the total achievable
information rates and capacity of a system. By means of the commonly used QPSK
modulation, we show that the use of differential coding does not decrease the
total amount of reliably conveyable information over the channel. It is a
common misconception that the use of differential coding introduces an
unavoidable differential loss. This perceived differential loss is rather a
consequence of simplified differential detection and decoding at the receiver.
Afterwards, we show how capacity-approaching coding schemes based on LDPC and
spatially coupled LDPC codes can be constructed by combining iterative
demodulation and decoding. For this, we first show how to modify the
differential decoder to account for phase slips and then how to use this
modified differential decoder to construct good LDPC codes. This construction
method can serve as a blueprint to construct good and practical LDPC codes for
other applications with iterative detection, such as higher order modulation
formats with non-square constellations, multi-dimensional optimized modulation
formats, turbo equalization to mitigate ISI (e.g., due to nonlinearities) and
many more. Finally, we introduce the class of spatially coupled (SC)-LDPC
codes, which are a generalization of LDPC codes with some outstanding
properties and which can be decoded with a very simple windowed decoder. We
show that the universal behavior of spatially coupled codes makes them an ideal
candidate for iterative differential demodulation/detection and decoding.
",1,0,0,0,0,0
9512,Space-time domain solutions of the wave equation by a non-singular boundary integral method and Fourier transform,"  The general space-time evolution of the scattering of an incident acoustic
plane wave pulse by an arbitrary configuration of targets is treated by
employing a recently developed non-singular boundary integral method to solve
the Helmholtz equation in the frequency domain from which the fast Fourier
transform is used to obtain the full space-time solution of the wave equation.
The non-singular boundary integral solution can enforce the radiation boundary
condition at infinity exactly and can account for multiple scattering effects
at all spacings between scatterers without adverse effects on the numerical
precision. More generally, the absence of singular kernels in the non-singular
integral equation confers high numerical stability and precision for smaller
numbers of degrees of freedom. The use of fast Fourier transform to obtain the
time dependence is not constrained to discrete time steps and is particularly
efficient for studying the response to different incident pulses by the same
configuration of scatterers. The precision that can be attained using a smaller
number of Fourier components is also quantified.
",0,1,0,0,0,0
739,Morse geodesics in torsion groups,"  In this paper we exhibit Morse geodesics, often called ""hyperbolic
directions"", in infinite unbounded torsion groups. The groups studied are
lacunary hyperbolic groups and constructed using graded small cancellation
conditions. In all previously known examples, Morse geodesics were found in
groups which also contained Morse elements, infinite order elements whose
cyclic subgroup gives a Morse quasi-geodesic. Our result presents the first
example of a group which contains Morse geodesics but no Morse elements. In
fact, we show that there is an isometrically embedded $7$-regular tree inside
such groups where every infinite, simple path is a Morse geodesic.
",0,0,1,0,0,0
16309,Sharp bounds for population recovery,"  The population recovery problem is a basic problem in noisy unsupervised
learning that has attracted significant research attention in recent years
[WY12,DRWY12, MS13, BIMP13, LZ15,DST16]. A number of different variants of this
problem have been studied, often under assumptions on the unknown distribution
(such as that it has restricted support size). In this work we study the sample
complexity and algorithmic complexity of the most general version of the
problem, under both bit-flip noise and erasure noise model. We give essentially
matching upper and lower sample complexity bounds for both noise models, and
efficient algorithms matching these sample complexity bounds up to polynomial
factors.
",1,0,1,1,0,0
1581,FLASH: Randomized Algorithms Accelerated over CPU-GPU for Ultra-High Dimensional Similarity Search,"  We present FLASH (\textbf{F}ast \textbf{L}SH \textbf{A}lgorithm for
\textbf{S}imilarity search accelerated with \textbf{H}PC), a similarity search
system for ultra-high dimensional datasets on a single machine, that does not
require similarity computations and is tailored for high-performance computing
platforms. By leveraging a LSH style randomized indexing procedure and
combining it with several principled techniques, such as reservoir sampling,
recent advances in one-pass minwise hashing, and count based estimations, we
reduce the computational and parallelization costs of similarity search, while
retaining sound theoretical guarantees.
We evaluate FLASH on several real, high-dimensional datasets from different
domains, including text, malicious URL, click-through prediction, social
networks, etc. Our experiments shed new light on the difficulties associated
with datasets having several million dimensions. Current state-of-the-art
implementations either fail on the presented scale or are orders of magnitude
slower than FLASH. FLASH is capable of computing an approximate k-NN graph,
from scratch, over the full webspam dataset (1.3 billion nonzeros) in less than
10 seconds. Computing a full k-NN graph in less than 10 seconds on the webspam
dataset, using brute-force ($n^2D$), will require at least 20 teraflops. We
provide CPU and GPU implementations of FLASH for replicability of our results.
",1,0,0,0,0,0
4983,Unsupervised Learning of Neural Networks to Explain Neural Networks (extended abstract),"  This paper presents an unsupervised method to learn a neural network, namely
an explainer, to interpret a pre-trained convolutional neural network (CNN),
i.e., the explainer uses interpretable visual concepts to explain features in
middle conv-layers of a CNN. Given feature maps of a conv-layer of the CNN, the
explainer performs like an auto-encoder, which decomposes the feature maps into
object-part features. The object-part features are learned to reconstruct CNN
features without much loss of information. We can consider the disentangled
representations of object parts a paraphrase of CNN features, which help people
understand the knowledge encoded by the CNN. More crucially, we learn the
explainer via knowledge distillation without using any annotations of object
parts or textures for supervision. In experiments, our method was widely used
to interpret features of different benchmark CNNs, and explainers significantly
boosted the feature interpretability without hurting the discrimination power
of the CNNs.
",1,0,0,1,0,0
5419,A Convex Parametrization of a New Class of Universal Kernel Functions for use in Kernel Learning,"  We propose a new class of universal kernel functions which admit a linear
parametrization using positive semidefinite matrices. These kernels are
generalizations of the Sobolev kernel and are defined by piecewise-polynomial
functions. The class of kernels is termed ""tessellated"" as the resulting
discriminant is defined piecewise with hyper-rectangular domains whose corners
are determined by the training data. The kernels have scalable complexity, but
each instance is universal in the sense that its hypothesis space is dense in
$L_2$. Using numerical testing, we show that for the soft margin SVM, this
class can eliminate the need for Gaussian kernels. Furthermore, we demonstrate
that when the ratio of the number of training data to features is high, this
method will significantly outperform other kernel learning algorithms. Finally,
to reduce the complexity associated with SDP-based kernel learning methods, we
use a randomized basis for the positive matrices to integrate with existing
multiple kernel learning algorithms such as SimpleMKL.
",1,0,0,1,0,0
17933,Photonic-chip supercontinuum with tailored spectra for precision frequency metrology,"  Supercontinuum generation using chip-integrated photonic waveguides is a
powerful approach for spectrally broadening pulsed laser sources with very low
pulse energies and compact form factors. When pumped with a mode-locked laser
frequency comb, these waveguides can coherently expand the comb spectrum to
more than an octave in bandwidth to enable self-referenced stabilization.
However, for applications in frequency metrology and precision spectroscopy, it
is desirable to not only support self-referencing, but also to generate
low-noise combs with customizable broadband spectra. In this work, we
demonstrate dispersion-engineered waveguides based on silicon nitride that are
designed to meet these goals and enable precision optical metrology experiments
across large wavelength spans. We perform a clock comparison measurement and
report a clock-limited relative frequency instability of $3.8\times10^{-15}$ at
$\tau = 2$ seconds between a 1550 nm cavity-stabilized reference laser and
NIST's calcium atomic clock laser at 657 nm using a two-octave
waveguide-supercontinuum comb.
",0,1,0,0,0,0
12615,From the simple reacting sphere kinetic model to the reaction-diffusion system of Maxwell-Stefan type,"  In this paper we perform a formal asymptotic analysis on a kinetic model for
reactive mixtures in order to derive a reaction-diffusion system of
Maxwell-Stefan type. More specifically, we start from the kinetic model of
simple reacting spheres for a quaternary mixture of monatomic ideal gases that
undergoes a reversible chemical reaction of bimolecular type. Then, we consider
a scaling describing a physical situation in which mechanical collisions play a
dominant role in the evolution process, while chemical reactions are slow, and
compute explicitly the production terms associated to the concentration and
momentum balance equations for each species in the reactive mixture. Finally,
we prove that, under isothermal assumptions, the limit equations for the scaled
kinetic model is the reaction diffusion system of Maxwell-Stefan type.
",0,1,0,0,0,0
20457,Vico-Greengard-Ferrando quadratures in the tensor solver for integral equations,"  Convolution with Green's function of a differential operator appears in a lot
of applications e.g. Lippmann-Schwinger integral equation. Algorithms for
computing such are usually non-trivial and require non-uniform mesh. However,
recently Vico, Greengard and Ferrando developed method for computing
convolution with smooth functions with compact support with spectral accuracy,
requiring nothing more than Fast Fourier Transform (FFT). Their approach is
very suitable for the low-rank tensor implementation which we develop using
Quantized Tensor Train (QTT) decomposition.
",1,0,0,0,0,0
14178,Optimal $k$-Coverage Charging Problem,"  Wireless rechargeable sensor networks, consisting of sensor nodes with
rechargeable batteries and mobile chargers to replenish their batteries, have
gradually become a promising solution to the bottleneck of energy limitation
that hinders the wide deployment of wireless sensor networks (WSN). In this
paper, we focus on the mobile charger scheduling and path optimization scenario
in which the $k$-coverage ability of a network system needs to be maintained.
We formulate the optimal $k$-coverage charging problem of finding a feasible
path for a mobile charger to charge a set of sensor nodes within their
estimated charging time windows under the constraint of maintaining the
$k$-coverage ability of the network system, with an objective of minimizing the
energy consumption on traveling per tour. We show the hardness of the problem
that even finding a feasible path for the trivial case of the problem is an
NP-complete one with no polytime constant-factor approximation algorithm.
",1,0,0,0,0,0
9749,Towards CNN map representation and compression for camera relocalisation,"  This paper presents a study on the use of Convolutional Neural Networks for
camera relocalisation and its application to map compression. We follow state
of the art visual relocalisation results and evaluate the response to different
data inputs. We use a CNN map representation and introduce the notion of map
compression under this paradigm by using smaller CNN architectures without
sacrificing relocalisation performance. We evaluate this approach in a series
of publicly available datasets over a number of CNN architectures with
different sizes, both in complexity and number of layers. This formulation
allows us to improve relocalisation accuracy by increasing the number of
training trajectories while maintaining a constant-size CNN.
",1,0,0,0,0,0
12382,Mutation invariance for the zeroth coefficients of the colored HOMFLY polynomial,"  We show that the zeroth coefficient of the cables of the HOMFLY polynomial
(colored HOMFLY polynomials) does not distinguish mutants. This makes a sharp
contrast with the total HOMFLY polynomial whose 3-cables can distinguish
mutants.
",0,0,1,0,0,0
10629,Radiative Transfer for Exoplanet Atmospheres,"  Remote sensing of the atmospheres of distant worlds motivates a firm
understanding of radiative transfer. In this review, we provide a pedagogical
cookbook that describes the principal ingredients needed to perform a radiative
transfer calculation and predict the spectrum of an exoplanet atmosphere,
including solving the radiative transfer equation, calculating opacities (and
chemistry), iterating for radiative equilibrium (or not), and adapting the
output of the calculations to the astronomical observations. A review of the
state of the art is performed, focusing on selected milestone papers.
Outstanding issues, including the need to understand aerosols or clouds and
elucidating the assumptions and caveats behind inversion methods, are
discussed. A checklist is provided to assist referees/reviewers in their
scrutiny of works involving radiative transfer. A table summarizing the
methodology employed by past studies is provided.
",0,1,0,0,0,0
14417,Almost sharp nonlinear scattering in one-dimensional Born-Infeld equations arising in nonlinear Electrodynamics,"  We study decay of small solutions of the Born-Infeld equation in 1+1
dimensions, a quasilinear scalar field equation modeling nonlinear
electromagnetism, as well as branes in String theory and minimal surfaces in
Minkowski space-times. From the work of Whitham, it is well-known that there is
no decay because of arbitrary solutions traveling to the speed of light just as
linear wave equation. However, even if there is no global decay in 1+1
dimensions, we are able to show that all globally small $H^{s+1}\times H^s$,
$s>\frac12$ solutions do decay to the zero background state in space, inside a
strictly proper subset of the light cone. We prove this result by constructing
a Virial identity related to a momentum law, in the spirit of works
\cite{KMM,KMM1}, as well as a Lyapunov functional that controls the $\dot H^1
\times L^2$ energy.
",0,0,1,0,0,0
2128,Dynamic Rank Maximal Matchings,"  We consider the problem of matching applicants to posts where applicants have
preferences over posts. Thus the input to our problem is a bipartite graph G =
(A U P,E), where A denotes a set of applicants, P is a set of posts, and there
are ranks on edges which denote the preferences of applicants over posts. A
matching M in G is called rank-maximal if it matches the maximum number of
applicants to their rank 1 posts, subject to this the maximum number of
applicants to their rank 2 posts, and so on.
We consider this problem in a dynamic setting, where vertices and edges can
be added and deleted at any point. Let n and m be the number of vertices and
edges in an instance G, and r be the maximum rank used by any rank-maximal
matching in G. We give a simple O(r(m+n))-time algorithm to update an existing
rank-maximal matching under each of these changes. When r = o(n), this is
faster than recomputing a rank-maximal matching completely using a known
algorithm like that of Irving et al., which takes time O(min((r + n,
r*sqrt(n))m).
",1,0,0,0,0,0
12342,Electronic origin of melting T-P curves of alkali metals with negative slope and minimum,"  Group I elements - alkali metals Li, Na, K, Rb and Cs - are examples of
simple metals with one s electron in the valence band. Under pressure these
elements display unusually complex structural behaviour transforming from
close-packed to low symmetry open structures. Unexpectedly complex form was
found for melting curves of alkalis under compression with initial increasing
in accordance to Lindemann criterion and further decreasing to very low melting
point. To understand complex and low symmetry structures in compressed alkalis
a transformation of the electron energy levels was suggested which involves an
overlap between the valence band and outer core electrons. Within the model of
the Fermi sphere - Brillouin zone interaction one can understand the complex
melting curve of alkalis.
",0,1,0,0,0,0
4942,Fine-resolution analysis of exoplanetary distributions by wavelets: hints of an overshooting iceline accumulation,"  We investigate 1D exoplanetary distributions using a novel analysis algorithm
based on the continuous wavelet transform. The analysis pipeline includes an
estimation of the wavelet transform of the probability density function
(p.d.f.) without pre-binning, use of optimized wavelets, a rigorous
significance testing of the patterns revealed in the p.d.f., and an optimized
minimum-noise reconstruction of the p.d.f. via matching pursuit iterations.
In the distribution of orbital periods, $P$, our analysis revealed a narrow
subfamily of exoplanets within the broad family of ""warm jupiters"", or massive
giants with $P\gtrsim 300$~d, which are often deemed to be related with the
iceline accumulation in a protoplanetary disk. We detected a p.d.f. pattern
that represents an upturn followed by an overshooting peak spanning $P\sim
300-600$~d, right beyond the ""period valley"". It is separated from the other
planets by p.d.f. concavities from both sides. It has at least two-sigma
significance.
In the distribution of planet radii, $R$, and using the California Kepler
Survey sample properly cleaned, we confirm the hints of a bimodality with two
peaks about $R=1.3 R_\oplus$ and $R=2.4 R_\oplus$, and the ""evaporation valley""
between them. However, we obtain just a modest significance for this pattern,
two-sigma only at the best. Besides, our follow-up application of the Hartigan
& Hartigan dip test for unimodality returns $3$ per cent false alarm
probability (merely $2.2$-sigma significance), contrary to $0.14$ per cent (or
$3.2$-sigma), as claimed by Fulton et al. (2017).
",0,1,0,0,0,0
16846,From Monte Carlo to Las Vegas: Improving Restricted Boltzmann Machine Training Through Stopping Sets,"  We propose a Las Vegas transformation of Markov Chain Monte Carlo (MCMC)
estimators of Restricted Boltzmann Machines (RBMs). We denote our approach
Markov Chain Las Vegas (MCLV). MCLV gives statistical guarantees in exchange
for random running times. MCLV uses a stopping set built from the training data
and has maximum number of Markov chain steps K (referred as MCLV-K). We present
a MCLV-K gradient estimator (LVS-K) for RBMs and explore the correspondence and
differences between LVS-K and Contrastive Divergence (CD-K), with LVS-K
significantly outperforming CD-K training RBMs over the MNIST dataset,
indicating MCLV to be a promising direction in learning generative models.
",1,0,0,1,0,0
10827,MatlabCompat.jl: helping Julia understand Your Matlab/Octave Code,"  Scientific legacy code in MATLAB/Octave not compatible with modernization of
research workflows is vastly abundant throughout academic community.
Performance of non-vectorized code written in MATLAB/Octave represents a major
burden. A new programming language for technical computing Julia, promises to
address these issues. Although Julia syntax is similar to MATLAB/Octave,
porting code to Julia may be cumbersome for researchers. Here we present
MatlabCompat.jl - a library aimed at simplifying the conversion of your
MATLAB/Octave code to Julia. We show using a simplistic image analysis use case
that MATLAB/Octave code can be easily ported to high performant Julia using
MatlabCompat.jl.
",1,0,0,0,0,0
4731,Active Decision Boundary Annotation with Deep Generative Models,"  This paper is on active learning where the goal is to reduce the data
annotation burden by interacting with a (human) oracle during training.
Standard active learning methods ask the oracle to annotate data samples.
Instead, we take a profoundly different approach: we ask for annotations of the
decision boundary. We achieve this using a deep generative model to create
novel instances along a 1d line. A point on the decision boundary is revealed
where the instances change class. Experimentally we show on three data sets
that our method can be plugged-in to other active learning schemes, that human
oracles can effectively annotate points on the decision boundary, that our
method is robust to annotation noise, and that decision boundary annotations
improve over annotating data samples.
",1,0,0,0,0,0
5549,Tuning Majorana zero modes with temperature in $π$-phase Josephson junctions,"  We study a superconductor-normal state-superconductor (SNS) Josephson
junction along the edge of a quantum spin Hall insulator (QSHI) with a
superconducting $\pi$-phase across the junction. We solve self-consistently for
the superconducting order parameter and find both real junctions, where the
order parameter is fully real throughout the system, and junctions where the
order parameter has a complex phase winding. Real junctions host two Majorana
zero modes (MZMs), while phase-winding junctions have no subgap states close to
zero energy. At zero temperature we find that the phase-winding solution always
has the lowest free energy, which we establish being due to a strong
proximity-effect into the N region. With increasing temperature this
proximity-effect is dramatically decreased and we find a phase transition into
a real junction with two MZMs.
",0,1,0,0,0,0
5386,Understanding low-temperature bulk transport in samarium hexaboride without relying on in-gap bulk states,"  We present a new model to explain the difference between the transport and
spectroscopy gaps in samarium hexaboride (SmB$_6$), which has been a mystery
for some time. We propose that SmB$_6$ can be modeled as an intrinsic
semiconductor with a depletion length that diverges at cryogenic temperatures.
In this model, we find a self-consistent solution to Poisson's equation in the
bulk, with boundary conditions based on Fermi energy pinning due to surface
charges. The solution yields band bending in the bulk; this explains the
difference between the two gaps because spectroscopic methods measure the gap
near the surface, while transport measures the average over the bulk. We also
connect the model to transport parameters, including the Hall coefficient and
thermopower, using semiclassical transport theory. The divergence of the
depletion length additionally explains the 10-12 K feature in data for these
parameters, demonstrating a crossover from bulk dominated transport above this
temperature to surface-dominated transport below this temperature. We find good
agreement between our model and a collection of transport data from 4-40 K.
This model can also be generalized to materials with similar band structure.
",0,1,0,0,0,0
7610,Semi-blind source separation with multichannel variational autoencoder,"  This paper proposes a multichannel source separation technique called the
multichannel variational autoencoder (MVAE) method, which uses a conditional
VAE (CVAE) to model and estimate the power spectrograms of the sources in a
mixture. By training the CVAE using the spectrograms of training examples with
source-class labels, we can use the trained decoder distribution as a universal
generative model capable of generating spectrograms conditioned on a specified
class label. By treating the latent space variables and the class label as the
unknown parameters of this generative model, we can develop a
convergence-guaranteed semi-blind source separation algorithm that consists of
iteratively estimating the power spectrograms of the underlying sources as well
as the separation matrices. In experimental evaluations, our MVAE produced
better separation performance than a baseline method.
",0,0,0,1,0,0
15848,The SoLid anti-neutrino detector's readout system,"  The SoLid collaboration have developed an intelligent readout system to
reduce their 3200 silicon photomultiplier detector's data rate by a factor of
10000 whilst maintaining high efficiency for storing data from anti-neutrino
interactions. The system employs an FPGA-level waveform characterisation to
trigger on neutron signals. Following a trigger, data from a space time region
of interest around the neutron will be read out using the IPbus protocol. In
these proceedings the design of the readout system is explained and results
showing the performance of a prototype version of the system are presented.
",0,1,0,0,0,0
4591,Sampling-based vs. Design-based Uncertainty in Regression Analysis,"  Consider a researcher estimating the parameters of a regression function
based on data for all 50 states in the United States or on data for all visits
to a website. What is the interpretation of the estimated parameters and the
standard errors? In practice, researchers typically assume that the sample is
randomly drawn from a large population of interest and report standard errors
that are designed to capture sampling variation. This is common practice, even
in applications where it is difficult to articulate what that population of
interest is, and how it differs from the sample. In this article, we explore an
alternative approach to inference, which is partly design-based. In a
design-based setting, the values of some of the regressors can be manipulated,
perhaps through a policy intervention. Design-based uncertainty emanates from
lack of knowledge about the values that the regression outcome would have taken
under alternative interventions. We derive standard errors that account for
design-based uncertainty instead of, or in addition to, sampling-based
uncertainty. We show that our standard errors in general are smaller than the
infinite-population sampling-based standard errors and provide conditions under
which they coincide.
",0,0,1,1,0,0
6648,Reconstruction of a compact Riemannian manifold from the scattering data of internal sources,"  Given a smooth non-trapping compact manifold with strictly con- vex boundary,
we consider an inverse problem of reconstructing the manifold from the
scattering data initiated from internal sources. This data consist of the exit
directions of geodesics that are emaneted from interior points of the manifold.
We show that under certain generic assumption of the metric, one can
reconstruct an isometric copy of the manifold from such scattering data
measured on the boundary.
",0,0,1,0,0,0
2711,Nonconvex Sparse Logistic Regression with Weakly Convex Regularization,"  In this work we propose to fit a sparse logistic regression model by a weakly
convex regularized nonconvex optimization problem. The idea is based on the
finding that a weakly convex function as an approximation of the $\ell_0$
pseudo norm is able to better induce sparsity than the commonly used $\ell_1$
norm. For a class of weakly convex sparsity inducing functions, we prove the
nonconvexity of the corresponding sparse logistic regression problem, and study
its local optimality conditions and the choice of the regularization parameter
to exclude trivial solutions. Despite the nonconvexity, a method based on
proximal gradient descent is used to solve the general weakly convex sparse
logistic regression, and its convergence behavior is studied theoretically.
Then the general framework is applied to a specific weakly convex function, and
a necessary and sufficient local optimality condition is provided. The solution
method is instantiated in this case as an iterative firm-shrinkage algorithm,
and its effectiveness is demonstrated in numerical experiments by both randomly
generated and real datasets.
",1,0,0,1,0,0
18378,Stellar population synthesis based modelling of the Milky Way using asteroseismology of dwarfs and subgiants from Kepler,"  Early attempts to apply asteroseismology to study the Galaxy have already
shown unexpected discrepancies for the mass distribution of stars between the
Galactic models and the data; a result that is still unexplained. Here, we
revisit the analysis of the asteroseismic sample of dwarf and subgiant stars
observed by Kepler and investigate in detail the possible causes for the
reported discrepancy. We investigate two models of the Milky Way based on
stellar population synthesis, Galaxia and TRILEGAL. In agreement with previous
results, we find that TRILEGAL predicts more massive stars compared to Galaxia,
and that TRILEGAL predicts too many blue stars compared to 2MASS observations.
Both models fail to match the distribution of the stellar sample in $(\log
g,T_{\rm eff})$ space, pointing to inaccuracies in the models and/or the
assumed selection function. When corrected for this mismatch in $(\log g,T_{\rm
eff})$ space, the mass distribution calculated by Galaxia is broader and the
mean is shifted toward lower masses compared to that of the observed stars.
This behaviour is similar to what has been reported for the Kepler red giant
sample. The shift between the mass distributions is equivalent to a change of
2\% in $\nu_{\rm max}$, which is within the current uncertainty in the
$\nu_{\rm max}$ scaling relation. Applying corrections to the $\Delta \nu$
scaling relation predicted by the stellar models makes the observed mass
distribution significantly narrower, but there is no change to the mean.
",0,1,0,0,0,0
18170,Weakly-Private Information Retrieval,"  Private information retrieval (PIR) protocols make it possible to retrieve a
file from a database without disclosing any information about the identity of
the file being retrieved. These protocols have been rigorously explored from an
information-theoretic perspective in recent years. While existing protocols
strictly impose that no information is leaked on the file's identity, this work
initiates the study of the tradeoffs that can be achieved by relaxing the
requirement of perfect privacy. In case the user is willing to leak some
information on the identity of the retrieved file, we study how the PIR rate,
as well as the upload cost and access complexity, can be improved. For the
particular case of replicated servers, we propose two weakly-private
information retrieval schemes based on two recent PIR protocols and a family of
schemes based on partitioning. Lastly, we compare the performance of the
proposed schemes.
",1,0,0,0,0,0
13417,New Integral representations for the Fox-Wright functions and its applications,"  Our aim in this paper is to derive several new integral representations of
the Fox-Wright functions. In particular, we give new Laplace and Stieltjes
transform for this special functions under a special restriction on parameters.
From the positivity conditions for the weight in these representations, we
found sufficient conditions to be imposed on the parameters of the Fox-Wright
functions that it be completely monotonic. As applications, we derive a class
of function related to the Fox H-functions is positive definite and an
investigation of a class of the Fox H-function is non-negative. Moreover, we
extended the Luke's inequalities and we establish a new Turán type
inequalities for the Fox-Wright function. Finally, by appealing to each of the
Luke's inequalities, two sets of two-sided bounding inequalities for the
generalized Mathieu's type series are proved.
",0,0,1,0,0,0
5981,Schrödinger operators periodic in octants,"  We consider Schrödinger operators with periodic potentials in the positive
quadrant for dim $>1$ with Dirichlet boundary condition. We show that for any
integer $N$ and any interval $I$ there exists a periodic potential such that
the Schrödinger operator has $N$ eigenvalues counted with the multiplicity on
this interval and there is no other spectrum on the interval. Furthermore, to
the right and to the left of it there is a essential spectrum.
Moreover, we prove similar results for Schrödinger operators for other
domains. The proof is based on the inverse spectral theory for Hill operators
on the real line.
",0,0,1,0,0,0
128,Detection of Nonlinearly Distorted OFDM Signals via Generalized Approximate Message Passing,"  In this paper, we propose a practical receiver for multicarrier signals
subjected to a strong memoryless nonlinearity. The receiver design is based on
a generalized approximate message passing (GAMP) framework, and this allows
real-time algorithm implementation in software or hardware with moderate
complexity. We demonstrate that the proposed receiver can provide more than a
2dB gain compared with an ideal uncoded linear OFDM transmission at a BER range
$10^{-4}\div10^{-6}$ in the AWGN channel, when the OFDM signal is subjected to
clipping nonlinearity and the crest-factor of the clipped waveform is only
1.9dB. Simulation results also demonstrate that the proposed receiver provides
significant performance gain in frequency-selective multipath channels
",1,0,0,0,0,0
18326,Time-dependent population imaging for solid high harmonic generation,"  We propose an intuitive method, called time-dependent population imaging
(TDPI), to map the dynamical processes of high harmonic generation (HHG) in
solids by solving the time-dependent Schrödinger equation (TDSE). It is
shown that the real-time dynamical characteristics of HHG in solids, such as
the instantaneous photon energies of emitted harmonics, can be read directly
from the energy-resolved population oscillations of electrons in the TDPIs.
Meanwhile, the short and long trajectories of solid HHG are illustrated clearly
from TDPI. By using the TDPI, we also investigate the effects of
carrier-envelope phase (CEP) in few-cycle pulses and intuitively demonstrate
the HHG dynamics driven by two-color fields. Our results show that the TDPI
provides a powerful tool to study the ultrafast dynamics in strong fields for
various laser-solid configurations and gain an insight into HHG processes in
solids.
",0,1,0,0,0,0
19032,Level set shape and topology optimization of finite strain bilateral contact problems,"  This paper presents a method for the optimization of multi-component
structures comprised of two and three materials considering large motion
sliding contact and separation along interfaces. The structural geometry is
defined by an explicit level set method, which allows for both shape and
topology changes. The mechanical model assumes finite strains, a nonlinear
elastic material behavior, and a quasi-static response. Identification of
overlapping surface position is handled by a coupled parametric representation
of contact surfaces. A stabilized Lagrange method and an active set strategy
are used to model frictionless contact and separation. The mechanical model is
discretized by the extended finite element method which maintains a clear
definition of geometry. Face-oriented ghost penalization and dynamic relaxation
are implemented to improve the stability of the physical response prediction. A
nonlinear programming scheme is used to solve the optimization problem, which
is regularized by introducing a perimeter penalty into the objective function.
Sensitivities are determined by the adjoint method. The main characteristics of
the proposed method are studied by numerical examples in two dimensions. The
numerical results demonstrate improved design performance when compared to
models optimized with a small strain assumption. Additionally, examples with
load path dependent objectives display non-intuitive designs.
",0,0,1,0,0,0
3972,Derivation of a Non-autonomous Linear Boltzmann Equation from a Heterogeneous Rayleigh Gas,"  A linear Boltzmann equation with nonautonomous collision operator is
rigorously derived in the Boltzmann-Grad limit for the deterministic dynamics
of a Rayleigh gas where a tagged particle is undergoing hard-sphere collisions
with heterogeneously distributed background particles, which do not interact
among each other. The validity of the linear Boltzmann equation holds for
arbitrary long times under moderate assumptions on spatial continuity and
higher moments of the initial distributions of the tagged particle and the
heterogeneous, non-equilibrium distribution of the background. The empiric
particle dynamics are compared to the Boltzmann dynamics using evolution
semigroups for Kolmogorov equations of associated probability measures on
collision histories.
",0,0,1,0,0,0
13897,Boundary Hamiltonian theory for gapped topological orders,"  In this letter, we report our systematic construction of the lattice
Hamiltonian model of topological orders on open surfaces, with explicit
boundary terms. We do this mainly for the Levin-Wen stringnet model. The full
Hamiltonian in our approach yields a topologically protected, gapped energy
spectrum, with the corresponding wave functions robust under
topology-preserving transformations of the lattice of the system. We explicitly
present the wavefunctions of the ground states and boundary elementary
excitations. We construct the creation and hopping operators of boundary
quasi-particles. We find that given a bulk topological order, the gapped
boundary conditions are classified by Frobenius algebras in its input data.
Emergent topological properties of the ground states and boundary excitations
are characterized by (bi-) modules over Frobenius algebras.
",0,1,1,0,0,0
17143,Topology and stability of the Kondo phase in quark matter,"  We investigate properties of the ground state of a light quark matter with
heavy quark impurities. This system exhibits the ""QCD Kondo effect"" where the
interaction strength between a light quark near the Fermi surface and a heavy
quark increases with decreasing energy of the light quark towards the Fermi
energy, and diverges at some scale near the Fermi energy, called the Kondo
scale. Around and below the Kondo scale, we must treat the dynamics
nonperturbatively. As a typical nonperturbative method to treat the strong
coupling regime, we adopt a mean-field approach where we introduce a
condensate, the Kondo condensate, representing a mixing between a light quark
and a heavy quark, and determine the ground state in the presence of the Kondo
condensate. We show that the ground state is a topologically non-trivial state
and the heavy quark spin forms the hedgehog configuration in the momentum
space. We can define the Berry phase for the ground-state wavefunction in the
momentum space which is associated with a monopole at the position of a heavy
quark. We also investigate fluctuations around the mean field in the
random-phase approximation, and show the existence of (exciton-like) collective
excitations made of a hole $h$ of a light quark and a heavy quark $Q$.
",0,1,0,0,0,0
10302,Power-Constrained Secrecy Rate Maximization for Joint Relay and Jammer Selection Assisted Wireless Networks,"  In this paper, we examine the physical layer security for cooperative
wireless networks with multiple intermediate nodes, where the
decode-and-forward (DF) protocol is considered. We propose a new joint relay
and jammer selection (JRJS) scheme for protecting wireless communications
against eavesdropping, where an intermediate node is selected as the relay for
the sake of forwarding the source signal to the destination and meanwhile, the
remaining intermediate nodes are employed to act as friendly jammers which
broadcast the artificial noise for disturbing the eavesdropper. We further
investigate the power allocation among the source, relay and friendly jammers
for maximizing the secrecy rate of proposed JRJS scheme and derive a
closed-form sub-optimal solution. Specificially, all the intermediate nodes
which successfully decode the source signal are considered as relay candidates.
For each candidate, we derive the sub-optimal closed-form power allocation
solution and obtain the secrecy rate result of the corresponding JRJS scheme.
Then, the candidate which is capable of achieving the highest secrecy rate is
selected as the relay. Two assumptions about the channel state information
(CSI), namely the full CSI (FCSI) and partial CSI (PCSI), are considered.
Simulation results show that the proposed JRJS scheme outperforms the
conventional pure relay selection, pure jamming and GSVD based beamforming
schemes in terms of secrecy rate. Additionally, the proposed FCSI based power
allocation (FCSI-PA) and PCSI based power allocation (PCSI-PA) schemes both
achieve higher secrecy rates than the equal power allocation (EPA) scheme.
",1,0,1,0,0,0
16873,Detection and Resolution of Rumours in Social Media: A Survey,"  Despite the increasing use of social media platforms for information and news
gathering, its unmoderated nature often leads to the emergence and spread of
rumours, i.e. pieces of information that are unverified at the time of posting.
At the same time, the openness of social media platforms provides opportunities
to study how users share and discuss rumours, and to explore how natural
language processing and data mining techniques may be used to find ways of
determining their veracity. In this survey we introduce and discuss two types
of rumours that circulate on social media; long-standing rumours that circulate
for long periods of time, and newly-emerging rumours spawned during fast-paced
events such as breaking news, where reports are released piecemeal and often
with an unverified status in their early stages. We provide an overview of
research into social media rumours with the ultimate goal of developing a
rumour classification system that consists of four components: rumour
detection, rumour tracking, rumour stance classification and rumour veracity
classification. We delve into the approaches presented in the scientific
literature for the development of each of these four components. We summarise
the efforts and achievements so far towards the development of rumour
classification systems and conclude with suggestions for avenues for future
research in social media mining for detection and resolution of rumours.
",1,0,0,0,0,0
7932,Dynamics of observables in rank-based models and performance of functionally generated portfolios,"  In the seminal work [9], several macroscopic market observables have been
introduced, in an attempt to find characteristics capturing the diversity of a
financial market. Despite the crucial importance of such observables for
investment decisions, a concise mathematical description of their dynamics has
been missing. We fill this gap in the setting of rank-based models and expect
our ideas to extend to other models of large financial markets as well. The
results are then used to study the performance of multiplicatively and
additively functionally generated portfolios, in particular, over short-term
and medium-term horizons.
",0,0,0,0,0,1
2313,Measuring the unmeasurable - a project of domestic violence risk prediction and management,"  The prevention of domestic violence (DV) have aroused serious concerns in
Taiwan because of the disparity between the increasing amount of reported DV
cases that doubled over the past decade and the scarcity of social workers.
Additionally, a large amount of data was collected when social workers use the
predominant case management approach to document case reports information.
However, these data were not properly stored or organized.
To improve the efficiency of DV prevention and risk management, we worked
with Taipei City Government and utilized the 2015 data from its DV database to
perform a spatial pattern analysis of the reports of DV cases to build a DV
risk map. However, during our map building process, the issue of confounding
bias arose because we were not able to verify if reported cases truly reflected
real violence occurrence or were simply false reports from potential victim's
neighbors. Therefore, we used the random forest method to build a repeat
victimization risk prediction model. The accuracy and F1-measure of our model
were 96.3% and 62.8%. This model helped social workers differentiate the risk
level of new cases, which further reduced their major workload significantly.
To our knowledge, this is the first project that utilized machine learning in
DV prevention. The research approach and results of this project not only can
improve DV prevention process, but also be applied to other social work or
criminal prevention areas.
",1,0,0,0,0,0
20946,Non-cocompact Group Actions and $π_1$-Semistability at Infinity,"  A finitely presented 1-ended group $G$ has {\it semistable fundamental group
at infinity} if $G$ acts geometrically on a simply connected and locally
compact ANR $Y$ having the property that any two proper rays in $Y$ are
properly homotopic. This property of $Y$ captures a notion of connectivity at
infinity stronger than ""1-ended"", and is in fact a feature of $G$, being
independent of choices. It is a fundamental property in the homotopical study
of finitely presented groups. While many important classes of groups have been
shown to have semistable fundamental group at infinity, the question of whether
every $G$ has this property has been a recognized open question for nearly
forty years. In this paper we attack the problem by considering a proper {\it
but non-cocompact} action of a group $J$ on such an $Y$. This $J$ would
typically be a subgroup of infinite index in the geometrically acting
over-group $G$; for example $J$ might be infinite cyclic or some other subgroup
whose semistability properties are known. We divide the semistability property
of $G$ into a $J$-part and a ""perpendicular to $J$"" part, and we analyze how
these two parts fit together. Among other things, this analysis leads to a
proof (in a companion paper) that a class of groups previously considered to be
likely counter examples do in fact have the semistability property.
",0,0,1,0,0,0
13785,Spelling Correction as a Foreign Language,"  In this paper, we reformulated the spell correction problem as a machine
translation task under the encoder-decoder framework. This reformulation
enabled us to use a single model for solving the problem that is traditionally
formulated as learning a language model and an error model. This model employs
multi-layer recurrent neural networks as an encoder and a decoder. We
demonstrate the effectiveness of this model using an internal dataset, where
the training data is automatically obtained from user logs. The model offers
competitive performance as compared to the state of the art methods but does
not require any feature engineering nor hand tuning between models.
",1,0,0,0,0,0
13019,Bayesian Uncertainty Directed Trial Designs,"  Most Bayesian response-adaptive designs unbalance randomization rates towards
the most promising arms with the goal of increasing the number of positive
treatment outcomes during the study, even though the primary aim of the trial
is different. We discuss Bayesian uncertainty directed designs (BUD), a class
of Bayesian designs in which the investigator specifies an information measure
tailored to the experiment. All decisions during the trial are selected to
optimize the available information at the end of the study. The approach can be
applied to several designs, ranging from early stage multi-arm trials to
biomarker-driven and multi-endpoint studies. We discuss the asymptotic limit of
the patient allocation proportion to treatments, and illustrate the
finite-sample operating characteristics of BUD designs through examples,
including multi-arm trials, biomarker-stratified trials, and trials with
multiple co-primary endpoints.
",0,0,0,1,0,0
17203,Measuring the Galactic Cosmic Ray Flux with the LISA Pathfinder Radiation Monitor,"  Test mass charging caused by cosmic rays will be a significant source of
acceleration noise for space-based gravitational wave detectors like LISA.
Operating between December 2015 and July 2017, the technology demonstration
mission LISA Pathfinder included a bespoke monitor to help characterise the
relationship between test mass charging and the local radiation environment.
The radiation monitor made in situ measurements of the cosmic ray flux while
also providing information about its energy spectrum. We describe the monitor
and present measurements which show a gradual 40% increase in count rate
coinciding with the declining phase of the solar cycle. Modulations of up to
10% were also observed with periods of 13 and 26 days that are associated with
co-rotating interaction regions and heliospheric current sheet crossings. These
variations in the flux above the monitor detection threshold (approximately 70
MeV) are shown to be coherent with measurements made by the IREM monitor
on-board the Earth orbiting INTEGRAL spacecraft. Finally we use the measured
deposited energy spectra, in combination with a GEANT4 model, to estimate the
galactic cosmic ray differential energy spectrum over the course of the
mission.
",0,1,0,0,0,0
9688,High-power closed-cycle $^4$He cryostat with top-loading sample exchange,"  We report on the development of a versatile cryogen-free laboratory cryostat
based upon a commercial pulse tube cryocooler. It provides enough cooling power
for continuous recondensation of circulating $^4$He gas at a condensation
pressure of approximately 250~mbar. Moreover, the cryostat allows for exchange
of different cryostat-inserts as well as fast and easy ""wet"" top-loading of
samples directly into the 1 K pot with a turn-over time of less than 75~min.
Starting from room temperature and using a $^4$He cryostat-insert, a base
temperature of 1.0~K is reached within approximately seven hours and a cooling
power of 250~mW is established at 1.24~K.
",0,1,0,0,0,0
19447,Machine Learning by Two-Dimensional Hierarchical Tensor Networks: A Quantum Information Theoretic Perspective on Deep Architectures,"  The resemblance between the methods used in quantum-many body physics and in
machine learning has drawn considerable attention. In particular, tensor
networks (TNs) and deep learning architectures bear striking similarities to
the extent that TNs can be used for machine learning. Previous results used
one-dimensional TNs in image recognition, showing limited scalability and
flexibilities. In this work, we train two-dimensional hierarchical TNs to solve
image recognition problems, using a training algorithm derived from the
multipartite entanglement renormalization ansatz. This approach introduces
novel mathematical connections among quantum many-body physics, quantum
information theory, and machine learning. While keeping the TN unitary in the
training phase, TN states are defined, which optimally encode classes of images
into quantum many-body states. We study the quantum features of the TN states,
including quantum entanglement and fidelity. We find these quantities could be
novel properties that characterize the image classes, as well as the machine
learning tasks. Our work could contribute to the research on
identifying/modeling quantum artificial intelligences.
",0,1,0,1,0,0
12658,Heterogeneous nucleation of catalyst-free InAs nanowires on silicon,"  We report on the heterogeneous nucleation of catalyst-free InAs nanowires on
Si (111) substrates by chemical beam epitaxy. We show that nanowire nucleation
is enhanced by sputtering the silicon substrate with energetic particles. We
argue that particle bombardment introduces lattice defects on the silicon
surface that serve as preferential nucleation sites. The formation of these
nucleation sites can be controlled by the sputtering parameters, allowing the
control of nanowire density in a wide range. Nanowire nucleation is accompanied
by unwanted parasitic islands, but by careful choice of annealing and growth
temperature allows to strongly reduce the relative density of these islands and
to realize samples with high nanowire yield.
",0,1,0,0,0,0
9751,Evaluating Predictive Models of Student Success: Closing the Methodological Gap,"  Model evaluation -- the process of making inferences about the performance of
predictive models -- is a critical component of predictive modeling research in
learning analytics. We survey the state of the practice with respect to model
evaluation in learning analytics, which overwhelmingly uses only naive methods
for model evaluation or statistical tests which are not appropriate for
predictive model evaluation. We conduct a critical comparison of both null
hypothesis significance testing (NHST) and a preferred Bayesian method for
model evaluation. Finally, we apply three methods -- the na{ï}ve average
commonly used in learning analytics, NHST, and Bayesian -- to a predictive
modeling experiment on a large set of MOOC data. We compare 96 different
predictive models, including different feature sets, statistical modeling
algorithms, and tuning hyperparameters for each, using this case study to
demonstrate the different experimental conclusions these evaluation techniques
provide.
",0,0,0,1,0,0
6268,Average treatment effects in the presence of unknown interference,"  We investigate large-sample properties of treatment effect estimators under
unknown interference in randomized experiments. The inferential target is a
generalization of the average treatment effect estimand that marginalizes over
potential spillover effects. We show that estimators commonly used to estimate
treatment effects under no-interference are consistent for the generalized
estimand for several common experimental designs under limited but otherwise
arbitrary and unknown interference. The rates of convergence depend on the rate
at which the amount of interference grows and the degree to which it aligns
with dependencies in treatment assignment. Importantly for practitioners, the
results imply that if one erroneously assumes that units do not interfere in a
setting with limited, or even moderate, interference, standard estimators are
nevertheless likely to be close to an average treatment effect if the sample is
sufficiently large.
",0,0,1,1,0,0
9015,"Transformation thermal convection: Cloaking, concentrating, and camouflage","  Heat can generally transfer via thermal conduction, thermal radiation, and
thermal convection. All the existing theories of transformation thermotics and
optics can treat thermal conduction and thermal radiation, respectively.
Unfortunately, thermal convection has never been touched in transformation
theories due to the lack of a suitable theory, thus limiting applications
associated with heat transfer through fluids (liquid or gas). Here, we develop,
for the first time, a general theory of transformation thermal convection by
considering the convection-diffusion equation, the Navier-Stokes equation, and
the Darcy law. By introducing porous media, we get a set of coupled equations
keeping their forms under coordinate transformation. As model applications, the
theory helps to show the effects of cloaking, concentrating, and camouflage.
Our finite element simulations confirm the theoretical findings. This work
offers a general transformation theory for thermal convection, thus revealing
some novel behaviors of thermal convection; it not only provides new hints on
how to control heat transfer by combining thermal conduction, thermal
radiation, and thermal convection, but also benefits the study of mass
diffusion and other related fields that contain a set of equations and need to
transform velocities at the same time.
",0,1,0,0,0,0
3617,Ergodic Exploration of Distributed Information,"  This paper presents an active search trajectory synthesis technique for
autonomous mobile robots with nonlinear measurements and dynamics. The
presented approach uses the ergodicity of a planned trajectory with respect to
an expected information density map to close the loop during search. The
ergodic control algorithm does not rely on discretization of the search or
action spaces, and is well posed for coverage with respect to the expected
information density whether the information is diffuse or localized, thus
trading off between exploration and exploitation in a single objective
function. As a demonstration, we use a robotic electrolocation platform to
estimate location and size parameters describing static targets in an
underwater environment. Our results demonstrate that the ergodic exploration of
distributed information (EEDI) algorithm outperforms commonly used
information-oriented controllers, particularly when distractions are present.
",1,0,0,0,0,0
1007,Inference in Sparse Graphs with Pairwise Measurements and Side Information,"  We consider the statistical problem of recovering a hidden ""ground truth""
binary labeling for the vertices of a graph up to low Hamming error from noisy
edge and vertex measurements. We present new algorithms and a sharp
finite-sample analysis for this problem on trees and sparse graphs with poor
expansion properties such as hypergrids and ring lattices. Our method
generalizes and improves over that of Globerson et al. (2015), who introduced
the problem for two-dimensional grid lattices.
For trees we provide a simple, efficient, algorithm that infers the ground
truth with optimal Hamming error has optimal sample complexity and implies
recovery results for all connected graphs. Here, the presence of side
information is critical to obtain a non-trivial recovery rate. We then show how
to adapt this algorithm to tree decompositions of edge-subgraphs of certain
graph families such as lattices, resulting in optimal recovery error rates that
can be obtained efficiently
The thrust of our analysis is to 1) use the tree decomposition along with
edge measurements to produce a small class of viable vertex labelings and 2)
apply an analysis influenced by statistical learning theory to show that we can
infer the ground truth from this class using vertex measurements. We show the
power of our method in several examples including hypergrids, ring lattices,
and the Newman-Watts model for small world graphs. For two-dimensional grids,
our results improve over Globerson et al. (2015) by obtaining optimal recovery
in the constant-height regime.
",1,0,0,0,0,0
3170,D-optimal designs for complex Ornstein-Uhlenbeck processes,"  Complex Ornstein-Uhlenbeck (OU) processes have various applications in
statistical modelling. They play role e.g. in the description of the motion of
a charged test particle in a constant magnetic field or in the study of
rotating waves in time-dependent reaction diffusion systems, whereas Kolmogorov
used such a process to model the so-called Chandler wobble, small deviation in
the Earth's axis of rotation. In these applications parameter estimation and
model fitting is based on discrete observations of the underlying stochastic
process, however, the accuracy of the results strongly depend on the
observation points.
This paper studies the properties of D-optimal designs for estimating the
parameters of a complex OU process with a trend. We show that in contrast with
the case of the classical real OU process, a D-optimal design exists not only
for the trend parameter, but also for joint estimation of the covariance
parameters, moreover, these optimal designs are equidistant.
",0,0,1,1,0,0
19170,Higher-Order Bounded Model Checking,"  We present a Bounded Model Checking technique for higher-order programs. The
vehicle of our study is a higher-order calculus with general references. Our
technique is a symbolic state syntactical translation based on SMT solvers,
adapted to a setting where the values passed and stored during computation can
be functions of arbitrary order. We prove that our algorithm is sound, and
devise an optimisation based on points-to analysis to improve scalability. We
moreover provide a prototype implementation of the algorithm with experimental
results showcasing its performance.
",1,0,0,0,0,0
19775,Time optimal sampled-data controls for heat equations,"  In this paper, we first design a time optimal control problem for the heat
equation with sampled-data controls, and then use it to approximate a time
optimal control problem for the heat equation with distributed controls. Our
design is reasonable from perspective of sampled-data controls. And it might
provide a right way for the numerical approach of a time optimal distributed
control problem, via the corresponding semi-discretized (in time variable) time
optimal control problem.
The study of such a time optimal sampled-data control problem is not easy,
because it may have infinitely many optimal controls. We find connections among
this problem, a minimal norm sampled-data control problem and a minimization
problem. And obtain some properties on these problems. Based on these, we not
only build up error estimates for optimal time and optimal controls between the
time optimal sampled-data control problem and the time optimal distributed
control problem, in terms of the sampling period, but also prove that such
estimates are optimal in some sense.
",0,0,1,0,0,0
8021,Emulation of the space radiation environment for materials testing and radiobiological experiments,"  Radiobiology studies on the effects of galactic cosmic ray radiation utilize
mono-energetic single-ion particle beams, where the projected doses for
exploration missions are given using highly-acute exposures. This methodology
does not replicate the multi-ion species and energies found in the space
radiation environment, nor does it reflect the low dose rate found in
interplanetary space. In radiation biology studies, as well as in the
assessment of health risk to astronaut crews, the differences in the biological
effectiveness of different ions is primarily attributed to differences in the
linear energy transfer of the radiation spectrum. Here we show that the linear
energy transfer spectrum of the intravehicular environment of, e.g.,
spaceflight vehicles can be accurately generated experimentally by perturbing
the intrinsic properties of hydrogen-rich crystalline materials in order to
instigate specific nuclear spallation and fragmentation processes when placed
in an accelerated mono-energetic heavy ion beam. Modifications to the internal
geometry and chemical composition of the materials allow for the shaping of the
emerging field to specific spectra that closely resemble the intravehicular
field. Our approach can also be utilized to emulate the external galactic
cosmic ray field, the planetary surface spectrum (e.g., Mars), and the local
radiation environment of orbiting satellites. This provides the first instance
of a true ground-based analog for characterizing the effects of space
radiation.
",0,1,0,0,0,0
5051,Coupling Load-Following Control with OPF,"  In this paper, the optimal power flow (OPF) problem is augmented to account
for the costs associated with the load-following control of a power network.
Load-following control costs are expressed through the linear quadratic
regulator (LQR). The power network is described by a set of nonlinear
differential algebraic equations (DAEs). By linearizing the DAEs around a known
equilibrium, a linearized OPF that accounts for steady-state operational
constraints is formulated first. This linearized OPF is then augmented by a set
of linear matrix inequalities that are algebraically equivalent to the
implementation of an LQR controller. The resulting formulation, termed LQR-OPF,
is a semidefinite program which furnishes optimal steady-state setpoints and an
optimal feedback law to steer the system to the new steady state with minimum
load-following control costs. Numerical tests demonstrate that the setpoints
computed by LQR-OPF result in lower overall costs and frequency deviations
compared to the setpoints of a scheme where OPF and load-following control are
considered separately.
",1,0,1,0,0,0
2931,When Work Matters: Transforming Classical Network Structures to Graph CNN,"  Numerous pattern recognition applications can be formed as learning from
graph-structured data, including social network, protein-interaction network,
the world wide web data, knowledge graph, etc. While convolutional neural
network (CNN) facilitates great advances in gridded image/video understanding
tasks, very limited attention has been devoted to transform these successful
network structures (including Inception net, Residual net, Dense net, etc.) to
establish convolutional networks on graph, due to its irregularity and
complexity geometric topologies (unordered vertices, unfixed number of adjacent
edges/vertices). In this paper, we aim to give a comprehensive analysis of when
work matters by transforming different classical network structures to graph
CNN, particularly in the basic graph recognition problem. Specifically, we
firstly review the general graph CNN methods, especially in its spectral
filtering operation on the irregular graph data. We then introduce the basic
structures of ResNet, Inception and DenseNet into graph CNN and construct these
network structures on graph, named as G_ResNet, G_Inception, G_DenseNet. In
particular, it seeks to help graph CNNs by shedding light on how these
classical network structures work and providing guidelines for choosing
appropriate graph network frameworks. Finally, we comprehensively evaluate the
performance of these different network structures on several public graph
datasets (including social networks and bioinformatic datasets), and
demonstrate how different network structures work on graph CNN in the graph
recognition task.
",0,0,0,1,0,0
20888,Scalable and Robust Sparse Subspace Clustering Using Randomized Clustering and Multilayer Graphs,"  Sparse subspace clustering (SSC) is one of the current state-of-the-art
methods for partitioning data points into the union of subspaces, with strong
theoretical guarantees. However, it is not practical for large data sets as it
requires solving a LASSO problem for each data point, where the number of
variables in each LASSO problem is the number of data points. To improve the
scalability of SSC, we propose to select a few sets of anchor points using a
randomized hierarchical clustering method, and, for each set of anchor points,
solve the LASSO problems for each data point allowing only anchor points to
have a non-zero weight (this reduces drastically the number of variables). This
generates a multilayer graph where each layer corresponds to a different set of
anchor points. Using the Grassmann manifold of orthogonal matrices, the shared
connectivity among the layers is summarized within a single subspace. Finally,
we use $k$-means clustering within that subspace to cluster the data points,
similarly as done by spectral clustering in SSC. We show on both synthetic and
real-world data sets that the proposed method not only allows SSC to scale to
large-scale data sets, but that it is also much more robust as it performs
significantly better on noisy data and on data with close susbspaces and
outliers, while it is not prone to oversegmentation.
",0,0,0,1,0,0
8730,On the complexity of generalized chromatic polynomials,"  J. Makowsky and B. Zilber (2004) showed that many variations of graph
colorings, called CP-colorings in the sequel, give rise to graph polynomials.
This is true in particular for harmonious colorings, convex colorings,
mcc_t-colorings, and rainbow colorings, and many more. N. Linial (1986) showed
that the chromatic polynomial $\chi(G;X)$ is #P-hard to evaluate for all but
three values X=0,1,2, where evaluation is in P. This dichotomy includes
evaluation at real or complex values, and has the further property that the set
of points for which evaluation is in P is finite. We investigate how the
complexity of evaluating univariate graph polynomials that arise from
CP-colorings varies for different evaluation points. We show that for some
CP-colorings (harmonious, convex) the complexity of evaluation follows a
similar pattern to the chromatic polynomial. However, in other cases (proper
edge colorings, mcc_t-colorings, H-free colorings) we could only obtain a
dichotomy for evaluations at non-negative integer points. We also discuss some
CP-colorings where we only have very partial results.
",1,0,1,0,0,0
12824,Nonlinear Instability of Half-Solitons on Star Graphs,"  We consider a half-soliton stationary state of the nonlinear Schrodinger
equation with the power nonlinearity on a star graph consisting of N edges and
a single vertex. For the subcritical power nonlinearity, the half-soliton state
is a degenerate critical point of the action functional under the mass
constraint such that the second variation is nonnegative. By using normal
forms, we prove that the degenerate critical point is a nonlinear saddle point,
for which the small perturbations to the half-soliton state grow slowly in time
resulting in the nonlinear instability of the half-soliton state. The result
holds for any $N \geq 3$ and arbitrary subcritical power nonlinearity. It gives
a precise dynamical characterization of the previous result of Adami {\em et
al.}, where the half-soliton state was shown to be a saddle point of the action
functional under the mass constraint for $N = 3$ and for cubic nonlinearity.
",0,1,1,0,0,0
20135,Narrow-line Laser Cooling by Adiabatic Transfer,"  We propose and demonstrate a novel laser cooling mechanism applicable to
particles with narrow-linewidth optical transitions. By sweeping the frequency
of counter-propagating laser beams in a sawtooth manner, we cause adiabatic
transfer back and forth between the ground state and a long-lived optically
excited state. The time-ordering of these adiabatic transfers is determined by
Doppler shifts, which ensures that the associated photon recoils are in the
opposite direction to the particle's motion. This ultimately leads to a robust
cooling mechanism capable of exerting large forces via a weak transition and
with reduced reliance on spontaneous emission. We present a simple intuitive
model for the resulting frictional force, and directly demonstrate its efficacy
for increasing the total phase-space density of an atomic ensemble. We rely on
both simulation and experimental studies using the 7.5~kHz linewidth $^1$S$_0$
to $^3$P$_1$ transition in $^{88}$Sr. The reduced reliance on spontaneous
emission may allow this adiabatic sweep method to be a useful tool for cooling
particles that lack closed cycling transitions, such as molecules.
",0,1,0,0,0,0
7756,$M$-QAM Precoder Design for MIMO Directional Modulation Transceivers,"  Spectrally efficient multi-antenna wireless communication systems are a key
challenge as service demands continue to increase. At the same time, powering
up radio access networks is facing environmental and regulation limitations. In
order to achieve more power efficiency, we design a directional modulation
precoder by considering an $M$-QAM constellation, particularly with
$M=4,8,16,32$. First, extended detection regions are defined for desired
constellations using analytical geometry. Then, constellation points are placed
in the optimal positions of these regions while the minimum Euclidean distance
to adjacent constellation points and detection region boundaries is kept as in
the conventional $M$-QAM modulation. For further power efficiency and symbol
error rate similar to that of fixed design in high SNR, relaxed detection
regions are modeled for inner points of $M=16,32$ constellations. The modeled
extended and relaxed detection regions as well as the modulation
characteristics are utilized to formulate symbol-level precoder design problems
for directional modulation to minimize the transmission power while preserving
the minimum required SNR at the destination. In addition, the extended and
relaxed detection regions are used for precoder design to minimize the output
of each power amplifier. We transform the design problems into convex ones and
devise an interior point path-following iterative algorithm to solve the
mentioned problems and provide details on finding the initial values of the
parameters and the starting point. Results show that compared to the benchmark
schemes, the proposed method performs better in terms of power and peak power
reduction as well as symbol error rate reduction for a wide range of SNRs.
",1,0,0,0,0,0
16731,ChemGAN challenge for drug discovery: can AI reproduce natural chemical diversity?,"  Generating molecules with desired chemical properties is important for drug
discovery. The use of generative neural networks is promising for this task.
However, from visual inspection, it often appears that generated samples lack
diversity. In this paper, we quantify this internal chemical diversity, and we
raise the following challenge: can a nontrivial AI model reproduce natural
chemical diversity for desired molecules? To illustrate this question, we
consider two generative models: a Reinforcement Learning model and the recently
introduced ORGAN. Both fail at this challenge. We hope this challenge will
stimulate research in this direction.
",1,0,0,1,0,0
4348,Toward Unsupervised Text Content Manipulation,"  Controlled generation of text is of high practical use. Recent efforts have
made impressive progress in generating or editing sentences with given textual
attributes (e.g., sentiment). This work studies a new practical setting of text
content manipulation. Given a structured record, such as `(PLAYER: Lebron,
POINTS: 20, ASSISTS: 10)', and a reference sentence, such as `Kobe easily
dropped 30 points', we aim to generate a sentence that accurately describes the
full content in the record, with the same writing style (e.g., wording,
transitions) of the reference. The problem is unsupervised due to lack of
parallel data in practice, and is challenging to minimally yet effectively
manipulate the text (by rewriting/adding/deleting text portions) to ensure
fidelity to the structured content. We derive a dataset from a basketball game
report corpus as our testbed, and develop a neural method with unsupervised
competing objectives and explicit content coverage constraints. Automatic and
human evaluations show superiority of our approach over competitive methods
including a strong rule-based baseline and prior approaches designed for style
transfer.
",1,0,0,0,0,0
20007,Iterative bidding in electricity markets: rationality and robustness,"  This paper studies an electricity market consisting of an independent system
operator (ISO) and a group of generators. The goal is to solve the DC optimal
power flow (DC-OPF) problem: have the generators collectively meet the power
demand while minimizing the aggregate generation cost and respecting line flow
limits in the network. The ISO by itself cannot solve the DC-OPF problem as
generators are strategic and do not share their cost functions. Instead, each
generator submits to the ISO a bid, consisting of the price per unit of
electricity at which it is willing to provide power. Based on the bids, the ISO
decides how much production to allocate to each generator to minimize the total
payment while meeting the load and satisfying the line limits. We provide a
provably correct, decentralized iterative scheme, termed BID ADJUSTMENT
ALGORITHM, for the resulting Bertrand competition game. Regarding convergence,
we show that the algorithm takes the generators' bids to any desired
neighborhood of the efficient Nash equilibrium at a linear convergence rate. As
a consequence, the optimal production of the generators converges to the
optimizer of the DC-OPF problem. Regarding robustness, we show that the
algorithm is robust to affine perturbations in the bid adjustment scheme and
that there is no incentive for any individual generator to deviate from the
algorithm by using an alternative bid update scheme. We also establish the
algorithm robustness to collusion, i.e., we show that, as long as each bus with
generation has a generator following the strategy, there is no incentive for
any group of generators to share information with the intent of tricking the
system to obtain a higher payoff. Simulations illustrate our results.
",1,0,1,0,0,0
20532,Cross-referencing Social Media and Public Surveillance Camera Data for Disaster Response,"  Physical media (like surveillance cameras) and social media (like Instagram
and Twitter) may both be useful in attaining on-the-ground information during
an emergency or disaster situation. However, the intersection and reliability
of both surveillance cameras and social media during a natural disaster are not
fully understood. To address this gap, we tested whether social media is of
utility when physical surveillance cameras went off-line during Hurricane Irma
in 2017. Specifically, we collected and compared geo-tagged Instagram and
Twitter posts in the state of Florida during times and in areas where public
surveillance cameras went off-line. We report social media content and
frequency and content to determine the utility for emergency managers or first
responders during a natural disaster.
",1,0,0,0,0,0
4231,Classical and quantum systems: transport due to rare events,"  We review possible mechanisms for energy transfer based on 'rare' or
'non-perturbative' effects, in physical systems that present a many-body
localized phenomenology. The main focus is on classical systems, with or
without quenched disorder. For non-quantum systems, the breakdown of
localization is usually not regarded as an issue, and we thus aim at
identifying the fastest channels for transport. Next, we contemplate the
possibility of applying the same mechanisms in quantum systems, including
disorder free systems (e.g. Bose-Hubbard chain), disordered many-body localized
systems with mobility edges at energies below the edge, and strongly disordered
lattice systems in $d>1$. For quantum mechanical systems, the relevance of
these considerations for transport is currently a matter of debate.
",0,1,0,0,0,0
2880,Is One Hyperparameter Optimizer Enough?,"  Hyperparameter tuning is the black art of automatically finding a good
combination of control parameters for a data miner. While widely applied in
empirical Software Engineering, there has not been much discussion on which
hyperparameter tuner is best for software analytics. To address this gap in the
literature, this paper applied a range of hyperparameter optimizers (grid
search, random search, differential evolution, and Bayesian optimization) to
defect prediction problem. Surprisingly, no hyperparameter optimizer was
observed to be `best' and, for one of the two evaluation measures studied here
(F-measure), hyperparameter optimization, in 50\% cases, was no better than
using default configurations.
We conclude that hyperparameter optimization is more nuanced than previously
believed. While such optimization can certainly lead to large improvements in
the performance of classifiers used in software analytics, it remains to be
seen which specific optimizers should be applied to a new dataset.
",1,0,0,0,0,0
7699,Opinion diversity and community formation in adaptive networks,"  It is interesting and of significant importance to investigate how network
structures co-evolve with opinions. The existing models of such co-evolution
typically lead to the final states where network nodes either reach a global
consensus or break into separated communities, each of which holding its own
community consensus. Such results, however, can hardly explain the richness of
real-life observations that opinions are always diversified with no global or
even community consensus, and people seldom, if not never, totally cut off
themselves from dissenters. In this article, we show that, a simple model
integrating consensus formation, link rewiring and opinion change allows
complex system dynamics to emerge, driving the system into a dynamic
equilibrium with co-existence of diversified opinions. Specifically, similar
opinion holders may form into communities yet with no strict community
consensus; and rather than being separated into disconnected communities,
different communities remain to be interconnected by non-trivial proportion of
inter-community links. More importantly, we show that the complex dynamics may
lead to different numbers of communities at steady state with a given tolerance
between different opinion holders. We construct a framework for theoretically
analyzing the co-evolution process. Theoretical analysis and extensive
simulation results reveal some useful insights into the complex co-evolution
process, including the formation of dynamic equilibrium, the phase transition
between different steady states with different numbers of communities, and the
dynamics between opinion distribution and network modularity, etc.
",1,1,0,0,0,0
20594,Degeneration in VAE: in the Light of Fisher Information Loss,"  While enormous progress has been made to Variational Autoencoder (VAE) in
recent years, similar to other deep networks, VAE with deep networks suffers
from the problem of degeneration, which seriously weakens the correlation
between the input and the corresponding latent codes, deviating from the goal
of the representation learning. To investigate how degeneration affects VAE
from a theoretical perspective, we illustrate the information transmission in
VAE and analyze the intermediate layers of the encoders/decoders. Specifically,
we propose a Fisher Information measure for the layer-wise analysis. With such
measure, we demonstrate that information loss is ineluctable in feed-forward
networks and causes the degeneration in VAE. We show that skip connections in
VAE enable the preservation of information without changing the model
architecture. We call this class of VAE equipped with skip connections as SCVAE
and perform a range of experiments to show its advantages in information
preservation and degeneration mitigation.
",0,0,0,1,0,0
6726,"On the Hilbert coefficients, depth of associated graded rings and reduction numbers","  Let $(R,\mathfrak{m})$ be a $d$-dimensional Cohen-Macaulay local ring, $I$ an
$\mathfrak{m}$-primary ideal of $R$ and $J=(x_1,...,x_d)$ a minimal reduction
of $I$. We show that if $J_{d-1}=(x_1,...,x_{d-1})$ and
$\sum\limits_{n=1}^\infty\lambda{({I^{n+1}\cap J_{d-1}})/({J{I^n} \cap
J_{d-1}})=i}$ where i=0,1, then depth $G(I)\geq{d-i-1}$. Moreover, we prove
that if $e_2(I) = \sum_{n=2}^\infty (n-1) \lambda (I^n/JI^{n-1})-2;$ or if $I$
is integrally closed and $e_2(I) = \sum_{n=2}^\infty
(n-1)\lambda({I^{n}}/JI^{n-1})-i$ where $i=3,4$, then $e_1(I) =
\sum_{n=1}^\infty \lambda(I^n / JI^{n-1})-1.$ In addition, we show that $r(I)$
is independent. Furthermore, we study the independence of $r(I)$ with some
other conditions.
",0,0,1,0,0,0
19396,On quasi-hereditary algebras,"  In this paper we introduce an easily verifiable sufficient condition to
determine whether an algebra is quasi-hereditary. In the case of monomial
algebras, we give conditions that are both necessary and sufficient to show
whether an algebra is quasi-hereditary.
",0,0,1,0,0,0
18131,Introducing the Simulated Flying Shapes and Simulated Planar Manipulator Datasets,"  We release two artificial datasets, Simulated Flying Shapes and Simulated
Planar Manipulator that allow to test the learning ability of video processing
systems. In particular, the dataset is meant as a tool which allows to easily
assess the sanity of deep neural network models that aim to encode, reconstruct
or predict video frame sequences. The datasets each consist of 90000 videos.
The Simulated Flying Shapes dataset comprises scenes showing two objects of
equal shape (rectangle, triangle and circle) and size in which one object
approaches its counterpart. The Simulated Planar Manipulator shows a 3-DOF
planar manipulator that executes a pick-and-place task in which it has to place
a size-varying circle on a squared platform. Different from other widely used
datasets such as moving MNIST [1], [2], the two presented datasets involve
goal-oriented tasks (e.g. the manipulator grasping an object and placing it on
a platform), rather than showing random movements. This makes our datasets more
suitable for testing prediction capabilities and the learning of sophisticated
motions by a machine learning model. This technical document aims at providing
an introduction into the usage of both datasets.
",1,0,0,0,0,0
6357,Finite size effects for spiking neural networks with spatially dependent coupling,"  We study finite-size fluctuations in a network of spiking deterministic
neurons coupled with non-uniform synaptic coupling. We generalize a previously
developed theory of finite size effects for uniform globally coupled neurons.
In the uniform case, mean field theory is well defined by averaging over the
network as the number of neurons in the network goes to infinity. However, for
nonuniform coupling it is no longer possible to average over the entire network
if we are interested in fluctuations at a particular location within the
network. We show that if the coupling function approaches a continuous function
in the infinite system size limit then an average over a local neighborhood can
be defined such that mean field theory is well defined for a spatially
dependent field. We then derive a perturbation expansion in the inverse system
size around the mean field limit for the covariance of the input to a neuron
(synaptic drive) and firing rate fluctuations due to dynamical deterministic
finite-size effects.
",0,0,0,0,1,0
4771,Cascaded Coded Distributed Computing on Heterogeneous Networks,"  Coded distributed computing (CDC) introduced by Li et al. in 2015 offers an
efficient approach to trade computing power to reduce the communication load in
general distributed computing frameworks such as MapReduce. For the more
general cascaded CDC, Map computations are repeated at $r$ nodes to
significantly reduce the communication load among nodes tasked with computing
$Q$ Reduce functions $s$ times. While an achievable cascaded CDC scheme was
proposed, it only operates on homogeneous networks, where the storage,
computation load and communication load of each computing node is the same. In
this paper, we address this limitation by proposing a novel combinatorial
design which operates on heterogeneous networks where nodes have varying
storage and computing capabilities. We provide an analytical characterization
of the computation-communication trade-off and show that it is optimal within a
constant factor and could outperform the state-of-the-art homogeneous schemes.
",1,0,0,0,0,0
13896,Setting Players' Behaviors in World of Warcraft through Semi-Supervised Learning,"  Digital games are one of the major and most important fields on the
entertainment domain, which also involves cinema and music. Numerous attempts
have been done to improve the quality of the games including more realistic
artistic production and computer science. Assessing the player's behavior, a
task known as player modeling, is currently the need of the hour which leads to
possible improvements in terms of: (i) better game interaction experience, (ii)
better exploitation of the relationship between players, and (iii)
increasing/maintaining the number of players interested in the game. In this
paper we model players using the basic four behaviors proposed in
\cite{BartleArtigo}, namely: achiever, explorer, socializer and killer. Our
analysis is carried out using data obtained from the game ""World of Warcraft""
over 3 years (2006 $-$ 2009). We employ a semi-supervised learning technique in
order to find out characteristics that possibly impact player's behavior.
",1,0,0,0,0,0
18772,Traffic models with adversarial vehicle behaviour,"  We examine the impact of adversarial actions on vehicles in traffic. Current
advances in assisted/autonomous driving technologies are supposed to reduce the
number of casualties, but this seems to be desired despite the recently proved
insecurity of in-vehicle communication buses or components. Fortunately to some
extent, while compromised cars have become a reality, the numerous attacks
reported so far on in-vehicle electronics are exclusively concerned with
impairments of a single target. In this work we put adversarial behavior under
a more complex scenario where driving decisions deluded by corrupted
electronics can affect more than one vehicle. Particularly, we focus our
attention on chain collisions involving multiple vehicles that can be amplified
by simple adversarial interventions, e.g., delaying taillights or falsifying
speedometer readings. We provide metrics for assessing adversarial impact and
consider safety margins against adversarial actions. Moreover, we discuss
intelligent adversarial behaviour by which the creation of rogue platoons is
possible and speed manipulations become stealthy to human drivers. We emphasize
that our work does not try to show the mere fact that imprudent speeds and
headways lead to chain-collisions, but points out that an adversary may favour
such scenarios (eventually keeping his actions stealthy for human drivers) and
further asks for quantifying the impact of adversarial activity or whether
existing traffic regulations are prepared for such situations.
",1,0,0,0,0,0
8073,Efficient Compression and Indexing of Trajectories,"  We present a new compressed representation of free trajectories of moving
objects. It combines a partial-sums-based structure that retrieves in constant
time the position of the object at any instant, with a hierarchical
minimum-bounding-boxes representation that allows determining if the object is
seen in a certain rectangular area during a time period. Combined with spatial
snapshots at regular intervals, the representation is shown to outperform
classical ones by orders of magnitude in space, and also to outperform previous
compressed representations in time performance, when using the same amount of
space.
",1,0,0,0,0,0
6393,A Tutorial on Deep Learning for Music Information Retrieval,"  Following their success in Computer Vision and other areas, deep learning
techniques have recently become widely adopted in Music Information Retrieval
(MIR) research. However, the majority of works aim to adopt and assess methods
that have been shown to be effective in other domains, while there is still a
great need for more original research focusing on music primarily and utilising
musical knowledge and insight. The goal of this paper is to boost the interest
of beginners by providing a comprehensive tutorial and reducing the barriers to
entry into deep learning for MIR. We lay out the basic principles and review
prominent works in this hard to navigate the field. We then outline the network
structures that have been successful in MIR problems and facilitate the
selection of building blocks for the problems at hand. Finally, guidelines for
new tasks and some advanced topics in deep learning are discussed to stimulate
new research in this fascinating field.
",1,0,0,0,0,0
3639,ConsiDroid: A Concolic-based Tool for Detecting SQL Injection Vulnerability in Android Apps,"  In this paper, we present a concolic execution technique for detecting SQL
injection vulnerabilities in Android apps, with a new tool we called
ConsiDroid. We extend the source code of apps with mocking technique, such that
the execution of original source code is not affected. The extended source
codes can be treated as Java applications and may be executed by SPF with
concolic execution. We automatically produce a DummyMain class out of static
analysis such that the essential functions are called sequentially and, the
events leading to vulnerable functions are triggered. We extend SPF with taint
analysis in ConsiDroid. For making taint analysis possible, we introduce a new
technique of symbolic mock classes in order to ease the propagation of tainted
values in the code. An SQL injection vulnerability is detected through
receiving a tainted value by a vulnerable function. Besides, ConsiDroid takes
advantage of static analysis to adjust SPF in order to inspect only suspicious
paths. To illustrate the applicability of ConsiDroid, we have inspected
randomly selected 140 apps from F-Droid repository. From these apps, we found
three apps vulnerable to SQL injection. To verify their vulnerability, we
analyzed the apps manually based on ConsiDroid's reports by using Robolectric.
",1,0,0,0,0,0
12309,On Security and Sparsity of Linear Classifiers for Adversarial Settings,"  Machine-learning techniques are widely used in security-related applications,
like spam and malware detection. However, in such settings, they have been
shown to be vulnerable to adversarial attacks, including the deliberate
manipulation of data at test time to evade detection. In this work, we focus on
the vulnerability of linear classifiers to evasion attacks. This can be
considered a relevant problem, as linear classifiers have been increasingly
used in embedded systems and mobile devices for their low processing time and
memory requirements. We exploit recent findings in robust optimization to
investigate the link between regularization and security of linear classifiers,
depending on the type of attack. We also analyze the relationship between the
sparsity of feature weights, which is desirable for reducing processing cost,
and the security of linear classifiers. We further propose a novel octagonal
regularizer that allows us to achieve a proper trade-off between them. Finally,
we empirically show how this regularizer can improve classifier security and
sparsity in real-world application examples including spam and malware
detection.
",1,0,0,0,0,0
7840,Autoencoder Based Sample Selection for Self-Taught Learning,"  Self-taught learning is a technique that uses a large number of unlabeled
data as source samples to improve the task performance on target samples.
Compared with other transfer learning techniques, self-taught learning can be
applied to a broader set of scenarios due to the loose restrictions on source
data. However, knowledge transferred from source samples that are not
sufficiently related to the target domain may negatively influence the target
learner, which is referred to as negative transfer. In this paper, we propose a
metric for the relevance between a source sample and target samples. To be more
specific, both source and target samples are reconstructed through a
single-layer autoencoder with a linear relationship between source samples and
target samples simultaneously enforced. An l_{2,1}-norm sparsity constraint is
imposed on the transformation matrix to identify source samples relevant to the
target domain. Source domain samples that are deemed relevant are assigned
pseudo-labels reflecting their relevance to target domain samples, and are
combined with target samples in order to provide an expanded training set for
classifier training. Local data structures are also preserved during source
sample selection through spectral graph analysis. Promising results in
extensive experiments show the advantages of the proposed approach.
",0,0,0,1,0,0
10317,Laplace approximation and the natural gradient for Gaussian process regression with the heteroscedastic Student-t model,"  This paper considers the Laplace method to derive approximate inference for
the Gaussian process (GP) regression in the location and scale parameters of
the Student-t probabilistic model. This allows both mean and variance of the
data to vary as a function of covariates with the attractive feature that the
Student-t model has been widely used as a useful tool for robustifying data
analysis. The challenge in the approximate inference for the GP regression with
the Student-t probabilistic model, lies in the analytical intractability of the
posterior distribution and the lack of concavity of the log-likelihood
function. We present the natural gradient adaptation for the estimation process
which primarily relies on the property that the Student-t model naturally has
orthogonal parametrization with respect to the location and scale paramaters.
Due to this particular property of the model, we also introduce an alternative
Laplace approximation by using the Fisher information matrix in place of the
Hessian matrix of the negative log-likelihood function. According to
experiments this alternative approximation provides very similar posterior
approximations and predictive performance when compared to the traditional
Laplace approximation. We also compare both of these Laplace approximations
with the Monte Carlo Markov Chain (MCMC) method. Moreover, we compare our
heteroscedastic Student-t model and the GP regression with the heteroscedastic
Gaussian model. We also discuss how our approach can improve the inference
algorithm in cases where the probabilistic model assumed for the data is not
log-concave.
",0,0,0,1,0,0
10633,Online control of the false discovery rate with decaying memory,"  In the online multiple testing problem, p-values corresponding to different
null hypotheses are observed one by one, and the decision of whether or not to
reject the current hypothesis must be made immediately, after which the next
p-value is observed. Alpha-investing algorithms to control the false discovery
rate (FDR), formulated by Foster and Stine, have been generalized and applied
to many settings, including quality-preserving databases in science and
multiple A/B or multi-armed bandit tests for internet commerce. This paper
improves the class of generalized alpha-investing algorithms (GAI) in four
ways: (a) we show how to uniformly improve the power of the entire class of
monotone GAI procedures by awarding more alpha-wealth for each rejection,
giving a win-win resolution to a recent dilemma raised by Javanmard and
Montanari, (b) we demonstrate how to incorporate prior weights to indicate
domain knowledge of which hypotheses are likely to be non-null, (c) we allow
for differing penalties for false discoveries to indicate that some hypotheses
may be more important than others, (d) we define a new quantity called the
decaying memory false discovery rate (mem-FDR) that may be more meaningful for
truly temporal applications, and which alleviates problems that we describe and
refer to as ""piggybacking"" and ""alpha-death"". Our GAI++ algorithms incorporate
all four generalizations simultaneously, and reduce to more powerful variants
of earlier algorithms when the weights and decay are all set to unity. Finally,
we also describe a simple method to derive new online FDR rules based on an
estimated false discovery proportion.
",1,0,1,1,0,0
12600,Short DNA persistence length in a mesoscopic helical model,"  The flexibility of short DNA chains is investigated via computation of the
average correlation function between dimers which defines the persistence
length. Path integration techniques have been applied to confine the phase
space available to base pair fluctuations and derive the partition function.
The apparent persistence lengths of a set of short chains have been computed as
a function of the twist conformation both in the over-twisted and the untwisted
regimes, whereby the equilibrium twist is selected by free energy minimization.
The obtained values are significantly lower than those generally attributed to
kilo-base long DNA. This points to an intrinsic helix flexibility at short
length scales, arising from large fluctuational effects and local bending, in
line with recent experimental indications. The interplay between helical
untwisting and persistence length has been discussed for a heterogeneous
fragment by weighing the effects of the sequence specificities through the
non-linear stacking potential.
",0,0,0,0,1,0
1607,Magnetization spin dynamics in a (LuBi)3Fe5O12 (BLIG) epitaxial film,"  Bismuth substituted lutetium iron garnet (BLIG) films exhibit larger Faraday
rotation, and have a higher Curie temperature than yttrium iron garnet. We have
observed magnetic stripe domains and measured domain widths of 1.4 {\mu}{\mu}m
using Fourier domain polarization microscopy, Faraday rotation experiments
yield a coercive field of 5 Oe. These characterizations form the basis of
micromagnetic simulations that allow us to estimate and compare spin wave
excitations in BLIG films. We observed that these films support thermal magnons
with a precessional frequency of 7 GHz with a line width of 400 MHz. Further,
we studied the dependence of precessional frequency on the externally applied
magnetic field. Brillouin light scattering experiments and precession
frequencies predicted by simulations show similar trend with increasing field.
",0,1,0,0,0,0
16593,Infinite rank surface cluster algebras,"  We generalise surface cluster algebras to the case of infinite surfaces where
the surface contains finitely many accumulation points of boundary marked
points. To connect different triangulations of an infinite surface, we consider
infinite mutation sequences.
We show transitivity of infinite mutation sequences on triangulations of an
infinite surface and examine different types of mutation sequences. Moreover,
we use a hyperbolic structure on an infinite surface to extend the notion of
surface cluster algebras to infinite rank by giving cluster variables as lambda
lengths of arcs. Furthermore, we study the structural properties of infinite
rank surface cluster algebras in combinatorial terms, namely we extend ""snake
graph combinatorics"" to give an expansion formula for cluster variables. We
also show skein relations for infinite rank surface cluster algebras.
",0,0,1,0,0,0
9263,Iterative PET Image Reconstruction Using Convolutional Neural Network Representation,"  PET image reconstruction is challenging due to the ill-poseness of the
inverse problem and limited number of detected photons. Recently deep neural
networks have been widely and successfully used in computer vision tasks and
attracted growing interests in medical imaging. In this work, we trained a deep
residual convolutional neural network to improve PET image quality by using the
existing inter-patient information. An innovative feature of the proposed
method is that we embed the neural network in the iterative reconstruction
framework for image representation, rather than using it as a post-processing
tool. We formulate the objective function as a constraint optimization problem
and solve it using the alternating direction method of multipliers (ADMM)
algorithm. Both simulation data and hybrid real data are used to evaluate the
proposed method. Quantification results show that our proposed iterative neural
network method can outperform the neural network denoising and conventional
penalized maximum likelihood methods.
",0,1,0,1,0,0
2603,Biomedical Event Trigger Identification Using Bidirectional Recurrent Neural Network Based Models,"  Biomedical events describe complex interactions between various biomedical
entities. Event trigger is a word or a phrase which typically signifies the
occurrence of an event. Event trigger identification is an important first step
in all event extraction methods. However many of the current approaches either
rely on complex hand-crafted features or consider features only within a
window. In this paper we propose a method that takes the advantage of recurrent
neural network (RNN) to extract higher level features present across the
sentence. Thus hidden state representation of RNN along with word and entity
type embedding as features avoid relying on the complex hand-crafted features
generated using various NLP toolkits. Our experiments have shown to achieve
state-of-art F1-score on Multi Level Event Extraction (MLEE) corpus. We have
also performed category-wise analysis of the result and discussed the
importance of various features in trigger identification task.
",1,0,0,0,0,0
14512,SMT Queries Decomposition and Caching in Semi-Symbolic Model Checking,"  In semi-symbolic (control-explicit data-symbolic) model checking the
state-space explosion problem is fought by representing sets of states by
first-order formulas over the bit-vector theory. In this model checking
approach, most of the verification time is spent in an SMT solver on deciding
satisfiability of quantified queries, which represent equality of symbolic
states. In this paper, we introduce a new scheme for decomposition of symbolic
states, which can be used to significantly improve the performance of any
semi-symbolic model checker. Using the decomposition, a model checker can issue
much simpler and smaller queries to the solver when compared to the original
case. Some SMT calls may be even avoided completely, as the satisfaction of
some of the simplified formulas can be decided syntactically. Moreover, the
decomposition allows for an efficient caching scheme for quantified formulas.
To support our theoretical contribution, we show the performance gain of our
model checker SymDIVINE on a set of examples from the Software Verification
Competition.
",1,0,0,0,0,0
20034,Radiation Hardness Test of Eljen EJ-500 Optical Cement,"  We present a comprehensive account of the proton radiation hardness of Eljen
Technology's EJ-500 optical cement used in the construction of experiment
detectors. The cement was embedded into five plastic scintillator tiles which
were each exposed to one of five different levels of radiation by a 50 MeV
proton beam produced at the 88-Inch Cyclotron at Lawrence Berkeley National
Laboratory. A cosmic ray telescope setup was used to measure signal amplitudes
before and after irradiation. Another post-radiation measurement was taken four
months after the experiment to investigate whether the radiation damage to the
cement recovers after a short amount of time. We verified that the radiation
damage to the tiles increased with increasing dose but showed significant
improvement after the four months time interval.
",0,1,0,0,0,0
11813,Three dimensional free-surface flow over arbitrary bottom topography,"  We consider steady nonlinear free surface flow past an arbitrary bottom
topography in three dimensions, concentrating on the shape of the wave pattern
that forms on the surface of the fluid. Assuming ideal fluid flow, the problem
is formulated using a boundary integral method and discretised to produce a
nonlinear system of algebraic equations. The Jacobian of this system is dense
due to integrals being evaluated over the entire free surface. To overcome the
computational difficulty and large memory requirements, a Jacobian-free Newton
Krylov (JFNK) method is utilised. Using a block-banded approximation of the
Jacobian from the linearised system as a preconditioner for the JFNK scheme, we
find significant reductions in computational time and memory required for
generating numerical solutions. These improvements also allow for a larger
number of mesh points over the free surface and the bottom topography. We
present a range of numerical solutions for both subcritical and supercritical
regimes, and for a variety of bottom configurations. We discuss nonlinear
features of the wave patterns as well as their relationship to ship wakes.
",0,1,0,0,0,0
19454,Nash and Wardrop equilibria in aggregative games with coupling constraints,"  We consider the framework of aggregative games, in which the cost function of
each agent depends on his own strategy and on the average population strategy.
As first contribution, we investigate the relations between the concepts of
Nash and Wardrop equilibrium. By exploiting a characterization of the two
equilibria as solutions of variational inequalities, we bound their distance
with a decreasing function of the population size. As second contribution, we
propose two decentralized algorithms that converge to such equilibria and are
capable of coping with constraints coupling the strategies of different agents.
Finally, we study the applications of charging of electric vehicles and of
route choice on a road network.
",1,0,1,0,0,0
2113,Greedy Strategy Works for Clustering with Outliers and Coresets Construction,"  We study the problems of clustering with outliers in high dimension. Though a
number of methods have been developed in the past decades, it is still quite
challenging to design quality guaranteed algorithms with low complexities for
the problems. Our idea is inspired by the greedy method, Gonzalez's algorithm,
for solving the problem of ordinary $k$-center clustering. Based on some novel
observations, we show that this greedy strategy actually can handle
$k$-center/median/means clustering with outliers efficiently, in terms of
qualities and complexities. We further show that the greedy approach yields
small coreset for the problem in doubling metrics, so as to reduce the time
complexity significantly. Moreover, a by-product is that the coreset
construction can be applied to speedup the popular density-based clustering
approach DBSCAN.
",1,0,0,0,0,0
4449,Randomness-induced quantum spin liquid on honeycomb lattice,"  We present a quantu spin liquid state in a spin-1/2 honeycomb lattice with
randomness in the exchange interaction. That is, we successfully introduce
randomness into the organic radial-based complex and realize a random-singlet
(RS) state. All magnetic and thermodynamic experimental results indicate the
liquid-like behaviors, which are consistent with those expected in the RS
state. These results demonstrate that the randomness or inhomogeneity in the
actual systems stabilize the RS state and yield liquid-like behavior.
",0,1,0,0,0,0
15723,Multi-Pose Face Recognition Using Hybrid Face Features Descriptor,"  This paper presents a multi-pose face recognition approach using hybrid face
features descriptors (HFFD). The HFFD is a face descriptor containing of rich
discriminant information that is created by fusing some frequency-based
features extracted using both wavelet and DCT analysis of several different
poses of 2D face images. The main aim of this method is to represent the
multi-pose face images using a dominant frequency component with still having
reasonable achievement compared to the recent multi-pose face recognition
methods. The HFFD based face recognition tends to achieve better performance
than that of the recent 2D-based face recognition method. In addition, the
HFFD-based face recognition also is sufficiently to handle large face
variability due to face pose variations .
",1,0,0,0,0,0
2504,Model comparison for Gibbs random fields using noisy reversible jump Markov chain Monte Carlo,"  The reversible jump Markov chain Monte Carlo (RJMCMC) method offers an
across-model simulation approach for Bayesian estimation and model comparison,
by exploring the sampling space that consists of several models of possibly
varying dimensions. A naive implementation of RJMCMC to models like Gibbs
random fields suffers from computational difficulties: the posterior
distribution for each model is termed doubly-intractable since computation of
the likelihood function is rarely available. Consequently, it is simply
impossible to simulate a transition of the Markov chain in the presence of
likelihood intractability. A variant of RJMCMC is presented, called noisy
RJMCMC, where the underlying transition kernel is replaced with an
approximation based on unbiased estimators. Based on previous theoretical
developments, convergence guarantees for the noisy RJMCMC algorithm are
provided. The experiments show that the noisy RJMCMC algorithm can be much more
efficient than other exact methods, provided that an estimator with controlled
Monte Carlo variance is used, a fact which is in agreement with the theoretical
analysis.
",0,0,0,1,0,0
5842,How Complex is your classification problem? A survey on measuring classification complexity,"  Extracting characteristics from the training datasets of classification
problems has proven effective in a number of meta-analyses. Among them,
measures of classification complexity can estimate the difficulty in separating
the data points into their expected classes. Descriptors of the spatial
distribution of the data and estimates of the shape and size of the decision
boundary are among the existent measures for this characterization. This
information can support the formulation of new data-driven pre-processing and
pattern recognition techniques, which can in turn be focused on challenging
characteristics of the problems. This paper surveys and analyzes measures which
can be extracted from the training datasets in order to characterize the
complexity of the respective classification problems. Their use in recent
literature is also reviewed and discussed, allowing to prospect opportunities
for future work in the area. Finally, descriptions are given on an R package
named Extended Complexity Library (ECoL) that implements a set of complexity
measures and is made publicly available.
",0,0,0,1,0,0
452,"From bare interactions, low--energy constants and unitary gas to nuclear density functionals without free parameters: application to neutron matter","  We further progress along the line of Ref. [Phys. Rev. {\bf A 94}, 043614
(2016)] where a functional for Fermi systems with anomalously large $s$-wave
scattering length $a_s$ was proposed that has no free parameters. The
functional is designed to correctly reproduce the unitary limit in Fermi gases
together with the leading-order contributions in the s- and p-wave channels at
low density. The functional is shown to be predictive up to densities
$\sim0.01$ fm$^{-3}$ that is much higher densities compared to the Lee-Yang
functional, valid for $\rho < 10^{-6}$ fm$^{-3}$. The form of the functional
retained in this work is further motivated. It is shown that the new functional
corresponds to an expansion of the energy in $(a_s k_F)$ and $(r_e k_F)$ to all
orders, where $r_e$ is the effective range and $k_F$ is the Fermi momentum. One
conclusion from the present work is that, except in the extremely low--density
regime, nuclear systems can be treated perturbatively in $-(a_s k_F)^{-1}$ with
respect to the unitary limit. Starting from the functional, we introduce
density--dependent scales and show that scales associated to the bare
interaction are strongly renormalized by medium effects. As a consequence, some
of the scales at play around saturation are dominated by the unitary gas
properties and not directly to low-energy constants. For instance, we show that
the scale in the s-wave channel around saturation is proportional to the
so-called Bertsch parameter $\xi_0$ and becomes independent of $a_s$. We also
point out that these scales are of the same order of magnitude than those
empirically obtained in the Skyrme energy density functional. We finally
propose a slight modification of the functional such that it becomes accurate
up to the saturation density $\rho\simeq 0.16$ fm$^{-3}$.
",0,1,0,0,0,0
6533,Experimental determination of the frequency and field dependence of Specific Loss Power in Magnetic Fluid Hyperthermia,"  Magnetic nanoparticles are promising systems for biomedical applications and
in particular for Magnetic Fluid Hyperthermia, a promising therapy that
utilizes the heat released by such systems to damage tumor cells. We present an
experimental study of the physical properties that influences the capability of
heat release, i.e. the Specific Loss Power, SLP, of three biocompatible
ferrofluid samples having a magnetic core of maghemite with different core
diameter d= 10.2, 14.6 and 19.7 nm. The SLP was measured as a function of
frequency f and intensity of the applied alternating magnetic field H, and it
turned out to depend on the core diameter, as expected. The results allowed us
to highlight experimentally that the physical mechanism responsible for the
heating is size-dependent and to establish, at applied constant frequency, the
phenomenological functional relationship SLP=cH^x, with 2<x<3 for all samples.
The x-value depends on sample size and field frequency/ intensity, here chosen
in the typical range of operating magnetic hyperthermia devices. For the
smallest sample, the effective relaxation time Teff=19.5 ns obtained from SLP
data is in agreement with the value estimated from magnetization data, thus
confirming the validity of the Linear Response Theory model for this system at
properly chosen field intensity and frequency.
",0,1,0,0,0,0
8504,Verifying Asynchronous Interactions via Communicating Session Automata,"  The relationship between communicating automata and session types is the
cornerstone of many diverse theories and tools, including type checking, code
generation, and runtime verification. A serious limitation of session types is
that, while endpoint programs interact asynchronously, the underlying property
which guarantees safety of session types is too synchronous: it requires a
one-to-one synchronisation between send and receive actions. This paper
proposes a sound procedure to verify properties of communicating session
automata (CSA), i.e., communicating automata that correspond to multiparty
session types. We introduce a new asynchronous compatibility property for CSA,
called k-multiparty compatibility (k-MC), which is a strict superset of the
synchronous multiparty compatibility proposed in the literature. It is
decomposed into two bounded properties: (i) a condition called k-safety which
guarantees that, within the bound, all sent messages can be received and each
automaton can make a move; and (ii) a condition called k-exhaustivity which
guarantees that all k-reachable send actions can be fired within the bound. We
show that k-exhaustive systems soundly and completely characterise systems
where each automaton behaves uniformly for any bound greater or equal to k. We
show that checking k-MC is PSPACE-complete, but can be done efficiently over
large systems by using partial order reduction techniques. We demonstrate that
several examples from the literature are k-MC, but not synchronous compatible.
",1,0,0,0,0,0
1641,Learning to Draw Samples with Amortized Stein Variational Gradient Descent,"  We propose a simple algorithm to train stochastic neural networks to draw
samples from given target distributions for probabilistic inference. Our method
is based on iteratively adjusting the neural network parameters so that the
output changes along a Stein variational gradient direction (Liu & Wang, 2016)
that maximally decreases the KL divergence with the target distribution. Our
method works for any target distribution specified by their unnormalized
density function, and can train any black-box architectures that are
differentiable in terms of the parameters we want to adapt. We demonstrate our
method with a number of applications, including variational autoencoder (VAE)
with expressive encoders to model complex latent space structures, and
hyper-parameter learning of MCMC samplers that allows Bayesian inference to
adaptively improve itself when seeing more data.
",0,0,0,1,0,0
5108,Antiferromagnetic Chern insulators in non-centrosymmetric systems,"  We investigate a new class of topological antiferromagnetic (AF) Chern
insulators driven by electronic interactions in two-dimensional systems without
inversion symmetry. Despite the absence of a net magnetization, AF Chern
insulators (AFCI) possess a nonzero Chern number $C$ and exhibit the quantum
anomalous Hall effect (QAHE). Their existence is guaranteed by the bifurcation
of the boundary line of Weyl points between a quantum spin Hall insulator and a
topologically trivial phase with the emergence of AF long-range order. As a
concrete example, we study the phase structure of the honeycomb lattice
Kane-Mele model as a function of the inversion-breaking ionic potential and the
Hubbard interaction. We find an easy $z$-axis $C=1$ AFCI phase and a spin-flop
transition to a topologically trivial $xy$-plane collinear antiferromagnet. We
propose experimental realizations of the AFCI and QAHE in correlated electron
materials and cold atom systems.
",0,1,0,0,0,0
18530,How to construct wavelets on local fields of positive characteristic,"  We present an algorithm for construction step wavelets on local fields of
positive characteristic.
",0,0,1,0,0,0
11925,Motion of a thin elliptic plate under symmetric and asymmetric orthotropic friction forces,"  Anisotropy of friction force is proved to be an important factor in various
contact problems. We study dynamical behavior of thin plates with respect to
symmetric and asymmetric orthotropic friction. Terminal motion of plates with
circular and elliptic contact areas is mainly analyzed. Evaluation of friction
forces for both symmetric and asymmetric orthotropic cases are shown. Regular
pressure distribution is considered. Differential equations are formulated and
solved numerically for a number of initial conditions. Examples show
significant influence of friction force asymmetry on the motion.
",0,1,0,0,0,0
3799,Birecurrent sets,"  A set is called recurrent if its minimal automaton is strongly connected and
birecurrent if it is recurrent as well as its reversal. We prove a series of
results concerning birecurrent sets. It is already known that any birecurrent
set is completely reducible (that is, such that the minimal representation of
its characteristic series is completely reducible). The main result of this
paper characterizes completely reducible sets as linear combinations of
birecurrent sets
",1,0,1,0,0,0
4323,Extreme radio-wave scattering associated with hot stars,"  We use data on extreme radio scintillation to demonstrate that this
phenomenon is associated with hot stars in the solar neighbourhood. The ionized
gas responsible for the scattering is found at distances up to 1.75pc from the
host star, and on average must comprise 1.E5 distinct structures per star. We
detect azimuthal velocities of the plasma, relative to the host star, up to 9.7
km/s, consistent with warm gas expanding at the sound speed. The circumstellar
plasma structures that we infer are similar in several respects to the cometary
knots seen in the Helix, and in other planetary nebulae. There the ionized gas
appears as a skin around tiny molecular clumps. Our analysis suggests that
molecular clumps are ubiquitous circumstellar features, unrelated to the
evolutionary state of the star. The total mass in such clumps is comparable to
the stellar mass.
",0,1,0,0,0,0
4580,Degenerate cyclotomic Hecke algebras and higher level Heisenberg categorification,"  We associate a monoidal category $\mathcal{H}^\lambda$ to each dominant
integral weight $\lambda$ of $\widehat{\mathfrak{sl}}_p$ or
$\mathfrak{sl}_\infty$. These categories, defined in terms of planar diagrams,
act naturally on categories of modules for the degenerate cyclotomic Hecke
algebras associated to $\lambda$. We show that, in the $\mathfrak{sl}_\infty$
case, the level $d$ Heisenberg algebra embeds into the Grothendieck ring of
$\mathcal{H}^\lambda$, where $d$ is the level of $\lambda$. The categories
$\mathcal{H}^\lambda$ can be viewed as a graphical calculus describing
induction and restriction functors between categories of modules for degenerate
cyclotomic Hecke algebras, together with their natural transformations. As an
application of this tool, we prove a new result concerning centralizers for
degenerate cyclotomic Hecke algebras.
",0,0,1,0,0,0
11338,Simple Length Rigidity for Hitchin Representations,"  We show that a Hitchin representation is determined by the spectral radii of
the images of simple, non-separating closed curves. As a consequence, we
classify isometries of the intersection function on Hitchin components of
dimension 3 and on the self-dual Hitchin components in all dimensions. As an
important tool in the proof, we establish a transversality result for positive
quadruples of flags.
",0,0,1,0,0,0
15609,Quantum gap and spin-wave excitations in the Kitaev model on a triangular lattice,"  We study the effects of quantum fluctuations on the dynamical generation of a
gap and on the evolution of the spin-wave spectra of a frustrated magnet on a
triangular lattice with bond-dependent Ising couplings, analog of the Kitaev
honeycomb model. The quantum fluctuations lift the subextensive degeneracy of
the classical ground-state manifold by a quantum order-by-disorder mechanism.
Nearest-neighbor chains remain decoupled and the surviving discrete degeneracy
of the ground state is protected by a hidden model symmetry. We show how the
four-spin interaction, emergent from the fluctuations, generates a spin gap
shifting the nodal lines of the linear spin-wave spectrum to finite energies.
",0,1,0,0,0,0
6849,Reaction-Diffusion Systems in Epidemiology,"  A key problem in modelling the evolution dynamics of infectious diseases is
the mathematical representation of the mechanism of transmission of the
contagion. Models with a finite number of subpopulations can be described via
systems of ordinary differential equations. When dealing with populations with
space structure the relevant quantities are spatial densities, whose evolution
in time requires nonlinear partial differential equations, which are known as
reaction-diffusion systems. Here we present an (historical) outline of
mathematical epidemiology, with a particular attention to the role of spatial
heterogeneity and dispersal in the population dynamics of infectious diseases.
Two specific examples are discussed, which have been the subject of intensive
research by the authors, i.e. man-environment-man epidemics, and malaria. In
addition to the epidemiological relevance of these epidemics all over the
world, their treatment requires a large amount of different sophisticate
mathematical methods, and has even posed new non trivial mathematical problems,
as one can realize from the list of references. One of the most relevant
problems posed by the authors, i.e. regional control, has been emphasized here:
the public health concern consists of eradicating the disease in the relevant
population, as fast as possible. On the other hand, very often the entire
domain of interest for the epidemic, is either unknown, or difficult to manage
for an affordable implementation of suitable environmental programmes. For
regional control instead it might be sufficient to implement such programmes
only in a given subregion conveniently chosen so to lead to an effective
(exponentially fast) eradication of the epidemic in the whole habitat; it is
evident that this practice may have an enormous importance in real cases with
respect to both financial and practical affordability.
",0,0,1,0,0,0
5049,Modelling the evaporation of nanoparticle suspensions from heterogeneous surfaces,"  We present a Monte Carlo (MC) grid-based model for the drying of drops of a
nanoparticle suspension upon a heterogeneous surface. The model consists of a
generalised lattice-gas in which the interaction parameters in the Hamiltonian
can be varied to model different properties of the materials involved. We show
how to choose correctly the interactions, to minimise the effects of the
underlying grid so that hemispherical droplets form. We also include the
effects of surface roughness to examine the effects of contact-line pinning on
the dynamics. When there is a `lid' above the system, which prevents
evaporation, equilibrium drops form on the surface, which we use to determine
the contact angle and how it varies as the parameters of the model are changed.
This enables us to relate the interaction parameters to the materials used in
applications. The model has also been applied to drying on heterogeneous
surfaces, in particular to the case where the suspension is deposited on a
surface consisting of a pair of hydrophilic conducting metal surfaces that are
either side of a band of hydrophobic insulating polymer. This situation occurs
when using inkjet printing to manufacture electrical connections between the
metallic parts of the surface. The process is not always without problems,
since the liquid can dewet from the hydrophobic part of the surface, breaking
the bridge before the drying process is complete. The MC model reproduces the
observed dewetting, allowing the parameters to be varied so that the conditions
for the best connection can be established. We show that if the hydrophobic
portion of the surface is located at a step below the height of the
neighbouring metal, the chance of dewetting of the liquid during the drying
process is significantly reduced.
",0,1,0,0,0,0
8144,Collusions in Teichmüller expansions,"  If $\mathfrak{p} \subseteq \mathbb{Z}[\zeta]$ is a prime ideal over $p$ in
the $(p^d - 1)$th cyclotomic extension of $\mathbb{Z}$, then every element
$\alpha$ of the completion $\mathbb{Z}[\zeta]_\mathfrak{p}$ has a unique
expansion as a power series in $p$ with coefficients in $\mu_{p^d -1} \cup
\{0\}$ called the Teichmüller expansion of $\alpha$ at $\mathfrak{p}$. We
observe three peculiar and seemingly unrelated patterns that frequently appear
in the computation of Teichmüller expansions, then develop a unifying theory
to explain these patterns in terms of the dynamics of an affine group action on
$\mathbb{Z}[\zeta]$.
",0,0,1,0,0,0
6984,"Model compression as constrained optimization, with application to neural nets. Part II: quantization","  We consider the problem of deep neural net compression by quantization: given
a large, reference net, we want to quantize its real-valued weights using a
codebook with $K$ entries so that the training loss of the quantized net is
minimal. The codebook can be optimally learned jointly with the net, or fixed,
as for binarization or ternarization approaches. Previous work has quantized
the weights of the reference net, or incorporated rounding operations in the
backpropagation algorithm, but this has no guarantee of converging to a
loss-optimal, quantized net. We describe a new approach based on the recently
proposed framework of model compression as constrained optimization
\citep{Carreir17a}. This results in a simple iterative ""learning-compression""
algorithm, which alternates a step that learns a net of continuous weights with
a step that quantizes (or binarizes/ternarizes) the weights, and is guaranteed
to converge to local optimum of the loss for quantized nets. We develop
algorithms for an adaptive codebook or a (partially) fixed codebook. The latter
includes binarization, ternarization, powers-of-two and other important
particular cases. We show experimentally that we can achieve much higher
compression rates than previous quantization work (even using just 1 bit per
weight) with negligible loss degradation.
",1,0,1,1,0,0
6021,"Genetic algorithm-based control of birefringent filtering for self-tuning, self-pulsing fiber lasers","  Polarization-based filtering in fiber lasers is well-known to enable spectral
tunability and a wide range of dynamical operating states. This effect is
rarely exploited in practical systems, however, because optimization of cavity
parameters is non-trivial and evolves due to environmental sensitivity. Here,
we report a genetic algorithm-based approach, utilizing electronic control of
the cavity transfer function, to autonomously achieve broad wavelength tuning
and the generation of Q-switched pulses with variable repetition rate and
duration. The practicalities and limitations of simultaneous spectral and
temporal self-tuning from a simple fiber laser are discussed, paving the way to
on-demand laser properties through algorithmic control and machine learning
schemes.
",0,1,0,0,0,0
15499,Dynamic Stochastic Approximation for Multi-stage Stochastic Optimization,"  In this paper, we consider multi-stage stochastic optimization problems with
convex objectives and conic constraints at each stage. We present a new
stochastic first-order method, namely the dynamic stochastic approximation
(DSA) algorithm, for solving these types of stochastic optimization problems.
We show that DSA can achieve an optimal ${\cal O}(1/\epsilon^4)$ rate of
convergence in terms of the total number of required scenarios when applied to
a three-stage stochastic optimization problem. We further show that this rate
of convergence can be improved to ${\cal O}(1/\epsilon^2)$ when the objective
function is strongly convex. We also discuss variants of DSA for solving more
general multi-stage stochastic optimization problems with the number of stages
$T > 3$. The developed DSA algorithms only need to go through the scenario tree
once in order to compute an $\epsilon$-solution of the multi-stage stochastic
optimization problem. To the best of our knowledge, this is the first time that
stochastic approximation type methods are generalized for multi-stage
stochastic optimization with $T \ge 3$.
",1,0,1,1,0,0
18886,Optimal Algorithms for Distributed Optimization,"  In this paper, we study the optimal convergence rate for distributed convex
optimization problems in networks. We model the communication restrictions
imposed by the network as a set of affine constraints and provide optimal
complexity bounds for four different setups, namely: the function $F(\xb)
\triangleq \sum_{i=1}^{m}f_i(\xb)$ is strongly convex and smooth, either
strongly convex or smooth or just convex. Our results show that Nesterov's
accelerated gradient descent on the dual problem can be executed in a
distributed manner and obtains the same optimal rates as in the centralized
version of the problem (up to constant or logarithmic factors) with an
additional cost related to the spectral gap of the interaction matrix. Finally,
we discuss some extensions to the proposed setup such as proximal friendly
functions, time-varying graphs, improvement of the condition numbers.
",1,0,0,1,0,0
7560,Discrete Dynamic Causal Modeling and Its Relationship with Directed Information,"  This paper explores the discrete Dynamic Causal Modeling (DDCM) and its
relationship with Directed Information (DI). We prove the conditional
equivalence between DDCM and DI in characterizing the causal relationship
between two brain regions. The theoretical results are demonstrated using fMRI
data obtained under both resting state and stimulus based state. Our numerical
analysis is consistent with that reported in previous study.
",0,0,0,1,0,0
13124,On the Robustness and Asymptotic Properties for Maximum Likelihood Estimators of Parameters in Exponential Power and its Scale Mixture Form Distributions,"  The normality assumption on data set is very restrictive approach for
modelling. The generalized form of normal distribution, named as an exponential
power (EP) distribution, and its scale mixture form have been considered
extensively to overcome the problem for modelling non-normal data set since
last decades. However, examining the robustness properties of maximum
likelihood (ML) estimators of parameters in these distributions, such as the in
uence function, gross-error sensitivity, breakdown point and
information-standardized sensitivity, has not been considered together. The
well-known asymptotic properties of ML estimators of location, scale and added
skewness parameters in EP and its scale mixture form distributions are studied
and also these ML estimators for location, scale and scale variant (skewness)
parameters can be represented as an iterative reweighting algorithm to compute
the estimates of these parameters simultaneously.
",0,0,1,1,0,0
19107,Hybrid Dirac Semimetal in CaAgBi Materials Family,"  Based on their formation mechanisms, Dirac points in three-dimensional
systems can be classified as accidental or essential. The former can be further
distinguished into type-I and type-II, depending on whether the Dirac cone
spectrum is completely tipped over along certain direction. Here, we predict
the coexistence of all three kinds of Dirac points in the low-energy band
structure of CaAgBi-family materials with a stuffed Wurtzite structure. Two
pairs of accidental Dirac points reside on the rotational axis, with one pair
being type-I and the other pair type-II; while another essential Dirac point is
pinned at the high symmetry point on the Brillouin zone boundary. Due to broken
inversion symmetry, the band degeneracy around accidental Dirac points is
completely lifted except along the rotational axis, which may enable the
splitting of chiral carriers at a ballistic p-n junction with a double negative
refraction effect. We clarify their symmetry protections, and find both the
Dirac-cone and Fermi arc topological surface states.
",0,1,0,0,0,0
1808,Radio Frequency Interference Mitigation,"  Radio astronomy observational facilities are under constant upgradation and
development to achieve better capabilities including increasing the time and
frequency resolutions of the recorded data, and increasing the receiving and
recording bandwidth. As only a limited spectrum resource has been allocated to
radio astronomy by the International Telecommunication Union, this results in
the radio observational instrumentation being inevitably exposed to undesirable
radio frequency interference (RFI) signals which originate mainly from
terrestrial human activity and are becoming stronger with time. RFIs degrade
the quality of astronomical data and even lead to data loss. The impact of RFIs
on scientific outcome is becoming progressively difficult to manage. In this
article, we motivate the requirement for RFI mitigation, and review the RFI
characteristics, mitigation techniques and strategies. Mitigation strategies
adopted at some representative observatories, telescopes and arrays are also
introduced. We also discuss and present advantages and shortcomings of the four
classes of RFI mitigation strategies, applicable at the connected causal
stages: preventive, pre-detection, pre-correlation and post-correlation. The
proper identification and flagging of RFI is key to the reduction of data loss
and improvement in data quality, and is also the ultimate goal of developing
RFI mitigation techniques. This can be achieved through a strategy involving a
combination of the discussed techniques in stages. Recent advances in high
speed digital signal processing and high performance computing allow for
performing RFI excision of large data volumes generated from large telescopes
or arrays in both real time and offline modes, aiding the proposed strategy.
",0,1,0,0,0,0
5568,Bounds for the difference between two Čebyšev functionals,"  In this work, a generalization of pre-Grüss inequality is established.
Several bounds for the difference between two Čebyšev functional are
proved.
",0,0,1,0,0,0
5850,Spectrum of signless 1-Laplacian on simplicial complexes,"  We first develop a general framework for signless 1-Laplacian defined in
terms of the combinatorial structure of a simplicial complex. The structure of
the eigenvectors and the complex feature of eigenvalues are studied. The
Courant nodal domain theorem for partial differential equation is extended to
the signless 1-Laplacian on complex. We also study the effects of a wedge sum
and a duplication of a motif on the spectrum of the signless 1-Laplacian, and
identify some of the combinatorial features of a simplicial complex that are
encoded in its spectrum. A special result is that the independent number and
clique covering number on a complex provide lower and upper bounds of the
multiplicity of the largest eigenvalue of signless 1-Laplacian, respectively,
which has no counterpart of $p$-Laplacian for any $p>1$.
",0,0,1,0,0,0
460,Axiomatic quantum mechanics: Necessity and benefits for the physics studies,"  The ongoing progress in quantum theory emphasizes the crucial role of the
very basic principles of quantum theory. However, this is not properly followed
in teaching quantum mechanics on the graduate and undergraduate levels of
physics studies. The existing textbooks typically avoid the axiomatic
presentation of the theory. We emphasize usefulness of the systematic,
axiomatic approach to the basics of quantum theory as well as its importance in
the light of the modern scientific-research context.
",0,1,0,0,0,0
15706,Fluid photonic crystal from colloidal quantum dots,"  We study optical forces acting upon semiconductor quantum dots and the force
driven motion of the dots in a colloid. In the spectral range of exciton
transitions in uantum dots, when the photon energy is close to the exciton
energy, the polarizability of the dots is drastically increased. It leads to a
resonant increase of both the gradient and the scattering contributions to the
optical force, which enables the efficient manipulation with the dots. We
reveal that the optical grating of the colloid leads to the formation of a
fluid photonic crystal with spatially periodic circulating fluxes and density
of the dots. Pronounced resonant dielectric response of semiconductor quantum
dots enables a separation of the quantum dots with different exciton
frequencies.
",0,1,0,0,0,0
13769,Creating a Cybersecurity Concept Inventory: A Status Report on the CATS Project,"  We report on the status of our Cybersecurity Assessment Tools (CATS) project
that is creating and validating a concept inventory for cybersecurity, which
assesses the quality of instruction of any first course in cybersecurity. In
fall 2014, we carried out a Delphi process that identified core concepts of
cybersecurity. In spring 2016, we interviewed twenty-six students to uncover
their understandings and misconceptions about these concepts. In fall 2016, we
generated our first assessment tool--a draft Cybersecurity Concept Inventory
(CCI), comprising approximately thirty multiple-choice questions. Each question
targets a concept; incorrect answers are based on observed misconceptions from
the interviews. This year we are validating the draft CCI using cognitive
interviews, expert reviews, and psychometric testing. In this paper, we
highlight our progress to date in developing the CCI.
The CATS project provides infrastructure for a rigorous evidence-based
improvement of cybersecurity education. The CCI permits comparisons of
different instructional methods by assessing how well students learned the core
concepts of the field (especially adversarial thinking), where instructional
methods refer to how material is taught (e.g., lab-based, case-studies,
collaborative, competitions, gaming). Specifically, the CCI is a tool that will
enable researchers to scientifically quantify and measure the effect of their
approaches to, and interventions in, cybersecurity education.
",1,0,0,0,0,0
1028,Absolute versus convective helical magnetorotational instabilities in Taylor-Couette flows,"  We study magnetic Taylor-Couette flow in a system having nondimensional radii
$r_i=1$ and $r_o=2$, and periodic in the axial direction with wavelengths
$h\ge100$. The rotation ratio of the inner and outer cylinders is adjusted to
be slightly in the Rayleigh-stable regime, where magnetic fields are required
to destabilize the flow, in this case triggering the axisymmetric helical
magnetorotational instability (HMRI). Two choices of imposed magnetic field are
considered, both having the same azimuthal component $B_\phi=r^{-1}$, but
differing axial components. The first choice has $B_z=0.1$, and yields the
familiar HMRI, consisting of unidirectionally traveling waves. The second
choice has $B_z\approx0.1\sin(2\pi z/h)$, and yields HMRI waves that travel in
opposite directions depending on the sign of $B_z$. The first configuration
corresponds to a convective instability, the second to an absolute instability.
The two variants behave very similarly regarding both linear onset as well as
nonlinear equilibration.
",0,1,0,0,0,0
16982,Epidemic spreading in multiplex networks influenced by opinion exchanges on vaccination,"  We study the changes of opinions about vaccination together with the
evolution of a disease. In our model we consider a multiplex network consisting
of two layers. One of the layers corresponds to a social network where people
share their opinions and influence others opinions. The social model that rules
the dynamic is the M-model, which takes into account two different processes
that occurs in a society: persuasion and compromise. This two processes are
related through a parameter $r$, $r<1$ describes a moderate and committed
society, for $r>1$ the society tends to have extremist opinions, while $r=1$
represents a neutral society. This social network may be of real or virtual
contacts. On the other hand, the second layer corresponds to a network of
physical contacts where the disease spreading is described by the SIR-Model. In
this model the individuals may be in one of the following four states:
Susceptible ($S$), Infected($I$), Recovered ($R$) or Vaccinated ($V$). A
Susceptible individual can: i) get vaccinated, if his opinion in the other
layer is totally in favor of the vaccine, ii) get infected, with probability
$\beta$ if he is in contact with an infected neighbor. Those $I$ individuals
recover after a certain period $t_r=6$. Vaccinated individuals have an
extremist positive opinion that does not change. We consider that the vaccine
has a certain effectiveness $\omega$ and as a consequence vaccinated nodes can
be infected with probability $\beta (1 - \omega)$ if they are in contact with
an infected neighbor. In this case, if the infection process is successful, the
new infected individual changes his opinion from extremist positive to totally
against the vaccine. We find that depending on the trend in the opinion of the
society, which depends on $r$, different behaviors in the spread of the
epidemic occurs. An epidemic threshold was found.
",0,1,0,0,0,0
12888,Stochastic Subsampling for Factorizing Huge Matrices,"  We present a matrix-factorization algorithm that scales to input matrices
with both huge number of rows and columns. Learned factors may be sparse or
dense and/or non-negative, which makes our algorithm suitable for dictionary
learning, sparse component analysis, and non-negative matrix factorization. Our
algorithm streams matrix columns while subsampling them to iteratively learn
the matrix factors. At each iteration, the row dimension of a new sample is
reduced by subsampling, resulting in lower time complexity compared to a simple
streaming algorithm. Our method comes with convergence guarantees to reach a
stationary point of the matrix-factorization problem. We demonstrate its
efficiency on massive functional Magnetic Resonance Imaging data (2 TB), and on
patches extracted from hyperspectral images (103 GB). For both problems, which
involve different penalties on rows and columns, we obtain significant
speed-ups compared to state-of-the-art algorithms.
",1,0,1,1,0,0
18145,The Hurwitz Subgroups of $E_6(2)$,"  We prove that the exceptional group $E_6(2)$ is not a Hurwitz group. In the
course of proving this, we complete the classification up to conjugacy of all
Hurwitz subgroups of $E_6(2)$, in particular, those isomorphic to $L_2(8)$ and
$L_3(2)$.
",0,0,1,0,0,0
2584,Universal Function Approximation by Deep Neural Nets with Bounded Width and ReLU Activations,"  This article concerns the expressive power of depth in neural nets with ReLU
activations and bounded width. We are particularly interested in the following
questions: what is the minimal width $w_{\text{min}}(d)$ so that ReLU nets of
width $w_{\text{min}}(d)$ (and arbitrary depth) can approximate any continuous
function on the unit cube $[0,1]^d$ aribitrarily well? For ReLU nets near this
minimal width, what can one say about the depth necessary to approximate a
given function? Our approach to this paper is based on the observation that,
due to the convexity of the ReLU activation, ReLU nets are particularly
well-suited for representing convex functions. In particular, we prove that
ReLU nets with width $d+1$ can approximate any continuous convex function of
$d$ variables arbitrarily well. These results then give quantitative depth
estimates for the rate of approximation of any continuous scalar function on
the $d$-dimensional cube $[0,1]^d$ by ReLU nets with width $d+3.$
",1,0,1,1,0,0
16928,Far-infrared metallicity diagnostics: Application to local ultraluminous infrared galaxies,"  The abundance of metals in galaxies is a key parameter which permits to
distinguish between different galaxy formation and evolution models. Most of
the metallicity determinations are based on optical line ratios. However, the
optical spectral range is subject to dust extinction and, for high-z objects (z
> 3), some of the lines used in optical metallicity diagnostics are shifted to
wavelengths not accessible to ground based observatories. For this reason, we
explore metallicity diagnostics using far-infrared (IR) line ratios which can
provide a suitable alternative in such situations. To investigate these far-IR
line ratios, we modeled the emission of a starburst with the photoionization
code CLOUDY. The most sensitive far-IR ratios to measure metallicities are the
[OIII]52$\mu$m and 88$\mu$m to [NIII]57$\mu$m ratios. We show that this ratio
produces robust metallicities in the presence of an AGN and is insensitive to
changes in the age of the ionizing stellar. Another metallicity sensitive ratio
is the [OIII]88$\mu$m/[NII]122$\mu$m ratio, although it depends on the
ionization parameter. We propose various mid- and far-IR line ratios to break
this dependency. Finally, we apply these far-IR diagnostics to a sample of 19
local ultraluminous IR galaxies (ULIRGs) observed with Herschel and Spitzer. We
find that the gas-phase metallicity in these local ULIRGs is in the range 0.7 <
Z_gas/Z_sun < 1.5, which corresponds to 8.5 < 12 + log (O/H) < 8.9. The
inferred metallicities agree well with previous estimates for local ULIRGs and
this confirms that they lie below the local mass-metallicity relation.
",0,1,0,0,0,0
18471,Modeling Oral Multispecies Biofilm Recovery After Antibacterial Treatment,"  Recovery of multispecies oral biofilms is investigated following treatment by
chlorhexidine gluconate (CHX), iodine-potassium iodide (IPI) and Sodium
hypochlorite (NaOCl) both experimentally and theoretically. Experimentally,
biofilms taken from two donors were exposed to the three antibacterial
solutions (irrigants) for 10 minutes, respectively. We observe that (a) live
bacterial cell ratios decline for a week after the exposure and the trend
reverses beyond a week; after fifteen weeks, live bacterial cell ratios in
biofilms fully return to their pretreatment levels; (b) NaOCl is shown as the
strongest antibacterial agent for the oral biofilms; (c) multispecies oral
biofilms from different donors showed no difference in their susceptibility to
all the bacterial solutions. Guided by the experiment, a mathematical model for
biofilm dynamics is developed, accounting for multiple bacterial phenotypes,
quorum sensing, and growth factor proteins, to describe the nonlinear time
evolutionary behavior of the biofilms. The model captures time evolutionary
dynamics of biofilms before and after antibacterial treatment very well. It
reveals the crucial role played by quorum sensing molecules and growth factors
in biofilm recovery and verifies that the source of biofilms has a minimal to
their recovery. The model is also applied to describe the state of biofilms of
various ages treated by CHX, IPI and NaOCl, taken from different donors. Good
agreement with experimental data predicted by the model is obtained as well,
confirming its applicability to modeling biofilm dynamics in general.
",0,0,0,0,1,0
14302,Topology and edge modes in quantum critical chains,"  We show that topology can protect exponentially localized, zero energy edge
modes at critical points between one-dimensional symmetry protected topological
phases. This is possible even without gapped degrees of freedom in the bulk
---in contrast to recent work on edge modes in gapless chains. We present an
intuitive picture for the existence of these edge modes in the case of
non-interacting spinless fermions with time reversal symmetry (BDI class of the
tenfold way). The stability of this phenomenon relies on a topological
invariant defined in terms of a complex function, counting its zeros and poles
inside the unit circle. This invariant can prevent two models described by the
\emph{same} conformal field theory (CFT) from being smoothly connected. A full
classification of critical phases in the non-interacting BDI class is obtained:
each phase is labeled by the central charge of the CFT, $c \in
\frac{1}{2}\mathbb N$, and the topological invariant, $\omega \in \mathbb Z$.
Moreover, $c$ is determined by the difference in the number of edge modes
between the phases neighboring the transition. Numerical simulations show that
the topological edge modes of critical chains can be stable in the presence of
interactions and disorder.
",0,1,0,0,0,0
3529,Parametrizing modified gravity for cosmological surveys,"  One of the challenges in testing gravity with cosmology is the vast freedom
opened when extending General Relativity. For linear perturbations, one
solution consists in using the Effective Field Theory of Dark Energy (EFT of
DE). Even then, the theory space is described in terms of a handful of free
functions of time. This needs to be reduced to a finite number of parameters to
be practical for cosmological surveys. We explore in this article how well
simple parametrizations, with a small number of parameters, can fit observables
computed from complex theories. Imposing the stability of linear perturbations
appreciably reduces the theory space we explore. We find that observables are
not extremely sensitive to short time-scale variations and that simple, smooth
parametrizations are usually sufficient to describe this theory space. Using
the Bayesian Information Criterion, we find that using two parameters for each
function (an amplitude and a power law index) is preferred over complex models
for 86% of our theory space.
",0,1,0,0,0,0
9607,Learning with Correntropy-induced Losses for Regression with Mixture of Symmetric Stable Noise,"  In recent years, correntropy and its applications in machine learning have
been drawing continuous attention owing to its merits in dealing with
non-Gaussian noise and outliers. However, theoretical understanding of
correntropy, especially in the statistical learning context, is still limited.
In this study, within the statistical learning framework, we investigate
correntropy based regression in the presence of non-Gaussian noise or outliers.
Motivated by the practical way of generating non-Gaussian noise or outliers, we
introduce mixture of symmetric stable noise, which include Gaussian noise,
Cauchy noise, and their mixture as special cases, to model non-Gaussian noise
or outliers. We demonstrate that under the mixture of symmetric stable noise
assumption, correntropy based regression can learn the conditional mean
function or the conditional median function well without resorting to the
finite-variance or even the finite first-order moment condition on the noise.
In particular, for the above two cases, we establish asymptotic optimal
learning rates for correntropy based regression estimators that are
asymptotically of type $\mathcal{O}(n^{-1})$. These results justify the
effectiveness of the correntropy based regression estimators in dealing with
outliers as well as non-Gaussian noise. We believe that the present study
completes our understanding towards correntropy based regression from a
statistical learning viewpoint, and may also shed some light on robust
statistical learning for regression.
",0,0,0,1,0,0
20411,Extracting Syntactic Patterns from Databases,"  Many database columns contain string or numerical data that conforms to a
pattern, such as phone numbers, dates, addresses, product identifiers, and
employee ids. These patterns are useful in a number of data processing
applications, including understanding what a specific field represents when
field names are ambiguous, identifying outlier values, and finding similar
fields across data sets. One way to express such patterns would be to learn
regular expressions for each field in the database. Unfortunately, exist- ing
techniques on regular expression learning are slow, taking hundreds of seconds
for columns of just a few thousand values. In contrast, we develop XSystem, an
efficient method to learn patterns over database columns in significantly less
time. We show that these patterns can not only be built quickly, but are
expressive enough to capture a number of key applications, including detecting
outliers, measuring column similarity, and assigning semantic labels to columns
(based on a library of regular expressions). We evaluate these applications
with datasets that range from chemical databases (based on a collaboration with
a pharmaceutical company), our university data warehouse, and open data from
MassData.gov.
",1,0,0,0,0,0
20918,Land Cover Classification via Multi-temporal Spatial Data by Recurrent Neural Networks,"  Nowadays, modern earth observation programs produce huge volumes of satellite
images time series (SITS) that can be useful to monitor geographical areas
through time. How to efficiently analyze such kind of information is still an
open question in the remote sensing field. Recently, deep learning methods
proved suitable to deal with remote sensing data mainly for scene
classification (i.e. Convolutional Neural Networks - CNNs - on single images)
while only very few studies exist involving temporal deep learning approaches
(i.e Recurrent Neural Networks - RNNs) to deal with remote sensing time series.
In this letter we evaluate the ability of Recurrent Neural Networks, in
particular the Long-Short Term Memory (LSTM) model, to perform land cover
classification considering multi-temporal spatial data derived from a time
series of satellite images. We carried out experiments on two different
datasets considering both pixel-based and object-based classification. The
obtained results show that Recurrent Neural Networks are competitive compared
to state-of-the-art classifiers, and may outperform classical approaches in
presence of low represented and/or highly mixed classes. We also show that
using the alternative feature representation generated by LSTM can improve the
performances of standard classifiers.
",1,0,0,0,0,0
13249,Bouncy Hybrid Sampler as a Unifying Device,"  This work introduces a class of rejection-free Markov chain Monte Carlo
(MCMC) samplers, named the Bouncy Hybrid Sampler, which unifies several
existing methods from the literature. Examples include the Bouncy Particle
Sampler of Peters and de With (2012), Bouchard-Cote et al. (2015) and the
Hamiltonian MCMC. Following the introduced general framework, we derive a new
sampler called the Quadratic Bouncy Hybrid Sampler. We apply this novel sampler
to the problem of sampling from a truncated Gaussian distribution.
",0,0,0,1,0,0
8820,Mordell-Weil Groups of Linear Systems and the Hitchin Fibration,"  In this paper, we study rational sections of the relative Picard scheme of a
linear system on a smooth projective variety. We prove that if the linear
system is basepoint-free and the locus of non-integral divisors has codimension
at least two, then all rational sections of the relative Picard scheme come
from restrictions of line bundles on the variety. As a consequence, we describe
the group of sections of the Hitchin fibration for moduli spaces of Higgs
bundles on curves.
",0,0,1,0,0,0
14034,An evolutionary strategy for DeltaE - E identification,"  In this article we present an automatic method for charge and mass
identification of charged nuclear fragments produced in heavy ion collisions at
intermediate energies. The algorithm combines a generative model of DeltaE - E
relation and a Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES). The
CMA-ES is a stochastic and derivative-free method employed to search parameter
space of the model by means of a fitness function. The article describes
details of the method along with results of an application on simulated labeled
data.
",1,1,0,0,0,0
6422,Analytic Connectivity in General Hypergraphs,"  In this paper we extend the known results of analytic connectivity to
non-uniform hypergraphs. We prove a modified Cheeger's inequality and also give
a bound on analytic connectivity with respect to the degree sequence and
diameter of a hypergraph.
",1,0,0,0,0,0
11407,Stability and elasticity of metastable solid solutions and superlattices in the MoN-TaN system: a first-principles study,"  Employing ab initio calculations, we discuss chemical, mechanical, and
dynamical stability of MoN-TaN solid solutions together with cubic-like MoN/TaN
superlattices, as another materials design concept. Hexagonal-type structures
based on low-energy modifications of MoN and TaN are the most stable ones over
the whole composition range. Despite being metastable, disordered cubic
polymorphs are energetically significantly preferred over their ordered
counterparts. An in-depth analysis of atomic environments in terms of bond
lengths and angles reveals that the chemical disorder results in (partially)
broken symmetry, i.e., the disordered cubic structure relaxes towards a
hexagonal NiAs-type phase, the ground state of MoN. Surprisingly, also the
superlattice architecture is clearly favored over the ordered cubic solid
solution. We show that the bi-axial coherency stresses in superlattices break
the cubic symmetry beyond simple tetragonal distortions and lead to a new
tetragonal $\zeta$-phase (space group P4/nmm), which exhibits a more negative
formation energy than the symmetry-stabilized cubic structures of MoN and TaN.
Unlike cubic TaN, the $\zeta\text{-TaN}$ is elastically and vibrationally
stable, while $\zeta$-MoN is stabilized only by the superlattice structure. To
map compositional trends in elasticity, we establish mechanical stability of
various Mo$_{1-x}$Ta$_x$N systems and find the closest high-symmetry
approximants of the corresponding elastic tensors. According to the estimated
polycrystalline moduli, the hexagonal polymorphs are predicted to be extremely
hard, however, less ductile than the cubic phases and superlattices. The trends
in stability based on energetics and elasticity are corroborated by density of
electronic states.
",0,1,0,0,0,0
1982,"Lattice thermal expansion and anisotropic displacements in urea, bromomalonic aldehyde, pentachloropyridine and naphthalene","  Anisotropic displacement parameters (ADPs) are commonly used in
crystallography, chemistry and related fields to describe and quantify thermal
motion of atoms. Within the very recent years, these ADPs have become
predictable by lattice dynamics in combination with first-principles theory.
Here, we study four very different molecular crystals, namely urea,
bromomalonic aldehyde, pentachloropyridine, and naphthalene, by
first-principles theory to assess the quality of ADPs calculated in the
quasi-harmonic approximation. In addition, we predict both thermal expansion
and thermal motion within the quasi-harmonic approximation and compare the
predictions with experimental data. Very reliable ADPs are calculated within
the quasi-harmonic approximation for all four cases up to at least 200 K, and
they turn out to be in better agreement with experiment than the harmonic ones.
In one particular case, ADPs can even reliably be predicted up to room
temperature. Our results also hint at the importance of normal-mode
anharmonicity in the calculation of ADPs.
",0,1,0,0,0,0
17918,Global well-posedness for 2-D Boussinesq system with the temperature-dependent viscosity and supercritical dissipation,"  The present paper is dedicated to the global well-posedness issue for the
Boussinesq system with the temperature-dependent viscosity in $\mathbb{R}^2.$
We aim at extending the work by Abidi and Zhang ( Adv. Math. 2017 (305)
1202--1249 ) to a supercritical dissipation for temperature.
",0,0,1,0,0,0
3560,Analogy and duality between random channel coding and lossy source coding,"  Here we write in a unified fashion (using ""R(P, Q, D)"") the random coding
exponents in channel coding and lossy source coding. We derive their explicit
forms and show, that, for a given random codebook distribution Q, the channel
decoding error exponent can be viewed as an encoding success exponent in lossy
source coding, and the channel correct-decoding exponent can be viewed as an
encoding failure exponent in lossy source coding. We then extend the channel
exponents to arbitrary D, which corresponds for D > 0 to erasure decoding and
for D < 0 to list decoding. For comparison, we also derive the exact random
coding exponent for Forney's optimum tradeoff decoder.
",1,0,0,0,0,0
17731,On the stochastic phase stability of Ti2AlC-Cr2AlC,"  The quest towards expansion of the MAX design space has been accelerated with
the recent discovery of several solid solution and ordered phases involving at
least two MAX end members. Going beyond the nominal MAX compounds enables not
only fine tuning of existing properties but also entirely new functionality.
This search, however, has been mostly done through painstaking experiments as
knowledge of the phase stability of the relevant systems is rather scarce. In
this work, we report the first attempt to evaluate the finite-temperature
pseudo-binary phase diagram of the Ti2AlC-Cr2AlC via first-principles-guided
Bayesian CALPHAD framework that accounts for uncertainties not only in ab
initio calculations and thermodynamic models but also in synthesis conditions
in reported experiments. The phase stability analyses are shown to have good
agreement with previous experiments. The work points towards a promising way of
investigating phase stability in other MAX Phase systems providing the
knowledge necessary to elucidate possible synthesis routes for MAX systems with
unprecedented properties.
",0,1,0,0,0,0
14718,Volume of representations and mapping degree,"  Given a connected real Lie group and a contractible homogeneous proper
$G$--space $X$ furnished with a $G$--invariant volume form, a real valued
volume can be assigned to any representation $\rho\colon \pi_1(M)\to G$ for any
oriented closed smooth manifold $M$ of the same dimension as $X$. Suppose that
$G$ contains a closed and cocompact semisimple subgroup, it is shown in this
paper that the set of volumes is finite for any given $M$. From a perspective
of model geometries, examples are investigated and applications with mapping
degrees are discussed.
",0,0,1,0,0,0
20466,Thermoelectric phase diagram of the SrTiO3-SrNbO3 solid solution system,"  Thermoelectric energy conversion - the exploitation of the Seebeck effect to
convert waste heat into electricity - has attracted an increasing amount of
research attention for energy harvesting technology. Niobium-doped strontium
titanate (SrTi1-xNbxO3) is one of the most promising thermoelectric material
candidates, particularly as it poses a much lesser environmental risk in
comparison to materials based on heavy metal elements. Two-dimensional electron
confinement, e.g. through the formation of superlattices or two-dimensional
electron gases, is recognized as an effective strategy to improve the
thermoelectric performance of SrTi1-xNbxO3. Although electron confinement is
closely related to the electronic structure, the fundamental electronic phase
behavior of the SrTi1-xNbxO3 solid solution system has yet to be
comprehensively investigated. Here, we present a thermoelectric phase diagram
for the SrTi1-xNbxO3 (0.05 =< x =< 1) solid solution system, which we derived
from the characterization of epitaxial films. We observed two thermoelectric
phase boundaries in the system, which originate from the step-like decrease in
carrier effective mass at x ~ 0.3, and from a local minimum in carrier
relaxation time at x ~ 0.5. The origins of these phase boundaries are
considered to be related to isovalent/heterovalent B-site substitution:
parabolic Ti 3d orbitals dominate electron conduction for compositions with x <
0.3, whereas the Nb 4d orbital dominates when x > 0.3. At x ~ 0.5, a tetragonal
distortion of the lattice, in which the B-site is composed of Ti4+ and Nb4+
ions, leads to the formation of tail-like impurity bands, which maximizes the
electron scattering. These results provide a foundation for further research
into improving the thermoelectric performance of SrTi1-xNbxO3.
",0,1,0,0,0,0
17414,Recover Fine-Grained Spatial Data from Coarse Aggregation,"  In this paper, we study a new type of spatial sparse recovery problem, that
is to infer the fine-grained spatial distribution of certain density data in a
region only based on the aggregate observations recorded for each of its
subregions. One typical example of this spatial sparse recovery problem is to
infer spatial distribution of cellphone activities based on aggregate mobile
traffic volumes observed at sparsely scattered base stations. We propose a
novel Constrained Spatial Smoothing (CSS) approach, which exploits the local
continuity that exists in many types of spatial data to perform sparse recovery
via finite-element methods, while enforcing the aggregated observation
constraints through an innovative use of the ADMM algorithm. We also improve
the approach to further utilize additional geographical attributes. Extensive
evaluations based on a large dataset of phone call records and a demographical
dataset from the city of Milan show that our approach significantly outperforms
various state-of-the-art approaches, including Spatial Spline Regression (SSR).
",1,0,0,0,0,0
14021,Self-Motion of the 3-PPPS Parallel Robot with Delta-Shaped Base,"  This paper presents the kinematic analysis of the 3-PPPS parallel robot with
an equi-lateral mobile platform and an equilateral-shaped base. Like the other
3-PPPS robots studied in the literature, it is proved that the parallel
singularities depend only on the orientation of the end-effector. The
quaternion parameters are used to represent the singularity surfaces. The study
of the direct kinematic model shows that this robot admits a self-motion of the
Cardanic type. This explains why the direct kinematic model admits an infinite
number of solutions in the center of the workspace at the ""home"" position but
has never been studied until now.
",1,0,0,0,0,0
18227,Emotion Specification from Musical Stimuli: An EEG Study with AFA and DFA,"  The present study reports interesting findings in regard to emotional arousal
based activities while listening to two Hindustani classical ragas of contrast
emotion. EEG data was taken on 5 naive listeners while they listened to two
ragas Bahar and Mia ki Malhar which are conventionally known to portray
contrast emotions. The EEG data were analyzed with the help of two robust non
linear tools viz. Adaptive Fractal Analysis (AFA) and Detrended Fluctuation
Analysis (DFA). A comparative study of the Hurst Exponents obtained from the
two methods have been shown which shows that DFA provides more rigorous results
compared to AFA when it comes to the scaling analysis of biosignal data. The
results and implications have been discussed in detail.
",0,1,0,0,0,0
18776,On Identifying Disaster-Related Tweets: Matching-based or Learning-based?,"  Social media such as tweets are emerging as platforms contributing to
situational awareness during disasters. Information shared on Twitter by both
affected population (e.g., requesting assistance, warning) and those outside
the impact zone (e.g., providing assistance) would help first responders,
decision makers, and the public to understand the situation first-hand.
Effective use of such information requires timely selection and analysis of
tweets that are relevant to a particular disaster. Even though abundant tweets
are promising as a data source, it is challenging to automatically identify
relevant messages since tweet are short and unstructured, resulting to
unsatisfactory classification performance of conventional learning-based
approaches. Thus, we propose a simple yet effective algorithm to identify
relevant messages based on matching keywords and hashtags, and provide a
comparison between matching-based and learning-based approaches. To evaluate
the two approaches, we put them into a framework specifically proposed for
analyzing disaster-related tweets. Analysis results on eleven datasets with
various disaster types show that our technique provides relevant tweets of
higher quality and more interpretable results of sentiment analysis tasks when
compared to learning approach.
",1,0,0,0,0,0
7754,Goldstone and Higgs Hydrodynamics in the BCS-BEC Crossover,"  We discuss the derivation of a low-energy effective field theory of phase
(Goldstone) and amplitude (Higgs) modes of the pairing field from a microscopic
theory of attractive fermions. The coupled equations for Goldstone and Higgs
fields are critically analyzed in the Bardeen-Cooper-Schrieffer (BCS) to
Bose-Einstein condensate (BEC) crossover both in three spatial dimensions and
in two spatial dimensions. The crucial role of pair fluctuations is
investigated, and the beyond-mean-field Gaussian theory of the BCS-BEC
crossover is compared with available experimental data of the two-dimensional
ultracold Fermi superfluid.
",0,1,0,0,0,0
5082,Large-scale diversity estimation through surname origin inference,"  The study of surnames as both linguistic and geographical markers of the past
has proven valuable in several research fields spanning from biology and
genetics to demography and social mobility. This article builds upon the
existing literature to conceive and develop a surname origin classifier based
on a data-driven typology. This enables us to explore a methodology to describe
large-scale estimates of the relative diversity of social groups, especially
when such data is scarcely available. We subsequently analyze the
representativeness of surname origins for 15 socio-professional groups in
France.
",0,0,0,1,0,0
12366,Complete Semantics to empower Touristic Service Providers,"  The tourism industry has a significant impact on the world's economy,
contributes 10.2% of the world's gross domestic product in 2016. It becomes a
very competitive industry, where having a strong online presence is an
essential aspect for business success. To achieve this goal, the proper usage
of latest Web technologies, particularly schema.org annotations is crucial. In
this paper, we present our effort to improve the online visibility of touristic
service providers in the region of Tyrol, Austria, by creating and deploying a
substantial amount of semantic annotations according to schema.org, a widely
used vocabulary for structured data on the Web. We started our work from
Tourismusverband (TVB) Mayrhofen-Hippach and all touristic service providers in
the Mayrhofen-Hippach region and applied the same approach to other TVBs and
regions, as well as other use cases. The rationale for doing this is
straightforward. Having schema.org annotations enables search engines to
understand the content better, and provide better results for end users, as
well as enables various intelligent applications to utilize them. As a direct
consequence, the region of Tyrol and its touristic service increase their
online visibility and decrease the dependency on intermediaries, i.e. Online
Travel Agency (OTA).
",1,0,0,0,0,0
13431,Convex Hull of the Quadratic Branch AC Power Flow Equations and Its Application in Radial Distribution Networks,"  A branch flow model (BFM) is used to formulate the AC power flow in general
networks. For each branch/line, the BFM contains a nonconvex quadratic
equality. A mathematical formulation of its convex hull is proposed, which is
the tightest convex relaxation of this quadratic equation. The convex hull
formulation consists of a second order cone inequality and a linear inequality
within the physical bounds of power flows. The convex hull formulation is
analytically proved and geometrically validated. An optimal scheduling problem
of distributed energy storage (DES) in radial distribution systems with high
penetration of photovoltaic resources is investigated in this paper. To capture
the performance of both the battery and converter, a second-order DES model is
proposed. Following the convex hull of the quadratic branch flow equation, the
convex hull formulation of the nonconvex constraint in the DES model is also
derived. The proposed convex hull models are used to generate a tight convex
relaxation of the DES optimal scheduling (DESOS) problem. The proposed approach
is tested on several radial systems. A discussion on the extension to meshed
networks is provided.
",0,0,1,0,0,0
8758,The Covering Principle: A New Approach to Address Multiplicity in Hypotheses Testing,"  The closure and the partitioning principles have been used to build various
multiple testing procedures in the past three decades. The essence of these two
principles is based on parameter space partitioning. In this article, we
propose a novel approach coined the covering principle from the perspective of
rejection region coverage in the sample space. The covering principle divides
the whole family of null hypotheses into a few overlapped sub-families when
there is a priority of making decisions for hypothesis testing. We have proven
that the multiple testing procedure constructed by the covering principle
strongly controls the familywise error rate as long as the multiple tests for
each sub-familiy strongly control the type I error. We have illustrated the
covering principle can be applied to solve the general gate-keeping problems.
",0,0,0,1,0,0
181,A simulation technique for slurries interacting with moving parts and deformable solids with applications,"  A numerical method for particle-laden fluids interacting with a deformable
solid domain and mobile rigid parts is proposed and implemented in a full
engineering system. The fluid domain is modeled with a lattice Boltzmann
representation, the particles and rigid parts are modeled with a discrete
element representation, and the deformable solid domain is modeled using a
Lagrangian mesh. The main issue of this work, since separately each of these
methods is a mature tool, is to develop coupling and model-reduction approaches
in order to efficiently simulate coupled problems of this nature, as occur in
various geological and engineering applications. The lattice Boltzmann method
incorporates a large-eddy simulation technique using the Smagorinsky turbulence
model. The discrete element method incorporates spherical and polyhedral
particles for stiff contact interactions. A neo-Hookean hyperelastic model is
used for the deformable solid. We provide a detailed description of how to
couple the three solvers within a unified algorithm. The technique we propose
for rubber modeling/coupling exploits a simplification that prevents having to
solve a finite-element problem each time step. We also develop a technique to
reduce the domain size of the full system by replacing certain zones with
quasi-analytic solutions, which act as effective boundary conditions for the
lattice Boltzmann method. The major ingredients of the routine are are
separately validated. To demonstrate the coupled method in full, we simulate
slurry flows in two kinds of piston-valve geometries. The dynamics of the valve
and slurry are studied and reported over a large range of input parameters.
",1,0,0,0,0,0
19187,Using Nonlinear Normal Modes for Execution of Efficient Cyclic Motions in Soft Robots,"  With the aim of getting closer to the performance of the animal
muscleskeletal system, elastic elements are purposefully introduced in the
mechanical structure of soft robots. Indeed, previous works have extensively
shown that elasticity can endow robots with the ability of performing tasks
with increased efficiency, peak performances, and mechanical robustness.
However, despite the many achievements, a general theory of efficient motions
in soft robots is still lacking. Most of the literature focuses on specific
examples, or imposes a prescribed behavior through dynamic cancellations, thus
defeating the purpose of introducing elasticity in the first place. This paper
aims at making a step towards establishing such a general framework. To this
end, we leverage on the theory of oscillations in nonlinear dynamical systems,
and we take inspiration from state of the art theories about how the human
central nervous system manages the muscleskeletal system. We propose to
generate regular and efficient motions in soft robots by stabilizing
sub-manifolds of the state space on which the system would naturally evolve. We
select these sub-manifolds as the nonlinear continuation of linear eigenspaces,
called nonlinear normal modes. In such a way, efficient oscillatory behaviors
can be excited. We show the effectiveness of the methods in simulations on an
elastic inverted pendulum, and experimentally on a segmented elastic leg.
",1,0,0,0,0,0
3520,Projecting UK Mortality using Bayesian Generalised Additive Models,"  Forecasts of mortality provide vital information about future populations,
with implications for pension and health-care policy as well as for decisions
made by private companies about life insurance and annuity pricing. Stochastic
mortality forecasts allow the uncertainty in mortality predictions to be taken
into consideration when making policy decisions and setting product prices.
Longer lifespans imply that forecasts of mortality at ages 90 and above will
become more important in such calculations.
This paper presents a Bayesian approach to the forecasting of mortality that
jointly estimates a Generalised Additive Model (GAM) for mortality for the
majority of the age-range and a parametric model for older ages where the data
are sparser. The GAM allows smooth components to be estimated for age, cohort
and age-specific improvement rates, together with a non-smoothed period effect.
Forecasts for the United Kingdom are produced using data from the Human
Mortality Database spanning the period 1961-2013. A metric that approximates
predictive accuracy under Leave-One-Out cross-validation is used to estimate
weights for the `stacking' of forecasts with different points of transition
between the GAM and parametric elements.
Mortality for males and females are estimated separately at first, but a
joint model allows the asymptotic limit of mortality at old ages to be shared
between sexes, and furthermore provides for forecasts accounting for
correlations in period innovations. The joint and single sex model forecasts
estimated using data from 1961-2003 are compared against observed data from
2004-2013 to facilitate model assessment.
",0,0,0,1,0,0
9882,Weighted Low-Rank Approximation of Matrices and Background Modeling,"  We primarily study a special a weighted low-rank approximation of matrices
and then apply it to solve the background modeling problem. We propose two
algorithms for this purpose: one operates in the batch mode on the entire data
and the other one operates in the batch-incremental mode on the data and
naturally captures more background variations and computationally more
effective. Moreover, we propose a robust technique that learns the background
frame indices from the data and does not require any training frames. We
demonstrate through extensive experiments that by inserting a simple weight in
the Frobenius norm, it can be made robust to the outliers similar to the
$\ell_1$ norm. Our methods match or outperform several state-of-the-art online
and batch background modeling methods in virtually all quantitative and
qualitative measures.
",1,0,0,0,0,0
18234,Cross-lingual Distillation for Text Classification,"  Cross-lingual text classification(CLTC) is the task of classifying documents
written in different languages into the same taxonomy of categories. This paper
presents a novel approach to CLTC that builds on model distillation, which
adapts and extends a framework originally proposed for model compression. Using
soft probabilistic predictions for the documents in a label-rich language as
the (induced) supervisory labels in a parallel corpus of documents, we train
classifiers successfully for new languages in which labeled training data are
not available. An adversarial feature adaptation technique is also applied
during the model training to reduce distribution mismatch. We conducted
experiments on two benchmark CLTC datasets, treating English as the source
language and German, French, Japan and Chinese as the unlabeled target
languages. The proposed approach had the advantageous or comparable performance
of the other state-of-art methods.
",1,0,0,0,0,0
981,Smooth Neighbors on Teacher Graphs for Semi-supervised Learning,"  The recently proposed self-ensembling methods have achieved promising results
in deep semi-supervised learning, which penalize inconsistent predictions of
unlabeled data under different perturbations. However, they only consider
adding perturbations to each single data point, while ignoring the connections
between data samples. In this paper, we propose a novel method, called Smooth
Neighbors on Teacher Graphs (SNTG). In SNTG, a graph is constructed based on
the predictions of the teacher model, i.e., the implicit self-ensemble of
models. Then the graph serves as a similarity measure with respect to which the
representations of ""similar"" neighboring points are learned to be smooth on the
low-dimensional manifold. We achieve state-of-the-art results on
semi-supervised learning benchmarks. The error rates are 9.89%, 3.99% for
CIFAR-10 with 4000 labels, SVHN with 500 labels, respectively. In particular,
the improvements are significant when the labels are fewer. For the
non-augmented MNIST with only 20 labels, the error rate is reduced from
previous 4.81% to 1.36%. Our method also shows robustness to noisy labels.
",1,0,0,1,0,0
9335,Analyzing Knowledge Transfer in Deep Q-Networks for Autonomously Handling Multiple Intersections,"  We analyze how the knowledge to autonomously handle one type of intersection,
represented as a Deep Q-Network, translates to other types of intersections
(tasks). We view intersection handling as a deep reinforcement learning
problem, which approximates the state action Q function as a deep neural
network. Using a traffic simulator, we show that directly copying a network
trained for one type of intersection to another type of intersection decreases
the success rate. We also show that when a network that is pre-trained on Task
A and then is fine-tuned on a Task B, the resulting network not only performs
better on the Task B than an network exclusively trained on Task A, but also
retained knowledge on the Task A. Finally, we examine a lifelong learning
setting, where we train a single network on five different types of
intersections sequentially and show that the resulting network exhibited
catastrophic forgetting of knowledge on previous tasks. This result suggests a
need for a long-term memory component to preserve knowledge.
",1,0,0,0,0,0
17406,Differences Among Noninformative Stopping Rules Are Often Relevant to Bayesian Decisions,"  L.J. Savage once hoped to show that ""the superficially incompatible systems
of ideas associated on the one hand with [subjective Bayesianism] and on the
other hand with [classical statistics]...lend each other mutual support and
clarification."" By 1972, however, he had largely ""lost faith in the devices"" of
classical statistics. One aspect of those ""devices"" that he found objectionable
is that differences among the ""stopping rules"" that are used to decide when to
end an experiment which are ""noninformative"" from a Bayesian perspective can
affect decisions made using a classical approach. Two experiments that produce
the same data using different stopping rules seem to differ only in the
intentions of the experimenters regarding whether or not they would have
carried on if the data had been different, which seem irrelevant to the
evidential import of the data and thus to facts about what actions the data
warrant.
I argue that classical and Bayesian ideas about stopping rules do in fact
""lend each other"" the kind of ""mutual support and clarification"" that Savage
had originally hoped to find. They do so in a kind of case that is common in
scientific practice, in which those who design an experiment have different
interests from those who will make decisions in light of its results. I show
that, in cases of this kind, Bayesian principles provide qualified support for
the classical statistical practice of ""penalizing"" ""biased"" stopping rules.
However, they require this practice in a narrower range of circumstances than
classical principles do, and for different reasons. I argue that classical
arguments for this practice are compelling in precisely the class of cases in
which Bayesian principles also require it, and thus that we should regard
Bayesian principles as clarifying classical statistical ideas about stopping
rules rather than the reverse.
",0,0,1,1,0,0
17980,Weighted parallel SGD for distributed unbalanced-workload training system,"  Stochastic gradient descent (SGD) is a popular stochastic optimization method
in machine learning. Traditional parallel SGD algorithms, e.g., SimuParallel
SGD, often require all nodes to have the same performance or to consume equal
quantities of data. However, these requirements are difficult to satisfy when
the parallel SGD algorithms run in a heterogeneous computing environment;
low-performance nodes will exert a negative influence on the final result. In
this paper, we propose an algorithm called weighted parallel SGD (WP-SGD).
WP-SGD combines weighted model parameters from different nodes in the system to
produce the final output. WP-SGD makes use of the reduction in standard
deviation to compensate for the loss from the inconsistency in performance of
nodes in the cluster, which means that WP-SGD does not require that all nodes
consume equal quantities of data. We also analyze the theoretical feasibility
of running two other parallel SGD algorithms combined with WP-SGD in a
heterogeneous environment. The experimental results show that WP-SGD
significantly outperforms the traditional parallel SGD algorithms on
distributed training systems with an unbalanced workload.
",1,0,0,1,0,0
2648,Efficient Adjoint Computation for Wavelet and Convolution Operators,"  First-order optimization algorithms, often preferred for large problems,
require the gradient of the differentiable terms in the objective function.
These gradients often involve linear operators and their adjoints, which must
be applied rapidly. We consider two example problems and derive methods for
quickly evaluating the required adjoint operator. The first example is an image
deblurring problem, where we must compute efficiently the adjoint of
multi-stage wavelet reconstruction. Our formulation of the adjoint works for a
variety of boundary conditions, which allows the formulation to generalize to a
larger class of problems. The second example is a blind channel estimation
problem taken from the optimization literature where we must compute the
adjoint of the convolution of two signals. In each example, we show how the
adjoint operator can be applied efficiently while leveraging existing software.
",0,0,1,0,0,0
12928,Meridian Surfaces on Rotational Hypersurfaces with Lightlike Axis in ${\mathbb E}^4_2$,"  We construct a special class of Lorentz surfaces in the pseudo-Euclidean
4-space with neutral metric which are one-parameter systems of meridians of
rotational hypersurfaces with lightlike axis and call them meridian surfaces.
We give the complete classification of the meridian surfaces with constant
Gauss curvature and prove that there are no meridian surfaces with parallel
mean curvature vector field other than CMC surfaces lying in a hyperplane. We
also classify the meridian surfaces with parallel normalized mean curvature
vector field. We show that in the family of the meridian surfaces there exist
Lorentz surfaces which have parallel normalized mean curvature vector field but
not parallel mean curvature vector.
",0,0,1,0,0,0
9615,Calibration for Stratified Classification Models,"  In classification problems, sampling bias between training data and testing
data is critical to the ranking performance of classification scores. Such bias
can be both unintentionally introduced by data collection and intentionally
introduced by the algorithm, such as under-sampling or weighting techniques
applied to imbalanced data. When such sampling bias exists, using the raw
classification score to rank observations in the testing data can lead to
suboptimal results. In this paper, I investigate the optimal calibration
strategy in general settings, and develop a practical solution for one specific
sampling bias case, where the sampling bias is introduced by stratified
sampling. The optimal solution is developed by analytically solving the problem
of optimizing the ROC curve. For practical data, I propose a ranking algorithm
for general classification models with stratified data. Numerical experiments
demonstrate that the proposed algorithm effectively addresses the stratified
sampling bias issue. Interestingly, the proposed method shows its potential
applicability in two other machine learning areas: unsupervised learning and
model ensembling, which can be future research topics.
",1,0,0,1,0,0
1979,Manifold Mixup: Learning Better Representations by Interpolating Hidden States,"  Deep networks often perform well on the data distribution on which they are
trained, yet give incorrect (and often very confident) answers when evaluated
on points from off of the training distribution. This is exemplified by the
adversarial examples phenomenon but can also be seen in terms of model
generalization and domain shift. Ideally, a model would assign lower confidence
to points unlike those from the training distribution. We propose a regularizer
which addresses this issue by training with interpolated hidden states and
encouraging the classifier to be less confident at these points. Because the
hidden states are learned, this has an important effect of encouraging the
hidden states for a class to be concentrated in such a way so that
interpolations within the same class or between two different classes do not
intersect with the real data points from other classes. This has a major
advantage in that it avoids the underfitting which can result from
interpolating in the input space. We prove that the exact condition for this
problem of underfitting to be avoided by Manifold Mixup is that the
dimensionality of the hidden states exceeds the number of classes, which is
often the case in practice. Additionally, this concentration can be seen as
making the features in earlier layers more discriminative. We show that despite
requiring no significant additional computation, Manifold Mixup achieves large
improvements over strong baselines in supervised learning, robustness to
single-step adversarial attacks, semi-supervised learning, and Negative
Log-Likelihood on held out samples.
",0,0,0,1,0,0
1035,A Method of Generating Random Weights and Biases in Feedforward Neural Networks with Random Hidden Nodes,"  Neural networks with random hidden nodes have gained increasing interest from
researchers and practical applications. This is due to their unique features
such as very fast training and universal approximation property. In these
networks the weights and biases of hidden nodes determining the nonlinear
feature mapping are set randomly and are not learned. Appropriate selection of
the intervals from which weights and biases are selected is extremely
important. This topic has not yet been sufficiently explored in the literature.
In this work a method of generating random weights and biases is proposed. This
method generates the parameters of the hidden nodes in such a way that
nonlinear fragments of the activation functions are located in the input space
regions with data and can be used to construct the surface approximating a
nonlinear target function. The weights and biases are dependent on the input
data range and activation function type. The proposed methods allows us to
control the generalization degree of the model. These all lead to improvement
in approximation performance of the network. Several experiments show very
promising results.
",1,0,0,1,0,0
689,Robust Gesture-Based Communication for Underwater Human-Robot Interaction in the context of Search and Rescue Diver Missions,"  We propose a robust gesture-based communication pipeline for divers to
instruct an Autonomous Underwater Vehicle (AUV) to assist them in performing
high-risk tasks and helping in case of emergency. A gesture communication
language (CADDIAN) is developed, based on consolidated and standardized diver
gestures, including an alphabet, syntax and semantics, ensuring a logical
consistency. A hierarchical classification approach is introduced for hand
gesture recognition based on stereo imagery and multi-descriptor aggregation to
specifically cope with underwater image artifacts, e.g. light backscatter or
color attenuation. Once the classification task is finished, a syntax check is
performed to filter out invalid command sequences sent by the diver or
generated by errors in the classifier. Throughout this process, the diver
receives constant feedback from an underwater tablet to acknowledge or abort
the mission at any time. The objective is to prevent the AUV from executing
unnecessary, infeasible or potentially harmful motions. Experimental results
under different environmental conditions in archaeological exploration and
bridge inspection applications show that the system performs well in the field.
",1,0,0,0,0,0
15755,Emotion Controlled Spectrum Mobility Scheme for Efficient Syntactic Interoperability In Cognitive Radio Based Internet of Vehicles,"  Blind spots are one of the causes of road accidents in the hilly and flat
areas. These blind spot accidents can be decreased by establishing an Internet
of Vehicles (IoV) using Vehicle-2-Vehicle (V2V) and Vehicle-2-Infrastrtructure
(V2I) communication systems. But the problem with these IoV is that most of
them are using DSRC or single Radio Access Technology (RAT) as a wireless
technology, which has been proven to be failed for efficient communication
between vehicles. Recently, Cognitive Radio (CR) based IoV have to be proven
best wireless communication systems for vehicular networks. However, the
spectrum mobility is a challenging task to keep CR based vehicular networks
interoperable and has not been addressed sufficiently in existing research. In
our previous research work, the Cognitive Radio Site (CR-Site) has been
proposed as in-vehicle CR-device, which can be utilized to establish efficient
IoV systems. H In this paper, we have introduced the Emotions Inspired
Cognitive Agent (EIC_Agent) based spectrum mobility mechanism in CR-Site and
proposed a novel emotions controlled spectrum mobility scheme for efficient
syntactic interoperability between vehicles. For this purpose, a probabilistic
deterministic finite automaton using fear factor is proposed to perform
efficient spectrum mobility using fuzzy logic. In addition, the quantitative
computation of different fear intensity levels has been performed with the help
of fuzzy logic. The system has been tested using active data from different GSM
service providers on Mangla-Mirpur road. This is supplemented by extensive
simulation experiments which validate the proposed scheme for CR based
high-speed vehicular networks. The qualitative comparison with the
existing-state-of the-art has proven the superiority of the proposed emotions
controlled syntactic interoperable spectrum mobility scheme within cognitive
radio based IoV systems.
",1,0,0,0,0,0
9617,What Sets the Radial Locations of Warm Debris Disks?,"  The architectures of debris disks encode the history of planet formation in
these systems. Studies of debris disks via their spectral energy distributions
(SEDs) have found infrared excesses arising from cold dust, warm dust, or a
combination of the two. The cold outer belts of many systems have been imaged,
facilitating their study in great detail. Far less is known about the warm
components, including the origin of the dust. The regularity of the disk
temperatures indicates an underlying structure that may be linked to the water
snow line. If the dust is generated from collisions in an exo-asteroid belt,
the dust will likely trace the location of the water snow line in the
primordial protoplanetary disk where planetesimal growth was enhanced. If
instead the warm dust arises from the inward transport from a reservoir of icy
material farther out in the system, the dust location is expected to be set by
the current snow line. We analyze the SEDs of a large sample of debris disks
with warm components. We find that warm components in single-component systems
(those without detectable cold components) follow the primordial snow line
rather than the current snow line, so they likely arise from exo-asteroid
belts. While the locations of many warm components in two-component systems are
also consistent with the primordial snow line, there is more diversity among
these systems, suggesting additional effects play a role.
",0,1,0,0,0,0
16650,Single-particle dispersion in stably stratified turbulence,"  We present models for single-particle dispersion in vertical and horizontal
directions of stably stratified flows. The model in the vertical direction is
based on the observed Lagrangian spectrum of the vertical velocity, while the
model in the horizontal direction is a combination of a continuous-time
eddy-constrained random walk process with a contribution to transport from
horizontal winds. Transport at times larger than the Lagrangian turnover time
is not universal and dependent on these winds. The models yield results in good
agreement with direct numerical simulations of stratified turbulence, for which
single-particle dispersion differs from the well studied case of homogeneous
and isotropic turbulence.
",0,1,0,0,0,0
2936,Approximation Algorithms for Rectangle Packing Problems (PhD Thesis),"  In rectangle packing problems we are given the task of placing axis-aligned
rectangles in a given plane region, so that they do not overlap with each
other. In Maximum Weight Independent Set of Rectangles (MWISR), their position
is given and we can only select which rectangles to choose, while trying to
maximize their total weight. In Strip Packing (SP), we have to pack all the
given rectangles in a rectangular region of fixed width, while minimizing its
height. In 2-Dimensional Geometric Knapsack (2DGK), the target region is a
square of a given size, and our goal is to select and pack a subset of the
given rectangles of maximum weight. We study a generalization of MWISR and use
it to improve the approximation for a resource allocation problem called
bagUFP. We revisit some classical results on SP and 2DGK, by proposing a
framework based on smaller containers that are packed with simpler rules; while
variations of this scheme are indeed a standard technique in this area, we
abstract away some of the problem-specific differences, obtaining simpler
algorithms that work for different problems. We obtain improved approximations
for SP in pseudo-polynomial time, and for a variant of 2DGK where one can to
rotate the rectangles by 90°. For the latter, we propose the first
algorithms with approximation factor better than 2. For the main variant of
2DGK (without rotations), a container-based approach seems to face a natural
barrier of 2 in the approximation factor. Thus, we consider a generalized kind
of packing that combines container packings with another packing problem that
we call L-packing problem, where we have to pack rectangles in an L-shaped
region of the plane. By finding a (1 + {\epsilon})-approximation for this
problem and exploiting the combinatorial structure of 2DGK, we obtain the first
algorithms that break the barrier of 2 for the approximation factor of this
problem.
",1,0,0,0,0,0
13385,On the predictability of infectious disease outbreaks,"  Infectious disease outbreaks recapitulate biology: they emerge from the
multi-level interaction of hosts, pathogens, and their shared environment. As a
result, predicting when, where, and how far diseases will spread requires a
complex systems approach to modeling. Recent studies have demonstrated that
predicting different components of outbreaks--e.g., the expected number of
cases, pace and tempo of cases needing treatment, demand for prophylactic
equipment, importation probability etc.--is feasible. Therefore, advancing both
the science and practice of disease forecasting now requires testing for the
presence of fundamental limits to outbreak prediction. To investigate the
question of outbreak prediction, we study the information theoretic limits to
forecasting across a broad set of infectious diseases using permutation entropy
as a model independent measure of predictability. Studying the predictability
of a diverse collection of historical outbreaks--including, chlamydia, dengue,
gonorrhea, hepatitis A, influenza, measles, mumps, polio, and whooping
cough--we identify a fundamental entropy barrier for infectious disease time
series forecasting. However, we find that for most diseases this barrier to
prediction is often well beyond the time scale of single outbreaks. We also
find that the forecast horizon varies by disease and demonstrate that both
shifting model structures and social network heterogeneity are the most likely
mechanisms for the observed differences across contagions. Our results
highlight the importance of moving beyond time series forecasting, by embracing
dynamic modeling approaches, and suggest challenges for performing model
selection across long time series. We further anticipate that our findings will
contribute to the rapidly growing field of epidemiological forecasting and may
relate more broadly to the predictability of complex adaptive systems.
",0,1,0,0,0,0
15493,Network Analysis of Particles and Grains,"  The arrangements of particles and forces in granular materials have a complex
organization on multiple spatial scales that ranges from local structures to
mesoscale and system-wide ones. This multiscale organization can affect how a
material responds or reconfigures when exposed to external perturbations or
loading. The theoretical study of particle-level, force-chain, domain, and bulk
properties requires the development and application of appropriate physical,
mathematical, statistical, and computational frameworks. Traditionally,
granular materials have been investigated using particulate or continuum
models, each of which tends to be implicitly agnostic to multiscale
organization. Recently, tools from network science have emerged as powerful
approaches for probing and characterizing heterogeneous architectures across
different scales in complex systems, and a diverse set of methods have yielded
fascinating insights into granular materials. In this paper, we review work on
network-based approaches to studying granular matter and explore the potential
of such frameworks to provide a useful description of these systems and to
enhance understanding of their underlying physics. We also outline a few open
questions and highlight particularly promising future directions in the
analysis and design of granular matter and other kinds of material networks.
",0,1,1,0,0,0
8049,A geometrical analysis of global stability in trained feedback networks,"  Recurrent neural networks have been extensively studied in the context of
neuroscience and machine learning due to their ability to implement complex
computations. While substantial progress in designing effective learning
algorithms has been achieved in the last years, a full understanding of trained
recurrent networks is still lacking. Specifically, the mechanisms that allow
computations to emerge from the underlying recurrent dynamics are largely
unknown. Here we focus on a simple, yet underexplored computational setup: a
feedback architecture trained to associate a stationary output to a stationary
input. As a starting point, we derive an approximate analytical description of
global dynamics in trained networks which assumes uncorrelated connectivity
weights in the feedback and in the random bulk. The resulting mean-field theory
suggests that the task admits several classes of solutions, which imply
different stability properties. Different classes are characterized in terms of
the geometrical arrangement of the readout with respect to the input vectors,
defined in the high-dimensional space spanned by the network population. We
find that such approximate theoretical approach can be used to understand how
standard training techniques implement the input-output task in finite-size
feedback networks. In particular, our simplified description captures the local
and the global stability properties of the target solution, and thus predicts
training performance.
",0,0,0,0,1,0
5195,Stability of a Volterra Integral Equation on Time Scales,"  In this paper, we study Hyers-Ulam stability for integral equation of
Volterra type in time scale setting. Moreover we study the stability of the
considered equation in Hyers-Ulam-Rassias sense. Our technique depends on
successive approximation method, and we use time scale variant of induction
principle to show that equation (1.1) is stable on unbounded domains in
Hyers-Ulam-Rassias sense.
",0,0,1,0,0,0
17068,Collective decision for open set recognition,"  In open set recognition (OSR), almost all existing methods are designed
specially for recognizing individual instances, even these instances are
collectively coming in batch. Recognizers in decision either reject or
categorize them to some known class using empirically-set threshold. Thus the
threshold plays a key role, however, the selection for it usually depends on
the knowledge of known classes, inevitably incurring risks due to lacking
available information from unknown classes. On the other hand, a more realistic
OSR system should NOT just rest on a reject decision but should go further,
especially for discovering the hidden unknown classes among the reject
instances, whereas existing OSR methods do not pay special attention. In this
paper, we introduce a novel collective/batch decision strategy with an aim to
extend existing OSR for new class discovery while considering correlations
among the testing instances. Specifically, a collective decision-based OSR
framework (CD-OSR) is proposed by slightly modifying the Hierarchical Dirichlet
process (HDP). Thanks to the HDP, our CD-OSR does not need to define the
specific threshold and can automatically reserve space for unknown classes in
testing, naturally resulting in a new class discovery function. Finally,
extensive experiments on benchmark datasets indicate the validity of CD-OSR.
",0,0,0,1,0,0
18964,Predicate Specialization for Definitional Higher-order Logic Programs,"  Higher-order logic programming is an interesting extension of traditional
logic programming that allows predicates to appear as arguments and variables
to be used where predicates typically occur. Higher-order characteristics are
indeed desirable but on the other hand they are also usually more expensive to
support. In this paper we propose a program specialization technique based on
partial evaluation that can be applied to a modest but useful class of
higher-order logic programs and can transform them into first-order programs
without introducing additional data structures. The resulting first-order
programs can be executed by conventional logic programming interpreters and
benefit from other optimizations that might be available. We provide an
implementation and experimental results that suggest the efficiency of the
transformation.
",1,0,0,0,0,0
8646,Quantum criticality in many-body parafermion chains,"  We construct local generalizations of 3-state Potts models with exotic
critical points. We analytically show that these are described by non-diagonal
modular invariant partition functions of products of $Z_3$ parafermion or
$u(1)_6$ conformal field theories (CFTs). These correspond either to
non-trivial permutation invariants or block diagonal invariants, that one can
understand in terms of anyon condensation. In terms of lattice parafermion
operators, the constructed models correspond to parafermion chains with
many-body terms. Our construction is based on how the partition function of a
CFT depends on symmetry sectors and boundary conditions. This enables to write
the partition function corresponding to one modular invariant as a linear
combination of another over different sectors and boundary conditions, which
translates to a general recipe how to write down a microscopic model, tuned to
criticality. We show that the scheme can also be extended to construct critical
generalizations of $k$-state Potts models.
",0,1,0,0,0,0
13260,Verifying Probabilistic Timed Automata Against Omega-Regular Dense-Time Properties,"  Probabilistic timed automata (PTAs) are timed automata (TAs) extended with
discrete probability distributions.They serve as a mathematical model for a
wide range of applications that involve both stochastic and timed behaviours.
In this work, we consider the problem of model-checking linear
\emph{dense-time} properties over {PTAs}. In particular, we study linear
dense-time properties that can be encoded by TAs with infinite acceptance
criterion.First, we show that the problem of model-checking PTAs against
deterministic-TA specifications can be solved through a product construction.
Based on the product construction, we prove that the computational complexity
of the problem with deterministic-TA specifications is EXPTIME-complete. Then
we show that when relaxed to general (nondeterministic) TAs, the model-checking
problem becomes undecidable.Our results substantially extend state of the art
with both the dense-time feature and the nondeterminism in TAs.
",1,0,0,0,0,0
14268,Integrability of dispersionless Hirota type equations in 4D and the symplectic Monge-Ampere property,"  We prove that integrability of a dispersionless Hirota type equation implies
the symplectic Monge-Ampere property in any dimension $\geq 4$. In 4D this
yields a complete classification of integrable dispersionless PDEs of Hirota
type through a list of heavenly type equations arising in self-dual gravity. As
a by-product of our approach we derive an involutive system of relations
characterising symplectic Monge-Ampere equations in any dimension.
Moreover, we demonstrate that in 4D the requirement of integrability is
equivalent to self-duality of the conformal structure defined by the
characteristic variety of the equation on every solution, which is in turn
equivalent to the existence of a dispersionless Lax pair. We also give a
criterion of linerisability of a Hirota type equation via flatness of the
corresponding conformal structure, and study symmetry properties of integrable
equations.
",0,1,1,0,0,0
8351,Matter fields interacting with photons,"  We have extended the biquaternionic Dirac's equation to include interactions
with photons. The electric field is found to be perpendicular to the matter
magnetic field, and the magnetic field is perpendicular to the matter inertial
field. Inertial and magnetic masses are found to be conserved separately. The
magnetic mass density is a consequence of the coupling between the vector
potential and the matter inertial field. The presence of the vector and scalar
potentials, and the matter inertial and magnetic fields are found to modify the
standard form of the derived Maxwell's equations. The resulting interacting
electrodynamics equations are found to generalize those of axion-like fields of
Frank Wilczek or Chern-Simons equations. The axion field satisfies massive
Klein-Gordon equation if Lorenz gauge condition is violated. Therefore, axion
could be our massive photon. The electromagnetic field vector,
$\vec{F}=\vec{E}+ic\vec{B}$, is found to satisfy massive Dirac's equation in
addition to the fact that $\vec{\nabla}\cdot\vec{F}=0$, where $\vec{E}$ and
$\vec{B}$ are the electric and magnetic fields, respectively.
",0,1,0,0,0,0
18860,"Uniform rank gradient, cost and local-global convergence","  We analyze the rank gradient of finitely generated groups with respect to
sequences of subgroups of finite index that do not necessarily form a chain, by
connecting it to the cost of p.m.p. actions. We generalize several results that
were only known for chains before. The connection is made by the notion of
local-global convergence.
In particular, we show that for a finitely generated group $\Gamma$ with
fixed price $c$, every Farber sequence has rank gradient $c-1$. By adapting
Lackenby's trichotomy theorem to this setting, we also show that in a finitely
presented amenable group, every sequence of subgroups with index tending to
infinity has vanishing rank gradient.
",0,0,1,0,0,0
410,J-MOD$^{2}$: Joint Monocular Obstacle Detection and Depth Estimation,"  In this work, we propose an end-to-end deep architecture that jointly learns
to detect obstacles and estimate their depth for MAV flight applications. Most
of the existing approaches either rely on Visual SLAM systems or on depth
estimation models to build 3D maps and detect obstacles. However, for the task
of avoiding obstacles this level of complexity is not required. Recent works
have proposed multi task architectures to both perform scene understanding and
depth estimation. We follow their track and propose a specific architecture to
jointly estimate depth and obstacles, without the need to compute a global map,
but maintaining compatibility with a global SLAM system if needed. The network
architecture is devised to exploit the joint information of the obstacle
detection task, that produces more reliable bounding boxes, with the depth
estimation one, increasing the robustness of both to scenario changes. We call
this architecture J-MOD$^{2}$. We test the effectiveness of our approach with
experiments on sequences with different appearance and focal lengths and
compare it to SotA multi task methods that jointly perform semantic
segmentation and depth estimation. In addition, we show the integration in a
full system using a set of simulated navigation experiments where a MAV
explores an unknown scenario and plans safe trajectories by using our detection
model.
",1,0,0,0,0,0
10130,Birth of the GUP and its effect on the entropy of the Universe in Lie-$N$-algebra,"  In this paper, the origin of the generalized uncertainty principle (GUP) in
an $M$-dimensional theory with Lie-$N$-algebra is considered. This theory which
we name GLNA(Generalized Lie-$N$-Algebra)-theory can be reduced to $M$-theory
with $M=11$ and $N=3$. In this theory, at the beginning, two energies with
positive and negative signs are created from nothing and produce two types of
branes with opposite quantum numbers and different numbers of timing
dimensions. Coincidence with the birth of these branes, various derivatives of
bosonic fields emerge in the action of the system which produce the $r$ GUP for
bosons. These branes interact with each other, compact and various derivatives
of spinor fields appear in the action of the system which leads to the creation
of the GUP for fermions. The previous predicted entropy of branes in the GUP is
corrected as due to the emergence of higher orders of derivatives and different
number of timing dimensions.
",0,1,0,0,0,0
3001,Mean field limits for nonlinear spatially extended hawkes processes with exponential memory kernels,"  We consider spatially extended systems of interacting nonlinear Hawkes
processes modeling large systems of neurons placed in Rd and study the
associated mean field limits. As the total number of neurons tends to infinity,
we prove that the evolution of a typical neuron, attached to a given spatial
position, can be described by a nonlinear limit differential equation driven by
a Poisson random measure. The limit process is described by a neural field
equation. As a consequence, we provide a rigorous derivation of the neural
field equation based on a thorough mean field analysis.
",0,0,1,0,0,0
1511,High Order Hierarchical Divergence-free Constrained Transport $H(div)$ Finite Element Method for Magnetic Induction Equation,"  In this paper, we will use the interior functions of an hierarchical basis
for high order $BDM_p$ elements to enforce the divergence-free condition of a
magnetic field $B$ approximated by the H(div) $BDM_p$ basis. The resulting
constrained finite element method can be used to solve magnetic induction
equation in MHD equations. The proposed procedure is based on the fact that the
scalar $(p-1)$-th order polynomial space on each element can be decomposed as
an orthogonal sum of the subspace defined by the divergence of the interior
functions of the $p$-th order $BDM_p$ basis and the constant function.
Therefore, the interior functions can be used to remove element-wise all higher
order terms except the constant in the divergence error of the finite element
solution of $B$-field. The constant terms from each element can be then easily
corrected using a first order H(div) basis globally. Numerical results for a
3-D magnetic induction equation show the effectiveness of the proposed method
in enforcing divergence-free condition of the magnetic field.
",0,0,1,0,0,0
18783,Diffeomorphic random sampling using optimal information transport,"  In this article we explore an algorithm for diffeomorphic random sampling of
nonuniform probability distributions on Riemannian manifolds. The algorithm is
based on optimal information transport (OIT)---an analogue of optimal mass
transport (OMT). Our framework uses the deep geometric connections between the
Fisher-Rao metric on the space of probability densities and the right-invariant
information metric on the group of diffeomorphisms. The resulting sampling
algorithm is a promising alternative to OMT, in particular as our formulation
is semi-explicit, free of the nonlinear Monge--Ampere equation. Compared to
Markov Chain Monte Carlo methods, we expect our algorithm to stand up well when
a large number of samples from a low dimensional nonuniform distribution is
needed.
",0,0,1,1,0,0
10712,High Dimensional Inference in Partially Linear Models,"  We propose two semiparametric versions of the debiased Lasso procedure for
the model $Y_i = X_i\beta_0 + g_0(Z_i) + \epsilon_i$, where $\beta_0$ is high
dimensional but sparse (exactly or approximately). Both versions are shown to
have the same asymptotic normal distribution and do not require the minimal
signal condition for statistical inference of any component in $\beta_0$. Our
method also works when $Z_i$ is high dimensional provided that the function
classes $E(X_{ij} |Z_i)$s and $E(Y_i|Z_i)$ belong to exhibit certain sparsity
features, e.g., a sparse additive decomposition structure. We further develop a
simultaneous hypothesis testing procedure based on multiplier bootstrap. Our
testing method automatically takes into account of the dependence structure
within the debiased estimates, and allows the number of tested components to be
exponentially high.
",0,0,1,1,0,0
2151,Finding low-tension communities,"  Motivated by applications that arise in online social media and collaboration
networks, there has been a lot of work on community-search and team-formation
problems. In the former class of problems, the goal is to find a subgraph that
satisfies a certain connectivity requirement and contains a given collection of
seed nodes. In the latter class of problems, on the other hand, the goal is to
find individuals who collectively have the skills required for a task and form
a connected subgraph with certain properties.
In this paper, we extend both the community-search and the team-formation
problems by associating each individual with a profile. The profile is a
numeric score that quantifies the position of an individual with respect to a
topic. We adopt a model where each individual starts with a latent profile and
arrives to a conformed profile through a dynamic conformation process, which
takes into account the individual's social interaction and the tendency to
conform with one's social environment. In this framework, social tension arises
from the differences between the conformed profiles of neighboring individuals
as well as from differences between individuals' conformed and latent profiles.
Given a network of individuals, their latent profiles and this conformation
process, we extend the community-search and the team-formation problems by
requiring the output subgraphs to have low social tension. From the technical
point of view, we study the complexity of these problems and propose algorithms
for solving them effectively. Our experimental evaluation in a number of social
networks reveals the efficacy and efficiency of our methods.
",1,1,0,0,0,0
2324,A multi-instrument non-parametric reconstruction of the electron pressure profile in the galaxy cluster CLJ1226.9+3332,"  Context: In the past decade, sensitive, resolved Sunyaev-Zel'dovich (SZ)
studies of galaxy clusters have become common. Whereas many previous SZ studies
have parameterized the pressure profiles of galaxy clusters, non-parametric
reconstructions will provide insights into the thermodynamic state of the
intracluster medium (ICM). Aims: We seek to recover the non-parametric pressure
profiles of the high redshift ($z=0.89$) galaxy cluster CLJ 1226.9+3332 as
inferred from SZ data from the MUSTANG, NIKA, Bolocam, and Planck instruments,
which all probe different angular scales. Methods: Our non-parametric algorithm
makes use of logarithmic interpolation, which under the assumption of
ellipsoidal symmetry is analytically integrable. For MUSTANG, NIKA, and Bolocam
we derive a non-parametric pressure profile independently and find good
agreement among the instruments. In particular, we find that the non-parametric
profiles are consistent with a fitted gNFW profile. Given the ability of Planck
to constrain the total signal, we include a prior on the integrated Compton Y
parameter as determined by Planck. Results: For a given instrument, constraints
on the pressure profile diminish rapidly beyond the field of view. The overlap
in spatial scales probed by these four datasets is therefore critical in
checking for consistency between instruments. By using multiple instruments,
our analysis of CLJ 1226.9+3332 covers a large radial range, from the central
regions to the cluster outskirts: $0.05 R_{500} < r < 1.1 R_{500}$. This is a
wider range of spatial scales than is typical recovered by SZ instruments.
Similar analyses will be possible with the new generation of SZ instruments
such as NIKA2 and MUSTANG2.
",0,1,0,0,0,0
10224,Star chromatic index of subcubic multigraphs,"  The star chromatic index of a multigraph $G$, denoted $\chi'_{s}(G)$, is the
minimum number of colors needed to properly color the edges of $G$ such that no
path or cycle of length four is bi-colored. A multigraph $G$ is star
$k$-edge-colorable if $\chi'_{s}(G)\le k$. Dvořák, Mohar and Šámal
[Star chromatic index, J. Graph Theory 72 (2013), 313--326] proved that every
subcubic multigraph is star $7$-edge-colorable. They conjectured in the same
paper that every subcubic multigraph should be star $6$-edge-colorable. In this
paper, we first prove that it is NP-complete to determine whether
$\chi'_s(G)\le3$ for an arbitrary graph $G$. This answers a question of Mohar.
We then establish some structure results on subcubic multigraphs $G$ with
$\delta(G)\le2$ such that $\chi'_s(G)>k$ but $\chi'_s(G-v)\le k$ for any $v\in
V(G)$, where $k\in\{5,6\}$. We finally apply the structure results, along with
a simple discharging method, to prove that every subcubic multigraph $G$ is
star $6$-edge-colorable if $mad(G)<5/2$, and star $5$-edge-colorable if
$mad(G)<24/11$, respectively, where $mad(G)$ is the maximum average degree of a
multigraph $G$. This partially confirms the conjecture of Dvořák, Mohar
and Šámal.
",0,0,1,0,0,0
6537,Finding influential nodes for integration in brain networks using optimal percolation theory,"  Global integration of information in the brain results from complex
interactions of segregated brain networks. Identifying the most influential
neuronal populations that efficiently bind these networks is a fundamental
problem of systems neuroscience. Here we apply optimal percolation theory and
pharmacogenetic interventions in-vivo to predict and subsequently target nodes
that are essential for global integration of a memory network in rodents. The
theory predicts that integration in the memory network is mediated by a set of
low-degree nodes located in the nucleus accumbens. This result is confirmed
with pharmacogenetic inactivation of the nucleus accumbens, which eliminates
the formation of the memory network, while inactivations of other brain areas
leave the network intact. Thus, optimal percolation theory predicts essential
nodes in brain networks. This could be used to identify targets of
interventions to modulate brain function.
",0,0,0,0,1,0
7722,Extreme value statistics for censored data with heavy tails under competing risks,"  This paper addresses the problem of estimating, in the presence of random
censoring as well as competing risks, the extreme value index of the
(sub)-distribution function associated to one particular cause, in the
heavy-tail case. Asymptotic normality of the proposed estimator (which has the
form of an Aalen-Johansen integral, and is the first estimator proposed in this
context) is established. A small simulation study exhibits its performances for
finite samples. Estimation of extreme quantiles of the cumulative incidence
function is also addressed.
",0,0,1,1,0,0
20194,A refined count of Coxeter element factorizations,"  For well-generated complex reflection groups, Chapuy and Stump gave a simple
product for a generating function counting reflection factorizations of a
Coxeter element by their length. This is refined here to record the number of
reflections used from each orbit of hyperplanes. The proof is case-by-case via
the classification of well-generated groups. It implies a new expression for
the Coxeter number, expressed via data coming from a hyperplane orbit; a
case-free proof of this due to J. Michel is included.
",0,0,1,0,0,0
7803,Global existence for the nonlinear fractional Schrödinger equation with fractional dissipation,"  We consider the initial value problem for the fractional nonlinear
Schrödinger equation with a fractional dissipation. Global existence and
scattering are proved depending on the order of the fractional dissipation.
",0,0,1,0,0,0
14517,Bet-hedging against demographic fluctuations,"  Biological organisms have to cope with stochastic variations in both the
external environment and the internal population dynamics. Theoretical studies
and laboratory experiments suggest that population diversification could be an
effective bet-hedging strategy for adaptation to varying environments. Here we
show that bet-hedging can also be effective against demographic fluctuations
that pose a trade-off between growth and survival for populations even in a
constant environment. A species can maximize its overall abundance in the long
term by diversifying into coexisting subpopulations of both ""fast-growing"" and
""better-surviving"" individuals. Our model generalizes statistical physics
models of birth-death processes to incorporate dispersal, during which new
populations are founded, and can further incorporate variations of local
environments. In this way we unify different bet-hedging strategies against
demographic and environmental variations as a general means of adaptation to
both types of uncertainties in population growth.
",0,1,0,0,0,0
20674,A quality model for evaluating and choosing a stream processing framework architecture,"  Today, we have to deal with many data (Big data) and we need to make
decisions by choosing an architectural framework to analyze these data coming
from different area. Due to this, it become problematic when we want to process
these data, and even more, when it is continuous data. When you want to process
some data, you have to first receive it, store it, and then query it. This is
what we call Batch Processing. It works well when you process big amount of
data, but it finds its limits when you want to get fast (or real-time)
processing results, such as financial trades, sensors, user session activity,
etc. The solution to this problem is stream processing. Stream processing
approach consists of data arriving record by record and rather than storing it,
the processing should be done directly. Therefore, direct results are needed
with a latency that may vary in real-time.
In this paper, we propose an assessment quality model to evaluate and choose
stream processing frameworks. We describe briefly different architectural
frameworks such as Kafka, Spark Streaming and Flink that address the stream
processing. Using our quality model, we present a decision tree to support
engineers to choose a framework following the quality aspects. Finally, we
evaluate our model doing a case study to Twitter and Netflix streaming.
",1,0,0,0,0,0
867,A Robust Multi-Batch L-BFGS Method for Machine Learning,"  This paper describes an implementation of the L-BFGS method designed to deal
with two adversarial situations. The first occurs in distributed computing
environments where some of the computational nodes devoted to the evaluation of
the function and gradient are unable to return results on time. A similar
challenge occurs in a multi-batch approach in which the data points used to
compute function and gradients are purposely changed at each iteration to
accelerate the learning process. Difficulties arise because L-BFGS employs
gradient differences to update the Hessian approximations, and when these
gradients are computed using different data points the updating process can be
unstable. This paper shows how to perform stable quasi-Newton updating in the
multi-batch setting, studies the convergence properties for both convex and
nonconvex functions, and illustrates the behavior of the algorithm in a
distributed computing platform on binary classification logistic regression and
neural network training problems that arise in machine learning.
",1,0,1,1,0,0
16727,Maximum Regularized Likelihood Estimators: A General Prediction Theory and Applications,"  Maximum regularized likelihood estimators (MRLEs) are arguably the most
established class of estimators in high-dimensional statistics. In this paper,
we derive guarantees for MRLEs in Kullback-Leibler divergence, a general
measure of prediction accuracy. We assume only that the densities have a convex
parametrization and that the regularization is definite and positive
homogenous. The results thus apply to a very large variety of models and
estimators, such as tensor regression and graphical models with convex and
non-convex regularized methods. A main conclusion is that MRLEs are broadly
consistent in prediction - regardless of whether restricted eigenvalues or
similar conditions hold.
",0,0,1,1,0,0
2172,"Navier-Stokes flow past a rigid body: attainability of steady solutions as limits of unsteady weak solutions, starting and landing cases","  Consider the Navier-Stokes flow in 3-dimensional exterior domains, where a
rigid body is translating with prescribed translational velocity
$-h(t)u_\infty$ with constant vector $u_\infty\in \mathbb R^3\setminus\{0\}$.
Finn raised the question whether his steady slutions are attainable as limits
for $t\to\infty$ of unsteady solutions starting from motionless state when
$h(t)=1$ after some finite time and $h(0)=0$ (starting problem). This was
affirmatively solved by Galdi, Heywood and Shibata for small $u_\infty$. We
study some generalized situation in which unsteady solutions start from large
motions being in $L^3$. We then conclude that the steady solutions for small
$u_\infty$ are still attainable as limits of evolution of those fluid motions
which are found as a sort of weak solutions. The opposite situation, in which
$h(t)=0$ after some finite time and $h(0)=1$ (landing problem), is also
discussed. In this latter case, the rest state is attainable no matter how
large $u_\infty$ is.
",0,0,1,0,0,0
18149,On the shape operator of relatively parallel hypersurfaces in the $n$-dimensional relative differential geometry,"  We deal with hypersurfaces in the framework of the $n$-dimensional relative
differential geometry. We consider a hypersurface $\varPhi$ of
$\mathbb{R}^{n+1}$ with position vector field $\mathbf{x}$, which is relatively
normalized by a relative normalization $\mathbf{y}$. Then $\mathbf{y}$ is also
a relative normalization of every member of the one-parameter family
$\mathcal{F}$ of hypersurfaces $\varPhi_\mu$ with position vector field
$$\mathbf{x}_\mu = \mathbf{x} + \mu \, \mathbf{y},$$ where $\mu$ is a real
constant. We call every hypersurface $\varPhi_\mu \in \mathcal{F}$ relatively
parallel to $\varPhi$ at the ""relative distance"" $\mu$. In this paper we study
(a) the shape (or Weingarten) operator,
(b) the relative principal curvatures,
(c) the relative mean curvature functions and
(d) the affine normalization
of a relatively parallel hypersurface $\left( \varPhi_\mu,\mathbf{y}\right)$
to $\left(\varPhi,\mathbf{y}\right)$.
",0,0,1,0,0,0
994,Anomaly detecting and ranking of the cloud computing platform by multi-view learning,"  Anomaly detecting as an important technical in cloud computing is applied to
support smooth running of the cloud platform. Traditional detecting methods
based on statistic, analysis, etc. lead to the high false-alarm rate due to
non-adaptive and sensitive parameters setting. We presented an online model for
anomaly detecting using machine learning theory. However, most existing methods
based on machine learning linked all features from difference sub-systems into
a long feature vector directly, which is difficult to both exploit the
complement information between sub-systems and ignore multi-view features
enhancing the classification performance. Aiming to this problem, the proposed
method automatic fuses multi-view features and optimize the discriminative
model to enhance the accuracy. This model takes advantage of extreme learning
machine (ELM) to improve detection efficiency. ELM is the single hidden layer
neural network, which is transforming iterative solution the output weights to
solution of linear equations and avoiding the local optimal solution. Moreover,
we rank anomies according to the relationship between samples and the
classification boundary, and then assigning weights for ranked anomalies,
retraining the classification model finally. Our method exploits the complement
information between sub-systems sufficiently, and avoids the influence from
imbalance dataset, therefore, deal with various challenges from the cloud
computing platform. We deploy the privately cloud platform by Openstack,
verifying the proposed model and comparing results to the state-of-the-art
methods with better efficiency and simplicity.
",1,0,0,1,0,0
5747,STACCATO: A Novel Solution to Supernova Photometric Classification with Biased Training Sets,"  We present a new solution to the problem of classifying Type Ia supernovae
from their light curves alone given a spectroscopically confirmed but biased
training set, circumventing the need to obtain an observationally expensive
unbiased training set. We use Gaussian processes (GPs) to model the
supernovae's (SN) light curves, and demonstrate that the choice of covariance
function has only a small influence on the GPs ability to accurately classify
SNe. We extend and improve the approach of Richards et al (2012} -- a diffusion
map combined with a random forest classifier -- to deal specifically with the
case of biassed training sets. We propose a novel method, called STACCATO
(SynThetically Augmented Light Curve ClassificATiOn') that synthetically
augments a biased training set by generating additional training data from the
fitted GPs. Key to the success of the method is the partitioning of the
observations into subgroups based on their propensity score of being included
in the training set. Using simulated light curve data, we show that STACCATO
increases performance, as measured by the area under the Receiver Operating
Characteristic curve (AUC), from 0.93 to 0.96, close to the AUC of 0.977
obtained using the 'gold standard' of an unbiased training set and
significantly improving on the previous best result of 0.88. STACCATO also
increases the true positive rate for SNIa classification by up to a factor of
50 for high-redshift/low brightness SNe.
",0,1,0,0,0,0
4630,Exploring a search for long-duration transient gravitational waves associated with magnetar bursts,"  Soft gamma repeaters and anomalous X-ray pulsars are thought to be magnetars,
neutron stars with strong magnetic fields of order $\mathord{\sim}
10^{13}$--$10^{15} \, \mathrm{gauss}$. These objects emit intermittent bursts
of hard X-rays and soft gamma rays. Quasiperiodic oscillations in the X-ray
tails of giant flares imply the existence of neutron star oscillation modes
which could emit gravitational waves powered by the magnetar's magnetic energy
reservoir. We describe a method to search for transient gravitational-wave
signals associated with magnetar bursts with durations of 10s to 1000s of
seconds. The sensitivity of this method is estimated by adding simulated
waveforms to data from the sixth science run of Laser Interferometer
Gravitational-wave Observatory (LIGO). We find a search sensitivity in terms of
the root sum square strain amplitude of $h_{\mathrm{rss}} = 1.3 \times 10^{-21}
\, \mathrm{Hz}^{-1/2}$ for a half sine-Gaussian waveform with a central
frequency $f_0 = 150 \, \mathrm{Hz}$ and a characteristic time $\tau = 400 \,
\mathrm{s}$. This corresponds to a gravitational wave energy of
$E_{\mathrm{GW}} = 4.3 \times 10^{46} \, \mathrm{erg}$, the same order of
magnitude as the 2004 giant flare which had an estimated electromagnetic energy
of $E_{\mathrm{EM}} = \mathord{\sim} 1.7 \times 10^{46} (d/ 8.7 \,
\mathrm{kpc})^2 \, \mathrm{erg}$, where $d$ is the distance to SGR 1806-20. We
present an extrapolation of these results to Advanced LIGO, estimating a
sensitivity to a gravitational wave energy of $E_{\mathrm{GW}} = 3.2 \times
10^{43} \, \mathrm{erg}$ for a magnetar at a distance of $1.6 \, \mathrm{kpc}$.
These results suggest this search method can probe significantly below the
energy budgets for magnetar burst emission mechanisms such as crust cracking
and hydrodynamic deformation.
",0,1,0,0,0,0
19367,D-optimal design for multivariate polynomial regression via the Christoffel function and semidefinite relaxations,"  We present a new approach to the design of D-optimal experiments with
multivariate polynomial regressions on compact semi-algebraic design spaces. We
apply the moment-sum-of-squares hierarchy of semidefinite programming problems
to solve numerically and approximately the optimal design problem. The geometry
of the design is recovered with semidefinite programming duality theory and the
Christoffel polynomial.
",0,0,1,1,0,0
1167,Identifiability of phylogenetic parameters from k-mer data under the coalescent,"  Distances between sequences based on their $k$-mer frequency counts can be
used to reconstruct phylogenies without first computing a sequence alignment.
Past work has shown that effective use of k-mer methods depends on 1)
model-based corrections to distances based on $k$-mers and 2) breaking long
sequences into blocks to obtain repeated trials from the sequence-generating
process. Good performance of such methods is based on having many high-quality
blocks with many homologous sites, which can be problematic to guarantee a
priori.
Nature provides natural blocks of sequences into homologous regions---namely,
the genes. However, directly using past work in this setting is problematic
because of possible discordance between different gene trees and the underlying
species tree. Using the multispecies coalescent model as a basis, we derive
model-based moment formulas that involve the divergence times and the
coalescent parameters. From this setting, we prove identifiability results for
the tree and branch length parameters under the Jukes-Cantor model of sequence
mutations.
",0,0,1,0,0,0
18596,Relative Error Tensor Low Rank Approximation,"  We consider relative error low rank approximation of $tensors$ with respect
to the Frobenius norm: given an order-$q$ tensor $A \in
\mathbb{R}^{\prod_{i=1}^q n_i}$, output a rank-$k$ tensor $B$ for which
$\|A-B\|_F^2 \leq (1+\epsilon)$OPT, where OPT $= \inf_{\textrm{rank-}k~A'}
\|A-A'\|_F^2$. Despite the success on obtaining relative error low rank
approximations for matrices, no such results were known for tensors. One
structural issue is that there may be no rank-$k$ tensor $A_k$ achieving the
above infinum. Another, computational issue, is that an efficient relative
error low rank approximation algorithm for tensors would allow one to compute
the rank of a tensor, which is NP-hard. We bypass these issues via (1)
bicriteria and (2) parameterized complexity solutions:
(1) We give an algorithm which outputs a rank $k' = O((k/\epsilon)^{q-1})$
tensor $B$ for which $\|A-B\|_F^2 \leq (1+\epsilon)$OPT in $nnz(A) + n \cdot
\textrm{poly}(k/\epsilon)$ time in the real RAM model. Here $nnz(A)$ is the
number of non-zero entries in $A$.
(2) We give an algorithm for any $\delta >0$ which outputs a rank $k$ tensor
$B$ for which $\|A-B\|_F^2 \leq (1+\epsilon)$OPT and runs in $ ( nnz(A) + n
\cdot \textrm{poly}(k/\epsilon) + \exp(k^2/\epsilon) ) \cdot n^\delta$ time in
the unit cost RAM model.
For outputting a rank-$k$ tensor, or even a bicriteria solution with
rank-$Ck$ for a certain constant $C > 1$, we show a $2^{\Omega(k^{1-o(1)})}$
time lower bound under the Exponential Time Hypothesis.
Our results give the first relative error low rank approximations for tensors
for a large number of robust error measures for which nothing was known, as
well as column row and tube subset selection. We also obtain new results for
matrices, such as $nnz(A)$-time CUR decompositions, improving previous
$nnz(A)\log n$-time algorithms, which may be of independent interest.
",1,0,0,0,0,0
1828,Room-temperature 1.54 $μ$m photoluminescence of Er:O$_x$ centers at extremely low concentration in silicon,"  The demand for single photon sources at $\lambda~=~1.54~\mu$m, which follows
from the consistent development of quantum networks based on commercial optical
fibers, makes Er:O$_x$ centers in Si still a viable resource thanks to the
optical transition of $Er^{3+}~:~^4I_{13/2}~\rightarrow~^4I_{15/2}$. Yet, to
date, the implementation of such system remains hindered by its extremely low
emission rate. In this Letter, we explore the room-temperature
photoluminescence (PL) at the telecomm wavelength of very low implantation
doses of $Er:O_x$ in $Si$. The emitted photons, excited by a $\lambda~=~792~nm$
laser in both large areas and confined dots of diameter down to $5~\mu$m, are
collected by an inverted confocal microscope. The lower-bound number of
detectable emission centers within our diffraction-limited illumination spot is
estimated to be down to about 10$^4$, corresponding to an emission rate per
individual ion of about $4~\times~10^{3}$ photons/s.
",0,1,0,0,0,0
14981,Tight contact structures on Seifert surface complements,"  We consider complements of standard Seifert surfaces of special alternating
links. On these handlebodies, we use Honda's method to enumerate those tight
contact structures whose dividing sets are isotopic to the link, and find their
number to be the leading coefficient of the Alexander polynomial. The Euler
classes of the contact structures are identified with hypertrees in a certain
hypergraph. Using earlier work, this establishes a connection between contact
topology and the Homfly polynomial. We also show that the contact invariants of
our tight contact structures form a basis for sutured Floer homology. Finally,
we relate our methods and results to Kauffman's formal knot theory.
",0,0,1,0,0,0
15614,SoaAlloc: Accelerating Single-Method Multiple-Objects Applications on GPUs,"  We propose SoaAlloc, a dynamic object allocator for Single-Method
Multiple-Objects applications in CUDA. SoaAlloc is the first allocator for GPUs
that (a) arranges allocations in a SIMD-friendly Structure of Arrays (SOA) data
layout, (b) provides a do-all operation for maximizing the benefit of SOA, and
(c) is on par with state-of-the-art memory allocators for raw (de)allocation
time. Our benchmarks show that the SOA layout leads to significantly better
memory bandwidth utilization, resulting in a 2x speedup of application code.
",1,0,0,0,0,0
19675,"Gravitational wave, collider and dark matter signals from a scalar singlet electroweak baryogenesis","  We analyse a simple extension of the SM with just an additional scalar
singlet coupled to the Higgs boson. We discuss the possible probes for
electroweak baryogenesis in this model including collider searches,
gravitational wave and direct dark matter detection signals. We show that a
large portion of the model parameter space exists where the observation of
gravitational waves would allow detection while the indirect collider searches
would not.
",0,1,0,0,0,0
13652,Calculation of the bulk modulus of mixed ionic crystal NH_4Cl_{1-x}Br_x,"  The ammonium halides present an interesting system for study in view of their
polymorphism and the possible internal rotation of the ammonium ion. The static
properties of the mixed ionic crystal NH$_4$Cl$_{1-x}$Br$_x$ have been recently
investigated, using three-body potential model (TDPM) by the application of
Vegard's law. Here, by using a simple theoretical model, we estimate the bulk
modulus of their ternary alloys NH$_4$Cl$_{1-x}$Br$_x$, in terms of the bulk
modulus of the end members alone. The calculated values are comparable to those
deduced from the three-body potential model (TDPM) by the application of
Vegard's law.
",0,1,0,0,0,0
18691,"Enlargeability, foliations, and positive scalar curvature","  We extend the deep and important results of Lichnerowicz, Connes, and
Gromov-Lawson which relate geometry and characteristic numbers to the existence
and non-existence of metrics of positive scalar curvature (PSC). In particular,
we show: that a spin foliation with Hausdorff homotopy groupoid of an
enlargeable manifold admits no PSC metric; that any metric of PSC on such a
foliation is bounded by a multiple of the reciprocal of the foliation K-area of
the ambient manifold; and that Connes' vanishing theorem for characteristic
numbers of PSC foliations extends to a vanishing theorem for Haefliger
cohomology classes.
",0,0,1,0,0,0
2116,L-Graphs and Monotone L-Graphs,"  In an $\mathsf{L}$-embedding of a graph, each vertex is represented by an
$\mathsf{L}$-segment, and two segments intersect each other if and only if the
corresponding vertices are adjacent in the graph. If the corner of each
$\mathsf{L}$-segment in an $\mathsf{L}$-embedding lies on a straight line, we
call it a monotone $\mathsf{L}$-embedding. In this paper we give a full
characterization of monotone $\mathsf{L}$-embeddings by introducing a new class
of graphs which we call ""non-jumping"" graphs. We show that a graph admits a
monotone $\mathsf{L}$-embedding if and only if the graph is a non-jumping
graph. Further, we show that outerplanar graphs, convex bipartite graphs,
interval graphs, 3-leaf power graphs, and complete graphs are subclasses of
non-jumping graphs. Finally, we show that distance-hereditary graphs and
$k$-leaf power graphs ($k\le 4$) admit $\mathsf{L}$-embeddings.
",1,0,0,0,0,0
7746,Spin controlled atom-ion inelastic collisions,"  The control of the ultracold collisions between neutral atoms is an extensive
and successful field of study. The tools developed allow for ultracold chemical
reactions to be managed using magnetic fields, light fields and spin-state
manipulation of the colliding particles among other methods. The control of
chemical reactions in ultracold atom-ion collisions is a young and growing
field of research. Recently, the collision energy and the ion electronic state
were used to control atom-ion interactions. Here, we demonstrate
spin-controlled atom-ion inelastic processes. In our experiment, both
spin-exchange and charge-exchange reactions are controlled in an ultracold
Rb-Sr$^+$ mixture by the atomic spin state. We prepare a cloud of atoms in a
single hyperfine spin-state. Spin-exchange collisions between atoms and ion
subsequently polarize the ion spin. Electron transfer is only allowed for
(RbSr)$^+$ colliding in the singlet manifold. Initializing the atoms in various
spin states affects the overlap of the collision wavefunction with the singlet
molecular manifold and therefore also the reaction rate. We experimentally show
that by preparing the atoms in different spin states one can vary the
charge-exchange rate in agreement with theoretical predictions.
",0,1,0,0,0,0
2459,Comparison of Polynomial Chaos and Gaussian Process surrogates for uncertainty quantification and correlation estimation of spatially distributed open-channel steady flows,"  Data assimilation is widely used to improve flood forecasting capability,
especially through parameter inference requiring statistical information on the
uncertain input parameters (upstream discharge, friction coefficient) as well
as on the variability of the water level and its sensitivity with respect to
the inputs. For particle filter or ensemble Kalman filter, stochastically
estimating probability density function and covariance matrices from a Monte
Carlo random sampling requires a large ensemble of model evaluations, limiting
their use in real-time application. To tackle this issue, fast surrogate models
based on Polynomial Chaos and Gaussian Process can be used to represent the
spatially distributed water level in place of solving the shallow water
equations. This study investigates the use of these surrogates to estimate
probability density functions and covariance matrices at a reduced
computational cost and without the loss of accuracy, in the perspective of
ensemble-based data assimilation. This study focuses on 1-D steady state flow
simulated with MASCARET over the Garonne River (South-West France). Results
show that both surrogates feature similar performance to the Monte-Carlo random
sampling, but for a much smaller computational budget; a few MASCARET
simulations (on the order of 10-100) are sufficient to accurately retrieve
covariance matrices and probability density functions all along the river, even
where the flow dynamic is more complex due to heterogeneous bathymetry. This
paves the way for the design of surrogate strategies suitable for representing
unsteady open-channel flows in data assimilation.
",0,1,0,1,0,0
6063,On the nature of the magnetic phase transition in a Weyl semimetal,"  We investigate the nature of the magnetic phase transition induced by the
short-ranged electron-electron interactions in a Weyl semimetal by using the
perturbative renormalization-group method. We find that the critical point
associated with the quantum phase transition is characterized by a Gaussian
fixed point perturbed by a dangerously irrelevant operator. Although the
low-energy and long-distance physics is governed by a free theory, the
velocities of the fermionic quasiparticles and the magnetic excitations suffer
from nontrivial renormalization effects. In particular, their ratio approaches
one, which indicates an emergent Lorentz symmetry at low energies. We further
investigate the stability of the fixed point in the presence of weak disorder.
We show that while the fixed point is generally stable against weak disorder,
among those disorders that are consistent with the emergent chiral symmetry of
the clean system, a moderately strong random chemical potential and/or random
vector potential may induce a quantum phase transition towards a
disorder-dominated phase. We propose a global phase diagram of the Weyl
semimetal in the presence of both electron-electron interactions and disorder
based on our results.
",0,1,0,0,0,0
13665,Beyond Backprop: Online Alternating Minimization with Auxiliary Variables,"  We propose a novel online alternating minimization (AltMin) algorithm for
training deep neural networks, provide theoretical convergence guarantees and
demonstrate its advantages on several classification tasks as compared both to
standard backpropagation with stochastic gradient descent (backprop-SGD) and to
offline alternating minimization. The key difference from backpropagation is an
explicit optimization over hidden activations, which eliminates gradient chain
computation in backprop, and breaks the weight training problem into
independent, local optimization subproblems; this allows to avoid vanishing
gradient issues, simplify handling non-differentiable nonlinearities, and
perform parallel weight updates across the layers. Moreover, parallel local
synaptic weight optimization with explicit activation propagation is a step
closer to a more biologically plausible learning model than backpropagation,
whose biological implausibility has been frequently criticized. Finally, the
online nature of our approach allows to handle very large datasets, as well as
continual, lifelong learning, which is our key contribution on top of recently
proposed offline alternating minimization schemes (e.g., (Carreira-Perpinan
andWang 2014), (Taylor et al. 2016)).
",0,0,0,1,0,0
15205,On tidal energy in Newtonian two-body motion,"  In this work, which is based on an essential linear analysis carried out by
Christodoulou, we study the evolution of tidal energy for the motion of two
gravitating incompressible fluid balls with free boundaries obeying the
Euler-Poisson equations. The orbital energy is defined as the mechanical energy
of the two bodies' center of mass. According to the classical analysis of
Kepler and Newton, when the fluids are replaced by point masses, the conic
curve describing the trajectories of the masses is a hyperbola when the orbital
energy is positive and an ellipse when the orbital energy is negative. The
orbital energy is conserved in the case of point masses. If the point masses
are initially very far, then the orbital energy is positive, corresponding to
hyperbolic motion. However, in the motion of fluid bodies the orbital energy is
no longer conserved because part of the conserved energy is used in deforming
the boundaries of the bodies. In this case the total energy
$\tilde{\mathcal{E}}$ can be decomposed into a sum
$\tilde{\mathcal{E}}:=\widetilde{\mathcal{E}_{\mathrm{orbital}}}+\widetilde{\mathcal{E}_{\mathrm{tidal}}}$,
with $\widetilde{\mathcal{E}_{\mathrm{tidal}}}$ measuring the energy used in
deforming the boundaries, such that if
$\widetilde{\mathcal{E}_{\mathrm{orbital}}}<-c<0$ for some absolute constant
$c>0$, then the orbit of the bodies must be bounded. In this work we prove that
under appropriate conditions on the initial configuration of the system, the
fluid boundaries and velocity remain regular up to the point of the first
closest approach in the orbit, and that the tidal energy
$\widetilde{\mathcal{E}_{\mathrm{tidal}}}$ can be made arbitrarily large
relative to the total energy $\tilde{\mathcal{E}}$. In particular under these
conditions $\widetilde{\mathcal{E}_{\mathrm{orbital}}}$, which is initially
positive, becomes negative before the point of the first closest approach.
",0,1,1,0,0,0
19580,DeepSD: Generating High Resolution Climate Change Projections through Single Image Super-Resolution,"  The impacts of climate change are felt by most critical systems, such as
infrastructure, ecological systems, and power-plants. However, contemporary
Earth System Models (ESM) are run at spatial resolutions too coarse for
assessing effects this localized. Local scale projections can be obtained using
statistical downscaling, a technique which uses historical climate observations
to learn a low-resolution to high-resolution mapping. Depending on statistical
modeling choices, downscaled projections have been shown to vary significantly
terms of accuracy and reliability. The spatio-temporal nature of the climate
system motivates the adaptation of super-resolution image processing techniques
to statistical downscaling. In our work, we present DeepSD, a generalized
stacked super resolution convolutional neural network (SRCNN) framework for
statistical downscaling of climate variables. DeepSD augments SRCNN with
multi-scale input channels to maximize predictability in statistical
downscaling. We provide a comparison with Bias Correction Spatial
Disaggregation as well as three Automated-Statistical Downscaling approaches in
downscaling daily precipitation from 1 degree (~100km) to 1/8 degrees (~12.5km)
over the Continental United States. Furthermore, a framework using the NASA
Earth Exchange (NEX) platform is discussed for downscaling more than 20 ESM
models with multiple emission scenarios.
",1,0,0,0,0,0
19515,"Locally Repairable Codes with Multiple $(r_{i}, δ_{i})$-Localities","  In distributed storage systems, locally repairable codes (LRCs) are
introduced to realize low disk I/O and repair cost. In order to tolerate
multiple node failures, the LRCs with \emph{$(r, \delta)$-locality} are further
proposed. Since hot data is not uncommon in a distributed storage system, both
Zeh \emph{et al.} and Kadhe \emph{et al.} focus on the LRCs with \emph{multiple
localities or unequal localities} (ML-LRCs) recently, which said that the
localities among the code symbols can be different. ML-LRCs are attractive and
useful in reducing repair cost for hot data. In this paper, we generalize the
ML-LRCs to the $(r,\delta)$-locality case of multiple node failures, and define
an LRC with multiple $(r_{i}, \delta_{i})_{i\in [s]}$ localities ($s\ge 2$),
where $r_{1}\leq r_{2}\leq\dots\leq r_{s}$ and
$\delta_{1}\geq\delta_{2}\geq\dots\geq\delta_{s}\geq2$. Such codes ensure that
some hot data could be repaired more quickly and have better failure-tolerance
in certain cases because of relatively smaller $r_{i}$ and larger $\delta_{i}$.
Then, we derive a Singleton-like upper bound on the minimum distance for the
proposed LRCs by employing the regenerating-set technique. Finally, we obtain a
class of explicit and structured constructions of optimal ML-LRCs, and further
extend them to the cases of multiple $(r_{i}, \delta)_{i\in [s]}$ localities.
",1,0,0,0,0,0
14811,The logic of pseudo-uninorms and their residua,"  Our method of density elimination is generalized to the non-commutative
substructural logic GpsUL*. Then the standard completeness of GpsUL* follows as
a lemma by virtue of previous work by Metcalfe and Montagna. This result shows
that GpsUL* is the logic of pseudo-uninorms and their residua and answered the
question posed by Prof. Metcalfe, Olivetti, Gabbay and Tsinakis.
",0,0,1,0,0,0
7460,Active learning machine learns to create new quantum experiments,"  How useful can machine learning be in a quantum laboratory? Here we raise the
question of the potential of intelligent machines in the context of scientific
research. A major motivation for the present work is the unknown reachability
of various entanglement classes in quantum experiments. We investigate this
question by using the projective simulation model, a physics-oriented approach
to artificial intelligence. In our approach, the projective simulation system
is challenged to design complex photonic quantum experiments that produce
high-dimensional entangled multiphoton states, which are of high interest in
modern quantum experiments. The artificial intelligence system learns to create
a variety of entangled states, and improves the efficiency of their
realization. In the process, the system autonomously (re)discovers experimental
techniques which are only now becoming standard in modern quantum optical
experiments - a trait which was not explicitly demanded from the system but
emerged through the process of learning. Such features highlight the
possibility that machines could have a significantly more creative role in
future research.
",1,0,0,1,0,0
14095,Data Interpolations in Deep Generative Models under Non-Simply-Connected Manifold Topology,"  Exploiting the deep generative model's remarkable ability of learning the
data-manifold structure, some recent researches proposed a geometric data
interpolation method based on the geodesic curves on the learned data-manifold.
However, this interpolation method often gives poor results due to a
topological difference between the model and the dataset. The model defines a
family of simply-connected manifolds, whereas the dataset generally contains
disconnected regions or holes that make them non-simply-connected. To
compensate this difference, we propose a novel density regularizer that make
the interpolation path circumvent the holes denoted by low probability density.
We confirm that our method gives consistently better interpolation results from
the experiments with real-world image datasets.
",1,0,0,1,0,0
3795,A metric of mutual energy and unlikely intersections for dynamical systems,"  We introduce a metric of mutual energy for adelic measures associated to the
Arakelov-Zhang pairing. Using this metric and potential theoretic techniques
involving discrete approximations to energy integrals, we prove an effective
bound on a problem of Baker and DeMarco on unlikely intersections of dynamical
systems, specifically, for the set of complex parameters $c$ for which $z=0$
and $1$ are both preperiodic under iteration of $f_c(z)=z^2 + c$.
",0,0,1,0,0,0
9925,Gated-Attention Architectures for Task-Oriented Language Grounding,"  To perform tasks specified by natural language instructions, autonomous
agents need to extract semantically meaningful representations of language and
map it to visual elements and actions in the environment. This problem is
called task-oriented language grounding. We propose an end-to-end trainable
neural architecture for task-oriented language grounding in 3D environments
which assumes no prior linguistic or perceptual knowledge and requires only raw
pixels from the environment and the natural language instruction as input. The
proposed model combines the image and text representations using a
Gated-Attention mechanism and learns a policy to execute the natural language
instruction using standard reinforcement and imitation learning methods. We
show the effectiveness of the proposed model on unseen instructions as well as
unseen maps, both quantitatively and qualitatively. We also introduce a novel
environment based on a 3D game engine to simulate the challenges of
task-oriented language grounding over a rich set of instructions and
environment states.
",1,0,0,0,0,0
327,Mathematics of Topological Quantum Computing,"  In topological quantum computing, information is encoded in ""knotted"" quantum
states of topological phases of matter, thus being locked into topology to
prevent decay. Topological precision has been confirmed in quantum Hall liquids
by experiments to an accuracy of $10^{-10}$, and harnessed to stabilize quantum
memory. In this survey, we discuss the conceptual development of this
interdisciplinary field at the juncture of mathematics, physics and computer
science. Our focus is on computing and physical motivations, basic mathematical
notions and results, open problems and future directions related to and/or
inspired by topological quantum computing.
",0,1,1,0,0,0
18870,Novel Feature-Based Clustering of Micro-Panel Data (CluMP),"  Micro-panel data are collected and analysed in many research and industry
areas. Cluster analysis of micro-panel data is an unsupervised learning
exploratory method identifying subgroup clusters in a data set which include
homogeneous objects in terms of the development dynamics of monitored
variables. The supply of clustering methods tailored to micro-panel data is
limited. The present paper focuses on a feature-based clustering method,
introducing a novel two-step characteristic-based approach designed for this
type of data. The proposed CluMP method aims to identify clusters that are at
least as internally homogeneous and externally heterogeneous as those obtained
by alternative methods already implemented in the statistical system R. We
compare the clustering performance of the devised algorithm with two extant
methods using simulated micro-panel data sets. Our approach has yielded similar
or better outcomes than the other methods, the advantage of the proposed
algorithm being time efficiency which makes it applicable for large data sets.
",0,0,0,1,0,0
12839,A Simple Fusion of Deep and Shallow Learning for Acoustic Scene Classification,"  In the past, Acoustic Scene Classification systems have been based on hand
crafting audio features that are input to a classifier. Nowadays, the common
trend is to adopt data driven techniques, e.g., deep learning, where audio
representations are learned from data. In this paper, we propose a system that
consists of a simple fusion of two methods of the aforementioned types: a deep
learning approach where log-scaled mel-spectrograms are input to a
convolutional neural network, and a feature engineering approach, where a
collection of hand-crafted features is input to a gradient boosting machine. We
first show that both methods provide complementary information to some extent.
Then, we use a simple late fusion strategy to combine both methods. We report
classification accuracy of each method individually and the combined system on
the TUT Acoustic Scenes 2017 dataset. The proposed fused system outperforms
each of the individual methods and attains a classification accuracy of 72.8%
on the evaluation set, improving the baseline system by 11.8%.
",0,0,0,1,0,0
12015,Greed Works - Online Algorithms For Unrelated Machine Stochastic Scheduling,"  This paper establishes the first performance guarantees for a combinatorial
online algorithm that schedules stochastic, nonpreemptive jobs on unrelated
machines to minimize the expected total weighted completion time. Prior work on
unrelated machine scheduling with stochastic jobs was restricted to the offline
case, and required sophisticated linear or convex programming relaxations for
the assignment of jobs to machines. The algorithm introduced in this paper is
based on a purely combinatorial assignment of jobs to machines, hence it also
works online. The performance bounds are of the same order of magnitude as
those of earlier work, and depend linearly on an upper bound $\Delta$ on the
squared coefficient of variation of the jobs' processing times. They are
$4+2\Delta$ when there are no release dates, and $12+6\Delta$ when jobs are
released over time. For the special case of deterministic processing times,
without and with release times, this paper shows that the same combinatorial
greedy algorithm has a competitive ratio of 4 and 6, respectively. As to the
technical contribution, the paper shows for the first time how dual fitting
techniques can be used for stochastic and nonpreemptive scheduling problems.
",1,0,0,0,0,0
19186,"Atomic Data Revisions for Transitions Relevant to Observations of Interstellar, Circumgalactic, and Intergalactic Matter","  Measurements of element abundances in galaxies from astrophysical
spectroscopy depend sensitively on the atomic data used. With the goal of
making the latest atomic data accessible to the community, we present a
compilation of selected atomic data for resonant absorption lines at
wavelengths longward of 911.753 {\AA} (the \ion{H}{1} Lyman limit), for key
heavy elements (heavier than atomic number 5) of astrophysical interest. In
particular, we focus on the transitions of those ions that have been observed
in the Milky Way interstellar medium (ISM), the circumgalactic medium (CGM) of
the Milky Way and/or other galaxies, and the intergalactic medium (IGM).
We provide wavelengths, oscillator strengths, associated accuracy grades, and
references to the oscillator strength determinations. We also attempt to
compare and assess the recent oscillator strength determinations. For about
22\% of the lines that have updated oscillator strength values, the differences
between the former values and the updated ones are $\gtrsim$~0.1 dex.
Our compilation will be a useful resource for absorption line studies of the
ISM, as well as studies of the CGM and IGM traced by sight lines to quasars and
gamma-ray bursts. Studies (including those enabled by future generations of
extremely large telescopes) of absorption by galaxies against the light of
background galaxies will also benefit from our compilation.
",0,1,0,0,0,0
18815,Limit Theorems in Mallows Distance for Processes with Gibssian Dependence,"  In this paper, we explore the connection between convergence in distribution
and Mallows distance in the context of positively associated random variables.
Our results extend some known invariance principles for sequences with FKG
property. Applications for processes with Gibbssian dependence structures are
included.
",0,0,1,0,0,0
2993,Small-dimensional representations of algebraic groups of type $A_l$,"  For $G$ an algebraic group of type $A_l$ over an algebraically closed field
of characteristic $p$, we determine all irreducible rational representations of
$G$ in defining characteristic with dimensions $\le (l+1)^s$ for $s = 3, 4$,
provided that $l > 18$, $l > 35$ respectively. We also give explicit
descriptions of the corresponding modules for $s = 3$.
",0,0,1,0,0,0
18551,Low Energy Phonons in $Bi_2Sr_2CaCu_2O_{8+δ}$ and their Possible Interaction with Electrons Measured by Inelastic Neutron Scattering,"  Angle-resolved photoemission (ARPES) experiments on copper oxide
superconductors revealed enigmatic kinks in electronic dispersions near 10 meV
presumably due to phonons or impuritites. We used inelastic neutron scattering
to measure phonon branches below 15 meV in a large single crystal sample of
optimally-doped $Bi_2Sr_2CaCu_2O_{8+\delta}$ (BSCCO). The high quality dataset
covered several Brilloiun zones with different final energies. In addition to
acoustic branches, optic branches disperse from 4 meV and 7 meV zone center
energies. The 4 meV branch interacts with acoustic phonons at small
wavevectors, which destroys the LA character of the acoustic branch beyond
~0.15 reciprocal lattice units. We propose a mechanism that explains the low
energy electronic dispersion features based on this observation.
",0,1,0,0,0,0
8797,Improving text classification with vectors of reduced precision,"  This paper presents the analysis of the impact of a floating-point number
precision reduction on the quality of text classification. The precision
reduction of the vectors representing the data (e.g. TF-IDF representation in
our case) allows for a decrease of computing time and memory footprint on
dedicated hardware platforms. The impact of precision reduction on the
classification quality was performed on 5 corpora, using 4 different
classifiers. Also, dimensionality reduction was taken into account. Results
indicate that the precision reduction improves classification accuracy for most
cases (up to 25% of error reduction). In general, the reduction from 64 to 4
bits gives the best scores and ensures that the results will not be worse than
with the full floating-point representation.
",1,0,0,0,0,0
13255,Introducing the anatomy of disciplinary discernment: an example from astronomy,"  Education is increasingly being framed by a competence mindset; the value of
knowledge lies much more in competence performativity and innovation than in
simply knowing. Reaching such competency in areas such as astronomy and physics
has long been known to be challenging. The movement from everyday conceptions
of the world around us to a disciplinary interpretation is fraught with
pitfalls and problems. Thus, what underpins the characteristics of the
disciplinary trajectory to competence becomes an important educational
consideration. In this article we report on a study involving what students and
lecturers discern from the same disciplinary semiotic resource. We use this to
propose an Anatomy of Disciplinary Discernment (ADD), a hierarchy of what is
focused on and how it is interpreted in an appropriate, disciplinary manner, as
an overarching fundamental aspect of disciplinary learning. Students and
lecturers in astronomy and physics were asked to describe what they could
discern from a video simulation of travel through our Galaxy and beyond. 137
people from nine countries participated. The descriptions were analysed using a
hermeneutic interpretive study approach. The analysis resulted in the
formulation of five qualitatively different categories of discernment; the ADD,
reflecting a view of participants' competence levels. The ADD reveals four
increasing levels of disciplinary discernment: Identification, Explanation,
Appreciation, and Evaluation. This facilitates the identification of a clear
relationship between educational level and the level of disciplinary
discernment. The analytical outcomes of the study suggest how teachers of
science, after using the ADD to assess the students disciplinary knowledge, may
attain new insights into how to create more effective learning environments by
explicitly crafting their teaching to support the crossing of boundaries in the
ADD model.
",0,1,0,0,0,0
6385,Decoupling of graphene from Ni(111) via oxygen intercalation,"  The combination of the surface science techniques (STM, XPS, ARPES) and
density-functional theory calculations was used to study the decoupling of
graphene from Ni(111) by oxygen intercalation. The formation of the
antiferromagnetic (AFM) NiO layer at the interface between graphene and
ferromagnetic (FM) Ni is found, where graphene protects the underlying AFM/FM
sandwich system. It is found that graphene is fully decoupled in this system
and strongly $p$-doped via charge transfer with a position of the Dirac point
of $(0.69\pm0.02)$ eV above the Fermi level. Our theoretical analysis confirms
all experimental findings, addressing also the interface properties between
graphene and AFM NiO.
",0,1,0,0,0,0
16980,The Fundamental Infinity-Groupoid of a Parametrized Family,"  Given an infinity-category C, one can naturally construct an
infinity-category Fam(C) of families of objects in C indexed by
infinity-groupoids. An ordinary categorical version of this construction was
used by Borceux and Janelidze in the study of generalized covering maps in
categorical Galois theory. In this paper, we develop the homotopy theory of
such ""parametrized families"" as generalization of the classical homotopy theory
of spaces. In particular, we study homotopy-theoretical constructions that
arise from the fundamental infinity-groupoids of families in an
infinity-category. In the same spirit, we show that Fam(C) admits a
Grothendieck topology which generalizes the canonical/epimorphism topology on
the infinity-topos of infinity-groupoids in the sense of Carchedi.
",0,0,1,0,0,0
20561,Lagrangian Flow Network approach to an open flow model,"  Concepts and tools from network theory, the so-called Lagrangian Flow Network
framework, have been successfully used to obtain a coarse-grained description
of transport by closed fluid flows. Here we explore the application of this
methodology to open chaotic flows, and check it with numerical results for a
model open flow, namely a jet with a localized wave perturbation. We find that
network nodes with high values of out-degree and of finite-time entropy in the
forward-in-time direction identify the location of the chaotic saddle and its
stable manifold, whereas nodes with high in-degree and backwards finite-time
entropy highlight the location of the saddle and its unstable manifold. The
cyclic clustering coefficient, associated to the presence of periodic orbits,
takes non-vanishing values at the location of the saddle itself.
",0,1,0,0,0,0
13003,"Trajectory Tracking Control of a Flexible Spine Robot, With and Without a Reference Input","  The Underactuated Lightweight Tensegrity Robotic Assistive Spine (ULTRA
Spine) project is an ongoing effort to develop a flexible, actuated backbone
for quadruped robots. In this work, model-predictive control is used to track a
trajectory in the robot's state space, in simulation. The state trajectory used
here corresponds to a bending motion of the spine, with translations and
rotations of the moving vertebrae. Two different controllers are presented in
this work: one that does not use a reference input but includes smoothing
constrants, and a second one that uses a reference input without smoothing. For
the smoothing controller, without reference inputs, the error converges to
zero, while the simpler-to-tune controller with an input reference shows small
errors but not complete convergence. It is expected that this controller will
converge as it is improved further.
",1,0,0,0,0,0
4773,Thermal and non-thermal emission from the cocoon of a gamma-ray burst jet,"  We present hydrodynamic simulations of the hot cocoon produced when a
relativistic jet passes through the gamma-ray burst (GRB) progenitor star and
its environment, and we compute the lightcurve and spectrum of the radiation
emitted by the cocoon. The radiation from the cocoon has a nearly thermal
spectrum with a peak in the X-ray band, and it lasts for a few minutes in the
observer frame; the cocoon radiation starts at roughly the same time as when
$\gamma$-rays from a burst trigger detectors aboard GRB satellites. The
isotropic cocoon luminosity ($\sim 10^{47}$ erg s$^{-1}$) is of the same order
of magnitude as the X-ray luminosity of a typical long-GRB afterglow during the
plateau phase. This radiation should be identifiable in the Swift data because
of its nearly thermal spectrum which is distinct from the somewhat brighter
power-law component. The detection of this thermal component would provide
information regarding the size and density stratification of the GRB progenitor
star. Photons from the cocoon are also inverse-Compton (IC) scattered by
electrons in the relativistic jet. We present the IC lightcurve and spectrum,
by post-processing the results of the numerical simulations. The IC spectrum
lies in 10 keV--MeV band for typical GRB parameters. The detection of this IC
component would provide an independent measurement of GRB jet Lorentz factor
and it would also help to determine the jet magnetisation parameter.
",0,1,0,0,0,0
14013,Anomaly Detection in Multivariate Non-stationary Time Series for Automatic DBMS Diagnosis,"  Anomaly detection in database management systems (DBMSs) is difficult because
of increasing number of statistics (stat) and event metrics in big data system.
In this paper, I propose an automatic DBMS diagnosis system that detects
anomaly periods with abnormal DB stat metrics and finds causal events in the
periods. Reconstruction error from deep autoencoder and statistical process
control approach are applied to detect time period with anomalies. Related
events are found using time series similarity measures between events and
abnormal stat metrics. After training deep autoencoder with DBMS metric data,
efficacy of anomaly detection is investigated from other DBMSs containing
anomalies. Experiment results show effectiveness of proposed model, especially,
batch temporal normalization layer. Proposed model is used for publishing
automatic DBMS diagnosis reports in order to determine DBMS configuration and
SQL tuning.
",1,0,0,1,0,0
6229,SU(2) Pfaffian systems and gauge theory,"  Motivated by the description of Nurowski's conformal structure for maximally
symmetric homogeneous examples of bracket-generating rank 2 distributions in
dimension 5, aka $(2,3,5)$-distributions, we consider a rank $3$ Pfaffian
system in dimension 5 with $SU(2)$ symmetry. We find the conditions for which
this Pfaffian system has the maximal symmetry group (in the real case this is
the split real form of $G_2$), and give the associated Nurowski's conformal
classes. We also present a $SU(2)$ gauge-theoretic interpretation of the
results obtained.
",0,0,1,0,0,0
18675,On the virtual singular braid monoid,"  We study the algebraic structures of the virtual singular braid monoid,
$VSB_n$, and the virtual singular pure braid monoid, $VSP_n$. The monoid
$VSB_n$ is the splittable extension of $VSP_n$ by the symmetric group $S_n$. We
also construct a representation of $VSB_n$.
",0,0,1,0,0,0
5315,Exploring light mediators with low-threshold direct detection experiments,"  We explore the potential of future cryogenic direct detection experiments to
determine the properties of the mediator that communicates the interactions
between dark matter and nuclei. Due to their low thresholds and large
exposures, experiments like CRESST-III, SuperCDMS SNOLAB and EDELWEISS-III will
have excellent capability to reconstruct mediator masses in the MeV range for a
large class of models. Combining the information from several experiments
further improves the parameter reconstruction, even when taking into account
additional nuisance parameters related to background uncertainties and the dark
matter velocity distribution. These observations may offer the intriguing
possibility of studying dark matter self-interactions with direct detection
experiments.
",0,1,0,0,0,0
17367,Grid-forming Control for Power Converters based on Matching of Synchronous Machines,"  We consider the problem of grid-forming control of power converters in
low-inertia power systems. Starting from an average-switch three-phase inverter
model, we draw parallels to a synchronous machine (SM) model and propose a
novel grid-forming converter control strategy which dwells upon the main
characteristic of a SM: the presence of an internal rotating magnetic field. In
particular, we augment the converter system with a virtual oscillator whose
frequency is driven by the DC-side voltage measurement and which sets the
converter pulse-width-modulation signal, thereby achieving exact matching
between the converter in closed-loop and the SM dynamics. We then provide a
sufficient condition assuring existence, uniqueness, and global asymptotic
stability of equilibria in a coordinate frame attached to the virtual
oscillator angle. By actuating the DC-side input of the converter we are able
to enforce this sufficient condition. In the same setting, we highlight strict
incremental passivity, droop, and power-sharing properties of the proposed
framework, which are compatible with conventional requirements of power system
operation. We subsequently adopt disturbance decoupling techniques to design
additional control loops that regulate the DC-side voltage, as well as AC-side
frequency and amplitude, while in the end validating them with numerical
experiments.
",0,0,1,0,0,0
18598,Global existence and convergence of $Q$-curvature flow on manifolds of even dimension,"  Using a negative gradient flow approach, we generalize and unify some
existence theorems for the problem of prescribing $Q$-curvature first by Baird,
Fardoun, and Regbaoui (Calc. Var. 27 75-104) for $4$-manifolds with a possible
sign-changing curvature candidate then by Brendle (Ann. Math. 158 323-343) for
$n$-manifolds with even $n$ with positive curvature candidate to the case of
$n$-manifolds of all even dimension with sign-changing curvature candidates.
Making use of the \L ojasiewicz--Simon inequality, we also address the rate of
the convergence.
",0,0,1,0,0,0
323,Yu-Shiba-Rusinov bands in superconductors in contact with a magnetic insulator,"  Superconductor-Ferromagnet (SF) heterostructures are of interest due to
numerous phenomena related to the spin-dependent interaction of Cooper pairs
with the magnetization. Here we address the effects of a magnetic insulator on
the density of states of a superconductor based on a recently developed
boundary condition for strongly spin-dependent interfaces. We show that the
boundary to a magnetic insulator has a similar effect like the presence of
magnetic impurities. In particular we find that the impurity effects of
strongly scattering localized spins leading to the formation of Shiba bands can
be mapped onto the boundary problem.
",0,1,0,0,0,0
5565,Quantum machine learning: a classical perspective,"  Recently, increased computational power and data availability, as well as
algorithmic advances, have led machine learning techniques to impressive
results in regression, classification, data-generation and reinforcement
learning tasks. Despite these successes, the proximity to the physical limits
of chip fabrication alongside the increasing size of datasets are motivating a
growing number of researchers to explore the possibility of harnessing the
power of quantum computation to speed-up classical machine learning algorithms.
Here we review the literature in quantum machine learning and discuss
perspectives for a mixed readership of classical machine learning and quantum
computation experts. Particular emphasis will be placed on clarifying the
limitations of quantum algorithms, how they compare with their best classical
counterparts and why quantum resources are expected to provide advantages for
learning problems. Learning in the presence of noise and certain
computationally hard problems in machine learning are identified as promising
directions for the field. Practical questions, like how to upload classical
data into quantum form, will also be addressed.
",1,0,0,1,0,0
20072,"The circumstellar disk HD$\,$169142: gas, dust and planets acting in concert?","  HD$\,$169142 is an excellent target to investigate signs of planet-disk
interaction due to the previous evidence of gap structures. We performed J-band
(~1.2{\mu}m) polarized intensity imaging of HD169142 with VLT/SPHERE. We
observe polarized scattered light down to 0.16"" (~19 au) and find an inner gap
with a significantly reduced scattered light flux. We confirm the previously
detected double ring structure peaking at 0.18"" (~21 au) and 0.56"" (~66 au),
and marginally detect a faint third gap at 0.70""-0.73"" (~82-85 au). We explore
dust evolution models in a disk perturbed by two giant planets, as well as
models with a parameterized dust size distribution. The dust evolution model is
able to reproduce the ring locations and gap widths in polarized intensity, but
fails to reproduce their depths. It, however, gives a good match with the ALMA
dust continuum image at 1.3 mm. Models with a parameterized dust size
distribution better reproduce the gap depth in scattered light, suggesting that
dust filtration at the outer edges of the gaps is less effective. The pile-up
of millimeter grains in a dust trap and the continuous distribution of small
grains throughout the gap likely require a more efficient dust fragmentation
and dust diffusion in the dust trap. Alternatively, turbulence or charging
effects might lead to a reservoir of small grains at the surface layer that is
not affected by the dust growth and fragmentation cycle dominating the dense
disk midplane. The exploration of models shows that extracting planet
properties such as mass from observed gap profiles is highly degenerate.
",0,1,0,0,0,0
8909,Mask R-CNN,"  We present a conceptually simple, flexible, and general framework for object
instance segmentation. Our approach efficiently detects objects in an image
while simultaneously generating a high-quality segmentation mask for each
instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a
branch for predicting an object mask in parallel with the existing branch for
bounding box recognition. Mask R-CNN is simple to train and adds only a small
overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to
generalize to other tasks, e.g., allowing us to estimate human poses in the
same framework. We show top results in all three tracks of the COCO suite of
challenges, including instance segmentation, bounding-box object detection, and
person keypoint detection. Without bells and whistles, Mask R-CNN outperforms
all existing, single-model entries on every task, including the COCO 2016
challenge winners. We hope our simple and effective approach will serve as a
solid baseline and help ease future research in instance-level recognition.
Code has been made available at: this https URL
",1,0,0,0,0,0
5894,Technological Parasitism,"  Technological parasitism is a new theory to explain the evolution of
technology in society. In this context, this study proposes a model to analyze
the interaction between a host technology (system) and a parasitic technology
(subsystem) to explain evolutionary pathways of technologies as complex
systems. The coefficient of evolutionary growth of the model here indicates the
typology of evolution of parasitic technology in relation to host technology:
i.e., underdevelopment, growth and development. This approach is illustrated
with realistic examples using empirical data of product and process
technologies. Overall, then, the theory of technological parasitism can be
useful for bringing a new perspective to explain and generalize the evolution
of technology and predict which innovations are likely to evolve rapidly in
society.
",0,0,0,0,0,1
384,PRE-render Content Using Tiles (PRECUT). 1. Large-Scale Compound-Target Relationship Analyses,"  Visualizing a complex network is computationally intensive process and
depends heavily on the number of components in the network. One way to solve
this problem is not to render the network in real time. PRE-render Content
Using Tiles (PRECUT) is a process to convert any complex network into a
pre-rendered network. Tiles are generated from pre-rendered images at different
zoom levels, and navigating the network simply becomes delivering relevant
tiles. PRECUT is exemplified by performing large-scale compound-target
relationship analyses. Matched molecular pair (MMP) networks were created using
compounds and the target class description found in the ChEMBL database. To
visualize MMP networks, the MMP network viewer has been implemented in COMBINE
and as a web application, hosted at this http URL.
",1,0,0,0,0,0
6507,Ramsey expansions of metrically homogeneous graphs,"  We discuss the Ramsey property, the existence of a stationary independence
relation and the coherent extension property for partial isometries (coherent
EPPA) for all classes of metrically homogeneous graphs from Cherlin's
catalogue, which is conjectured to include all such structures. We show that,
with the exception of tree-like graphs, all metric spaces in the catalogue have
precompact Ramsey expansions (or lifts) with the expansion property. With two
exceptions we can also characterise the existence of a stationary independence
relation and the coherent EPPA.
Our results can be seen as a new contribution to Nešetřil's
classification programme of Ramsey classes and as empirical evidence of the
recent convergence in techniques employed to establish the Ramsey property, the
expansion (or lift or ordering) property, EPPA and the existence of a
stationary independence relation. At the heart of our proof is a canonical way
of completing edge-labelled graphs to metric spaces in Cherlin's classes. The
existence of such a ""completion algorithm"" then allows us to apply several
strong results in the areas that imply EPPA and respectively the Ramsey
property.
The main results have numerous corollaries on the automorphism groups of the
Fraïssé limits of the classes, such as amenability, unique ergodicity,
existence of universal minimal flows, ample generics, small index property,
21-Bergman property and Serre's property (FA).
",1,0,1,0,0,0
14198,Relative stability of a ferroelectric state in (Na0.5Bi0.5)TiO3-based compounds under substitutions: Role of a tolerance factor in expansion of the temperature interval of stable ferroelectric state,"  The influence of the B-site ion substitutions in
(1-x)(Bi1/2Na1/2)TiO3-xBaTiO3 system of solid solutions on the relative
stability of the ferroelectric and antiferroelectric phases has been studied.
The ions of zirconium, tin, along with (In0.5Nb0.5), (Fe0.5Nb0.5), (Al0.5V0.5)
ion complexes have been used as substituting elements. An increase in the
concentration of the substituting ion results in a near linear variation in the
size of the crystal lattice cell. Along with the cell size variation a change
in the relative stability of the ferroelectric and antiferroelectric phases
takes place according to the changes of the tolerance factor of the solid
solution. An increase in the tolerance factor leads to the increase in the
temperature of the ferroelectric-antiferroelectric phase transition, and vice
versa. All obtained results demonstrate the predominant influence of the ion
size factor on the relative stability of the ferroelectric and
antiferroelectric states in the (Na0.5Bi0.5)TiO3-based solid solutions and
indicate the way for raising the temperature of the
ferroelectric-antiferroelectric phase transition.
",0,1,0,0,0,0
7597,Normal-state Properties of a Unitary Bose-Fermi Mixture: A Combined Strong-coupling Approach with Universal Thermodynamics,"  We theoretically investigate normal-state properties of a unitary Bose-Fermi
mixture. Including strong hetero-pairing fluctuations, we evaluate the Bose and
Fermi chemical potential, internal energy, pressure, entropy, as well as
specific heat at constant volume $C_V$, within the framework of a combined
strong-coupling theory with exact thermodynamic identities. We show that
hetero-pairing fluctuations at the unitarity cause non-monotonic temperature
dependence of $C_V$, being qualitatively different from the monotonic behavior
of this quantity in the weak- and strong-coupling limit. On the other hand,
such an anomalous behavior is not seen in the other quantities. Our results
indicate that the specific heat $C_V$, which has recently become observable in
cold atom physics, is a useful quantity for understanding strong-coupling
aspects of this quantum system.
",0,1,0,0,0,0
1806,Cautious Model Predictive Control using Gaussian Process Regression,"  Gaussian process (GP) regression has been widely used in supervised machine
learning due to its flexibility and inherent ability to describe uncertainty in
function estimation. In the context of control, it is seeing increasing use for
modeling of nonlinear dynamical systems from data, as it allows the direct
assessment of residual model uncertainty. We present a model predictive control
(MPC) approach that integrates a nominal system with an additive nonlinear part
of the dynamics modeled as a GP. Approximation techniques for propagating the
state distribution are reviewed and we describe a principled way of formulating
the chance constrained MPC problem, which takes into account residual
uncertainties provided by the GP model to enable cautious control. Using
additional approximations for efficient computation, we finally demonstrate the
approach in a simulation example, as well as in a hardware implementation for
autonomous racing of remote controlled race cars, highlighting improvements
with regard to both performance and safety over a nominal controller.
",1,0,1,0,0,0
18581,Maximum Margin Principal Components,"  Principal Component Analysis (PCA) is a very successful dimensionality
reduction technique, widely used in predictive modeling. A key factor in its
widespread use in this domain is the fact that the projection of a dataset onto
its first $K$ principal components minimizes the sum of squared errors between
the original data and the projected data over all possible rank $K$
projections. Thus, PCA provides optimal low-rank representations of data for
least-squares linear regression under standard modeling assumptions. On the
other hand, when the loss function for a prediction problem is not the
least-squares error, PCA is typically a heuristic choice of dimensionality
reduction -- in particular for classification problems under the zero-one loss.
In this paper we target classification problems by proposing a straightforward
alternative to PCA that aims to minimize the difference in margin distribution
between the original and the projected data. Extensive experiments show that
our simple approach typically outperforms PCA on any particular dataset, in
terms of classification error, though this difference is not always
statistically significant, and despite being a filter method is frequently
competitive with Partial Least Squares (PLS) and Lasso on a wide range of
datasets.
",1,0,0,1,0,0
4845,(non)-automaticity of completely multiplicative sequences having negligible many non-trivial prime factors,"  In this article we consider the completely multiplicative sequences $(a_n)_{n
\in \mathbf{N}}$ defined on a field $\mathbf{K}$ and satisfying $$\sum_{p| p
\leq n, a_p \neq 1, p \in \mathbf{P}}\frac{1}{p}<\infty,$$ where $\mathbf{P}$
is the set of prime numbers. We prove that if such sequences are automatic then
they cannot have infinitely many prime numbers $p$ such that $a_{p}\neq 1$.
Using this fact, we prove that if a completely multiplicative sequence
$(a_n)_{n \in \mathbf{N}}$, vanishing or not, can be written in the form
$a_n=b_n\chi_n$ such that $(b_n)_{n \in \mathbf{N}}$ is a non ultimately
periodic, completely multiplicative automatic sequence satisfying the above
condition, and $(\chi_n)_{n \in \mathbf{N}}$ is a Dirichlet character or a
constant sequence, then there exists only one prime number $p$ such that $b_p
\neq 1$ or $0$.
",0,0,1,0,0,0
8853,Learning to Generalize: Meta-Learning for Domain Generalization,"  Domain shift refers to the well known problem that a model trained in one
source domain performs poorly when applied to a target domain with different
statistics. {Domain Generalization} (DG) techniques attempt to alleviate this
issue by producing models which by design generalize well to novel testing
domains. We propose a novel {meta-learning} method for domain generalization.
Rather than designing a specific model that is robust to domain shift as in
most previous DG work, we propose a model agnostic training procedure for DG.
Our algorithm simulates train/test domain shift during training by synthesizing
virtual testing domains within each mini-batch. The meta-optimization objective
requires that steps to improve training domain performance should also improve
testing domain performance. This meta-learning procedure trains models with
good generalization ability to novel domains. We evaluate our method and
achieve state of the art results on a recent cross-domain image classification
benchmark, as well demonstrating its potential on two classic reinforcement
learning tasks.
",1,0,0,0,0,0
1912,Bootstrapping Exchangeable Random Graphs,"  We introduce two new bootstraps for exchangeable random graphs. One, the
""empirical graphon"", is based purely on resampling, while the other, the
""histogram stochastic block model"", is a model-based ""sieve"" bootstrap. We show
that both of them accurately approximate the sampling distributions of motif
densities, i.e., of the normalized counts of the number of times fixed
subgraphs appear in the network. These densities characterize the distribution
of (infinite) exchangeable networks. Our bootstraps therefore give, for the
first time, a valid quantification of uncertainty in inferences about
fundamental network statistics, and so of parameters identifiable from them.
",0,0,0,1,0,0
15567,A Note on Multiparty Communication Complexity and the Hales-Jewett Theorem,"  For integers $n$ and $k$, the density Hales-Jewett number $c_{n,k}$ is
defined as the maximal size of a subset of $[k]^n$ that contains no
combinatorial line. We show that for $k \ge 3$ the density Hales-Jewett number
$c_{n,k}$ is equal to the maximal size of a cylinder intersection in the
problem $Part_{n,k}$ of testing whether $k$ subsets of $[n]$ form a partition.
It follows that the communication complexity, in the Number On the Forehead
(NOF) model, of $Part_{n,k}$, is equal to the minimal size of a partition of
$[k]^n$ into subsets that do not contain a combinatorial line. Thus, the bound
in \cite{chattopadhyay2007languages} on $Part_{n,k}$ using the Hales-Jewett
theorem is in fact tight, and the density Hales-Jewett number can be thought of
as a quantity in communication complexity. This gives a new angle to this well
studied quantity.
As a simple application we prove a lower bound on $c_{n,k}$, similar to the
lower bound in \cite{polymath2010moser} which is roughly $c_{n,k}/k^n \ge
\exp(-O(\log n)^{1/\lceil \log_2 k\rceil})$. This lower bound follows from a
protocol for $Part_{n,k}$. It is interesting to better understand the
communication complexity of $Part_{n,k}$ as this will also lead to the better
understanding of the Hales-Jewett number. The main purpose of this note is to
motivate this study.
",1,0,0,0,0,0
5847,On Bousfield's problem for solvable groups of finite Prüfer rank,"  For a group $G$ and $R=\mathbb Z,\mathbb Z/p,\mathbb Q$ we denote by $\hat
G_R$ the $R$-completion of $G.$ We study the map $H_n(G,K)\to H_n(\hat G_R,K),$
where $(R,K)=(\mathbb Z,\mathbb Z/p),(\mathbb Z/p,\mathbb Z/p),(\mathbb
Q,\mathbb Q).$ We prove that $H_2(G,K)\to H_2(\hat G_R,K)$ is an epimorphism
for a finitely generated solvable group $G$ of finite Prüfer rank. In
particular, Bousfield's $HK$-localisation of such groups coincides with the
$K$-completion for $K=\mathbb Z/p,\mathbb Q.$ Moreover, we prove that
$H_n(G,K)\to H_n(\hat G_R,K)$ is an epimorphism for any $n$ if $G$ is a
finitely presented group of the form $G=M\rtimes C,$ where $C$ is the infinite
cyclic group and $M$ is a $C$-module.
",0,0,1,0,0,0
13267,Euler characteristic and Akashi series for Selmer groups over global function fields,"  Let $A$ be an abelian variety defined over a global function field $F$ of
positive characteristic $p$ and let $K/F$ be a $p$-adic Lie extension with
Galois group $G$. We provide a formula for the Euler characteristic
$\chi(G,Sel_A(K)_p)$ of the $p$-part of the Selmer group of $A$ over $K$. In
the special case $G=\mathbb{Z}_p^d$ and $A$ a constant ordinary variety, using
Akashi series, we show how the Euler characteristic of the dual of $Sel_A(K)_p$
is related to special values of a $p$-adic $\mathcal{L}$-function.
",0,0,1,0,0,0
9802,Study of Electro-Caloric Effect in Ca and Sn co-Doped BaTiO3 Ceramics,"  The present work deals with the study of structural, ferroelectric,
dielectric and electro-caloric effects in lead free ferroelectric
polycrystalline Ba1-xCaxTi0.95Sn0.05O3 (x= 2, 5 and 10 %) i.e., Ca, Sn co-doped
BaTiO3 (BTO). Phase purity of the samples is confirmed from X-ray data by using
Rietveld refinement. 119Sn Mössbauer reveals homogenous phase as well as
iso-valent substitution of Sn at Ti site. Enhancements in ferroelectric and
dielectric properties have been observed. Indirect method which is based on
Maxwell equation has been used to determine the electro-caloric (EC) effect in
the studied ferroelectric ceramics and maximum EC coefficient is observed for
Ba0.95Ca0.05Ti0.95Sn0.05O3.
",0,1,0,0,0,0
7713,"Comment on ""Kinetic decoupling of WIMPs: Analytic expressions""","  Visinelli and Gondolo (2015, hereafter VG15) derived analytic expressions for
the evolution of the dark matter temperature in a generic cosmological model.
They then calculated the dark matter kinetic decoupling temperature
$T_{\mathrm{kd}}$ and compared their results to the Gelmini and Gondolo (2008,
hereafter GG08) calculation of $T_{\mathrm{kd}}$ in an early matter-dominated
era (EMDE), which occurs when the Universe is dominated by either a decaying
oscillating scalar field or a semistable massive particle before Big Bang
nucleosynthesis. VG15 found that dark matter decouples at a lower temperature
in an EMDE than it would in a radiation-dominated era, while GG08 found that
dark matter decouples at a higher temperature in an EMDE than it would in a
radiation-dominated era. VG15 attributed this discrepancy to the presence of a
matching constant that ensures that the dark matter temperature is continuous
during the transition from the EMDE to the subsequent radiation-dominated era
and concluded that the GG08 result is incorrect. We show that the disparity is
due to the fact that VG15 compared $T_\mathrm{kd}$ in an EMDE to the decoupling
temperature in a radiation-dominated universe that would result in the same
dark matter temperature at late times. Since decoupling during an EMDE leaves
the dark matter colder than it would be if it decoupled during radiation
domination, this temperature is much higher than $T_\mathrm{kd}$ in a standard
thermal history, which is indeed lower than $T_{\mathrm{kd}}$ in an EMDE, as
stated by GG08.
",0,1,0,0,0,0
14695,Universal Reinforcement Learning Algorithms: Survey and Experiments,"  Many state-of-the-art reinforcement learning (RL) algorithms typically assume
that the environment is an ergodic Markov Decision Process (MDP). In contrast,
the field of universal reinforcement learning (URL) is concerned with
algorithms that make as few assumptions as possible about the environment. The
universal Bayesian agent AIXI and a family of related URL algorithms have been
developed in this setting. While numerous theoretical optimality results have
been proven for these agents, there has been no empirical investigation of
their behavior to date. We present a short and accessible survey of these URL
algorithms under a unified notation and framework, along with results of some
experiments that qualitatively illustrate some properties of the resulting
policies, and their relative performance on partially-observable gridworld
environments. We also present an open-source reference implementation of the
algorithms which we hope will facilitate further understanding of, and
experimentation with, these ideas.
",1,0,0,0,0,0
9392,Two-dimensional Schrödinger symmetry and three-dimensional breathers and Kelvin-ripple complexes as quasi-massive-Nambu-Goldstone modes,"  Bose-Einstein condensates (BECs) confined in a two-dimensional (2D) harmonic
trap are known to possess a hidden 2D Schrödinger symmetry, that is, the
Schrödinger symmetry modified by a trapping potential. Spontaneous breaking
of this symmetry gives rise to a breathing motion of the BEC, whose oscillation
frequency is robustly determined by the strength of the harmonic trap. In this
paper, we demonstrate that the concept of the 2D Schrödinger symmetry can be
applied to predict the nature of three dimensional (3D) collective modes
propagating along a condensate confined in an elongated trap. We find three
kinds of collective modes whose existence is robustly ensured by the
Schrödinger symmetry, which are physically interpreted as one breather mode
and two Kelvin-ripple complex modes, i.e., composite modes in which the vortex
core and the condensate surface oscillate interactively. We provide analytical
expressions for the dispersion relations (energy-momentum relation) of these
modes using the Bogoliubov theory [D. A. Takahashi and M. Nitta, Ann. Phys.
354, 101 (2015)]. Furthermore, we point out that these modes can be interpreted
as ""quasi-massive-Nambu-Goldstone (NG) modes"", that is, they have the
properties of both quasi-NG and massive NG modes: quasi-NG modes appear when a
symmetry of a part of a Lagrangian, which is not a symmetry of full a
Lagrangian, is spontaneously broken, while massive NG modes appear when a
modified symmetry is spontaneously broken.
",0,1,0,0,0,0
2618,Supercongruences between truncated ${}_3F_2$ hypergeometric series,"  We establish four supercongruences between truncated ${}_3F_2$ hypergeometric
series involving $p$-adic Gamma functions, which extend some of the
Rodriguez-Villegas supercongruences.
",0,0,1,0,0,0
16369,Automated Synthesis of Secure Platform Mappings,"  System development often involves decisions about how a high-level design is
to be implemented using primitives from a low-level platform. Certain
decisions, however, may introduce undesirable behavior into the resulting
implementation, possibly leading to a violation of a desired property that has
already been established at the design level. In this paper, we introduce the
problem of synthesizing a property-preserving platform mapping: A set of
implementation decisions ensuring that a desired property is preserved from a
high-level design into a low-level platform implementation. We provide a
formalization of the synthesis problem and propose a technique for synthesizing
a mapping based on symbolic constraint search. We describe our prototype
implementation, and a real-world case study demonstrating the application of
our technique to synthesizing secure mappings for the popular web authorization
protocols OAuth 1.0 and 2.0.
",1,0,0,0,0,0
10412,Identification of multiple hard X-ray sources in solar flares: A Bayesian analysis of the February 20 2002 event,"  The hard X-ray emission in a solar flare is typically characterized by a
number of discrete sources, each with its own spectral, temporal, and spatial
variability. Establishing the relationship amongst these sources is critical to
determine the role of each in the energy release and transport processes that
occur within the flare. In this paper we present a novel method to identify and
characterize each source of hard X-ray emission. The method permits a
quantitative determination of the most likely number of subsources present, and
of the relative probabilities that the hard X-ray emission in a given subregion
of the flare is represented by a complicated multiple source structure or by a
simpler single source. We apply the method to a well-studied flare on
2002~February~20 in order to assess competing claims as to the number of
chromospheric footpoint sources present, and hence to the complexity of the
underlying magnetic geometry/toplogy. Contrary to previous claims of the need
for multiple sources to account for the chromospheric hard X-ray emission at
different locations and times, we find that a simple
two-footpoint-plus-coronal-source model is the most probable explanation for
the data. We also find that one of the footpoint sources moves quite rapidly
throughout the event, a factor that presumably complicated previous analyses.
The inferred velocity of the footpoint corresponds to a very high induced
electric field, compatible with those in thin reconnecting current sheets.
",0,0,0,1,0,0
12326,Estimators of the correlation coefficient in the bivariate exponential distribution,"  A finite-support constraint on the parameter space is used to derive a lower
bound on the error of an estimator of the correlation coefficient in the
bivariate exponential distribution. The bound is then exploited to examine
optimality of three estimators, each being a nonlinear function of moments of
exponential or Rayleigh observables. The estimator based on a measure of cosine
similarity is shown to be highly efficient for values of the correlation
coefficient greater than 0.35; for smaller values, however, it is the
transformed Pearson correlation coefficient that exhibits errors closer to the
derived bound.
",0,0,0,1,0,0
2451,Exploring nucleon spin structure through neutrino neutral-current interactions in MicroBooNE,"  The net contribution of the strange quark spins to the proton spin, $\Delta
s$, can be determined from neutral current elastic neutrino-proton interactions
at low momentum transfer combined with data from electron-proton scattering.
The probability of neutrino-proton interactions depends in part on the axial
form factor, which represents the spin structure of the proton and can be
separated into its quark flavor contributions. Low momentum transfer neutrino
neutral current interactions can be measured in MicroBooNE, a high-resolution
liquid argon time projection chamber (LArTPC) in its first year of running in
the Booster Neutrino Beamline at Fermilab. The signal for these interactions in
MicroBooNE is a single short proton track. We present our work on the automated
reconstruction and classification of proton tracks in LArTPCs, an important
step in the determination of neutrino- nucleon cross sections and the
measurement of $\Delta s$.
",0,1,0,0,0,0
3949,Convergence rate of a simulated annealing algorithm with noisy observations,"  In this paper we propose a modified version of the simulated annealing
algorithm for solving a stochastic global optimization problem. More precisely,
we address the problem of finding a global minimizer of a function with noisy
evaluations. We provide a rate of convergence and its optimized parametrization
to ensure a minimal number of evaluations for a given accuracy and a confidence
level close to 1. This work is completed with a set of numerical
experimentations and assesses the practical performance both on benchmark test
cases and on real world examples.
",0,0,1,1,0,0
10819,Data-Driven Estimation Of Mutual Information Between Dependent Data,"  We consider the problem of estimating mutual information between dependent
data, an important problem in many science and engineering applications. We
propose a data-driven, non-parametric estimator of mutual information in this
paper. The main novelty of our solution lies in transforming the data to
frequency domain to make the problem tractable. We define a novel
metric--mutual information in frequency--to detect and quantify the dependence
between two random processes across frequency using Cramér's spectral
representation. Our solution calculates mutual information as a function of
frequency to estimate the mutual information between the dependent data over
time. We validate its performance on linear and nonlinear models. In addition,
mutual information in frequency estimated as a part of our solution can also be
used to infer cross-frequency coupling in the data.
",1,0,0,1,0,0
12069,A Practical Randomized CP Tensor Decomposition,"  The CANDECOMP/PARAFAC (CP) decomposition is a leading method for the analysis
of multiway data. The standard alternating least squares algorithm for the CP
decomposition (CP-ALS) involves a series of highly overdetermined linear least
squares problems. We extend randomized least squares methods to tensors and
show the workload of CP-ALS can be drastically reduced without a sacrifice in
quality. We introduce techniques for efficiently preprocessing, sampling, and
computing randomized least squares on a dense tensor of arbitrary order, as
well as an efficient sampling-based technique for checking the stopping
condition. We also show more generally that the Khatri-Rao product (used within
the CP-ALS iteration) produces conditions favorable for direct sampling. In
numerical results, we see improvements in speed, reductions in memory
requirements, and robustness with respect to initialization.
",1,0,0,0,0,0
7450,Superregular grammars do not provide additional explanatory power but allow for a compact analysis of animal song,"  A pervasive belief with regard to the differences between human language and
animal vocal sequences (song) is that they belong to different classes of
computational complexity, with animal song belonging to regular languages,
whereas human language is superregular. This argument, however, lacks empirical
evidence since superregular analyses of animal song are understudied. The goal
of this paper is to perform a superregular analysis of animal song, using data
from gibbons as a case study, and demonstrate that a superregular analysis can
be effectively used with non-human data. A key finding is that a superregular
analysis does not increase explanatory power but rather provides for compact
analysis. For instance, fewer grammatical rules are necessary once
superregularity is allowed. This pattern is analogous to a previous
computational analysis of human language, and accordingly, the null hypothesis,
that human language and animal song are governed by the same type of
grammatical systems, cannot be rejected.
",0,0,0,0,1,0
11136,Trace your sources in large-scale data: one ring to find them all,"  An important preprocessing step in most data analysis pipelines aims to
extract a small set of sources that explain most of the data. Currently used
algorithms for blind source separation (BSS), however, often fail to extract
the desired sources and need extensive cross-validation. In contrast, their
rarely used probabilistic counterparts can get away with little
cross-validation and are more accurate and reliable but no simple and scalable
implementations are available. Here we present a novel probabilistic BSS
framework (DECOMPOSE) that can be flexibly adjusted to the data, is extensible
and easy to use, adapts to individual sources and handles large-scale data
through algorithmic efficiency. DECOMPOSE encompasses and generalises many
traditional BSS algorithms such as PCA, ICA and NMF and we demonstrate
substantial improvements in accuracy and robustness on artificial and real
data.
",0,0,0,1,0,0
2899,Fourier dimension and spectral gaps for hyperbolic surfaces,"  We obtain an essential spectral gap for a convex co-compact hyperbolic
surface $M=\Gamma\backslash\mathbb H^2$ which depends only on the dimension
$\delta$ of the limit set. More precisely, we show that when $\delta>0$ there
exists $\varepsilon_0=\varepsilon_0(\delta)>0$ such that the Selberg zeta
function has only finitely many zeroes $s$ with $\Re s>\delta-\varepsilon_0$.
The proof uses the fractal uncertainty principle approach developed by
Dyatlov-Zahl [arXiv:1504.06589]. The key new component is a Fourier decay bound
for the Patterson-Sullivan measure, which may be of independent interest. This
bound uses the fact that transformations in the group $\Gamma$ are nonlinear,
together with estimates on exponential sums due to Bourgain which follow from
the discretized sum-product theorem in $\mathbb R$.
",0,0,1,0,0,0
14941,Recovering sparse graphs,"  We construct a fixed parameter algorithm parameterized by d and k that takes
as an input a graph G' obtained from a d-degenerate graph G by complementing on
at most k arbitrary subsets of the vertex set of G and outputs a graph H such
that G and H agree on all but f(d,k) vertices.
Our work is motivated by the first order model checking in graph classes that
are first order interpretable in classes of sparse graphs. We derive as a
corollary that if G_0 is a graph class with bounded expansion, then the first
order model checking is fixed parameter tractable in the class of all graphs
that can obtained from a graph G from G_0 by complementing on at most k
arbitrary subsets of the vertex set of G; this implies an earlier result that
the first order model checking is fixed parameter tractable in graph classes
interpretable in classes of graphs with bounded maximum degree.
",1,0,0,0,0,0
8966,Flux-flow and vortex-glass phase in iron pnictide BaFe$_{2-x}$Ni$_x$As$_2$ single crystals with $T_c$ $\sim$ 20 K,"  We analysed the flux-flow region of isofield magneto resistivity data
obtained on three crystals of BaFe$_{2-x}$Ni$_x$As$_2$ with $T_c$$\sim$20 K for
three different geometries relative to the angle formed between the applied
magnetic field and the c-axis of the crystals. The field dependent activation
energy, $U_0$, was obtained from the TAFF and modified vortex-glass models,
which were compared with the values of $U_0$ obtained from flux-creep available
in the literature. We observed that the $U_0$ obtained from the TAFF model show
deviations among the different crystals, while the correspondent glass lines
obtained from the vortex glass model are virtually coincident. It is shown that
the data is well explained by the modified vortex glass model, allowing to
extract values of $T_g$, the glass transition temperature, and $T^*$, a
temperature which scales with the mean field critical temperature $T_c(H)$. The
resulting glass lines obey the anisotropic Ginzburg-Landau theory and are well
fitted by a theory developed in the literature by considering the effect of
disorder.
",0,1,0,0,0,0
7435,Improved $A_1-A_\infty$ and related estimates for commutators of rough singular integrals,"  An $A_1-A_\infty$ estimate improving a previous result in arXiv:1607.06432 is
obtained. Also new a result in terms of the ${A_\infty}$ constant and the one
supremum $A_q-A_\infty^{\exp}$ constant, is proved, providing a counterpart for
the result obained in arXiv:1705.08364. Both of the preceding results rely upon
a sparse domination in terms of bilinear forms for $[b,T_\Omega]$ with
$\Omega\in L^\infty(\mathbb{S}^{n-1})$ and $b\in BMO$ which is established
relying upon techniques from arXiv:1705.07397.
",0,0,1,0,0,0
1748,Algebraic operads up to homotopy,"  This paper deals with the homotopy theory of differential graded operads. We
endow the Koszul dual category of curved conilpotent cooperads, where the
notion of quasi-isomorphism barely makes sense, with a model category structure
Quillen equivalent to that of operads. This allows us to describe the homotopy
properties of differential graded operads in a simpler and richer way, using
obstruction methods.
",0,0,1,0,0,0
13943,The reactive-telegraph equation and a related kinetic model,"  We study the long-range, long-time behavior of the reactive-telegraph
equation and a related reactive-kinetic model. The two problems are equivalent
in one spatial dimension. We point out that the reactive-telegraph equation,
meant to model a population density, does not preserve positivity in higher
dimensions. In view of this, in dimensions larger than one, we consider a
reactive-kinetic model and investigate the long-range, long-time limit of the
solutions. We provide a general characterization of the speed of propagation
and we compute it explicitly in one and two dimensions. We show that a phase
transition between parabolic and hyperbolic behavior takes place only in one
dimension. Finally, we investigate the hydrodynamic limit of the limiting
problem.
",0,0,1,0,0,0
14477,Theoretical studies of superconductivity in doped BaCoSO,"  We investigate superconductivity that may exist in the doped BaCoSO, a
multi-orbital Mott insulator with a strong antiferromagnetic ground state. The
superconductivity is studied in both t-J type and Hubbard type multi-orbital
models by mean field approach and random phase approximation (RPA) analysis.
Even if there is no C4 rotational symmetry, it is found that the system still
carries a d-wave like pairing symmetry state with gapless nodes and sign
changed superconducting order parameters on Fermi surfaces. The results are
largely doping insensitive. In this superconducting state, the three t2g
orbitals have very different superconducting form factors in momentum space. In
particular, the intra-orbital pairing of the dx2-y2 orbital has a s-wave like
pairing form factor. The two methods also predict very different pairing
strength on different parts of Fermi surfaces.These results suggest that BaCoSO
and related materials can be a new ground to test and establish fundamental
principles for unconventional high temperature superconductivity.
",0,1,0,0,0,0
6799,A binary main belt comet,"  The asteroids are primitive solar system bodies which evolve both
collisionally and through disruptions due to rapid rotation [1]. These
processes can lead to the formation of binary asteroids [2-4] and to the
release of dust [5], both directly and, in some cases, through uncovering
frozen volatiles. In a sub-set of the asteroids called main-belt comets (MBCs),
the sublimation of excavated volatiles causes transient comet-like activity
[6-8]. Torques exerted by sublimation measurably influence the spin rates of
active comets [9] and might lead to the splitting of bilobate comet nuclei
[10]. The kilometer-sized main-belt asteroid 288P (300163) showed activity for
several months around its perihelion 2011 [11], suspected to be sustained by
the sublimation of water ice [12] and supported by rapid rotation [13], while
at least one component rotates slowly with a period of 16 hours [14]. 288P is
part of a young family of at least 11 asteroids that formed from a ~10km
diameter precursor during a shattering collision 7.5 million years ago [15].
Here we report that 288P is a binary main-belt comet. It is different from the
known asteroid binaries for its combination of wide separation, near-equal
component size, high eccentricity, and comet-like activity. The observations
also provide strong support for sublimation as the driver of activity in 288P
and show that sublimation torques may play a significant role in binary orbit
evolution.
",0,1,0,0,0,0
6688,Deep Learning: A Bayesian Perspective,"  Deep learning is a form of machine learning for nonlinear high dimensional
pattern matching and prediction. By taking a Bayesian probabilistic
perspective, we provide a number of insights into more efficient algorithms for
optimisation and hyper-parameter tuning. Traditional high-dimensional data
reduction techniques, such as principal component analysis (PCA), partial least
squares (PLS), reduced rank regression (RRR), projection pursuit regression
(PPR) are all shown to be shallow learners. Their deep learning counterparts
exploit multiple deep layers of data reduction which provide predictive
performance gains. Stochastic gradient descent (SGD) training optimisation and
Dropout (DO) regularization provide estimation and variable selection. Bayesian
regularization is central to finding weights and connections in networks to
optimize the predictive bias-variance trade-off. To illustrate our methodology,
we provide an analysis of international bookings on Airbnb. Finally, we
conclude with directions for future research.
",1,0,0,1,0,0
5228,A sparse linear algebra algorithm for fast computation of prediction variances with Gaussian Markov random fields,"  Gaussian Markov random fields are used in a large number of disciplines in
machine vision and spatial statistics. The models take advantage of sparsity in
matrices introduced through the Markov assumptions, and all operations in
inference and prediction use sparse linear algebra operations that scale well
with dimensionality. Yet, for very high-dimensional models, exact computation
of predictive variances of linear combinations of variables is generally
computationally prohibitive, and approximate methods (generally interpolation
or conditional simulation) are typically used instead. A set of conditions are
established under which the variances of linear combinations of random
variables can be computed exactly using the Takahashi recursions. The ensuing
computational simplification has wide applicability and may be used to enhance
several software packages where model fitting is seated in a maximum-likelihood
framework. The resulting algorithm is ideal for use in a variety of spatial
statistical applications, including \emph{LatticeKrig} modelling, statistical
downscaling, and fixed rank kriging. It can compute hundreds of thousands exact
predictive variances of linear combinations on a standard desktop with ease,
even when large spatial GMRF models are used.
",0,0,0,1,0,0
19104,"automan: a simple, Python-based, automation framework for numerical computing","  We present an easy-to-use, Python-based framework that allows a researcher to
automate their computational simulations. In particular the framework
facilitates assembling several long-running computations and producing various
plots from the data produced by these computations. The framework makes it
possible to reproduce every figure made for a publication with a single
command. It also allows one to distribute the computations across a network of
computers. The framework has been used to write research papers in numerical
computing. This paper discusses the design of the framework, and the benefits
of using it. The ideas presented are general and should help researchers
organize their computations for better reproducibility.
",1,0,0,0,0,0
17119,Errors and secret data in the Italian research assessment exercise. A comment to a reply,"  Italy adopted a performance-based system for funding universities that is
centered on the results of a national research assessment exercise, realized by
a governmental agency (ANVUR). ANVUR evaluated papers by using 'a dual system
of evaluation', that is by informed peer review or by bibliometrics. In view of
validating that system, ANVUR performed an experiment for estimating the
agreement between informed review and bibliometrics. Ancaiani et al. (2015)
presents the main results of the experiment. Baccini and De Nicolao (2017)
documented in a letter, among other critical issues, that the statistical
analysis was not realized on a random sample of articles. A reply to the letter
has been published by Research Evaluation (Benedetto et al. 2017). This note
highlights that in the reply there are (1) errors in data, (2) problems with
'representativeness' of the sample, (3) unverifiable claims about weights used
for calculating kappas, (4) undisclosed averaging procedures; (5) a statement
about 'same protocol in all areas' contradicted by official reports. Last but
not least: the data used by the authors continue to be undisclosed. A general
warning concludes: many recently published papers use data originating from
Italian research assessment exercise. These data are not accessible to the
scientific community and consequently these papers are not reproducible. They
can be hardly considered as containing sound evidence at least until authors or
ANVUR disclose the data necessary for replication.
",1,0,0,0,0,0
4344,Doping-induced spin-orbit splitting in Bi-doped ZnO nanowires,"  Our predictions, based on density-functional calculations, reveal that
surface doping of ZnO nanowires with Bi leads to a linear-in-$k$ splitting of
the conduction-band states, through spin-orbit interaction, due to the lowering
of the symmetry in the presence of the dopant. This finding implies that spin
polarization of the conduction electrons in Bi-doped ZnO nanowires could be
controlled with applied electric (as opposed to magnetic) fields, making them
candidate materials for spin-orbitronic applications. Our findings also show
that the degree of spin splitting could be tuned by adjusting the dopant
concentration. Defect calculations and ab initio molecular dynamics simulations
indicate that stable doping configurations exhibiting the foregoing
linear-in-$k$ splitting could be realized under reasonable thermodynamic
conditions.
",0,1,0,0,0,0
10572,3D Object Reconstruction from Hand-Object Interactions,"  Recent advances have enabled 3d object reconstruction approaches using a
single off-the-shelf RGB-D camera. Although these approaches are successful for
a wide range of object classes, they rely on stable and distinctive geometric
or texture features. Many objects like mechanical parts, toys, household or
decorative articles, however, are textureless and characterized by minimalistic
shapes that are simple and symmetric. Existing in-hand scanning systems and 3d
reconstruction techniques fail for such symmetric objects in the absence of
highly distinctive features. In this work, we show that extracting 3d hand
motion for in-hand scanning effectively facilitates the reconstruction of even
featureless and highly symmetric objects and we present an approach that fuses
the rich additional information of hands into a 3d reconstruction pipeline,
significantly contributing to the state-of-the-art of in-hand scanning.
",1,0,0,0,0,0
18205,Polynomially and Infinitesimally Injective Modules,"  The injective polynomial modules for a general linear group $G$ of degree $n$
are labelled by the partitions with at most $n$ parts. Working over an
algebraically closed field of characteristic $p$, we consider the question of
which partitions correspond to polynomially injective modules that are also
injective as modules for the restricted enveloping algebra of the Lie algebra
of $G$. The question is related to the ""index of divisibility"" of a polynomial
module in general, and an explicit answer is given for $n=2$.
",0,0,1,0,0,0
12931,"Mermin-Wagner physics, (H,T) phase diagram, and candidate quantum spin-liquid phase in the spin-1/2 triangular-lattice antiferromagnet Ba8CoNb6O24","  Ba$_8$CoNb$_6$O$_{24}$ presents a system whose Co$^{2+}$ ions have an
effective spin 1/2 and construct a regular triangular-lattice antiferromagnet
(TLAFM) with a very large interlayer spacing, ensuring purely two-dimensional
character. We exploit this ideal realization to perform a detailed experimental
analysis of the $S = 1/2$ TLAFM, which is one of the keystone models in
frustrated quantum magnetism. We find strong low-energy spin fluctuations and
no magnetic ordering, but a diverging correlation length down to 0.1 K,
indicating a Mermin-Wagner trend towards zero-temperature order. Below 0.1 K,
however, our low-field measurements show an nexpected magnetically disordered
state, which is a candidate quantum spin liquid. We establish the $(H,T)$ phase
diagram, mapping in detail the quantum fluctuation corrections to the available
theoretical analysis. These include a strong upshift in field of the maximum
ordering temperature, qualitative changes to both low- and high-field phase
boundaries, and an ordered regime apparently dominated by the collinear
""up-up-down"" state. Ba$_8$CoNb$_6$O$_{24}$ therefore offers fresh input for the
development of theoretical approaches to the field-induced quantum phase
transitions of the $S = 1/2$ Heisenberg TLAFM.
",0,1,0,0,0,0
2699,On the Prospects for Detecting a Net Photon Circular Polarization Produced by Decaying Dark Matter,"  If dark matter interactions with Standard Model particles are $CP$-violating,
then dark matter annihilation/decay can produce photons with a net circular
polarization. We consider the prospects for experimentally detecting evidence
for such a circular polarization. We identify optimal models for dark matter
interactions with the Standard Model, from the point of view of detectability
of the net polarization, for the case of either symmetric or asymmetric dark
matter. We find that, for symmetric dark matter, evidence for net polarization
could be found by a search of the Galactic Center by an instrument sensitive to
circular polarization with an efficiency-weighted exposure of at least
$50000~\text{cm}^2~\text{yr}$, provided the systematic detector uncertainties
are constrained at the $1\%$ level. Better sensitivity can be obtained in the
case of asymmetric dark matter. We discuss the prospects for achieving the
needed level of performance using possible detector technologies.
",0,1,0,0,0,0
5926,A numerical study of the F-model with domain-wall boundaries,"  We perform a numerical study of the F-model with domain-wall boundary
conditions. Various exact results are known for this particular case of the
six-vertex model, including closed expressions for the partition function for
any system size as well as its asymptotics and leading finite-size corrections.
To complement this picture we use a full lattice multi-cluster algorithm to
study equilibrium properties of this model for systems of moderate size, up to
L=512. We compare the energy to its exactly known large-L asymptotics. We
investigate the model's infinite-order phase transition by means of finite-size
scaling for an observable derived from the staggered polarization in order to
test the method put forward in our recent joint work with Duine and Barkema. In
addition we analyse local properties of the model. Our data are perfectly
consistent with analytical expressions for the arctic curves. We investigate
the structure inside the temperate region of the lattice, confirming the
oscillations in vertex densities that were first observed by Sylju{\aa}sen and
Zvonarev, and recently studied by Lyberg et al. We point out
'(anti)ferroelectric' oscillations close to the corresponding frozen regions as
well as 'higher-order' oscillations forming an intricate pattern with
saddle-point-like features.
",0,1,0,0,0,0
17926,A Recursive Bayesian Approach To Describe Retinal Vasculature Geometry,"  Demographic studies suggest that changes in the retinal vasculature geometry,
especially in vessel width, are associated with the incidence or progression of
eye-related or systemic diseases. To date, the main information source for
width estimation from fundus images has been the intensity profile between
vessel edges. However, there are many factors affecting the intensity profile:
pathologies, the central light reflex and local illumination levels, to name a
few. In this study, we introduce three information sources for width
estimation. These are the probability profiles of vessel interior, centreline
and edge locations generated by a deep network. The probability profiles
provide direct access to vessel geometry and are used in the likelihood
calculation for a Bayesian method, particle filtering. We also introduce a
geometric model which can handle non-ideal conditions of the probability
profiles. Our experiments conducted on the REVIEW dataset yielded consistent
estimates of vessel width, even in cases when one of the vessel edges is
difficult to identify. Moreover, our results suggest that the method is better
than human observers at locating edges of low contrast vessels.
",1,0,0,0,0,0
13202,Impact of Continuous Integration on Code Reviews,"  Peer code review and continuous integration often interleave with each other
in the modern software quality management. Although several studies investigate
how non-technical factors (e.g., reviewer workload), developer participation
and even patch size affect the code review process, the impact of continuous
integration on code reviews is not yet properly understood. In this paper, we
report an exploratory study using 578K automated build entries where we
investigate the impact of automated builds on the code reviews. Our
investigation suggests that successfully passed builds are more likely to
encourage new code review participation in a pull request. Frequently built
projects are found to be maintaining a steady level of reviewing activities
over the years, which was quite missing from the rarely built projects.
Experiments with 26,516 automated build entries reported that our proposed
model can identify 64% of the builds that triggered new code reviews later.
",1,0,0,0,0,0
10606,Functional limit laws for the increments of Lévy processes,"  We present a functional form of the Erdös-Renyi law of large numbers for
Levy processes.
",0,0,1,1,0,0
9793,Latent Association Mining in Binary Data,"  We consider the problem of identifying groups of mutually associated
variables in moderate or high dimensional data. In many cases, ordinary Pearson
correlation provides useful information concerning the linear relationship
between variables. However, for binary data, ordinary correlation may lose
power and may lack interpretability. In this paper, we develop and investigate
a new method called Latent Association Mining in Binary Data (LAMB). The LAMB
method is built on the assumption that the binary observations represent a
random thresholding of a latent continuous variable that may have a complex
correlation structure. We consider a new measure of association, latent
correlation, that is designed to assess association in the underlying
continuous variable, without bias due to the mediating effects of the
thresholding procedure. The full LAMB procedure makes use of iterative
hypothesis testing to identify groups of latently correlated variables. LAMB is
shown to improve power over existing methods in simulated settings, to be
computationally efficient for large datasets, and to uncover new meaningful
results from common real data types.
",0,0,0,1,0,0
5533,Finding Influential Training Samples for Gradient Boosted Decision Trees,"  We address the problem of finding influential training samples for a
particular case of tree ensemble-based models, e.g., Random Forest (RF) or
Gradient Boosted Decision Trees (GBDT). A natural way of formalizing this
problem is studying how the model's predictions change upon leave-one-out
retraining, leaving out each individual training sample. Recent work has shown
that, for parametric models, this analysis can be conducted in a
computationally efficient way. We propose several ways of extending this
framework to non-parametric GBDT ensembles under the assumption that tree
structures remain fixed. Furthermore, we introduce a general scheme of
obtaining further approximations to our method that balance the trade-off
between performance and computational complexity. We evaluate our approaches on
various experimental setups and use-case scenarios and demonstrate both the
quality of our approach to finding influential training samples in comparison
to the baselines and its computational efficiency.
",0,0,0,1,0,0
8125,All the people around me: face discovery in egocentric photo-streams,"  Given an unconstrained stream of images captured by a wearable photo-camera
(2fpm), we propose an unsupervised bottom-up approach for automatic clustering
appearing faces into the individual identities present in these data. The
problem is challenging since images are acquired under real world conditions;
hence the visible appearance of the people in the images undergoes intensive
variations. Our proposed pipeline consists of first arranging the photo-stream
into events, later, localizing the appearance of multiple people in them, and
finally, grouping various appearances of the same person across different
events. Experimental results performed on a dataset acquired by wearing a
photo-camera during one month, demonstrate the effectiveness of the proposed
approach for the considered purpose.
",1,0,0,0,0,0
9350,Legendre curves and singularities of a ruled surface according to rotation minimizing frame,"  In this paper, Legendre curves on unit tangent bundle are given using
rotation minimizing (RM) vector fields. Ruled surfaces corresponding to these
curves are represented. Singularities of these ruled surfaces are also analyzed
and classifed.
",0,0,1,0,0,0
17498,Unsupervised Object Discovery and Segmentation of RGBD-images,"  In this paper we introduce a system for unsupervised object discovery and
segmentation of RGBD-images. The system models the sensor noise directly from
data, allowing accurate segmentation without sensor specific hand tuning of
measurement noise models making use of the recently introduced Statistical
Inlier Estimation (SIE) method. Through a fully probabilistic formulation, the
system is able to apply probabilistic inference, enabling reliable segmentation
in previously challenging scenarios. In addition, we introduce new methods for
filtering out false positives, significantly improving the signal to noise
ratio. We show that the system significantly outperform state-of-the-art in on
a challenging real-world dataset.
",1,0,0,0,0,0
17852,Water flow in Carbon and Silicon Carbide nanotubes,"  In this work the conduction of ion-water solution through two discrete
bundles of armchair carbon and silicon carbide nanotubes, as useful membranes
for water desalination, is studied. In order that studies on different types of
nanotubes be comparable, the chiral vectors of C and Si-C nanotubes are
selected as (7,7) and (5,5), respectively, so that a similar volume of fluid is
investigated flowing through two similar dimension membranes. Different
hydrostatic pressures are applied and the flow rates of water and ions are
calculated through molecular dynamics simulations. Consequently, according to
conductance of water per each nanotube, per nanosecond, it is perceived that at
lower pressures (below 150 MPa) the Si-C nanotubes seem to be more applicable,
while higher hydrostatic pressures make carbon nanotube membranes more suitable
for water desalination.
",0,1,0,0,0,0
12957,On Sidorenko's conjecture for determinants and Gaussian Markov random fields,"  We study a class of determinant inequalities that are closely related to
Sidorenko's famous conjecture (Also conjectured by Erd\H os and Simonovits in a
different form). Our results can also be interpreted as entropy inequalities
for Gaussian Markov random fields (GMRF). We call a GMRF on a finite graph $G$
homogeneous if the marginal distributions on the edges are all identical. We
show that if $G$ satisfies Sidorenko's conjecture then the differential entropy
of any homogeneous GMRF on $G$ is at least $|E(G)|$ times the edge entropy plus
$|V(G)|-2|E(G)|$ times the point entropy. We also prove this inequality in a
large class of graphs for which Sidorenko's conjecture is not verified
including the so-called Möbius ladder: $K_{5,5}\setminus C_{10}$. The
connection between Sidorenko's conjecture and GMRF's is established via a large
deviation principle on high dimensional spheres combined with graph limit
theory.
",0,0,1,0,0,0
1555,Using Inertial Sensors for Position and Orientation Estimation,"  In recent years, MEMS inertial sensors (3D accelerometers and 3D gyroscopes)
have become widely available due to their small size and low cost. Inertial
sensor measurements are obtained at high sampling rates and can be integrated
to obtain position and orientation information. These estimates are accurate on
a short time scale, but suffer from integration drift over longer time scales.
To overcome this issue, inertial sensors are typically combined with additional
sensors and models. In this tutorial we focus on the signal processing aspects
of position and orientation estimation using inertial sensors. We discuss
different modeling choices and a selected number of important algorithms. The
algorithms include optimization-based smoothing and filtering as well as
computationally cheaper extended Kalman filter and complementary filter
implementations. The quality of their estimates is illustrated using both
experimental and simulated data.
",1,0,0,0,0,0
19336,Rewriting in Free Hypergraph Categories,"  We study rewriting for equational theories in the context of symmetric
monoidal categories where there is a separable Frobenius monoid on each object.
These categories, also called hypergraph categories, are increasingly relevant:
Frobenius structures recently appeared in cross-disciplinary applications,
including the study of quantum processes, dynamical systems and natural
language processing. In this work we give a combinatorial characterisation of
arrows of a free hypergraph category as cospans of labelled hypergraphs and
establish a precise correspondence between rewriting modulo Frobenius structure
on the one hand and double-pushout rewriting of hypergraphs on the other. This
interpretation allows to use results on hypergraphs to ensure decidability of
confluence for rewriting in a free hypergraph category. Our results generalise
previous approaches where only categories generated by a single object (props)
were considered.
",1,0,0,0,0,0
3227,Saturating sets in projective planes and hypergraph covers,"  Let $\Pi_q$ be an arbitrary finite projective plane of order $q$. A subset
$S$ of its points is called saturating if any point outside $S$ is collinear
with a pair of points from $S$. Applying probabilistic tools we improve the
upper bound on the smallest possible size of the saturating set to
$\lceil\sqrt{3q\ln{q}}\rceil+ \lceil(\sqrt{q}+1)/2\rceil$. The same result is
presented using an algorithmic approach as well, which points out the
connection with the transversal number of uniform multiple intersecting
hypergraphs.
",0,0,1,0,0,0
11899,Constructing grids for molecular quantum dynamics using an autoencoder,"  A challenge for molecular quantum dynamics (QD) calculations is the curse of
dimensionality with respect to the nuclear degrees of freedom. A common
approach that works especially well for fast reactive processes is to reduce
the dimensionality of the system to a few most relevant coordinates.
Identifying these can become a very difficult task, since they often are highly
unintuitive. We present a machine learning approach that utilizes an
autoencoder that is trained to find a low-dimensional representation of a set
of molecular configurations. These configurations are generated by trajectory
calculations performed on the reactive molecular systems of interest. The
resulting low-dimensional representation can be used to generate a potential
energy surface grid in the desired subspace. Using the G-matrix formalism to
calculate the kinetic energy operator, QD calculations can be carried out on
this grid. In addition to step-by-step instructions for the grid construction,
we present the application to a test system.
",0,1,0,0,0,0
11365,Kinetic Theory for Finance Brownian Motion from Microscopic Dynamics,"  Recent technological development has enabled researchers to study social
phenomena scientifically in detail and financial markets has particularly
attracted physicists since the Brownian motion has played the key role as in
physics. In our previous report (arXiv:1703.06739; to appear in Phys. Rev.
Lett.), we have presented a microscopic model of trend-following high-frequency
traders (HFTs) and its theoretical relation to the dynamics of financial
Brownian motion, directly supported by a data analysis of tracking trajectories
of individual HFTs in a financial market. Here we show the mathematical
foundation for the HFT model paralleling to the traditional kinetic theory in
statistical physics. We first derive the time-evolution equation for the
phase-space distribution for the HFT model exactly, which corresponds to the
Liouville equation in conventional analytical mechanics. By a systematic
reduction of the Liouville equation for the HFT model, the
Bogoliubov-Born-Green-Kirkwood-Yvon hierarchal equations are derived for
financial Brownian motion. We then derive the Boltzmann-like and Langevin-like
equations for the order-book and the price dynamics by making the assumption of
molecular chaos. The qualitative behavior of the model is asymptotically
studied by solving the Boltzmann-like and Langevin-like equations for the large
number of HFTs, which is numerically validated through the Monte-Carlo
simulation. Our kinetic description highlights the parallel mathematical
structure between the financial Brownian motion and the physical Brownian
motion.
",0,0,0,0,0,1
8899,Computable Isomorphisms for Certain Classes of Infinite Graphs,"  We investigate (2,1):1 structures, which consist of a countable set $A$
together with a function $f: A \to A$ such that for every element $x$ in $A$,
$f$ maps either exactly one element or exactly two elements of $A$ to $x$.
These structures extend the notions of injection structures, 2:1 structures,
and (2,0):1 structures studied by Cenzer, Harizanov, and Remmel, all of which
can be thought of as infinite directed graphs. We look at various
computability-theoretic properties of (2,1):1 structures, most notably that of
computable categoricity. We say that a structure $\mathcal{A}$ is computably
categorical if there exists a computable isomorphism between any two computable
copies of $\mathcal{A}$. We give a sufficient condition under which a (2,1):1
structure is computably categorical, and present some examples of (2,1):1
structures with different computability-theoretic properties.
",0,0,1,0,0,0
11244,Beyond the Erdős Matching Conjecture,"  A family $\mathcal F\subset {[n]\choose k}$ is $U(s,q)$ of for any
$F_1,\ldots, F_s\in \mathcal F$ we have $|F_1\cup\ldots\cup F_s|\le q$. This
notion generalizes the property of a family to be $t$-intersecting and to have
matching number smaller than $s$.
In this paper, we find the maximum $|\mathcal F|$ for $\mathcal F$ that are
$U(s,q)$, provided $n>C(s,q)k$ with moderate $C(s,q)$. In particular, we
generalize the result of the first author on the Erdős Matching Conjecture
and prove a generalization of the Erdős-Ko-Rado theorem, which states that
for $n> s^2k$ the largest family $\mathcal F\subset {[n]\choose k}$ with
property $U(s,s(k-1)+1)$ is the star and is in particular intersecting.
(Conversely, it is easy to see that any intersecting family in ${[n]\choose k}$
is $U(s,s(k-1)+1)$.)
We investigate the case $k=3$ more thoroughly, showing that, unlike in the
case of the Erdős Matching Conjecture, in general there may be $3$ extremal
families.
",1,0,0,0,0,0
9314,Equipping weak equivalences with algebraic structure,"  We investigate the extent to which the weak equivalences in a model category
can be equipped with algebraic structure. We prove, for instance, that there
exists a monad T such that a morphism of topological spaces admits T-algebra
structure if and only it is a weak homotopy equivalence. Likewise for
quasi-isomorphisms and many other examples. The basic trick is to consider
injectivity in arrow categories. Using algebraic injectivity and cone
injectivity we obtain general results about the extent to which the weak
equivalences in a combinatorial model category can be equipped with algebraic
structure.
",0,0,1,0,0,0
9986,Privacy-Preserving Adversarial Networks,"  We propose a data-driven framework for optimizing privacy-preserving data
release mechanisms toward the information-theoretically optimal tradeoff
between minimizing distortion of useful data and concealing sensitive
information. Our approach employs adversarially-trained neural networks to
implement randomized mechanisms and to perform a variational approximation of
mutual information privacy. We empirically validate our Privacy-Preserving
Adversarial Networks (PPAN) framework with experiments conducted on discrete
and continuous synthetic data, as well as the MNIST handwritten digits dataset.
With the synthetic data, we find that our model-agnostic PPAN approach achieves
tradeoff points very close to the optimal tradeoffs that are
analytically-derived from model knowledge. In experiments with the MNIST data,
we visually demonstrate a learned tradeoff between minimizing the pixel-level
distortion versus concealing the written digit.
",1,0,0,1,0,0
11552,Trading Strategies Generated by Path-dependent Functionals of Market Weights,"  Almost twenty years ago, E.R. Fernholz introduced portfolio generating
functions which can be used to construct a variety of portfolios, solely in the
terms of the individual companies' market weights. I. Karatzas and J. Ruf
recently developed another methodology for the functional construction of
portfolios, which leads to very simple conditions for strong relative arbitrage
with respect to the market. In this paper, both of these notions of functional
portfolio generation are generalized in a pathwise, probability-free setting;
portfolio generating functions are substituted by path-dependent functionals,
which involve the current market weights, as well as additional
bounded-variation functions of past and present market weights. This
generalization leads to a wider class of functionally-generated portfolios than
was heretofore possible, and yields improved conditions for outperforming the
market portfolio over suitable time-horizons.
",0,0,0,0,0,1
17436,Dynamic Word Embeddings,"  We present a probabilistic language model for time-stamped text data which
tracks the semantic evolution of individual words over time. The model
represents words and contexts by latent trajectories in an embedding space. At
each moment in time, the embedding vectors are inferred from a probabilistic
version of word2vec [Mikolov et al., 2013]. These embedding vectors are
connected in time through a latent diffusion process. We describe two scalable
variational inference algorithms--skip-gram smoothing and skip-gram
filtering--that allow us to train the model jointly over all times; thus
learning on all data while simultaneously allowing word and context vectors to
drift. Experimental results on three different corpora demonstrate that our
dynamic model infers word embedding trajectories that are more interpretable
and lead to higher predictive likelihoods than competing methods that are based
on static models trained separately on time slices.
",0,0,0,1,0,0
20214,Weakly Supervised Audio Source Separation via Spectrum Energy Preserved Wasserstein Learning,"  Separating audio mixtures into individual instrument tracks has been a long
standing challenging task. We introduce a novel weakly supervised audio source
separation approach based on deep adversarial learning. Specifically, our loss
function adopts the Wasserstein distance which directly measures the
distribution distance between the separated sources and the real sources for
each individual source. Moreover, a global regularization term is added to
fulfill the spectrum energy preservation property regardless separation. Unlike
state-of-the-art weakly supervised models which often involve deliberately
devised constraints or careful model selection, our approach need little prior
model specification on the data, and can be straightforwardly learned in an
end-to-end fashion. We show that the proposed method performs competitively on
public benchmark against state-of-the-art weakly supervised methods.
",1,0,0,0,0,0
6591,Statistically Optimal and Computationally Efficient Low Rank Tensor Completion from Noisy Entries,"  In this article, we develop methods for estimating a low rank tensor from
noisy observations on a subset of its entries to achieve both statistical and
computational efficiencies. There have been a lot of recent interests in this
problem of noisy tensor completion. Much of the attention has been focused on
the fundamental computational challenges often associated with problems
involving higher order tensors, yet very little is known about their
statistical performance. To fill in this void, in this article, we characterize
the fundamental statistical limits of noisy tensor completion by establishing
minimax optimal rates of convergence for estimating a $k$th order low rank
tensor under the general $\ell_p$ ($1\le p\le 2$) norm which suggest
significant room for improvement over the existing approaches. Furthermore, we
propose a polynomial-time computable estimating procedure based upon power
iteration and a second-order spectral initialization that achieves the optimal
rates of convergence. Our method is fairly easy to implement and numerical
experiments are presented to further demonstrate the practical merits of our
estimator.
",0,0,1,1,0,0
4519,Playing Music in Just Intonation - A Dynamically Adapting Tuning Scheme,"  We investigate a dynamically adapting tuning scheme for microtonal tuning of
musical instruments, allowing the performer to play music in just intonation in
any key. Unlike other methods, which are based on a procedural analysis of the
chordal structure, the tuning scheme continually solves a system of linear
equations without making explicit decisions. In complex situations, where not
all intervals of a chord can be tuned according to just frequency ratios, the
method automatically yields a tempered compromise. We outline the
implementation of the algorithm in an open-source software project that we have
provided in order to demonstrate the feasibility of the tuning method.
",0,1,0,0,0,0
16599,Doubled Khovanov Homology,"  We define a homology theory of virtual links built out of the direct sum of
the standard Khovanov complex with itself, motivating the name doubled Khovanov
homology. We demonstrate that it can be used to show that some virtual links
are non-classical, and that it yields a condition on a virtual knot being the
connect sum of two unknots. Further, we show that doubled Khovanov homology
possesses a perturbation analogous to that defined by Lee in the classical case
and define a doubled Rasmussen invariant. This invariant is used to obtain
various cobordism obstructions; in particular it is an obstruction to
sliceness. Finally, we show that the doubled Rasmussen invariant contains the
odd writhe of a virtual knot, and use this to show that knots with non-zero odd
writhe are not slice.
",0,0,1,0,0,0
11649,Minimal surfaces near short geodesics in hyperbolic $3$-manifolds,"  If $M$ is a finite volume complete hyperbolic $3$-manifold, the quantity
$\mathcal A_1(M)$ is defined as the infimum of the areas of closed minimal
surfaces in $M$. In this paper we study the continuity property of the
functional $\mathcal A_1$ with respect to the geometric convergence of
hyperbolic manifolds. We prove that it is lower semi-continuous and even
continuous if $\mathcal A_1(M)$ is realized by a minimal surface satisfying
some hypotheses. Understanding the interaction between minimal surfaces and
short geodesics in $M$ is the main theme of this paper
",0,0,1,0,0,0
18931,Towards an algebraic natural proofs barrier via polynomial identity testing,"  We observe that a certain kind of algebraic proof - which covers essentially
all known algebraic circuit lower bounds to date - cannot be used to prove
lower bounds against VP if and only if what we call succinct hitting sets exist
for VP. This is analogous to the Razborov-Rudich natural proofs barrier in
Boolean circuit complexity, in that we rule out a large class of lower bound
techniques under a derandomization assumption. We also discuss connections
between this algebraic natural proofs barrier, geometric complexity theory, and
(algebraic) proof complexity.
",1,0,1,0,0,0
20565,Nonlocal heat equations in the Heisenberg group,"  We study the following nonlocal diffusion equation in the Heisenberg group
$\mathbb{H}_n$, \[ u_t(z,s,t)=J\ast u(z,s,t)-u(z,s,t), \] where $\ast$ denote
convolution product and $J$ satisfies appropriated hypothesis. For the Cauchy
problem we obtain that the asymptotic behavior of the solutions is the same
form that the one for the heat equation in the Heisenberg group. To obtain this
result we use the spherical transform related to the pair
$(U(n),\mathbb{H}_n)$. Finally we prove that solutions of properly rescaled
nonlocal Dirichlet problem converge uniformly to the solution of the
corresponding Dirichlet problem for the classical heat equation in the
Heisenberg group.
",0,0,1,0,0,0
18028,A Generalization of Smillie's Theorem on Strongly Cooperative Tridiagonal Systems,"  Smillie (1984) proved an interesting result on the stability of nonlinear,
time-invariant, strongly cooperative, and tridiagonal dynamical systems. This
result has found many applications in models from various fields including
biology, ecology, and chemistry. Smith (1991) has extended Smillie's result and
proved entrainment in the case where the vector field is time-varying and
periodic. We use the theory of linear totally nonnegative differential systems
developed by Schwarz (1970) to give a generalization of these two results. This
is based on weakening the requirement for strong cooperativity to
cooperativity, and adding an additional observability-type condition.
",1,0,0,0,0,0
20356,A Matrix Factorization Approach for Learning Semidefinite-Representable Regularizers,"  Regularization techniques are widely employed in optimization-based
approaches for solving ill-posed inverse problems in data analysis and
scientific computing. These methods are based on augmenting the objective with
a penalty function, which is specified based on prior domain-specific expertise
to induce a desired structure in the solution. We consider the problem of
learning suitable regularization functions from data in settings in which
precise domain knowledge is not directly available. Previous work under the
title of `dictionary learning' or `sparse coding' may be viewed as learning a
regularization function that can be computed via linear programming. We
describe generalizations of these methods to learn regularizers that can be
computed and optimized via semidefinite programming. Our framework for learning
such semidefinite regularizers is based on obtaining structured factorizations
of data matrices, and our algorithmic approach for computing these
factorizations combines recent techniques for rank minimization problems along
with an operator analog of Sinkhorn scaling. Under suitable conditions on the
input data, our algorithm provides a locally linearly convergent method for
identifying the correct regularizer that promotes the type of structure
contained in the data. Our analysis is based on the stability properties of
Operator Sinkhorn scaling and their relation to geometric aspects of
determinantal varieties (in particular tangent spaces with respect to these
varieties). The regularizers obtained using our framework can be employed
effectively in semidefinite programming relaxations for solving inverse
problems.
",1,0,1,1,0,0
11549,BFGS convergence to nonsmooth minimizers of convex functions,"  The popular BFGS quasi-Newton minimization algorithm under reasonable
conditions converges globally on smooth convex functions. This result was
proved by Powell in 1976: we consider its implications for functions that are
not smooth. In particular, an analogous convergence result holds for functions,
like the Euclidean norm, that are nonsmooth at the minimizer.
",0,0,1,0,0,0
15693,Sparse Markov Decision Processes with Causal Sparse Tsallis Entropy Regularization for Reinforcement Learning,"  In this paper, a sparse Markov decision process (MDP) with novel causal
sparse Tsallis entropy regularization is proposed.The proposed policy
regularization induces a sparse and multi-modal optimal policy distribution of
a sparse MDP. The full mathematical analysis of the proposed sparse MDP is
provided.We first analyze the optimality condition of a sparse MDP. Then, we
propose a sparse value iteration method which solves a sparse MDP and then
prove the convergence and optimality of sparse value iteration using the Banach
fixed point theorem. The proposed sparse MDP is compared to soft MDPs which
utilize causal entropy regularization. We show that the performance error of a
sparse MDP has a constant bound, while the error of a soft MDP increases
logarithmically with respect to the number of actions, where this performance
error is caused by the introduced regularization term. In experiments, we apply
sparse MDPs to reinforcement learning problems. The proposed method outperforms
existing methods in terms of the convergence speed and performance.
",1,0,0,1,0,0
20016,Face R-CNN,"  Faster R-CNN is one of the most representative and successful methods for
object detection, and has been becoming increasingly popular in various
objection detection applications. In this report, we propose a robust deep face
detection approach based on Faster R-CNN. In our approach, we exploit several
new techniques including new multi-task loss function design, online hard
example mining, and multi-scale training strategy to improve Faster R-CNN in
multiple aspects. The proposed approach is well suited for face detection, so
we call it Face R-CNN. Extensive experiments are conducted on two most popular
and challenging face detection benchmarks, FDDB and WIDER FACE, to demonstrate
the superiority of the proposed approach over state-of-the-arts.
",1,0,0,0,0,0
16976,Completely Sidon sets in $C^*$-algebras (New title),"  A sequence in a $C^*$-algebra $A$ is called completely Sidon if its span in
$A$ is completely isomorphic to the operator space version of the space
$\ell_1$ (i.e. $\ell_1$ equipped with its maximal operator space structure).
The latter can also be described as the span of the free unitary generators in
the (full) $C^*$-algebra of the free group $\F_\infty$ with countably
infinitely many generators. Our main result is a generalization to this context
of Drury's classical theorem stating that Sidon sets are stable under finite
unions. In the particular case when $A=C^*(G)$ the (maximal) $C^*$-algebra of a
discrete group $G$, we recover the non-commutative (operator space) version of
Drury's theorem that we recently proved. We also give several non-commutative
generalizations of our recent work on uniformly bounded orthonormal systems to
the case of von Neumann algebras equipped with normal faithful tracial states.
",0,0,1,0,0,0
20364,"Dimers, crystals and quantum Kostka numbers","  We relate the counting of honeycomb dimer configurations on the cylinder to
the counting of certain vertices in Kirillov-Reshetikhin crystal graphs. We
show that these dimer configurations yield the quantum Kostka numbers of the
small quantum cohomology ring of the Grassmannian, i.e. the expansion
coefficients when multiplying a Schubert class repeatedly with different Chern
classes. This allows one to derive sum rules for Gromov-Witten invariants.
",0,0,1,0,0,0
8640,Periodic orbits of planets in binary systems,"  Periodic solutions of the three body problem are very important for
understanding its dynamics either in a theoretical framework or in various
applications in celestial mechanics. In this paper we discuss the computation
and continuation of periodic orbits for planetary systems. The study is
restricted to coplanar motion. Staring from known results of two-planet systems
around single stars, we perform continuation of solutions with respect to the
mass and approach periodic orbits of single planets in two-star systems. Also,
families of periodic solutions can be computed for fixed masses of the
primaries. When they are linearly stable, we can conclude about the existence
of phase space domains of long-term orbital stability.
",0,1,0,0,0,0
20376,Stable Signatures for Dynamic Graphs and Dynamic Metric Spaces via Zigzag Persistence,"  When studying flocking/swarming behaviors in animals one is interested in
quantifying and comparing the dynamics of the clustering induced by the
coalescence and disbanding of animals in different groups. In a similar vein,
studying the dynamics of social networks leads to the problem of characterizing
groups/communities as they form and disperse throughout time.
Motivated by this, we study the problem of obtaining persistent homology
based summaries of time-dependent data. Given a finite dynamic graph (DG), we
first construct a zigzag persistence module arising from linearizing the
dynamic transitive graph naturally induced from the input DG. Based on standard
results, we then obtain a persistence diagram or barcode from this zigzag
persistence module. We prove that these barcodes are stable under perturbations
in the input DG under a suitable distance between DGs that we identify.
More precisely, our stability theorem can be interpreted as providing a lower
bound for the distance between DGs. Since it relies on barcodes, and their
bottleneck distance, this lower bound can be computed in polynomial time from
the DG inputs.
Since DGs can be given rise by applying the Rips functor (with a fixed
threshold) to dynamic metric spaces, we are also able to derive related stable
invariants for these richer class of dynamic objects.
Along the way, we propose a summarization of dynamic graphs that captures
their time-dependent clustering features which we call formigrams. These
set-valued functions generalize the notion of dendrogram, a prevalent tool for
hierarchical clustering. In order to elucidate the relationship between our
distance between two DGs and the bottleneck distance between their associated
barcodes, we exploit recent advances in the stability of zigzag persistence due
to Botnan and Lesnick, and to Bjerkevik.
",0,0,1,0,0,0
9396,On the Limitation of Local Intrinsic Dimensionality for Characterizing the Subspaces of Adversarial Examples,"  Understanding and characterizing the subspaces of adversarial examples aid in
studying the robustness of deep neural networks (DNNs) to adversarial
perturbations. Very recently, Ma et al. (ICLR 2018) proposed to use local
intrinsic dimensionality (LID) in layer-wise hidden representations of DNNs to
study adversarial subspaces. It was demonstrated that LID can be used to
characterize the adversarial subspaces associated with different attack
methods, e.g., the Carlini and Wagner's (C&W) attack and the fast gradient sign
attack.
In this paper, we use MNIST and CIFAR-10 to conduct two new sets of
experiments that are absent in existing LID analysis and report the limitation
of LID in characterizing the corresponding adversarial subspaces, which are (i)
oblivious attacks and LID analysis using adversarial examples with different
confidence levels; and (ii) black-box transfer attacks. For (i), we find that
the performance of LID is very sensitive to the confidence parameter deployed
by an attack, and the LID learned from ensembles of adversarial examples with
varying confidence levels surprisingly gives poor performance. For (ii), we
find that when adversarial examples are crafted from another DNN model, LID is
ineffective in characterizing their adversarial subspaces. These two findings
together suggest the limited capability of LID in characterizing the subspaces
of adversarial examples.
",0,0,0,1,0,0
8571,Personalized Driver Stress Detection with Multi-task Neural Networks using Physiological Signals,"  Stress can be seen as a physiological response to everyday emotional, mental
and physical challenges. A long-term exposure to stressful situations can have
negative health consequences, such as increased risk of cardiovascular diseases
and immune system disorder. Therefore, a timely stress detection can lead to
systems for better management and prevention in future circumstances. In this
paper, we suggest a multi-task learning based neural network approach (with
hard parameter sharing of mutual representation and task-specific layers) for
personalized stress recognition using skin conductance and heart rate from
wearable devices. The proposed method is tested on multi-modal physiological
responses collected during real-world and simulator driving tasks.
",1,0,0,0,0,0
11153,A Branch-and-Bound Algorithm for Checkerboard Extraction in Camera-Laser Calibration,"  We address the problem of camera-to-laser-scanner calibration using a
checkerboard and multiple image-laser scan pairs. Distinguishing which laser
points measure the checkerboard and which lie on the background is essential to
any such system. We formulate the checkerboard extraction as a combinatorial
optimization problem with a clear cut objective function. We propose a
branch-and-bound technique that deterministically and globally optimizes the
objective. Unlike what is available in the literature, the proposed method is
not heuristic and does not require assumptions such as constraints on the
background or relying on discontinuity of the range measurements to partition
the data into line segments. The proposed approach is generic and can be
applied to both 3D or 2D laser scanners as well as the cases where multiple
checkerboards are present. We demonstrate the effectiveness of the proposed
approach by providing numerical simulations as well as experimental results.
",1,0,0,0,0,0
6370,Network of sensitive magnetometers for urban studies,"  The magnetic signature of an urban environment is investigated using a
geographically distributed network of fluxgate magnetometers deployed in and
around Berkeley, California. The system hardware and software are described and
results from initial operation of the network are reported. The sensors sample
the vector magnetic field with a 4 kHz resolution and are sensitive to
fluctuations below 0.1 $\textrm{nT}/\sqrt{\textrm{Hz}}$. Data from separate
stations are synchronized to around $\pm100$ $\mu{s}$ using GPS and computer
system clocks. Data from all sensors are automatically uploaded to a central
server. Anomalous events, such as lightning strikes, have been observed. A
wavelet analysis is used to study observations over a wide range of temporal
scales up to daily variations that show strong differences between weekend and
weekdays. The Bay Area Rapid Transit (BART) is identified as the most dominant
signal from these observations and a superposed epoch analysis is used to study
and extract the BART signal. Initial results of the correlation between sensors
are also presented.
",0,1,0,0,0,0
6518,"Joint Routing, Scheduling and Power Control Providing Hard Deadline in Wireless Multihop Networks","  We consider optimal/efficient power allocation policies in a single/multihop
wireless network in the presence of hard end-to-end deadline delay constraints
on the transmitted packets. Such constraints can be useful for real time voice
and video. Power is consumed in only transmission of the data. We consider the
case when the power used in transmission is a convex function of the data
transmitted. We develop a computationally efficient online algorithm, which
minimizes the average power for the single hop. We model this problem as
dynamic program (DP) and obtain the optimal solution. Next, we generalize it to
the multiuser, multihop scenario when there are multiple real time streams with
different hard deadline constraints.
",1,0,0,0,0,0
1528,"Implementing GraphQL as a Query Language for Deductive Databases in SWI-Prolog Using DCGs, Quasi Quotations, and Dicts","  The methods to access large relational databases in a distributed system are
well established: the relational query language SQL often serves as a language
for data access and manipulation, and in addition public interfaces are exposed
using communication protocols like REST. Similarly to REST, GraphQL is the
query protocol of an application layer developed by Facebook. It provides a
unified interface between the client and the server for data fetching and
manipulation. Using GraphQL's type system, it is possible to specify data
handling of various sources and to combine, e.g., relational with NoSQL
databases. In contrast to REST, GraphQL provides a single API endpoint and
supports flexible queries over linked data.
GraphQL can also be used as an interface for deductive databases. In this
paper, we give an introduction of GraphQL and a comparison to REST. Using
language features recently added to SWI-Prolog 7, we have developed the Prolog
library GraphQL.pl, which implements the GraphQL type system and query syntax
as a domain-specific language with the help of definite clause grammars (DCG),
quasi quotations, and dicts. Using our library, the type system created for a
deductive database can be validated, while the query system provides a unified
interface for data access and introspection.
",1,0,0,0,0,0
1789,PythonRobotics: a Python code collection of robotics algorithms,"  This paper describes an Open Source Software (OSS) project: PythonRobotics.
This is a collection of robotics algorithms implemented in the Python
programming language. The focus of the project is on autonomous navigation, and
the goal is for beginners in robotics to understand the basic ideas behind each
algorithm. In this project, the algorithms which are practical and widely used
in both academia and industry are selected. Each sample code is written in
Python3 and only depends on some standard modules for readability and ease of
use. It includes intuitive animations to understand the behavior of the
simulation.
",1,0,0,0,0,0
2849,Jet determination of smooth CR automorphisms and generalized stationary discs,"  We prove finite jet determination for (finitely) smooth CR diffeomorphisms of
(finitely) smooth Levi degenerate hypersurfaces in $\mathbb{C}^{n+1}$ by
constructing generalized stationary discs glued to such hypersurfaces.
",0,0,1,0,0,0
32,Equality of the usual definitions of Brakke flow,"  In 1978 Brakke introduced the mean curvature flow in the setting of geometric
measure theory. There exist multiple variants of the original definition. Here
we prove that most of them are indeed equal. One central point is to correct
the proof of Brakke's §3.5, where he develops an estimate for the evolution
of the measure of time-dependent test functions.
",0,0,1,0,0,0
3435,Direct Multitype Cardiac Indices Estimation via Joint Representation and Regression Learning,"  Cardiac indices estimation is of great importance during identification and
diagnosis of cardiac disease in clinical routine. However, estimation of
multitype cardiac indices with consistently reliable and high accuracy is still
a great challenge due to the high variability of cardiac structures and
complexity of temporal dynamics in cardiac MR sequences. While efforts have
been devoted into cardiac volumes estimation through feature engineering
followed by a independent regression model, these methods suffer from the
vulnerable feature representation and incompatible regression model. In this
paper, we propose a semi-automated method for multitype cardiac indices
estimation. After manual labelling of two landmarks for ROI cropping, an
integrated deep neural network Indices-Net is designed to jointly learn the
representation and regression models. It comprises two tightly-coupled
networks: a deep convolution autoencoder (DCAE) for cardiac image
representation, and a multiple output convolution neural network (CNN) for
indices regression. Joint learning of the two networks effectively enhances the
expressiveness of image representation with respect to cardiac indices, and the
compatibility between image representation and indices regression, thus leading
to accurate and reliable estimations for all the cardiac indices.
When applied with five-fold cross validation on MR images of 145 subjects,
Indices-Net achieves consistently low estimation error for LV wall thicknesses
(1.44$\pm$0.71mm) and areas of cavity and myocardium (204$\pm$133mm$^2$). It
outperforms, with significant error reductions, segmentation method (55.1% and
17.4%) and two-phase direct volume-only methods (12.7% and 14.6%) for wall
thicknesses and areas, respectively. These advantages endow the proposed method
a great potential in clinical cardiac function assessment.
",1,0,0,0,0,0
632,On the Limitations of Representing Functions on Sets,"  Recent work on the representation of functions on sets has considered the use
of summation in a latent space to enforce permutation invariance. In
particular, it has been conjectured that the dimension of this latent space may
remain fixed as the cardinality of the sets under consideration increases.
However, we demonstrate that the analysis leading to this conjecture requires
mappings which are highly discontinuous and argue that this is only of limited
practical use. Motivated by this observation, we prove that an implementation
of this model via continuous mappings (as provided by e.g. neural networks or
Gaussian processes) actually imposes a constraint on the dimensionality of the
latent space. Practical universal function representation for set inputs can
only be achieved with a latent dimension at least the size of the maximum
number of input elements.
",1,0,0,1,0,0
8158,Tailoring spin defects in diamond,"  Atomic-size spin defects in solids are unique quantum systems. Most
applications require nanometer positioning accuracy, which is typically
achieved by low energy ion implantation. So far, a drawback of this technique
is the significant residual implantation-induced damage to the lattice, which
strongly degrades the performance of spins in quantum applications. In this
letter we show that the charge state of implantation-induced defects
drastically influences the formation of lattice defects during thermal
annealing. We demonstrate that charging of vacancies localized at e.g.
individual nitrogen implantation sites suppresses the formation of vacancy
complexes, resulting in a tenfold-improved spin coherence time of single
nitrogen-vacancy (NV) centers in diamond. This has been achieved by confining
implantation defects into the space charge layer of free carriers generated by
a nanometer-thin boron-doped diamond structure. Besides, a twofold-improved
yield of formation of NV centers is observed. By combining these results with
numerical calculations, we arrive at a quantitative understanding of the
formation and dynamics of the implanted spin defects. The presented results
pave the way for improved engineering of diamond spin defect quantum devices
and other solid-state quantum systems.
",0,1,0,0,0,0
9398,Kähler differential algebras for 0-dimensional schemes,"  Given a 0-dimensional scheme in a projective space $\mathbb{P}^n$ over a
field $K$, we study the Kähler differential algebra $\Omega_{R/K}$ of its
homogeneous coordinate ring $R$. Using explicit presentations of the modules
$\Omega^m_{R/K}$ of Kähler differential $m$-forms, we determine many values
of their Hilbert functions explicitly and bound their Hilbert polynomials and
regularity indices. Detailed results are obtained for subschemes of
$\mathbb{P}^1$, fat point schemes, and subschemes of $\mathbb{P}^2$ supported
on a conic.
",0,0,1,0,0,0
14482,Incremental Adversarial Domain Adaptation for Continually Changing Environments,"  Continuous appearance shifts such as changes in weather and lighting
conditions can impact the performance of deployed machine learning models.
While unsupervised domain adaptation aims to address this challenge, current
approaches do not utilise the continuity of the occurring shifts. In
particular, many robotics applications exhibit these conditions and thus
facilitate the potential to incrementally adapt a learnt model over minor
shifts which integrate to massive differences over time. Our work presents an
adversarial approach for lifelong, incremental domain adaptation which benefits
from unsupervised alignment to a series of intermediate domains which
successively diverge from the labelled source domain. We empirically
demonstrate that our incremental approach improves handling of large appearance
changes, e.g. day to night, on a traversable-path segmentation task compared
with a direct, single alignment step approach. Furthermore, by approximating
the feature distribution for the source domain with a generative adversarial
network, the deployment module can be rendered fully independent of retaining
potentially large amounts of the related source training data for only a minor
reduction in performance.
",1,0,0,1,0,0
11513,Active tuning of high-Q dielectric metasurfaces,"  We demonstrate the active tuning of all-dielectric metasurfaces exhibiting
high-quality factor (high-Q) resonances. The active control is provided by
embedding the asymmetric silicon meta-atoms with liquid crystals, which allows
the relative index of refraction to be controlled through heating. It is found
that high quality factor resonances ($Q=270\pm30$) can be tuned over more than
three resonance widths. Our results demonstrate the feasibility of using
all-dielectric metasurfaces to construct tunable narrow-band filters.
",0,1,0,0,0,0
10905,Randomly cross-linked polymer models,"  Polymer models are used to describe chromatin, which can be folded at
different spatial scales by binding molecules. By folding, chromatin generates
loops of various sizes. We present here a randomly cross-linked (RCL) polymer
model, where monomer pairs are connected randomly. We obtain asymptotic
formulas for the steady-state variance, encounter probability, the radius of
gyration, instantaneous displacement and the mean first encounter time between
any two monomers. The analytical results are confirmed by Brownian simulations.
Finally, the present results can be used to extract the minimum number of
cross-links in a chromatin region from {conformation capture} data.
",0,1,0,0,0,0
12742,Multi-Block Interleaved Codes for Local and Global Read Access,"  We define multi-block interleaved codes as codes that allow reading
information from either a small sub-block or from a larger full block. The
former offers faster access, while the latter provides better reliability. We
specify the correction capability of the sub-block code through its gap $t$
from optimal minimum distance, and look to have full-block minimum distance
that grows with the parameter $t$. We construct two families of such codes when
the number of sub-blocks is $3$. The codes match the distance properties of
known integrated-interleaving codes, but with the added feature of mapping the
same number of information symbols to each sub-block. As such, they are the
first codes that provide read access in multiple size granularities and
correction capabilities.
",1,0,0,0,0,0
9057,Unimodal Category and the Monotonicity Conjecture,"  We completely characterize the unimodal category for functions $f:\mathbb
R\to[0,\infty)$ using a decomposition theorem obtained by generalizing the
sweeping algorithm of Baryshnikov and Ghrist. We also give a characterization
of the unimodal category for functions $f:S^1\to[0,\infty)$ and provide an
algorithm to compute the unimodal category of such a function in the case of
finitely many critical points.
We then turn to the monotonicity conjecture of Baryshnikov and Ghrist. We
show that this conjecture is true for functions on $\mathbb R$ and $S^1$ using
the above characterizations and that it is false on certain graphs and on the
Euclidean plane by providing explicit counterexamples. We also show that it
holds for functions on the Euclidean plane whose Morse-Smale graph is a tree
using a result of Hickok, Villatoro and Wang.
",0,0,1,0,0,0
20001,Asymmetric Spin-wave Dispersion on Fe(110): Direct Evidence of Dzyaloshinskii--Moriya Interaction,"  The influence of the Dzyaloshinskii-Moriya interaction on the spin-wave
dispersion in an Fe double layer grown on W(110) is measured for the first
time. It is demonstrated that the Dzyaloshinskii-Moriya interaction breaks the
degeneracy of spin waves and leads to an asymmetric spin-wave dispersion
relation. An extended Heisenberg spin Hamiltonian is employed to obtain the
longitudinal component of the Dzyaloshinskii-Moriya vectors from the
experimentally measured energy asymmetry.
",0,1,0,0,0,0
16291,Effective identifiability criteria for tensors and polynomials,"  A tensor $T$, in a given tensor space, is said to be $h$-identifiable if it
admits a unique decomposition as a sum of $h$ rank one tensors. A criterion for
$h$-identifiability is called effective if it is satisfied in a dense, open
subset of the set of rank $h$ tensors. In this paper we give effective
$h$-identifiability criteria for a large class of tensors. We then improve
these criteria for some symmetric tensors. For instance, this allows us to give
a complete set of effective identifiability criteria for ternary quintic
polynomial. Finally, we implement our identifiability algorithms in Macaulay2.
",1,0,1,0,0,0
12611,"The structure, capability and the Schur multiplier of generalized Heisenberg Lie algebras","  From [Problem 1729, Groups of prime power order, Vol. 3], Berkovich et al.
asked to obtain the Schur multiplier and the representation of a group $G$,
when $G$ is a special $p$-group minimally generated by $d$ elements and
$|G'|=p^{\frac{1}{2}d(d-1)}$. Since there are analogies between groups and Lie
algebras, we intend to give an answer to this question similarly for nilpotent
Lie algebras. Furthermore, we give some results about the tensor square and the
Schur multiplier of some nilpotent Lie algebras of class two.
",0,0,1,0,0,0
12091,Temporal Type Theory: A topos-theoretic approach to systems and behavior,"  This book introduces a temporal type theory, the first of its kind as far as
we know. It is based on a standard core, and as such it can be formalized in a
proof assistant such as Coq or Lean by adding a number of axioms. Well-known
temporal logics---such as Linear and Metric Temporal Logic (LTL and
MTL)---embed within the logic of temporal type theory.
The types in this theory represent ""behavior types"". The language is rich
enough to allow one to define arbitrary hybrid dynamical systems, which are
mixtures of continuous dynamics---e.g. as described by a differential
equation---and discrete jumps. In particular, the derivative of a continuous
real-valued function is internally defined.
We construct a semantics for the temporal type theory in the topos of sheaves
on a translation-invariant quotient of the standard interval domain. In fact,
domain theory plays a recurring role in both the semantics and the type theory.
",0,0,1,0,0,0
20587,Longitudinal electric field: from Maxwell equation to non-locality in time and space,"  In this paper we use the classical electrodynamics to show that the Lorenz
gauge can be incompatible with some particular solutions of the d Alembert
equations for electromagnetic potentials. In its turn, the d Alembert equations
for the elec- tromagnetic potentials is the result of application of the Lorenz
gauge to general equations for the potentials. The last ones is the
straightforward consequence of Maxwell equations. Since the d Alembert
equations and the electromagnetic poten- tials are necessary for quantum
electrodynamics formulation, one should oblige to satisfy these equations also
in classical case. The solution of d Alembert equations, which modifies
longitudinal electric field is found. The requirement of this modifi- cation
follows from the necessity to satisfy the physical condition of impossibility
of instantaneous transferring of interaction in space.
",0,1,0,0,0,0
2535,A Hierarchical Bayesian Linear Regression Model with Local Features for Stochastic Dynamics Approximation,"  One of the challenges in model-based control of stochastic dynamical systems
is that the state transition dynamics are involved, and it is not easy or
efficient to make good-quality predictions of the states. Moreover, there are
not many representational models for the majority of autonomous systems, as it
is not easy to build a compact model that captures the entire dynamical
subtleties and uncertainties. In this work, we present a hierarchical Bayesian
linear regression model with local features to learn the dynamics of a
micro-robotic system as well as two simpler examples, consisting of a
stochastic mass-spring damper and a stochastic double inverted pendulum on a
cart. The model is hierarchical since we assume non-stationary priors for the
model parameters. These non-stationary priors make the model more flexible by
imposing priors on the priors of the model. To solve the maximum likelihood
(ML) problem for this hierarchical model, we use the variational expectation
maximization (EM) algorithm, and enhance the procedure by introducing hidden
target variables. The algorithm yields parsimonious model structures, and
consistently provides fast and accurate predictions for all our examples
involving large training and test sets. This demonstrates the effectiveness of
the method in learning stochastic dynamics, which makes it suitable for future
use in a paradigm, such as model-based reinforcement learning, to compute
optimal control policies in real time.
",0,0,0,1,0,0
12105,AC-Biased Shift Registers as Fabrication Process Benchmark Circuits and Flux Trapping Diagnostic Tool,"  We develop an ac-biased shift register introduced in our previous work (V.K.
Semenov et al., IEEE Trans. Appl. Supercond., vol. 25, no. 3, 1301507, June
2015) into a benchmark circuit for evaluation of superconductor electronics
fabrication technology. The developed testing technique allows for extracting
margins of all individual cells in the shift register, which in turn makes it
possible to estimate statistical distribution of Josephson junctions in the
circuit. We applied this approach to successfully test registers having 8, 16,
36, and 202 thousand cells and, respectively, about 33000, 65000, 144000, and
809000 Josephson junctions. The circuits were fabricated at MIT Lincoln
Laboratory, using a fully planarized process, 0.4 {\mu}m inductor linewidth,
and 1.33x10^6 cm^-2 junction density. They are presently the largest
operational superconducting SFQ circuits ever made. The developed technique
distinguishes between hard defects (fabrication-related) and soft defects
(measurement-related) and locates them in the circuit. The soft defects are
specific to superconducting circuits and caused by magnetic flux trapping
either inside the active cells or in the dedicated flux-trapping moats near the
cells. The number and distribution of soft defects depend on the ambient
magnetic field and vary with thermal cycling even if done in the same magnetic
environment.
",0,1,0,0,0,0
14815,Universal Adversarial Perturbations Against Semantic Image Segmentation,"  While deep learning is remarkably successful on perceptual tasks, it was also
shown to be vulnerable to adversarial perturbations of the input. These
perturbations denote noise added to the input that was generated specifically
to fool the system while being quasi-imperceptible for humans. More severely,
there even exist universal perturbations that are input-agnostic but fool the
network on the majority of inputs. While recent work has focused on image
classification, this work proposes attacks against semantic image segmentation:
we present an approach for generating (universal) adversarial perturbations
that make the network yield a desired target segmentation as output. We show
empirically that there exist barely perceptible universal noise patterns which
result in nearly the same predicted segmentation for arbitrary inputs.
Furthermore, we also show the existence of universal noise which removes a
target class (e.g., all pedestrians) from the segmentation while leaving the
segmentation mostly unchanged otherwise.
",1,0,0,1,0,0
19793,Robotic Pick-and-Place of Novel Objects in Clutter with Multi-Affordance Grasping and Cross-Domain Image Matching,"  This paper presents a robotic pick-and-place system that is capable of
grasping and recognizing both known and novel objects in cluttered
environments. The key new feature of the system is that it handles a wide range
of object categories without needing any task-specific training data for novel
objects. To achieve this, it first uses a category-agnostic affordance
prediction algorithm to select and execute among four different grasping
primitive behaviors. It then recognizes picked objects with a cross-domain
image classification framework that matches observed images to product images.
Since product images are readily available for a wide range of objects (e.g.,
from the web), the system works out-of-the-box for novel objects without
requiring any additional training data. Exhaustive experimental results
demonstrate that our multi-affordance grasping achieves high success rates for
a wide variety of objects in clutter, and our recognition algorithm achieves
high accuracy for both known and novel grasped objects. The approach was part
of the MIT-Princeton Team system that took 1st place in the stowing task at the
2017 Amazon Robotics Challenge. All code, datasets, and pre-trained models are
available online at this http URL
",1,0,0,0,0,0
10646,Eight-cluster structure of chloroplast genomes differs from similar one observed for bacteria,"  Previously, a seven-cluster pattern claiming to be a universal one in
bacterial genomes has been reported. Keeping in mind the most popular theory of
chloroplast origin, we checked whether a similar pattern is observed in
chloroplast genomes. Surprisingly, eight cluster structure has been found, for
chloroplasts. The pattern observed for chloroplasts differs rather
significantly, from bacterial one, and from that latter observed for
cyanobacteria. The structure is provided by clustering of the fragments of
equal length isolated within a genome so that each fragment is converted in
triplet frequency dictionary with non-overlapping triplets with no gaps in
frame tiling. The points in 63-dimensional space were clustered due to elastic
map technique. The eight cluster found in chloroplasts comprises the fragments
of a genome bearing tRNA genes and exhibiting excessively high
$\mathsf{GC}$-content, in comparison to the entire genome.
",0,0,0,0,1,0
3107,Predicting Pulsar Scintillation from Refractive Plasma Sheets,"  The dynamic and secondary spectra of many pulsars show evidence for
long-lived, aligned images of the pulsar that are stationary on a thin
scattering sheet. One explanation for this phenomenon considers the effects of
wave crests along sheets in the ionized interstellar medium, such as those due
to Alfvén waves propagating along current sheets. If these sheets are closely
aligned to our line-of-sight to the pulsar, high bending angles arise at the
wave crests and a selection effect causes alignment of images produced at
different crests, similar to grazing reflection off of a lake. Using geometric
optics, we develop a simple parameterized model of these corrugated sheets that
can be constrained with a single observation and that makes observable
predictions for variations in the scintillation of the pulsar over time and
frequency. This model reveals qualitative differences between lensing from
overdense and underdense corrugated sheets: Only if the sheet is overdense
compared to the surrounding interstellar medium can the lensed images be
brighter than the line-of-sight image to the pulsar, and the faint lensed
images are closer to the pulsar at higher frequencies if the sheet is
underdense, but at lower frequencies if the sheet is overdense.
",0,1,0,0,0,0
8671,Proof of Correspondence between Keys and Encoding Maps in an Authentication Code,"  In a former paper the authors introduced two new systematic authentication
codes based on the Gray map over a Galois ring. In this paper, it is proved the
one-to-one onto correspondence between keys and encoding maps for the second
introduced authentication code.
",1,0,1,0,0,0
5353,Four-dimensional Lens Space Index from Two-dimensional Chiral Algebra,"  We study the supersymmetric partition function on $S^1 \times L(r, 1)$, or
the lens space index of four-dimensional $\mathcal{N}=2$ superconformal field
theories and their connection to two-dimensional chiral algebras. We primarily
focus on free theories as well as Argyres-Douglas theories of type $(A_1, A_k)$
and $(A_1, D_k)$. We observe that in specific limits, the lens space index is
reproduced in terms of the (refined) character of an appropriately twisted
module of the associated two-dimensional chiral algebra or a generalized vertex
operator algebra. The particular twisted module is determined by the choice of
discrete holonomies for the flavor symmetry in four-dimensions.
",0,0,1,0,0,0
13236,Do metric fluctuations affect the Higgs dynamics during inflation?,"  We show that the dynamics of the Higgs field during inflation is not affected
by metric fluctuations if the Higgs is an energetically subdominant light
spectator. For Standard Model parameters we find that couplings between Higgs
and metric fluctuations are suppressed by $\mathcal{O}(10^{-7})$. They are
negligible compared to both pure Higgs terms in the effective potential and the
unavoidable non-minimal Higgs coupling to background scalar curvature. The
question of the electroweak vacuum instability during high energy scale
inflation can therefore be studied consistently using the Jordan frame action
in a Friedmann--Lemaître--Robertson--Walker metric, where the Higgs-curvature
coupling enters as an effective mass contribution. Similar results apply for
other light spectator scalar fields during inflation.
",0,1,0,0,0,0
15164,Thermal Characterization of Microscale Heat Convection under Rare Gas Condition by a Modified Hot Wire Method,"  As power electronics shrinks down to sub-micron scale, the thermal transport
from a solid surface to environment becomes significant. Under circumstances
when the device works in rare gas environment, the scale for thermal transport
is comparable to the mean free path of molecules, and is difficult to
characterize. In this work, we present an experimental study about thermal
transport around a microwire in rare gas environment by using a steady state
hot wire method. Unlike conventional hot wire technique of using transient heat
transfer process, this method considers both the heat conduction along the wire
and convection effect from wire surface to surroundings. Convection heat
transfer coefficient from a platinum wire in diameter 25 um to air is
characterized under different heating power and air pressures to comprehend the
effect of temperature and density of gas molecules. It is observed that
convection heat transfer coefficient varies from 14 Wm-2K-1 at 7 Pa to 629
Wm-2K-1 at atmosphere pressure. In free molecule regime, Nusselt number has a
linear relationship with inverse Knudsen number and the slope of 0.274 is
employed to determined equivalent thermal dissipation boundary as 7.03E10-4 m.
In transition regime, the equivalent thermal dissipation boundary is obtained
as 5.02E10-4 m. Under a constant pressure, convection heat transfer coefficient
decreases with increasing temperature, and this correlation is more sensitive
to larger pressure. This work provides a pathway for studying both heat
conduction and heat convection effect at micro/nanoscale under rare gas
environment, the knowledge of which is essential for regulating heat
dissipation in various industrial applications.
",0,1,0,0,0,0
18525,The SPEDE spectrometer,"  The electron spectrometer, SPEDE, has been developed and will be employed in
conjunction with the Miniball spectrometer at the HIE-ISOLDE facility, CERN.
SPEDE allows for direct measurement of internal conversion electrons emitted
in-flight, without employing magnetic fields to transport or momentum filter
the electrons. Together with the Miniball spectrometer, it enables simultaneous
observation of {\gamma} rays and conversion electrons in Coulomb-excitation
experiments using radioactive ion beams.
",0,1,0,0,0,0
6441,Critical values in Bak-Sneppen type models,"  In the Bak-Sneppen model, the lowest fitness particle and its two nearest
neighbors are renewed at each temporal step with a uniform (0,1) fitness
distribution. The model presents a critical value that depends on the
interaction criteria (two nearest neighbors) and on the update procedure
(uniform). Here we calculate the critical value for models where one or both
properties are changed. We study models with non-uniform updates, models with
random neighbors and models with binary fitness and obtain exact results for
the average fitness and for $p_c$.
",0,1,0,0,0,0
248,Bayesian nonparametric inference for the M/G/1 queueing systems based on the marked departure process,"  In the present work we study Bayesian nonparametric inference for the
continuous-time M/G/1 queueing system. In the focus of the study is the
unobservable service time distribution. We assume that the only available data
of the system are the marked departure process of customers with the marks
being the queue lengths just after departure instants. These marks constitute
an embedded Markov chain whose distribution may be parametrized by stochastic
matrices of a special delta form. We develop the theory in order to obtain
integral mixtures of Markov measures with respect to suitable prior
distributions. We have found a sufficient statistic with a distribution of a
so-called S-structure sheding some new light on the inner statistical structure
of the M/G/1 queue. Moreover, it allows to update suitable prior distributions
to the posterior. Our inference methods are validated by large sample results
as posterior consistency and posterior normality.
",0,0,1,1,0,0
6492,Augmentor: An Image Augmentation Library for Machine Learning,"  The generation of artificial data based on existing observations, known as
data augmentation, is a technique used in machine learning to improve model
accuracy, generalisation, and to control overfitting. Augmentor is a software
package, available in both Python and Julia versions, that provides a high
level API for the expansion of image data using a stochastic, pipeline-based
approach which effectively allows for images to be sampled from a distribution
of augmented images at runtime. Augmentor provides methods for most standard
augmentation practices as well as several advanced features such as
label-preserving, randomised elastic distortions, and provides many helper
functions for typical augmentation tasks used in machine learning.
",1,0,0,1,0,0
10754,Computational Models of Tutor Feedback in Language Acquisition,"  This paper investigates the role of tutor feedback in language learning using
computational models. We compare two dominant paradigms in language learning:
interactive learning and cross-situational learning - which differ primarily in
the role of social feedback such as gaze or pointing. We analyze the
relationship between these two paradigms and propose a new mixed paradigm that
combines the two paradigms and allows to test algorithms in experiments that
combine no feedback and social feedback. To deal with mixed feedback
experiments, we develop new algorithms and show how they perform with respect
to traditional knn and prototype approaches.
",1,0,0,0,0,0
14416,Persistent Monitoring of Dynamically Changing Environments Using an Unmanned Vehicle,"  We consider the problem of planning a closed walk $\mathcal W$ for a UAV to
persistently monitor a finite number of stationary targets with equal
priorities and dynamically changing properties. A UAV must physically visit the
targets in order to monitor them and collect information therein. The frequency
of monitoring any given target is specified by a target revisit time, $i.e.$,
the maximum allowable time between any two successive visits to the target. The
problem considered in this paper is the following: Given $n$ targets and $k
\geq n$ allowed visits to them, find an optimal closed walk $\mathcal W^*(k)$
so that every target is visited at least once and the maximum revisit time over
all the targets, $\mathcal R(\mathcal W(k))$, is minimized. We prove the
following: If $k \geq n^2-n$, $\mathcal R(\mathcal W^*(k))$ (or simply,
$\mathcal R^*(k)$) takes only two values: $\mathcal R^*(n)$ when $k$ is an
integral multiple of $n$, and $\mathcal R^*(n+1)$ otherwise. This result
suggests significant computational savings - one only needs to determine
$\mathcal W^*(n)$ and $\mathcal W^*(n+1)$ to construct an optimal solution
$\mathcal W^*(k)$. We provide MILP formulations for computing $\mathcal W^*(n)$
and $\mathcal W^*(n+1)$. Furthermore, for {\it any} given $k$, we prove that
$\mathcal R^*(k) \geq \mathcal R^*(k+n)$.
",1,0,0,0,0,0
6046,On the non-vanishing of certain Dirichlet series,"  Given $k\in\mathbb N$, we study the vanishing of the Dirichlet series
$$D_k(s,f):=\sum_{n\geq1} d_k(n)f(n)n^{-s}$$ at the point $s=1$, where $f$ is a
periodic function modulo a prime $p$. We show that if $(k,p-1)=1$ or
$(k,p-1)=2$ and $p\equiv 3\mod 4$, then there are no odd rational-valued
functions $f\not\equiv 0$ such that $D_k(1,f)=0$, whereas in all other cases
there are examples of odd functions $f$ such that $D_k(1,f)=0$.
As a consequence, we obtain, for example, that the set of values
$L(1,\chi)^2$, where $\chi$ ranges over odd characters mod $p$, are linearly
independent over $\mathbb Q$.
",0,0,1,0,0,0
10993,Invariant theory of a special group action on irreducible polynomials over finite fields,"  In the past few years, an action of $\mathrm{PGL}_2(\mathbb F_q)$ on the set
of irreducible polynomials in $\mathbb F_q[x]$ has been introduced and many
questions have been discussed, such as the characterization and number of
invariant elements. In this paper, we analyze some recent works on this action
and provide full generalizations of them, yielding final theoretical results on
the characterization and number of invariant elements.
",0,0,1,0,0,0
2393,Resource Allocation for a Full-Duplex Base Station Aided OFDMA System,"  Exploiting full-duplex (FD) technology on base stations (BSs) is a promising
solution to enhancing the system performance. Motivated by this, we revisit a
full-duplex base station (FD-BS) aided OFDMA system, which consists of one BS,
several uplink/downlink users and multiple subcarriers. A joint 3-dimensional
(3D) mapping scheme among subcarriers, down-link users (DUEs), uplink users
(UUEs) is considered as well as an associated power allocation optimization. In
detail, we first decompose the complex 3D mapping problem into three
2-dimensional sub ones and solve them by using the iterative Hungarian method,
respectively. Then based on the Lagrange dual method, we sequentially solve the
power allocation and 3- dimensional mapping problem by fixing a dual point.
Finally, the optimal solution can be obtained by utilizing the sub-gradient
method. Unlike existing work that only solves either 3D mapping or power
allocation problem but with a high computation complexity, we tackle both of
them and have successfully reduced computation complexity from exponential to
polynomial order. Numerical simulations are conducted to verify the proposed
scheme.
",1,0,0,0,0,0
656,Evolutionary dynamics of N-person Hawk-Dove games,"  In the animal world, the competition between individuals belonging to
different species for a resource often requires the cooperation of several
individuals in groups. This paper proposes a generalization of the Hawk-Dove
Game for an arbitrary number of agents: the N-person Hawk-Dove Game. In this
model, doves exemplify the cooperative behavior without intraspecies conflict,
while hawks represent the aggressive behavior. In the absence of hawks, doves
share the resource equally and avoid conflict, but having hawks around lead to
doves escaping without fighting. Conversely, hawks fight for the resource at
the cost of getting injured. Nevertheless, if doves are present in sufficient
number to expel the hawks, they can aggregate to protect the resource, and thus
avoid being plundered by hawks. We derive and numerically solve an exact
equation for the evolution of the system in both finite and infinite well-mixed
populations, finding the conditions for stable coexistence between both
species. Furthermore, by varying the different parameters, we found a scenario
of bifurcations that leads the system from dominating hawks and coexistence to
bi-stability, multiple interior equilibria and dominating doves.
",0,1,0,0,0,0
6920,Non-equilibrium time dynamics of genetic evolution,"  Biological systems are typically highly open, non-equilibrium systems that
are very challenging to understand from a statistical mechanics perspective.
While statistical treatments of evolutionary biological systems have a long and
rich history, examination of the time-dependent non-equilibrium dynamics has
been less studied. In this paper we first derive a generalized master equation
in the genotype space for diploid organisms incorporating the processes of
selection, mutation, recombination, and reproduction. The master equation is
defined in terms of continuous time and can handle an arbitrary number of gene
loci and alleles, and can be defined in terms of an absolute population or
probabilities. We examine and analytically solve several prototypical cases
which illustrate the interplay of the various processes and discuss the
timescales of their evolution. The entropy production during the evolution
towards steady state is calculated and we find that it agrees with predictions
from non-equilibrium statistical mechanics where it is large when the
population distribution evolves towards a more viable genotype. The stability
of the non-equilibrium steady state is confirmed using the Glansdorff-Prigogine
criterion.
",0,0,0,0,1,0
10445,Precision of the ENDGame: Mixed-precision arithmetic in the iterative solver of the Unified Model,"  The Met Office's weather and climate simulation code the Unified Model is
used for both operational Numerical Weather Prediction and Climate modelling.
The computational performance of the model running on parallel supercomputers
is a key consideration. A Krylov sub-space solver is employed to solve the
equations of the dynamical core of the model, known as ENDGame. These describe
the evolution of the Earth's atmosphere. Typically, 64-bit precision is used
throughout weather and climate applications. This work presents a
mixed-precision implementation of the solver, the beneficial effect on run-time
and the impact on solver convergence. The complex interplay of errors arising
from accumulated round-off in floating-point arithmetic and other numerical
effects is discussed. A careful analysis is required, however, the
mixed-precision solver is now employed in the operational forecast to satisfy
run-time constraints without compromising the accuracy of the solution.
",1,0,0,0,0,0
2560,Large Spontaneous Hall Effects in Chiral Topological Magnets,"  As novel topological phases in correlated electron systems, we have found two
examples of non-ferromagnetic states that exhibit a large anomalous Hall
effect. One is the chiral spin liquid compound Pr$_{2}$Ir$_{2}$O$_{7}$, which
exhibits a spontaneous Hall effect in a spin liquid state due to spin ice
correlation. The other is the chiral antiferromagnets Mn$_{3}$Sn and Mn$_{3}$Ge
that exhibit a large anomalous Hall effect at room temperature. The latter
shows a sign change of the anomalous Hall effect by a small change in the
magnetic field by a few 100 G, which should be useful for various applications.
We will discuss that the magnetic Weyl metal states are the origin for such a
large anomalous Hall effect observed in both the spin liquid and
antiferromagnet that possess almost no magnetization.
",0,1,0,0,0,0
18867,B-spline-like bases for $C^2$ cubics on the Powell-Sabin 12-split,"  For spaces of constant, linear, and quadratic splines of maximal smoothness
on the Powell-Sabin 12-split of a triangle, the so-called S-bases were recently
introduced. These are simplex spline bases with B-spline-like properties on the
12-split of a single triangle, which are tied together across triangles in a
Bézier-like manner.
In this paper we give a formal definition of an S-basis in terms of certain
basic properties. We proceed to investigate the existence of S-bases for the
aforementioned spaces and additionally the cubic case, resulting in an
exhaustive list. From their nature as simplex splines, we derive simple
differentiation and recurrence formulas to other S-bases. We establish a
Marsden identity that gives rise to various quasi-interpolants and domain
points forming an intuitive control net, in terms of which conditions for
$C^0$-, $C^1$-, and $C^2$-smoothness are derived.
",1,0,0,0,0,0
15149,Cyclic Dominance in the Spatial Coevolutionary Optional Prisoner's Dilemma Game,"  This paper studies scenarios of cyclic dominance in a coevolutionary spatial
model in which game strategies and links between agents adaptively evolve over
time. The Optional Prisoner's Dilemma (OPD) game is employed. The OPD is an
extended version of the traditional Prisoner's Dilemma where players have a
third option to abstain from playing the game. We adopt an agent-based
simulation approach and use Monte Carlo methods to perform the OPD with
coevolutionary rules. The necessary conditions to break the scenarios of cyclic
dominance are also investigated. This work highlights that cyclic dominance is
essential in the sustenance of biodiversity. Moreover, we also discuss the
importance of a spatial coevolutionary model in maintaining cyclic dominance in
adverse conditions.
",1,1,1,0,0,0
3785,A Homological model for the coloured Jones polynomials,"  In this paper we will present a homological model for Coloured Jones
Polynomials. For each color $N \in \N$, we will describe the invariant
$J_N(L,q)$ as a graded intersection pairing of certain homological classes in a
covering of the configuration space on the punctured disk. This construction is
based on the Lawrence representation and a result due to Kohno that relates
quantum representations and homological representations of the braid groups.
",0,0,1,0,0,0
17921,Asymmetric Preheating,"  We study the generation of the matter-antimatter asymmetry during bosonic
preheating, focusing on the sources of the asymmetry. If the asymmetry appears
in the multiplication factor of the resonant particle production, the
matter-antimatter ratio will grow during preheating. On the other hand, if the
asymmetry does not grow during preheating, one has to find out another reason.
We consider several scenarios for the asymmetric preheating to distinguish the
sources of the asymmetry. We also discuss a new baryogenesis scenario, in which
the asymmetry is generated without introducing neither loop corrections nor
rotation of a field.
",0,1,0,0,0,0
6138,A fixed point formula and Harish-Chandra's character formula,"  The main result in this paper is a fixed point formula for equivariant
indices of elliptic differential operators, for proper actions by connected
semisimple Lie groups on possibly noncompact manifolds, with compact quotients.
For compact groups and manifolds, this reduces to the Atiyah-Segal-Singer fixed
point formula. Other special cases include an index theorem by Connes and
Moscovici for homogeneous spaces, and an earlier index theorem by the second
author, both in cases where the group acting is connected and semisimple. As an
application of this fixed point formula, we give a new proof of
Harish-Chandra's character formula for discrete series representations.
",0,0,1,0,0,0
14436,The infinite Fibonacci groups and relative asphericity,"  We prove that the generalised Fibonacci group F(r,n) is infinite for (r,n) in
{(7 + 5k,5), (8 + 5k,5)} where k is greater than or equal to 0. This together
with previously known results yields a complete classification of the finite
F(r,n), a problem that has its origins in a question by J H Conway in 1965. The
method is to show that a related relative presentation is aspherical from which
it can be deduced that the groups are infinite.
",0,0,1,0,0,0
13782,Coping with Construals in Broad-Coverage Semantic Annotation of Adpositions,"  We consider the semantics of prepositions, revisiting a broad-coverage
annotation scheme used for annotating all 4,250 preposition tokens in a 55,000
word corpus of English. Attempts to apply the scheme to adpositions and case
markers in other languages, as well as some problematic cases in English, have
led us to reconsider the assumption that a preposition's lexical contribution
is equivalent to the role/relation that it mediates. Our proposal is to embrace
the potential for construal in adposition use, expressing such phenomena
directly at the token level to manage complexity and avoid sense proliferation.
We suggest a framework to represent both the scene role and the adposition's
lexical function so they can be annotated at scale---supporting automatic,
statistical processing of domain-general language---and sketch how this
representation would inform a constructional analysis.
",1,0,0,0,0,0
14308,Regularisation of Neural Networks by Enforcing Lipschitz Continuity,"  We investigate the effect of explicitly enforcing the Lipschitz continuity of
neural networks with respect to their inputs. To this end, we provide a simple
technique for computing an upper bound to the Lipschitz constant of a feed
forward neural network composed of commonly used layer types and demonstrate
inaccuracies in previous work on this topic. Our technique is then used to
formulate training a neural network with a bounded Lipschitz constant as a
constrained optimisation problem that can be solved using projected stochastic
gradient methods. Our evaluation study shows that, in isolation, our method
performs comparatively to state-of-the-art regularisation techniques. Moreover,
when combined with existing approaches to regularising neural networks the
performance gains are cumulative. We also provide evidence that the
hyperparameters are intuitive to tune and demonstrate how the choice of norm
for computing the Lipschitz constant impacts the resulting model.
",0,0,0,1,0,0
3389,Understanding GANs: the LQG Setting,"  Generative Adversarial Networks (GANs) have become a popular method to learn
a probability model from data. In this paper, we aim to provide an
understanding of some of the basic issues surrounding GANs including their
formulation, generalization and stability on a simple benchmark where the data
has a high-dimensional Gaussian distribution. Even in this simple benchmark,
the GAN problem has not been well-understood as we observe that existing
state-of-the-art GAN architectures may fail to learn a proper generative
distribution owing to (1) stability issues (i.e., convergence to bad local
solutions or not converging at all), (2) approximation issues (i.e., having
improper global GAN optimizers caused by inappropriate GAN's loss functions),
and (3) generalizability issues (i.e., requiring large number of samples for
training). In this setup, we propose a GAN architecture which recovers the
maximum-likelihood solution and demonstrates fast generalization. Moreover, we
analyze global stability of different computational approaches for the proposed
GAN optimization and highlight their pros and cons. Finally, we outline an
extension of our model-based approach to design GANs in more complex setups
than the considered Gaussian benchmark.
",1,0,0,1,0,0
12625,Stability of patterns in the Abelian sandpile,"  We show that the patterns in the Abelian sandpile are stable. The proof
combines the structure theory for the patterns with the regularity machinery
for non-divergence form elliptic equations. The stability results allows one to
improve weak-* convergence of the Abelian sandpile to pattern convergence for
certain classes of solutions.
",0,0,1,0,0,0
16177,Optimal design with EGM approach in conjugate natural convection with surface radiation in a two-dimensional enclosure,"  Analysis of conjugate natural convection with surface radiation in a
two-dimensional enclosure is carried out in order to search the optimal
location of the heat source with entropy generation minimization (EGM) approach
and conventional heat transfer parameters. The air as an incompressible fluid
and transparent media is considered the fluid filling the enclosure with the
steady and laminar regime. The enclosure internal surfaces are also gray,
opaque and diffuse. The governing equations with stream function and vorticity
formulation are solved using finite difference approach. Results include the
effect of Rayleigh number and emissivity on the dimensionless average rate of
entropy generation and its optimum location. The optimum location search with
conventional heat transfer parameters including maximum temperature and Nusselt
numbers are also examined.
",0,1,0,0,0,0
13356,Quermassintegral preserving curvature flow in Hyperbolic space,"  We consider the quermassintegral preserving flow of closed \emph{h-convex}
hypersurfaces in hyperbolic space with the speed given by any positive power of
a smooth symmetric, strictly increasing, and homogeneous of degree one function
$f$ of the principal curvatures which is inverse concave and has dual $f_*$
approaching zero on the boundary of the positive cone. We prove that if the
initial hypersurface is \emph{h-convex}, then the solution of the flow becomes
strictly \emph{h-convex} for $t>0$, the flow exists for all time and converges
to a geodesic sphere exponentially in the smooth topology.
",0,0,1,0,0,0
6196,Measured Multiseries and Integration,"  A paper by Bruno Salvy and the author introduced measured multiseries and
gave an algorithm to compute these for a large class of elementary functions,
modulo a zero-equivalence method for constants. This gave a theoretical
background for the implementation that Salvy was developing at that time. The
main result of the present article is an algorithm to calculate measured
multiseries for integrals of functions of the form h*sin G, where h and G
belong to a Hardy field. The process can reiterated with the resulting algebra,
and also applied to solutions of a second order differential equation of a
particular form.
",1,0,0,0,0,0
929,On a question of Buchweitz about ranks of syzygies of modules of finite length,"  Let R be a local ring of dimension d. Buchweitz asks if the rank of the d-th
syzygy of a module of finite lengh is greater than or equal to the rank of the
d-th syzygy of the residue field, unless the module has finite projective
dimension. Assuming that R is Gorenstein, we prove that if the question is
affrmative, then R is a hypersurface. If moreover R has dimension two, then we
show that the converse also holds true.
",0,0,1,0,0,0
16889,"Switching and Data Injection Attacks on Stochastic Cyber-Physical Systems: Modeling, Resilient Estimation and Attack Mitigation","  In this paper, we consider the problem of attack-resilient state estimation,
that is to reliably estimate the true system states despite two classes of
attacks: (i) attacks on the switching mechanisms and (ii) false data injection
attacks on actuator and sensor signals, in the presence of unbounded stochastic
process and measurement noise signals. We model the systems under attack as
hidden mode stochastic switched linear systems with unknown inputs and propose
the use of a multiple-model inference algorithm to tackle these security
issues. Moreover, we characterize fundamental limitations to resilient
estimation (e.g., upper bound on the number of tolerable signal attacks) and
discuss the topics of attack detection, identification and mitigation under
this framework. Simulation examples of switching and false data injection
attacks on a benchmark system and an IEEE 68-bus test system show the efficacy
of our approach to recover resilient (i.e., asymptotically unbiased) state
estimates as well as to identify and mitigate the attacks.
",1,0,1,0,0,0
20141,The KLASH Proposal,"  We propose a search of galactic axions with mass about 0.2 microeV using a
large volume resonant cavity, about 50 m^3, cooled down to 4 K and immersed in
a moderate axial magnetic field of about 0.6 T generated inside the
superconducting magnet of the KLOE experiment located at the National
Laboratory of Frascati of INFN. This experiment, called KLASH (KLoe magnet for
Axion SearcH) in the following, has a potential sensitivity on the
axion-to-photon coupling, g_agg, of about 6x10^-17 GeV-1, reaching the region
predicted by KSVZ and DFSZ models of QCD axions.
",0,1,0,0,0,0
10719,The word and conjugacy problems in lacunary hyperbolic groups,"  We study the word and conjugacy problems in lacunary hyperbolic groups
(briefly, LHG). In particular, we describe a necessary and sufficient condition
for decidability of the word problem in LHG. Then, based on the graded
small-cancellation theory of Olshanskii, we develop a general framework which
allows us to construct lacunary hyperbolic groups with word and conjugacy
problems highly controllable and flexible both in terms of computability and
computational complexity.
As an application, we show that for any recursively enumerable subset
$\mathcal{L} \subseteq \mathcal{A}^*$, where $\mathcal{A}^*$ is the set of
words over arbitrarily chosen non-empty finite alphabet $\mathcal{A}$, there
exists a lacunary hyperbolic group $G_{\mathcal{L}}$ such that the membership
problem for $ \mathcal{L}$ is `almost' linear time equivalent to the conjugacy
problem in $G_{\mathcal{L}}$. Moreover, for the mentioned group the word and
individual conjugacy problems are decidable in `almost' linear time.
Another application is the construction of a lacunary hyperbolic group with
`almost' linear time word problem and with all the individual conjugacy
problems being undecidable except the word problem.
As yet another application of the developed framework, we construct infinite
verbally complete groups and torsion free Tarski monsters, i.e. infinite
torsion-free groups all of whose proper subgroups are cyclic, with `almost'
linear time word and polynomial time conjugacy problems. These groups are
constructed as quotients of arbitrarily given non-elementary torsion-free
hyperbolic groups and are lacunary hyperbolic.
Finally, as a consequence of the main results, we answer a few open
questions.
",0,0,1,0,0,0
13803,Approximate Structure Construction Using Large Statistical Swarms,"  In this paper we describe a novel local algorithm for large statistical
swarms using ""harmonic attractor dynamics"", by means of which a swarm can
construct harmonics of the environment. This in turn allows the swarm to
approximately reconstruct desired structures in the environment. The robots
navigate in a discrete environment, completely free of localization, being able
to communicate with other robots in its own discrete cell only, and being able
to sense or take reliable action within a disk of radius $r$ around itself. We
present the mathematics that underlie such dynamics and present initial results
demonstrating the proposed algorithm.
",1,0,0,1,0,0
16481,Baselines and a datasheet for the Cerema AWP dataset,"  This paper presents the recently published Cerema AWP (Adverse Weather
Pedestrian) dataset for various machine learning tasks and its exports in
machine learning friendly format. We explain why this dataset can be
interesting (mainly because it is a greatly controlled and fully annotated
image dataset) and present baseline results for various tasks. Moreover, we
decided to follow the very recent suggestions of datasheets for dataset, trying
to standardize all the available information of the dataset, with a
transparency objective.
",0,0,0,1,0,0
4825,Rocket Launching: A Universal and Efficient Framework for Training Well-performing Light Net,"  Models applied on real time response task, like click-through rate (CTR)
prediction model, require high accuracy and rigorous response time. Therefore,
top-performing deep models of high depth and complexity are not well suited for
these applications with the limitations on the inference time. In order to
further improve the neural networks' performance given the time and
computational limitations, we propose an approach that exploits a cumbersome
net to help train the lightweight net for prediction. We dub the whole process
rocket launching, where the cumbersome booster net is used to guide the
learning of the target light net throughout the whole training process. We
analyze different loss functions aiming at pushing the light net to behave
similarly to the booster net, and adopt the loss with best performance in our
experiments. We use one technique called gradient block to improve the
performance of the light net and booster net further. Experiments on benchmark
datasets and real-life industrial advertisement data present that our light
model can get performance only previously achievable with more complex models.
",1,0,0,1,0,0
19537,Multi-objective Model-based Policy Search for Data-efficient Learning with Sparse Rewards,"  The most data-efficient algorithms for reinforcement learning in robotics are
model-based policy search algorithms, which alternate between learning a
dynamical model of the robot and optimizing a policy to maximize the expected
return given the model and its uncertainties. However, the current algorithms
lack an effective exploration strategy to deal with sparse or misleading reward
scenarios: if they do not experience any state with a positive reward during
the initial random exploration, it is very unlikely to solve the problem. Here,
we propose a novel model-based policy search algorithm, Multi-DEX, that
leverages a learned dynamical model to efficiently explore the task space and
solve tasks with sparse rewards in a few episodes. To achieve this, we frame
the policy search problem as a multi-objective, model-based policy optimization
problem with three objectives: (1) generate maximally novel state trajectories,
(2) maximize the expected return and (3) keep the system in state-space regions
for which the model is as accurate as possible. We then optimize these
objectives using a Pareto-based multi-objective optimization algorithm. The
experiments show that Multi-DEX is able to solve sparse reward scenarios (with
a simulated robotic arm) in much lower interaction time than VIME, TRPO,
GEP-PG, CMA-ES and Black-DROPS.
",1,0,0,1,0,0
9595,Knockoffs for the mass: new feature importance statistics with false discovery guarantees,"  An important problem in machine learning and statistics is to identify
features that causally affect the outcome. This is often impossible to do from
purely observational data, and a natural relaxation is to identify features
that are correlated with the outcome even conditioned on all other observed
features. For example, we want to identify that smoking really is correlated
with cancer conditioned on demographics. The knockoff procedure is a recent
breakthrough in statistics that, in theory, can identify truly correlated
features while guaranteeing that the false discovery is limited. The idea is to
create synthetic data -knockoffs- that captures correlations amongst the
features. However there are substantial computational and practical challenges
to generating and using knockoffs. This paper makes several key advances that
enable knockoff application to be more efficient and powerful. We develop an
efficient algorithm to generate valid knockoffs from Bayesian Networks. Then we
systematically evaluate knockoff test statistics and develop new statistics
with improved power. The paper combines new mathematical guarantees with
systematic experiments on real and synthetic data.
",0,0,0,1,0,0
6713,Dark Matter in the Local Group of Galaxies,"  We describe the neutrino flavor (e = electron, u = muon, t = tau) masses as
m(i=e;u;t)= m + [Delta]mi with |[Delta]mij|/m < 1 and probably |[Delta]mij|/m
<< 1. The quantity m is the degenerate neutrino mass. Because neutrino flavor
is not a quantum number, this degenerate mass appears in the neutrino equation
of state. We apply a Monte Carlo computational physics technique to the Local
Group (LG) of galaxies to determine an approximate location for a Dark Matter
embedding condensed neutrino object(CNO). The calculation is based on the
rotational properties of the only spiral galaxies within the LG: M31, M33 and
the Milky Way. CNOs could be the Dark Matter everyone is looking for and we
estimate the CNO embedding the LG to have a mass 5.17x10^15 Mo and a radius
1.316 Mpc, with the estimated value of m ~= 0.8 eV/c2. The up-coming KATRIN
experiment will either be the definitive result or eliminate condensed
neutrinos as a Dark Matter candidate.
",0,1,0,0,0,0
3635,Nanopteron solutions of diatomic Fermi-Pasta-Ulam-Tsingou lattices with small mass-ratio,"  Consider an infinite chain of masses, each connected to its nearest neighbors
by a (nonlinear) spring. This is a Fermi-Pasta-Ulam-Tsingou lattice. We prove
the existence of traveling waves in the setting where the masses alternate in
size. In particular we address the limit where the mass ratio tends to zero.
The problem is inherently singular and we find that the traveling waves are not
true solitary waves but rather ""nanopterons"", which is to say, waves which
asymptotic at spatial infinity to very small amplitude periodic waves.
Moreover, we can only find solutions when the mass ratio lies in a certain open
set. The difficulties in the problem all revolve around understanding Jost
solutions of a nonlocal Schrödinger operator in its semi-classical limit.
",0,0,1,0,0,0
11160,"The Uranie platform: an Open-source software for optimisation, meta-modelling and uncertainty analysis","  The high-performance computing resources and the constant improvement of both
numerical simulation accuracy and the experimental measurements with which they
are confronted, bring a new compulsory step to strengthen the credence given to
the simulation results: uncertainty quantification. This can have different
meanings, according to the requested goals (rank uncertainty sources, reduce
them, estimate precisely a critical threshold or an optimal working point) and
it could request mathematical methods with greater or lesser complexity. This
paper introduces the Uranie platform, an Open-source framework which is
currently developed at the Alternative Energies and Atomic Energy Commission
(CEA), in the nuclear energy division, in order to deal with uncertainty
propagation, surrogate models, optimisation issues, code calibration... This
platform benefits from both its dependencies, but also from personal
developments, to offer an efficient data handling model, a C++ and Python
interpreter, advanced graphical tools, several parallelisation solutions...
These methods are very generic and can then be applied to many kinds of code
(as Uranie considers them as black boxes) so to many fields of physics as well.
In this paper, the example of thermal exchange between a plate-sheet and a
fluid is introduced to show how Uranie can be used to perform a large range of
analysis. The code used to produce the figures of this paper can be found in
this https URL along with the sources of the
platform.
",0,0,0,1,0,0
14635,Weak and smooth solutions for a fractional Yamabe flow: the case of general compact and locally conformally flat manifolds,"  As a counterpart of the classical Yamabe problem, a fractional Yamabe flow
has been introduced by Jin and Xiong (2014) on the sphere. Here we pursue its
study in the context of general compact smooth manifolds with positive
fractional curvature. First, we prove that the flow is locally well posed in
the weak sense on any compact manifold. If the manifold is locally conformally
flat with positive Yamabe invariant, we also prove that the flow is smooth and
converges to a constant scalar curvature metric. We provide different proofs
using extension properties introduced by Chang and González (2011) for the
conformally covariant fractional order operators.
",0,0,1,0,0,0
1399,Linear Time Clustering for High Dimensional Mixtures of Gaussian Clouds,"  Clustering mixtures of Gaussian distributions is a fundamental and
challenging problem that is ubiquitous in various high-dimensional data
processing tasks. While state-of-the-art work on learning Gaussian mixture
models has focused primarily on improving separation bounds and their
generalization to arbitrary classes of mixture models, less emphasis has been
paid to practical computational efficiency of the proposed solutions. In this
paper, we propose a novel and highly efficient clustering algorithm for $n$
points drawn from a mixture of two arbitrary Gaussian distributions in
$\mathbb{R}^p$. The algorithm involves performing random 1-dimensional
projections until a direction is found that yields a user-specified clustering
error $e$. For a 1-dimensional separation parameter $\gamma$ satisfying
$\gamma=Q^{-1}(e)$, the expected number of such projections is shown to be
bounded by $o(\ln p)$, when $\gamma$ satisfies $\gamma\leq
c\sqrt{\ln{\ln{p}}}$, with $c$ as the separability parameter of the two
Gaussians in $\mathbb{R}^p$. Consequently, the expected overall running time of
the algorithm is linear in $n$ and quasi-linear in $p$ at $o(\ln{p})O(np)$, and
the sample complexity is independent of $p$. This result stands in contrast to
prior works which provide polynomial, with at-best quadratic, running time in
$p$ and $n$. We show that our bound on the expected number of 1-dimensional
projections extends to the case of three or more Gaussian components, and we
present a generalization of our results to mixture distributions beyond the
Gaussian model.
",1,0,0,0,0,0
19978,TensorQuant - A Simulation Toolbox for Deep Neural Network Quantization,"  Recent research implies that training and inference of deep neural networks
(DNN) can be computed with low precision numerical representations of the
training/test data, weights and gradients without a general loss in accuracy.
The benefit of such compact representations is twofold: they allow a
significant reduction of the communication bottleneck in distributed DNN
training and faster neural network implementations on hardware accelerators
like FPGAs. Several quantization methods have been proposed to map the original
32-bit floating point problem to low-bit representations. While most related
publications validate the proposed approach on a single DNN topology, it
appears to be evident, that the optimal choice of the quantization method and
number of coding bits is topology dependent. To this end, there is no general
theory available, which would allow users to derive the optimal quantization
during the design of a DNN topology. In this paper, we present a quantization
tool box for the TensorFlow framework. TensorQuant allows a transparent
quantization simulation of existing DNN topologies during training and
inference. TensorQuant supports generic quantization methods and allows
experimental evaluation of the impact of the quantization on single layers as
well as on the full topology. In a first series of experiments with
TensorQuant, we show an analysis of fix-point quantizations of popular CNN
topologies.
",1,0,0,1,0,0
13983,Hot Phonon and Carrier Relaxation in Si(100) Determined by Transient Extreme Ultraviolet Spectroscopy,"  The thermalization of hot carriers and phonons gives direct insight into the
scattering processes that mediate electrical and thermal transport. Obtaining
the scattering rates for both hot carriers and phonons currently requires
multiple measurements with incommensurate timescales. Here, transient
extreme-ultraviolet (XUV) spectroscopy on the silicon 2p core level at 100 eV
is used to measure hot carrier and phonon thermalization in Si(100) from tens
of femtoseconds to 200 ps following photoexcitation of the indirect transition
to the {\Delta} valley at 800 nm. The ground state XUV spectrum is first
theoretically predicted using a combination of a single plasmon pole model and
the Bethe-Salpeter equation (BSE) with density functional theory (DFT). The
excited state spectrum is predicted by incorporating the electronic effects of
photo-induced state-filling, broadening, and band-gap renormalization into the
ground state XUV spectrum. A time-dependent lattice deformation and expansion
is also required to describe the excited state spectrum. The kinetics of these
structural components match the kinetics of phonons excited from the
electron-phonon and phonon-phonon scattering processes following
photoexcitation. Separating the contributions of electronic and structural
effects on the transient XUV spectra allows the carrier population, the
population of phonons involved in inter- and intra-valley electron-phonon
scattering, and the population of phonons involved in phonon-phonon scattering
to be quantified as a function of delay time.
",0,1,0,0,0,0
13310,Large Margin Learning in Set to Set Similarity Comparison for Person Re-identification,"  Person re-identification (Re-ID) aims at matching images of the same person
across disjoint camera views, which is a challenging problem in multimedia
analysis, multimedia editing and content-based media retrieval communities. The
major challenge lies in how to preserve similarity of the same person across
video footages with large appearance variations, while discriminating different
individuals. To address this problem, conventional methods usually consider the
pairwise similarity between persons by only measuring the point to point (P2P)
distance. In this paper, we propose to use deep learning technique to model a
novel set to set (S2S) distance, in which the underline objective focuses on
preserving the compactness of intra-class samples for each camera view, while
maximizing the margin between the intra-class set and inter-class set. The S2S
distance metric is consisted of three terms, namely the class-identity term,
the relative distance term and the regularization term. The class-identity term
keeps the intra-class samples within each camera view gathering together, the
relative distance term maximizes the distance between the intra-class class set
and inter-class set across different camera views, and the regularization term
smoothness the parameters of deep convolutional neural network (CNN). As a
result, the final learned deep model can effectively find out the matched
target to the probe object among various candidates in the video gallery by
learning discriminative and stable feature representations. Using the CUHK01,
CUHK03, PRID2011 and Market1501 benchmark datasets, we extensively conducted
comparative evaluations to demonstrate the advantages of our method over the
state-of-the-art approaches.
",1,0,0,1,0,0
11666,Ce 3$p$ hard x-ray photoelectron spectroscopy study of the topological Kondo insulator CeRu$_4$Sn$_6$,"  Bulk sensitive hard x-ray photoelectron spectroscopy data of the Ce 3$p$ core
level of CeRu$_4$Sn$_6$ are presented. Using a combination of full multiplet
and configuration iteration model we were able to obtain an accurate lineshape
analysis of the data, thereby taking into account correlations for the strong
plasmon intensities. We conclude that CeRu$_4$Sn$_6$ is a moderately mixed
valence compound with a weight of 8% for the Ce $f^0$ configuration in the
ground state.
",0,1,0,0,0,0
16675,Predicting the Gender of Indonesian Names,"  We investigated a way to predict the gender of a name using character-level
Long-Short Term Memory (char-LSTM). We compared our method with some
conventional machine learning methods, namely Naive Bayes, logistic regression,
and XGBoost with n-grams as the features. We evaluated the models on a dataset
consisting of the names of Indonesian people. It is not common to use a family
name as the surname in Indonesian culture, except in some ethnicities.
Therefore, we inferred the gender from both full names and first names. The
results show that we can achieve 92.25% accuracy from full names, while using
first names only yields 90.65% accuracy. These results are better than the ones
from applying the classical machine learning algorithms to n-grams.
",1,0,0,0,0,0
2768,Superconductivity at 33 - 37 K in $ALn_2$Fe$_4$As$_4$O$_2$ ($A$ = K and Cs; $Ln$ = Lanthanides),"  We have synthesized 10 new iron oxyarsenides, K$Ln_2$Fe$_4$As$_4$O$_2$ ($Ln$
= Gd, Tb, Dy, and Ho) and Cs$Ln_2$Fe$_4$As$_4$O$_2$ ($Ln$ = Nd, Sm, Gd, Tb, Dy,
and Ho), with the aid of lattice-match [between $A$Fe$_2$As$_2$ ($A$ = K and
Cs) and $Ln$FeAsO] approach. The resultant compounds possess hole-doped
conducting double FeAs layers, [$A$Fe$_4$As$_4$]$^{2-}$, that are separated by
the insulating [$Ln_2$O$_2$]$^{2+}$ slabs. Measurements of electrical
resistivity and dc magnetic susceptibility demonstrate bulk superconductivity
at $T_\mathrm{c}$ = 33 - 37 K. We find that $T_\mathrm{c}$ correlates with the
axis ratio $c/a$ for all 12442-type superconductors discovered. Also,
$T_\mathrm{c}$ tends to increase with the lattice mismatch, implying a role of
lattice instability for the enhancement of superconductivity.
",0,1,0,0,0,0
6247,A Dynamic Programming Principle for Distribution-Constrained Optimal Stopping,"  We consider an optimal stopping problem where a constraint is placed on the
distribution of the stopping time. Reformulating the problem in terms of
so-called measure-valued martingales allows us to transform the marginal
constraint into an initial condition and view the problem as a stochastic
control problem; we establish the corresponding dynamic programming principle.
",0,0,1,0,0,0
20259,First-Order vs. Second-Order Encodings for LTLf-to-Automata Translation,"  Translating formulas of Linear Temporal Logic (LTL) over finite traces, or
LTLf, to symbolic Deterministic Finite Automata (DFA) plays an important role
not only in LTLf synthesis, but also in synthesis for Safety LTL formulas. The
translation is enabled by using MONA, a powerful tool for symbolic, BDD-based,
DFA construction from logic specifications. Recent works used a first-order
encoding of LTLf formulas to translate LTLf to First Order Logic (FOL), which
is then fed to MONA to get the symbolic DFA. This encoding was shown to perform
well, but other encodings have not been studied. Specifically, the natural
question of whether second-order encoding, which has significantly simpler
quantificational structure, can outperform first-order encoding remained open.
In this paper we address this challenge and study second-order encodings for
LTLf formulas. We first introduce a specific MSO encoding that captures the
semantics of LTLf in a natural way and prove its correctness. We then explore
is a Compact MSO encoding, which benefits from automata-theoretic minimization,
thus suggesting a possible practical advantage. To that end, we propose a
formalization of symbolic DFA in second-order logic, thus developing a novel
connection between BDDs and MSO. We then show by empirical evaluations that the
first-order encoding does perform better than both second-order encodings. The
conclusion is that first-order encoding is a better choice than second-order
encoding in LTLf-to-Automata translation.
",1,0,0,0,0,0
11119,"Bombieri-Vinogradov for multiplicative functions, and beyond the $x^{1/2}$-barrier","  Part-and-parcel of the study of ""multiplicative number theory"" is the study
of the distribution of multiplicative functions in arithmetic progressions.
Although appropriate analogies to the Bombieri-Vingradov Theorem have been
proved for particular examples of multiplicative functions, there has not
previously been headway on a general theory; seemingly none of the different
proofs of the Bombieri-Vingradov Theorem for primes adapt well to this
situation. In this article we find out why such a result has been so elusive,
and discover what can be proved along these lines and develop some limitations.
For a fixed residue class $a$ we extend such averages out to moduli $\leq
x^{\frac {20}{39}-\delta}$.
",0,0,1,0,0,0
116,Gene regulatory network inference: an introductory survey,"  Gene regulatory networks are powerful abstractions of biological systems.
Since the advent of high-throughput measurement technologies in biology in the
late 90s, reconstructing the structure of such networks has been a central
computational problem in systems biology. While the problem is certainly not
solved in its entirety, considerable progress has been made in the last two
decades, with mature tools now available. This chapter aims to provide an
introduction to the basic concepts underpinning network inference tools,
attempting a categorisation which highlights commonalities and relative
strengths. While the chapter is meant to be self-contained, the material
presented should provide a useful background to the later, more specialised
chapters of this book.
",0,0,0,0,1,0
11914,Moment analysis of highway-traffic clearance distribution,"  To help with the planning of inter-vehicular communication networks, an
accurate understanding of traffic behavior and traffic phase transition is
required. We calculate inter-vehicle spacings from empirical data collected in
a multi-lane highway in California, USA. We calculate the correlation
coefficients for spacings between vehicles in individual lanes to show that the
flows are independent. We determine the first four moments for individual lanes
at regular time intervals, namely the mean, variance, skewness and kurtosis. We
follow the evolution of these moments as the traffic condition changes from the
low-density free flow to high-density congestion. We find that the higher
moments of inter-vehicle spacings have a well defined dependence on the mean
value. The variance of the spacing distribution monotonously increases with the
mean vehicle spacing. In contrast, our analysis suggests that the skewness and
kurtosis provide one of the most sensitive probes towards the search for the
critical points. We find two significant results. First, the kurtosis
calculated in different time intervals for different lanes varies smoothly with
the skewness. They share the same behavior with the skewness and kurtosis
calculated for probability density functions that depend on a single parameter.
Second, the skewness and kurtosis as functions of the mean intervehicle spacing
show sharp peaks at critical densities expected for transitions between
different traffic phases. The data show a considerable scatter near the peak
positions, which suggests that the critical behavior may depend on other
parameters in addition to the traffic density.
",0,1,0,0,0,0
4220,"Evolution of structure, magnetism and electronic transport in doped pyrochlore iridate Y$_2$Ir$_{2-x}$Ru$_{x}$O$_7$","  The interplay between spin-orbit coupling (SOC) and electron correlation
($U$) is considered for many exotic phenomena in iridium oxides. We have
investigated the evolution of structural, magnetic and electronic properties in
pyrochlore iridate Y$_2$Ir$_{2-x}$Ru$_{x}$O$_7$ where the substitution of Ru
has been aimed to tune this interplay. The Ru substitution does not introduce
any structural phase transition, however, we do observe an evolution of lattice
parameters with the doping level $x$. X-ray photoemission spectroscopy (XPS)
study indicates Ru adopts charge state of Ru$^{4+}$ and replaces the Ir$^{4+}$
accordingly. Magnetization data reveal both the onset of magnetic
irreversibility and the magnetic moment decreases with progressive substitution
of Ru. These materials show non-equilibrium low temperature magnetic state as
revealed by magnetic relaxation data. Interestingly, we find magnetic
relaxation rate increases with substitution of Ru. The electrical resistivity
shows an insulating behavior in whole temperature range, however, resistivity
decreases with substitution of Ru. Nature of electronic conduction has been
found to follow power-law behavior for all the materials.
",0,1,0,0,0,0
14214,Regression estimator for the tail index,"  Estimating the tail index parameter is one of the primal objectives in
extreme value theory. For heavy-tailed distributions the Hill estimator is the
most popular way to estimate the tail index parameter. Improving the Hill
estimator was aimed by recent works with different methods, for example by
using bootstrap, or Kolmogorov-Smirnov metric. These methods are asymptotically
consistent, but for tail index $\xi >1$ and smaller sample sizes the estimation
fails to approach the theoretical value for realistic sample sizes. In this
paper, we introduce a new empirical method, which can estimate high tail index
parameters well and might also be useful for relatively small sample sizes.
",0,0,1,1,0,0
17724,On the orders of the non-Frattini elements of a finite group,"  Let $G$ be a finite group and let $p_1,\dots,p_n$ be distinct primes. If $G$
contains an element of order $p_1\cdots p_n,$ then there is an element in $G$
which is not contained in the Frattini subgroup of $G$ and whose order is
divisible by $p_1\cdots p_n.$
",0,0,1,0,0,0
7923,Exact Tensor Completion from Sparsely Corrupted Observations via Convex Optimization,"  This paper conducts a rigorous analysis for provable estimation of
multidimensional arrays, in particular third-order tensors, from a random
subset of its corrupted entries. Our study rests heavily on a recently proposed
tensor algebraic framework in which we can obtain tensor singular value
decomposition (t-SVD) that is similar to the SVD for matrices, and define a new
notion of tensor rank referred to as the tubal rank. We prove that by simply
solving a convex program, which minimizes a weighted combination of tubal
nuclear norm, a convex surrogate for the tubal rank, and the $\ell_1$-norm, one
can recover an incoherent tensor exactly with overwhelming probability,
provided that its tubal rank is not too large and that the corruptions are
reasonably sparse. Interestingly, our result includes the recovery guarantees
for the problems of tensor completion (TC) and tensor principal component
analysis (TRPCA) under the same algebraic setup as special cases. An
alternating direction method of multipliers (ADMM) algorithm is presented to
solve this optimization problem. Numerical experiments verify our theory and
real-world applications demonstrate the effectiveness of our algorithm.
",1,0,0,1,0,0
3589,Origin of Operating Voltage Increase in InGaN-based Light-emitting Diodes under High Injection: Phase Space Filling Effect on Forward Voltage Characteristics,"  As an attempt to further elucidate the operating voltage increase in
InGaN-based light-emitting diodes (LEDs), the radiative and nonradiative
current components are separately analyzed in combination with the Shockley
diode equation. Through the analyses, we have shown that the increase in
operating voltage is caused by phase space filling effect in high injection. We
have also shown that the classical Shockley diode equation is insufficient to
comprehensively explain the I-V curve of the LED devices since the transport
and recombination characteristics of respective current components are
basically different. Hence, we have proposed a modified Shockley equation
suitable for modern LED devices. Our analysis gives a new insight on the cause
of the wall-plug-efficiency drop influenced by such factors as the efficiency
droop and the high operating voltage in InGaN LEDs.
",0,1,0,0,0,0
3116,A note on $p^λ$-convex set in a complete Riemannian manifold,"  In this paper we have generalized the notion of $\lambda$-radial contraction
in complete Riemannian manifold and developed the concept of $p^\lambda$-convex
function. We have also given a counter example proving the fact that in general
$\lambda$-radial contraction of a geodesic is not necessarily a geodesic. We
have also deduced some relations between geodesic convex sets and
$p^\lambda$-convex sets and showed that under certain conditions they are
equivalent.
",0,0,1,0,0,0
9569,"Belief Propagation, Bethe Approximation and Polynomials","  Factor graphs are important models for succinctly representing probability
distributions in machine learning, coding theory, and statistical physics.
Several computational problems, such as computing marginals and partition
functions, arise naturally when working with factor graphs. Belief propagation
is a widely deployed iterative method for solving these problems. However,
despite its significant empirical success, not much is known about the
correctness and efficiency of belief propagation.
Bethe approximation is an optimization-based framework for approximating
partition functions. While it is known that the stationary points of the Bethe
approximation coincide with the fixed points of belief propagation, in general,
the relation between the Bethe approximation and the partition function is not
well understood. It has been observed that for a few classes of factor graphs,
the Bethe approximation always gives a lower bound to the partition function,
which distinguishes them from the general case, where neither a lower bound,
nor an upper bound holds universally. This has been rigorously proved for
permanents and for attractive graphical models.
Here we consider bipartite normal factor graphs and show that if the local
constraints satisfy a certain analytic property, the Bethe approximation is a
lower bound to the partition function. We arrive at this result by viewing
factor graphs through the lens of polynomials. In this process, we reformulate
the Bethe approximation as a polynomial optimization problem. Our sufficient
condition for the lower bound property to hold is inspired by recent
developments in the theory of real stable polynomials. We believe that this way
of viewing factor graphs and its connection to real stability might lead to a
better understanding of belief propagation and factor graphs in general.
",1,0,0,1,0,0
16898,Size Constraints on Majorana Beamsplitter Interferometer: Majorana Coupling and Surface-Bulk Scattering,"  Topological insulator surfaces in proximity to superconductors have been
proposed as a way to produce Majorana fermions in condensed matter physics. One
of the simplest proposed experiments with such a system is Majorana
interferometry. Here, we consider two possibly conflicting constraints on the
size of such an interferometer. Coupling of a Majorana mode from the edge (the
arms) of the interferometer to vortices in the centre of the device sets a
lower bound on the size of the device. On the other hand, scattering to the
usually imperfectly insulating bulk sets an upper bound. From estimates of
experimental parameters, we find that typical samples may have no size window
in which the Majorana interferometer can operate, implying that a new
generation of more highly insulating samples must be explored.
",0,1,0,0,0,0
7701,Torsion of elliptic curves and unlikely intersections,"  We study effective versions of unlikely intersections of images of torsion
points of elliptic curves on the projective line.
",0,0,1,0,0,0
8850,"Absorption probabilities for Gaussian polytopes, and regular spherical simplices","  The Gaussian polytope $\mathcal P_{n,d}$ is the convex hull of $n$
independent standard normally distributed points in $\mathbb R^d$. We derive
explicit expressions for the probability that $\mathcal P_{n,d}$ contains a
fixed point $x\in\mathbb R^d$ as a function of the Euclidean norm of $x$, and
the probability that $\mathcal P_{n,d}$ contains the point $\sigma X$, where
$\sigma\geq 0$ is constant and $X$ is a standard normal vector independent of
$\mathcal P_{n,d}$. As a by-product, we also compute the expected number of
$k$-faces and the expected volume of $\mathcal P_{n,d}$, thus recovering the
results of Affentranger and Schneider [Discr. and Comput. Geometry, 1992] and
Efron [Biometrika, 1965], respectively. All formulas are in terms of the
volumes of regular spherical simplices, which, in turn, can be expressed
through the standard normal distribution function $\Phi(z)$ and its complex
version $\Phi(iz)$. The main tool used in the proofs is the conic version of
the Crofton formula.
",0,0,1,0,0,0
19173,"The Alexandrov-Fenchel type inequalities, revisited","  Various Alexandrov-Fenchel type inequalities have appeared and played
important roles in convex geometry, matrix theory and complex algebraic
geometry. It has been noticed for some time that they share some striking
analogies and have intimate relationships. The purpose of this article is to
shed new light on this by comparatively investigating them in several aspects.
\emph{The principal result} in this article is a complete solution to the
equality characterization problem of various Alexandrov-Fenchel type
inequalities for intersection numbers of nef and big classes on compact
Kähler manifolds, extending earlier results of Boucksom-Favre-Jonsson,
Fu-Xiao and Xiao-Lehmann. Our proof combines a result of Dinh-Nguyên on
Kähler geometry and an idea in convex geometry tracing back to Shephard. In
addition to this central result, we also give a geometric proof of the complex
version of the Alexandrov-Fenchel type inequality for mixed discriminants and a
determinantal type generalization of various Alexandrov-Fenchel type
inequalities.
",0,0,1,0,0,0
17472,DICOD: Distributed Convolutional Sparse Coding,"  In this paper, we introduce DICOD, a convolutional sparse coding algorithm
which builds shift invariant representations for long signals. This algorithm
is designed to run in a distributed setting, with local message passing, making
it communication efficient. It is based on coordinate descent and uses locally
greedy updates which accelerate the resolution compared to greedy coordinate
selection. We prove the convergence of this algorithm and highlight its
computational speed-up which is super-linear in the number of cores used. We
also provide empirical evidence for the acceleration properties of our
algorithm compared to state-of-the-art methods.
",1,0,0,1,0,0
3700,Dynamics of Charged Bulk Viscous Collapsing Cylindrical Source With Heat Flux,"  In this paper, we have explored the effects of dissipation on the dynamics of
charged bulk viscous collapsing cylindrical source which allows the out follow
of heat flux in the form of radiations. Misner-Sharp formulism has been
implemented to drive the dynamical equation in term of proper time and radial
derivatives. We have investigated the effects of charge and bulk viscosity on
the dynamics of collapsing cylinder. To determine the effects of radial heat
flux, we have formulated the heat transport equations in the context of
M$\ddot{u}$ller-Israel-Stewart theory by assuming that thermodynamics
viscous/heat coupling coefficients can be neglected within some approximations.
In our discussion, we have introduced the viscosity by the standard
(non-casual) thermodynamics approach. The dynamical equations have been coupled
with the heat transport equation equation, the consequences of resulting
coupled heat equation have been analyzed in detail.
",0,1,0,0,0,0
2120,On the universality of anomalous scaling exponents of structure functions in turbulent flows,"  All previous experiments in open turbulent flows (e.g. downstream of grids,
jet and atmospheric boundary layer) have produced quantitatively consistent
values for the scaling exponents of velocity structure functions. The only
measurement in closed turbulent flow (von Kármán swirling flow) using
Taylor-hypothesis, however, produced scaling exponents that are significantly
smaller, suggesting that the universality of these exponents are broken with
respect to change of large scale geometry of the flow. Here, we report
measurements of longitudinal structure functions of velocity in a von
Kármán setup without the use of Taylor-hypothesis. The measurements are
made using Stereo Particle Image Velocimetry at 4 different ranges of spatial
scales, in order to observe a combined inertial subrange spanning roughly one
and a half order of magnitude. We found scaling exponents (up to 9th order)
that are consistent with values from open turbulent flows, suggesting that they
might be in fact universal.
",0,1,0,0,0,0
10471,Designing Deterministic Polynomial-Space Algorithms by Color-Coding Multivariate Polynomials,"  In recent years, several powerful techniques have been developed to design
{\em randomized} polynomial-space parameterized algorithms. In this paper, we
introduce an enhancement of color coding to design deterministic
polynomial-space parameterized algorithms. Our approach aims at reducing the
number of random choices by exploiting the special structure of a solution.
Using our approach, we derive the following deterministic algorithms (see
Introduction for problem definitions).
1. Polynomial-space $O^*(3.86^k)$-time (exponential-space $O^*(3.41^k)$-time)
algorithm for {\sc $k$-Internal Out-Branching}, improving upon the previously
fastest {\em exponential-space} $O^*(5.14^k)$-time algorithm for this problem.
2. Polynomial-space $O^*((2e)^{k+o(k)})$-time (exponential-space
$O^*(4.32^k)$-time) algorithm for {\sc $k$-Colorful Out-Branching} on
arc-colored digraphs and {\sc $k$-Colorful Perfect Matching} on planar
edge-colored graphs.
To obtain our polynomial space algorithms, we show that $(n,k,\alpha
k)$-splitters ($\alpha\ge 1$) and in particular $(n,k)$-perfect hash families
can be enumerated one by one with polynomial delay.
",1,0,0,0,0,0
13178,Decentralized Optimal Control for Connected Automated Vehicles at Intersections Including Left and Right Turns,"  In prior work, we addressed the problem of optimally controlling on line
connected and automated vehicles crossing two adjacent intersections in an
urban area to minimize fuel consumption while achieving maximal throughput
without any explicit traffic signaling and without considering left and right
turns. In this paper, we extend the solution of this problem to account for
left and right turns under hard safety constraints. Furthermore, we formulate
and solve another optimization problem to minimize a measure of passenger
discomfort while the vehicle turns at the intersection and we investigate the
associated tradeoff between minimizing fuel consumption and passenger
discomfort.
",0,0,1,0,0,0
12164,Power-of-$d$-Choices with Memory: Fluid Limit and Optimality,"  In multi-server distributed queueing systems, the access of stochastically
arriving jobs to resources is often regulated by a dispatcher, also known as
load balancer. A fundamental problem consists in designing a load balancing
algorithm that minimizes the delays experienced by jobs. During the last two
decades, the power-of-$d$-choice algorithm, based on the idea of dispatching
each job to the least loaded server out of $d$ servers randomly sampled at the
arrival of the job itself, has emerged as a breakthrough in the foundations of
this area due to its versatility and appealing asymptotic properties. In this
paper, we consider the power-of-$d$-choice algorithm with the addition of a
local memory that keeps track of the latest observations collected over time on
the sampled servers. Then, each job is sent to a server with the lowest
observation. We show that this algorithm is asymptotically optimal in the sense
that the load balancer can always assign each job to an idle server in the
large-server limit. This holds true if and only if the system load $\lambda$ is
less than $1-\frac{1}{d}$. If this condition is not satisfied, we show that
queue lengths are bounded by $j^\star+1$, where $j^\star\in\mathbb{N}$ is given
by the solution of a polynomial equation. This is in contrast with the classic
version of the power-of-$d$-choice algorithm, where queue lengths are
unbounded. Our upper bound on the size of the most loaded server, $j^*+1$, is
tight and increases slowly when $\lambda$ approaches its critical value from
below. For instance, when $\lambda= 0.995$ and $d=2$ (respectively, $d=3$), we
find that no server will contain more than just $5$ ($3$) jobs in equilibrium.
Our results quantify and highlight the importance of using memory as a means to
enhance performance in randomized load balancing.
",1,0,0,0,0,0
12982,Symmetric Convex Sets with Minimal Gaussian Surface Area,"  Let $\Omega\subset\mathbb{R}^{n+1}$ have minimal Gaussian surface area among
all sets satisfying $\Omega=-\Omega$ with fixed Gaussian volume. Let $A=A_{x}$
be the second fundamental form of $\partial\Omega$ at $x$, i.e. $A$ is the
matrix of first order partial derivatives of the unit normal vector at
$x\in\partial\Omega$. For any $x=(x_{1},\ldots,x_{n+1})\in\mathbb{R}^{n+1}$,
let $\gamma_{n}(x)=(2\pi)^{-n/2}e^{-(x_{1}^{2}+\cdots+x_{n+1}^{2})/2}$. Let
$\|A\|^{2}$ be the sum of the squares of the entries of $A$, and let
$\|A\|_{2\to 2}$ denote the $\ell_{2}$ operator norm of $A$.
It is shown that if $\Omega$ or $\Omega^{c}$ is convex, and if either
$$\int_{\partial\Omega}(\|A_{x}\|^{2}-1)\gamma_{n}(x)dx>0\qquad\mbox{or}\qquad
\int_{\partial\Omega}\Big(\|A_{x}\|^{2}-1+2\sup_{y\in\partial\Omega}\|A_{y}\|_{2\to
2}^{2}\Big)\gamma_{n}(x)dx<0,$$ then $\partial\Omega$ must be a round cylinder.
That is, except for the case that the average value of $\|A\|^{2}$ is slightly
less than $1$, we resolve the convex case of a question of Barthe from 2001.
The main tool is the Colding-Minicozzi theory for Gaussian minimal surfaces,
which studies eigenfunctions of the Ornstein-Uhlenbeck type operator $L=
\Delta-\langle x,\nabla \rangle+\|A\|^{2}+1$ associated to the surface
$\partial\Omega$. A key new ingredient is the use of a randomly chosen degree 2
polynomial in the second variation formula for the Gaussian surface area. Our
actual results are a bit more general than the above statement. Also, some of
our results hold without the assumption of convexity.
",1,0,1,0,0,0
7705,A nested sampling code for targeted searches for continuous gravitational waves from pulsars,"  This document describes a code to perform parameter estimation and model
selection in targeted searches for continuous gravitational waves from known
pulsars using data from ground-based gravitational wave detectors. We describe
the general workings of the code and characterise it on simulated data
containing both noise and simulated signals. We also show how it performs
compared to a previous MCMC and grid-based approach to signal parameter
estimation. Details how to run the code in a variety of cases are provided in
Appendix A.
",0,1,0,0,0,0
7498,Non-Generic Unramified Representations in Metaplectic Covering Groups,"  Let $G^{(r)}$ denote the metaplectic covering group of the linear algebraic
group $G$. In this paper we study conditions on unramified representations of
the group $G^{(r)}$ not to have a nonzero Whittaker function. We state a
general Conjecture about the possible unramified characters $\chi$ such that
the unramified sub-representation of
$Ind_{B^{(r)}}^{G^{(r)}}\chi\delta_B^{1/2}$ will have no nonzero Whittaker
function. We prove this Conjecture for the groups $GL_n^{(r)}$ with $r\ge n-1$,
and for the exceptional groups $G_2^{(r)}$ when $r\ne 2$.
",0,0,1,0,0,0
15192,Maximizing the Mutual Information of Multi-Antenna Links Under an Interfered Receiver Power Constraint,"  Single-user multiple-input / multiple-output (SU-MIMO) communication systems
have been successfully used over the years and have provided a significant
increase on a wireless link's capacity by enabling the transmission of multiple
data streams. Assuming channel knowledge at the transmitter, the maximization
of the mutual information of a MIMO link is achieved by finding the optimal
power allocation under a given sum-power constraint, which is in turn obtained
by the water-filling (WF) algorithm. However, in spectrum sharing setups, such
as Licensed Shared Access (LSA), where a primary link (PL) and a secondary link
(SL) coexist, the power transmitted by the SL transmitter may induce harmful
interference to the PL receiver. While such co-existing links have been
considered extensively in various spectrum sharing setups, the mutual
information of the SL under a constraint on the interference it may cause to
the PL receiver has, quite astonishingly, not been evaluated so far. In this
paper, we solve this problem, find its unique optimal solution and provide the
power allocation policy and corresponding precoding solution that achieves the
optimal capacity under the imposed constraint. The performance of the optimal
solution and the penalty due to the interference constraint are evaluated over
some indicative Rayleigh fading channel conditions and interference thresholds.
We believe that the obtained results are of general nature and that they may
apply, beyond spectrum sharing, to a variety of applications that admit a
similar setup.
",1,0,0,0,0,0
19872,HPDedup: A Hybrid Prioritized Data Deduplication Mechanism for Primary Storage in the Cloud,"  Eliminating duplicate data in primary storage of clouds increases the
cost-efficiency of cloud service providers as well as reduces the cost of users
for using cloud services. Existing primary deduplication techniques either use
inline caching to exploit locality in primary workloads or use post-processing
deduplication running in system idle time to avoid the negative impact on I/O
performance. However, neither of them works well in the cloud servers running
multiple services or applications for the following two reasons: Firstly, the
temporal locality of duplicate data writes may not exist in some primary
storage workloads thus inline caching often fails to achieve good deduplication
ratio. Secondly, the post-processing deduplication allows duplicate data to be
written into disks, therefore does not provide the benefit of I/O deduplication
and requires high peak storage capacity. This paper presents HPDedup, a Hybrid
Prioritized data Deduplication mechanism to deal with the storage system shared
by applications running in co-located virtual machines or containers by fusing
an inline and a post-processing process for exact deduplication. In the inline
deduplication phase, HPDedup gives a fingerprint caching mechanism that
estimates the temporal locality of duplicates in data streams from different
VMs or applications and prioritizes the cache allocation for these streams
based on the estimation. HPDedup also allows different deduplication threshold
for streams based on their spatial locality to reduce the disk fragmentation.
The post-processing phase removes duplicates whose fingerprints are not able to
be cached due to the weak temporal locality from disks. Our experimental
results show that HPDedup clearly outperforms the state-of-the-art primary
storage deduplication techniques in terms of inline cache efficiency and
primary deduplication efficiency.
",1,0,0,0,0,0
9007,Variational Inference for Data-Efficient Model Learning in POMDPs,"  Partially observable Markov decision processes (POMDPs) are a powerful
abstraction for tasks that require decision making under uncertainty, and
capture a wide range of real world tasks. Today, effective planning approaches
exist that generate effective strategies given black-box models of a POMDP
task. Yet, an open question is how to acquire accurate models for complex
domains. In this paper we propose DELIP, an approach to model learning for
POMDPs that utilizes amortized structured variational inference. We empirically
show that our model leads to effective control strategies when coupled with
state-of-the-art planners. Intuitively, model-based approaches should be
particularly beneficial in environments with changing reward structures, or
where rewards are initially unknown. Our experiments confirm that DELIP is
particularly effective in this setting.
",0,0,0,1,0,0
3003,Mechanism of the double heterostructure TiO2/ZnO/TiO2 for photocatalytic and photovoltaic applications: A theoretical study,"  Understanding the mechanism of the heterojunction is an important step
towards controllable and tunable interfaces for photocatalytic and photovoltaic
based devices. To this aim, we propose a thorough study of a double
heterostructure system consisting of two semiconductors with large band gap,
namely, wurtzite ZnO and anatase TiO2. We demonstrate via first-principle
calculations two stable configurations of ZnO/TiO2 interfaces. Our structural
analysis provides a key information on the nature of the complex interface and
lattice distortions occurring when combining these materials. The study of the
electronic properties of the sandwich nanostructure TiO2/ZnO/TiO2 reveals that
conduction band arises mainly from Ti3d orbitals, while valence band is
maintained by O2p of ZnO, and that the trapped states within the gap region
frequent in single heterostructure are substantially reduced in the double
interface system. Moreover, our work explains the origin of certain optical
transitions observed in the experimental studies. Unexpectedly, as a
consequence of different bond distortions, the results on the band alignments
show electron accumulation in the left shell of TiO2 rather than the right one.
Such behavior provides more choice for the sensitization and functionalization
of TiO2 surfaces.
",0,1,0,0,0,0
7539,Multi-speaker Recognition in Cocktail Party Problem,"  This paper proposes an original statistical decision theory to accomplish a
multi-speaker recognition task in cocktail party problem. This theory relies on
an assumption that the varied frequencies of speakers obey Gaussian
distribution and the relationship of their voiceprints can be represented by
Euclidean distance vectors. This paper uses Mel-Frequency Cepstral Coefficients
to extract the feature of a voice in judging whether a speaker is included in a
multi-speaker environment and distinguish who the speaker should be. Finally, a
thirteen-dimension constellation drawing is established by mapping from
Manhattan distances of speakers in order to take a thorough consideration about
gross influential factors.
",1,0,0,0,0,0
1338,"p-FP: Extraction, Classification, and Prediction of Website Fingerprints with Deep Learning","  Recent advances in learning Deep Neural Network (DNN) architectures have
received a great deal of attention due to their ability to outperform
state-of-the-art classifiers across a wide range of applications, with little
or no feature engineering. In this paper, we broadly study the applicability of
deep learning to website fingerprinting. We show that unsupervised DNNs can be
used to extract low-dimensional feature vectors that improve the performance of
state-of-the-art website fingerprinting attacks. When used as classifiers, we
show that they can match or exceed performance of existing attacks across a
range of application scenarios, including fingerprinting Tor website traces,
fingerprinting search engine queries over Tor, defeating fingerprinting
defenses, and fingerprinting TLS-encrypted websites. Finally, we show that DNNs
can be used to predict the fingerprintability of a website based on its
contents, achieving 99% accuracy on a data set of 4500 website downloads.
",1,0,0,1,0,0
16322,"Comment on ""Eshelby twist and correlation effects in diffraction from nanocrystals"" [J. Appl. Phys. 117, 164304 (2015)]","  The aim of this comment is to show that anisotropic effects and image fields
should not be omitted as they are in the publication of A. Leonardi, S. Ryu, N.
M. Pugno, and P. Scardi (LRPS) [J. Appl. Phys. 117, 164304 (2015)] on Pd <011>
cylindrical nanowires containing an axial screw dislocation. Indeed, according
to our previous study [Phys. Rev. B 88, 224101 (2013)], the axial displacement
field along the nanowire exhibits both a radial and an azimuthal dependence
with a twofold symmetry due the <011> orientation. As a consequence, the
deviatoric strain term used by LRPS is not suitable to analyze the anisotropic
strain fields that should be observed in their atomistic simulations. In this
comment, we first illustrate the importance of anisotropy in <011> Pd nanowire
by calculating the azimuthal dependence of the deviatoric strain term. Then the
expression of the anisotropic elastic field is recalled in term of strain
tensor components to show that image fields should be also considered. The
other aspect of this comment concerns the supposedly loss of correlation along
the nanorod caused by the twist. It is claimed for instance by LRPS that : ""As
an effect of the dislocation strain and twist, if the cylinder is long enough,
upper/lower regions tend to lose correlation, as if the rod were made of
different sub-domains."". This assertion appears to us misleading since for any
twist the position of all the atoms in the nanorod is perfectly defined and
therefore prevents any loss of correlation. To clarify this point, it should be
specified that this apparent loss of correlation can not be ascribed to the
twisted state of the nanowire but is rather due to a limitation of the X-ray
powder diffraction. Considering for instance coherent X-ray diffraction, we
show an example of high twist where the simulated diffractogram presents a
clear signature of the perfect correlation.
",0,1,0,0,0,0
11082,Uncountable realtime probabilistic classes,"  We investigate the minimum cases for realtime probabilistic machines that can
define uncountably many languages with bounded error. We show that logarithmic
space is enough for realtime PTMs on unary languages. On binary case, we follow
the same result for double logarithmic space, which is tight. When replacing
the worktape with some limited memories, we can follow uncountable results on
unary languages for two counters.
",1,0,0,0,0,0
3490,Semi-Supervised Overlapping Community Finding based on Label Propagation with Pairwise Constraints,"  Algorithms for detecting communities in complex networks are generally
unsupervised, relying solely on the structure of the network. However, these
methods can often fail to uncover meaningful groupings that reflect the
underlying communities in the data, particularly when those structures are
highly overlapping. One way to improve the usefulness of these algorithms is by
incorporating additional background information, which can be used as a source
of constraints to direct the community detection process. In this work, we
explore the potential of semi-supervised strategies to improve algorithms for
finding overlapping communities in networks. Specifically, we propose a new
method, based on label propagation, for finding communities using a limited
number of pairwise constraints. Evaluations on synthetic and real-world
datasets demonstrate the potential of this approach for uncovering meaningful
community structures in cases where each node can potentially belong to more
than one community.
",1,0,0,0,0,0
11496,An Operational Framework for Specifying Memory Models using Instantaneous Instruction Execution,"  There has been great progress recently in formally specifying the memory
model of microprocessors like ARM and POWER. These specifications are, however,
too complicated for reasoning about program behaviors, verifying compilers
etc., because they involve microarchitectural details like the reorder buffer
(ROB), partial and speculative execution, instruction replay on speculation
failure, etc. In this paper we present a new Instantaneous Instruction
Execution (I2E) framework which allows us to specify weak memory models in the
same style as SC and TSO. Each instruction in I2E is executed instantaneously
and in-order such that the state of the processor is always correct. The effect
of instruction reordering is captured by the way data is moved between the
processors and the memory non-deterministically, using three conceptual
devices: invalidation buffers, timestamps and dynamic store buffers. We prove
that I2E models capture the behaviors of modern microarchitectures and
cache-coherent memory systems accurately, thus eliminating the need to think
about microarchitectural details.
",1,0,0,0,0,0
16427,Advanced Satellite-based Frequency Transfer at the 10^{-16} Level,"  Advanced satellite-based frequency transfers by TWCP and IPPP have been
performed between NICT and KRISS. We confirm that the disagreement between them
is less than 1x10^{-16} at an averaging time of several days. Additionally, an
intercontinental frequency ratio measurement of Sr and Yb optical lattice
clocks was directly performed by TWCP. We achieved an uncertainty at the
mid-10^{-16} level after a total measurement time of 12 hours. The frequency
ratio was consistent with the recently reported values within the uncertainty.
",0,1,0,0,0,0
13109,Solving (most) of a set of quadratic equalities: Composite optimization for robust phase retrieval,"  We develop procedures, based on minimization of the composition $f(x) =
h(c(x))$ of a convex function $h$ and smooth function $c$, for solving random
collections of quadratic equalities, applying our methodology to phase
retrieval problems. We show that the prox-linear algorithm we develop can solve
phase retrieval problems---even with adversarially faulty measurements---with
high probability as soon as the number of measurements $m$ is a constant factor
larger than the dimension $n$ of the signal to be recovered. The algorithm
requires essentially no tuning---it consists of solving a sequence of convex
problems---and it is implementable without any particular assumptions on the
measurements taken. We provide substantial experiments investigating our
methods, indicating the practical effectiveness of the procedures and showing
that they succeed with high probability as soon as $m / n \ge 2$ when the
signal is real-valued.
",0,0,1,1,0,0
8257,A Grouping Genetic Algorithm for Joint Stratification and Sample Allocation Designs,"  Predicting the cheapest sample size for the optimal stratification in
multivariate survey design is a problem in cases where the population frame is
large. A solution exists that iteratively searches for the minimum sample size
necessary to meet accuracy constraints in partitions of atomic strata created
by the Cartesian product of auxiliary variables into larger strata. The optimal
stratification can be found by testing all possible partitions. However the
number of possible partitions grows exponentially with the number of initial
strata. There are alternative ways of modelling this problem, one of the most
natural is using Genetic Algorithms (GA). These evolutionary algorithms use
recombination, mutation and selection to search for optimal solutions. They
often converge on optimal or near-optimal solution more quickly than exact
methods. We propose a new GA approach to this problem using grouping genetic
operators instead of traditional operators. The results show a significant
improvement in solution quality for similar computational effort, corresponding
to large monetary savings.
",0,0,0,1,0,0
1434,Geometric counting on wavefront real spherical spaces,"  We provide $L^p$-versus $L^\infty$-bounds for eigenfunctions on a real
spherical space $Z$ of wavefront type. It is shown that these bounds imply a
non-trivial error term estimate for lattice counting on $Z$. The paper also
serves as an introduction to geometric counting on spaces of the mentioned
type. Section 7 on higher rank is new and extends the result from v1 to higher
rank. Final version. To appear in Acta Math. Sinica.
",0,0,1,0,0,0
1871,Structured Connectivity Augmentation,"  We initiate the algorithmic study of the following ""structured augmentation""
question: is it possible to increase the connectivity of a given graph G by
superposing it with another given graph H? More precisely, graph F is the
superposition of G and H with respect to injective mapping \phi: V(H)->V(G) if
every edge uv of F is either an edge of G, or \phi^{-1}(u)\phi^{-1}(v) is an
edge of H. We consider the following optimization problem. Given graphs G,H,
and a weight function \omega assigning non-negative weights to pairs of
vertices of V(G), the task is to find \varphi of minimum weight
\omega(\phi)=\sum_{xy\in E(H)}\omega(\phi(x)\varphi(y)) such that the edge
connectivity of the superposition F of G and H with respect to \phi is higher
than the edge connectivity of G. Our main result is the following ""dichotomy""
complexity classification. We say that a class of graphs C has bounded
vertex-cover number, if there is a constant t depending on C only such that the
vertex-cover number of every graph from C does not exceed t. We show that for
every class of graphs C with bounded vertex-cover number, the problems of
superposing into a connected graph F and to 2-edge connected graph F, are
solvable in polynomial time when H\in C. On the other hand, for any hereditary
class C with unbounded vertex-cover number, both problems are NP-hard when H\in
C. For the unweighted variants of structured augmentation problems, i.e. the
problems where the task is to identify whether there is a superposition of
graphs of required connectivity, we provide necessary and sufficient
combinatorial conditions on the existence of such superpositions. These
conditions imply polynomial time algorithms solving the unweighted variants of
the problems.
",1,0,0,0,0,0
16676,Exploring Students Blended Learning Through Social Media,"  Information technology (IT) has been used widely in many aspects of our daily
life. After discuss politics related aspects for some articles. In this article
author would like to discuss social media for students learning environment.
Social media as a leading application on the internet has changed many aspects
of life become more globalized. This article discusses the use of social media
to support learning activities for students in the faculty of computer science.
The author uses Facebook and WordPress as an alternative to electronic
learning: 1) online attendance tool, 2) media storage and dissemination of
course materials, 3) event scheduling for the lectures. Social media succeed to
change the way of modern learning styles and environment. The results of this
study are some learning activities such as : 1) Preparation, 2) Weekly meeting
activities, 3) Course Page, 4) Social Media as Online Attendance Tool, 5)
Social Media as Learning Repository and Dissemination, and 6) Social Media as
Online Event Scheduling.
",1,0,0,0,0,0
18891,On conditional least squares estimation for affine diffusions based on continuous time observations,"  We study asymptotic properties of conditional least squares estimators for
the drift parameters of two-factor affine diffusions based on continuous time
observations. We distinguish three cases: subcritical, critical and
supercritical. For all the drift parameters, in the subcritical and
supercritical cases, asymptotic normality and asymptotic mixed normality is
proved, while in the critical case, non-standard asymptotic behavior is
described.
",0,0,1,1,0,0
20792,Detecting Oriented Text in Natural Images by Linking Segments,"  Most state-of-the-art text detection methods are specific to horizontal Latin
text and are not fast enough for real-time applications. We introduce Segment
Linking (SegLink), an oriented text detection method. The main idea is to
decompose text into two locally detectable elements, namely segments and links.
A segment is an oriented box covering a part of a word or text line; A link
connects two adjacent segments, indicating that they belong to the same word or
text line. Both elements are detected densely at multiple scales by an
end-to-end trained, fully-convolutional neural network. Final detections are
produced by combining segments connected by links. Compared with previous
methods, SegLink improves along the dimensions of accuracy, speed, and ease of
training. It achieves an f-measure of 75.0% on the standard ICDAR 2015
Incidental (Challenge 4) benchmark, outperforming the previous best by a large
margin. It runs at over 20 FPS on 512x512 images. Moreover, without
modification, SegLink is able to detect long lines of non-Latin text, such as
Chinese.
",1,0,0,0,0,0
13553,The effect of an offset polar cap dipolar magnetic field on the modeling of the Vela pulsar's $γ$-ray light curves,"  We performed geometric pulsar light curve modeling using static, retarded
vacuum, and offset polar cap (PC) dipole $B$-fields (the latter is
characterized by a parameter $\epsilon$), in conjunction with standard two-pole
caustic (TPC) and outer gap (OG) emission geometries. The offset-PC dipole
$B$-field mimics deviations from the static dipole (which corresponds to
$\epsilon=0$). In addition to constant-emissivity geometric models, we also
considered a slot gap (SG) $E$-field associated with the offset-PC dipole
$B$-field and found that its inclusion leads to qualitatively different light
curves. Solving the particle transport equation shows that the particle energy
only becomes large enough to yield significant curvature radiation at large
altitudes above the stellar surface, given this relatively low $E$-field.
Therefore, particles do not always attain the radiation-reaction limit. Our
overall optimal light curve fit is for the retarded vacuum dipole field and OG
model, at an inclination angle $\alpha=78{_{-1}^{+1}}^{\circ}$ and observer
angle $\zeta=69{_{-1}^{+2}}^{\circ}$. For this $B$-field, the TPC model is
statistically disfavored compared to the OG model. For the static dipole field,
neither model is significantly preferred. We found that smaller values of
$\epsilon$ are favored for the offset-PC dipole field when assuming constant
emissivity, and larger $\epsilon$ values favored for variable emissivity, but
not significantly so. When multiplying the SG $E$-field by a factor of 100, we
found improved light curve fits, with $\alpha$ and $\zeta$ being closer to best
fits from independent studies, as well as curvature radiation reaction at lower
altitudes.
",0,1,0,0,0,0
17306,Actively Learning what makes a Discrete Sequence Valid,"  Deep learning techniques have been hugely successful for traditional
supervised and unsupervised machine learning problems. In large part, these
techniques solve continuous optimization problems. Recently however, discrete
generative deep learning models have been successfully used to efficiently
search high-dimensional discrete spaces. These methods work by representing
discrete objects as sequences, for which powerful sequence-based deep models
can be employed. Unfortunately, these techniques are significantly hindered by
the fact that these generative models often produce invalid sequences. As a
step towards solving this problem, we propose to learn a deep recurrent
validator model. Given a partial sequence, our model learns the probability of
that sequence occurring as the beginning of a full valid sequence. Thus this
identifies valid versus invalid sequences and crucially it also provides
insight about how individual sequence elements influence the validity of
discrete objects. To learn this model we propose an approach inspired by
seminal work in Bayesian active learning. On a synthetic dataset, we
demonstrate the ability of our model to distinguish valid and invalid
sequences. We believe this is a key step toward learning generative models that
faithfully produce valid discrete objects.
",1,0,0,1,0,0
18600,General purpose graphics-processing-unit implementation of cosmological domain wall network evolution,"  Topological defects unavoidably form at symmetry breaking phase transitions
in the early Universe. To probe the parameter space of theoretical models and
set tighter experimental constraints (exploiting the recent advances in
astrophysical observations), one requires more and more demanding simulations,
and therefore more hardware resources and computation time. Improving the speed
and efficiency of existing codes is essential. Here we present a General
Purpose Graphics Processing Unit implementation of the canonical
Press-Ryden-Spergel algorithm for the evolution of cosmological domain wall
networks. This is ported to the Open Computing Language standard, and as a
consequence significant speed-ups are achieved both in 2D and 3D simulations.
",0,1,0,0,0,0
14414,"Brief Notes on Hard Takeoff, Value Alignment, and Coherent Extrapolated Volition","  I make some basic observations about hard takeoff, value alignment, and
coherent extrapolated volition, concepts which have been central in analyses of
superintelligent AI systems.
",1,0,0,0,0,0
16624,Efficient Use of Limited-Memory Accelerators for Linear Learning on Heterogeneous Systems,"  We propose a generic algorithmic building block to accelerate training of
machine learning models on heterogeneous compute systems. Our scheme allows to
efficiently employ compute accelerators such as GPUs and FPGAs for the training
of large-scale machine learning models, when the training data exceeds their
memory capacity. Also, it provides adaptivity to any system's memory hierarchy
in terms of size and processing speed. Our technique is built upon novel
theoretical insights regarding primal-dual coordinate methods, and uses duality
gap information to dynamically decide which part of the data should be made
available for fast processing. To illustrate the power of our approach we
demonstrate its performance for training of generalized linear models on a
large-scale dataset exceeding the memory size of a modern GPU, showing an
order-of-magnitude speedup over existing approaches.
",1,0,0,1,0,0
15131,Polynomial Time and Sample Complexity for Non-Gaussian Component Analysis: Spectral Methods,"  The problem of Non-Gaussian Component Analysis (NGCA) is about finding a
maximal low-dimensional subspace $E$ in $\mathbb{R}^n$ so that data points
projected onto $E$ follow a non-gaussian distribution. Although this is an
appropriate model for some real world data analysis problems, there has been
little progress on this problem over the last decade.
In this paper, we attempt to address this state of affairs in two ways.
First, we give a new characterization of standard gaussian distributions in
high-dimensions, which lead to effective tests for non-gaussianness. Second, we
propose a simple algorithm, \emph{Reweighted PCA}, as a method for solving the
NGCA problem. We prove that for a general unknown non-gaussian distribution,
this algorithm recovers at least one direction in $E$, with sample and time
complexity depending polynomially on the dimension of the ambient space. We
conjecture that the algorithm actually recovers the entire $E$.
",1,0,0,1,0,0
12377,Fluid dynamics of diving wedges,"  Diving induces large pressures during water entry, accompanied by the
creation of cavity and water splash ejected from the free water surface. To
minimize impact forces, divers streamline their shape at impact. Here, we
investigate the impact forces and splash evolution of diving wedges as a
function of the wedge opening angle. A gradual transition from impactful to
smooth entry is observed as the wedge angle decreases. After submersion, diving
wedges experience significantly smaller drag forces (two-fold smaller) than
immersed wedges. Our experimental findings compare favorably with existing
force models upon the introduction of empirically-based corrections. We
experimentally characterize the shapes of the cavity and splash created by the
wedge and find that they are independent of the entry velocity at short times,
but that the splash exhibits distinct variations in shape at later times. We
propose a one-dimensional model of the splash that takes into account gravity,
surface tension and aerodynamics forces. The model shows, in conjunction with
experimental data, that the splash shape is dominated by the interplay between
a destabilizing Venturi-suction force due to air rushing between the splash and
the water surface and a stabilizing force due to surface tension. Taken
together, these findings could direct future research aimed at understanding
and combining the mechanisms underlying all stages of water entry in
application to engineering and bio-related problems, including naval
engineering, disease spreading or platform diving.
",0,1,0,0,0,0
5351,FeSe(en)0.3 - Separated FeSe layers with stripe-type crystal structure by intercalation of neutral spacer molecules,"  Solvothermal intercalation of ethylenediamine molecules into FeSe separates
the layers by 1078 pm and creates a different stacking. FeSe(en)0.3 is not
superconducting although each layer exhibits the stripe-type crystal structure
and the Fermi surface topology of superconducting FeSe. FeSe(en)0.3 requires
electron-doping for high-Tc similar to monolayers of FeSe@SrTiO3, whose much
higher Tc may arise from the proximity of the oxide surface.
",0,1,0,0,0,0
7151,Markov Chain Lifting and Distributed ADMM,"  The time to converge to the steady state of a finite Markov chain can be
greatly reduced by a lifting operation, which creates a new Markov chain on an
expanded state space. For a class of quadratic objectives, we show an analogous
behavior where a distributed ADMM algorithm can be seen as a lifting of
Gradient Descent algorithm. This provides a deep insight for its faster
convergence rate under optimal parameter tuning. We conjecture that this gain
is always present, as opposed to the lifting of a Markov chain which sometimes
only provides a marginal speedup.
",1,0,1,1,0,0
2713,A giant with feet of clay: on the validity of the data that feed machine learning in medicine,"  This paper considers the use of Machine Learning (ML) in medicine by focusing
on the main problem that this computational approach has been aimed at solving
or at least minimizing: uncertainty. To this aim, we point out how uncertainty
is so ingrained in medicine that it biases also the representation of clinical
phenomena, that is the very input of ML models, thus undermining the clinical
significance of their output. Recognizing this can motivate both medical
doctors, in taking more responsibility in the development and use of these
decision aids, and the researchers, in pursuing different ways to assess the
value of these systems. In so doing, both designers and users could take this
intrinsic characteristic of medicine more seriously and consider alternative
approaches that do not ""sweep uncertainty under the rug"" within an objectivist
fiction, which everyone can come up by believing as true.
",1,0,0,1,0,0
19390,Riemannian almost product manifolds generated by a circulant structure,"  A 4-dimensional Riemannian manifold equipped with a circulant structure,
which is an isometry with respect to the metric and its fourth power is the
identity, is considered. The almost product manifold associated with the
considered manifold is studied. The relation between the covariant derivatives
of the almost product structure and the circulant structure is obtained. The
conditions for the covariant derivative of the circulant structure, which imply
that an almost product manifold belongs to each of the basic classes of the
Staikova-Gribachev classification, are given.
",0,0,1,0,0,0
2755,Color difference makes a difference: four planet candidates around tau Ceti,"  The removal of noise typically correlated in time and wavelength is one of
the main challenges for using the radial velocity method to detect Earth
analogues. We analyze radial velocity data of tau Ceti and find robust evidence
for wavelength dependent noise. We find this noise can be modeled by a
combination of moving average models and ""differential radial velocities"". We
apply this noise model to various radial velocity data sets for tau Ceti, and
find four periodic signals at 20.0, 49.3, 160 and 642 d which we interpret as
planets. We identify two new signals with orbital periods of 20.0 and 49.3 d
while the other two previously suspected signals around 160 and 600 d are
quantified to a higher precision. The 20.0 d candidate is independently
detected in KECK data. All planets detected in this work have minimum masses
less than 4$M_\oplus$ with the two long period ones located around the inner
and outer edges of the habitable zone, respectively. We find that the
instrumental noise gives rise to a precision limit of the HARPS around 0.2 m/s.
We also find correlation between the HARPS data and the central moments of the
spectral line profile at around 0.5 m/s level, although these central moments
may contain both noise and signals. The signals detected in this work have
semi-amplitudes as low as 0.3 m/s, demonstrating the ability of the radial
velocity technique to detect relatively weak signals.
",0,1,0,0,0,0
19825,Light Attenuation Length of High Quality Linear Alkyl Benzene as Liquid Scintillator Solvent for the JUNO Experiment,"  The Jiangmen Underground Neutrino Observatory (JUNO) is a multipurpose
neutrino experiment with a 20 kt liquid scintillator detector designed to
determine the neutrino mass hierarchy, and measure the neutrino oscillation
parameters. Linear alkyl benzene (LAB) will be used as the solvent for the
liquid scintillation system in the central detector of JUNO. For this purpose,
we have prepared LAB samples, and have measured their light attenuation
lengths, with one achieving a length of 25.8 m, comparable to the diameter of
the JUNO detector.
",0,1,0,0,0,0
18359,Regularity gradient estimates for weak solutions of singular quasi-linear parabolic equations,"  This paper studies the Sobolev regularity estimates of weak solutions of a
class of singular quasi-linear elliptic problems of the form $u_t -
\mbox{div}[\mathbb{A}(x,t,u,\nabla u)]= \mbox{div}[{\mathbf F}]$ with
homogeneous Dirichlet boundary conditions over bounded spatial domains. Our
main focus is on the case that the vector coefficients $\mathbb{A}$ are
discontinuous and singular in $(x,t)$-variables, and dependent on the solution
$u$. Global and interior weighted $W^{1,p}(\Omega, \omega)$-regularity
estimates are established for weak solutions of these equations, where $\omega$
is a weight function in some Muckenhoupt class of weights. The results obtained
are even new for linear equations, and for $\omega =1$, because of the
singularity of the coefficients in $(x,t)$-variables
",0,0,1,0,0,0
19270,A Probabilistic Framework for Location Inference from Social Media,"  We study the extent to which we can infer users' geographical locations from
social media. Location inference from social media can benefit many
applications, such as disaster management, targeted advertising, and news
content tailoring. In recent years, a number of algorithms have been proposed
for identifying user locations on social media platforms such as Twitter and
Facebook from message contents, friend networks, and interactions between
users. In this paper, we propose a novel probabilistic model based on factor
graphs for location inference that offers several unique advantages for this
task. First, the model generalizes previous methods by incorporating content,
network, and deep features learned from social context. The model is also
flexible enough to support both supervised learning and semi-supervised
learning. Second, we explore several learning algorithms for the proposed
model, and present a Two-chain Metropolis-Hastings (MH+) algorithm, which
improves the inference accuracy. Third, we validate the proposed model on three
different genres of data - Twitter, Weibo, and Facebook - and demonstrate that
the proposed model can substantially improve the inference accuracy (+3.3-18.5%
by F1-score) over that of several state-of-the-art methods.
",1,0,0,0,0,0
2044,A Generative Model for Natural Sounds Based on Latent Force Modelling,"  Recent advances in analysis of subband amplitude envelopes of natural sounds
have resulted in convincing synthesis, showing subband amplitudes to be a
crucial component of perception. Probabilistic latent variable analysis is
particularly revealing, but existing approaches don't incorporate prior
knowledge about the physical behaviour of amplitude envelopes, such as
exponential decay and feedback. We use latent force modelling, a probabilistic
learning paradigm that incorporates physical knowledge into Gaussian process
regression, to model correlation across spectral subband envelopes. We augment
the standard latent force model approach by explicitly modelling correlations
over multiple time steps. Incorporating this prior knowledge strengthens the
interpretation of the latent functions as the source that generated the signal.
We examine this interpretation via an experiment which shows that sounds
generated by sampling from our probabilistic model are perceived to be more
realistic than those generated by similar models based on nonnegative matrix
factorisation, even in cases where our model is outperformed from a
reconstruction error perspective.
",0,0,0,1,0,0
10854,Large Scale Graph Learning from Smooth Signals,"  Graphs are a prevalent tool in data science, as they model the inherent
structure of the data. They have been used successfully in unsupervised and
semi-supervised learning. Typically they are constructed either by connecting
nearest samples, or by learning them from data, solving an optimization
problem. While graph learning does achieve a better quality, it also comes with
a higher computational cost. In particular, the current state-of-the-art model
cost is $\mathcal{O}(n^2)$ for $n$ samples. In this paper, we show how to scale
it, obtaining an approximation with leading cost of $\mathcal{O}(n\log(n))$,
with quality that approaches the exact graph learning model. Our algorithm uses
known approximate nearest neighbor techniques to reduce the number of
variables, and automatically selects the correct parameters of the model,
requiring a single intuitive input: the desired edge density.
",1,0,0,1,0,0
15307,Cyclic Hypergraph Degree Sequences,"  The problem of efficiently characterizing degree sequences of simple
hypergraphs is a fundamental long-standing open problem in Graph Theory.
Several results are known for restricted versions of this problem. This paper
adds to the list of sufficient conditions for a degree sequence to be {\em
hypergraphic}. This paper proves a combinatorial lemma about cyclically
permuting the columns of a binary table with length $n$ binary sequences as
rows. We prove that for any set of cyclic permutations acting on its columns,
the resulting table has all of its $2^n$ rows distinct. Using this property, we
first define a subset {\em cyclic hyper degrees} of hypergraphic sequences and
show that they admit a polynomial time recognition algorithm. Next, we prove
that there are at least $2^{\frac{(n-1)(n-2)}{2}}$ {\em cyclic hyper degrees},
which also serves as a lower bound on the number of {\em hypergraphic}
sequences. The {\em cyclic hyper degrees} also enjoy a structural
characterization, they are the integral points contained in the union of some
$n$-dimensional rectangles.
",1,0,0,0,0,0
20064,Model-based Iterative Restoration for Binary Document Image Compression with Dictionary Learning,"  The inherent noise in the observed (e.g., scanned) binary document image
degrades the image quality and harms the compression ratio through breaking the
pattern repentance and adding entropy to the document images. In this paper, we
design a cost function in Bayesian framework with dictionary learning.
Minimizing our cost function produces a restored image which has better quality
than that of the observed noisy image, and a dictionary for representing and
encoding the image. After the restoration, we use this dictionary (from the
same cost function) to encode the restored image following the
symbol-dictionary framework by JBIG2 standard with the lossless mode.
Experimental results with a variety of document images demonstrate that our
method improves the image quality compared with the observed image, and
simultaneously improves the compression ratio. For the test images with
synthetic noise, our method reduces the number of flipped pixels by 48.2% and
improves the compression ratio by 36.36% as compared with the best encoding
methods. For the test images with real noise, our method visually improves the
image quality, and outperforms the cutting-edge method by 28.27% in terms of
the compression ratio.
",1,0,0,0,0,0
17836,Numerical investigation of supersonic shock-wave/boundary-layer interaction in transitional and turbulent regime,"  We perform direct numerical simulations of shock-wave/boundary-layer
interactions (SBLI) at Mach number M = 1.7 to investigate the influence of the
state of the incoming boundary layer on the interaction properties. We
reproduce and extend the flow conditions of the experiments performed by
Giepman et al., in which a spatially evolving laminar boundary layer over a
flat plate is initially tripped by an array of distributed roughness elements
and impinged further downstream by an oblique shock wave. Four SBLI cases are
considered, based on two different shock impingement locations along the
streamwise direction, corresponding to transitional and turbulent interactions,
and two different shock strengths, corresponding to flow deflection angles 3
degreees and 6 degrees. We find that, for all flow cases, shock induced
separation is not observed, the boundary layer remains attached for the 3
degrees case and close to incipient separation for the 6 degrees case,
independent of the state of the incoming boundary layer. The findings of this
work suggest that a transitional interaction might be the optimal solution for
practical SBLI applications, as it removes the large separation bubble typical
of laminar interactions and reduces the extent of the high-friction region
associated with an incoming turbulent boundary layer.
",0,1,0,0,0,0
1600,First non-icosahedral boron allotrope synthesized at high pressure and high temperature,"  Theoretical predictions of pressure-induced phase transformations often
become long-standing enigmas because of limitations of contemporary available
experimental possibilities. Hitherto the existence of a non-icosahedral boron
allotrope has been one of them. Here we report on the first non-icosahedral
boron allotrope, which we denoted as {\zeta}-B, with the orthorhombic
{\alpha}-Ga-type structure (space group Cmce) synthesized in a diamond anvil
cell at extreme high-pressure high-temperature conditions (115 GPa and 2100 K).
The structure of {\zeta}-B was solved using single-crystal synchrotron X-ray
diffraction and its compressional behavior was studied in the range of very
high pressures (115 GPa to 135 GPa). Experimental validation of theoretical
predictions reveals the degree of our up-to-date comprehension of condensed
matter and promotes further development of the solid state physics and
chemistry.
",0,1,0,0,0,0
17504,Can Who-Edits-What Predict Edit Survival?,"  As the number of contributors to online peer-production systems grows, it
becomes increasingly important to predict whether the edits that users make
will eventually be beneficial to the project. Existing solutions either rely on
a user reputation system or consist of a highly specialized predictor that is
tailored to a specific peer-production system. In this work, we explore a
different point in the solution space that goes beyond user reputation but does
not involve any content-based feature of the edits. We view each edit as a game
between the editor and the component of the project. We posit that the
probability that an edit is accepted is a function of the editor's skill, of
the difficulty of editing the component and of a user-component interaction
term. Our model is broadly applicable, as it only requires observing data about
who makes an edit, what the edit affects and whether the edit survives or not.
We apply our model on Wikipedia and the Linux kernel, two examples of
large-scale peer-production systems, and we seek to understand whether it can
effectively predict edit survival: in both cases, we provide a positive answer.
Our approach significantly outperforms those based solely on user reputation
and bridges the gap with specialized predictors that use content-based
features. It is simple to implement, computationally inexpensive, and in
addition it enables us to discover interesting structure in the data.
",1,0,0,1,0,0
11977,Heavy fermion quantum criticality at dilute carrier limit in CeNi$_{2-δ}$(As$_{1-x}$P$_{x}$)$_{2}$,"  We study the quantum phase transitions in the nickel pnctides,
CeNi$_{2-\delta}$(As$_{1-x}$P$_{x}$)$_{2}$ ($\delta$ $\approx$ 0.07-0.22). This
series displays the distinct heavy fermion behavior in the rarely studied
parameter regime of dilute carrier limit. We systematically investigate the
magnetization, specific heat and electrical transport down to low temperatures.
Upon increasing the P-content, the antiferromagnetic order of the Ce-4$f$
moment is suppressed continuously and vanishes at $x_c \sim$ 0.55. At this
doping, the temperature dependences of the specific heat and longitudinal
resistivity display non-Fermi liquid behavior. Both the residual resistivity
$\rho_0$ and the Sommerfeld coefficient $\gamma_0$ are sharply peaked around
$x_c$. When the P-content reaches close to 100\%, we observe a clear
low-temperature crossover into the Fermi liquid regime. In contrast to what
happens in the parent compound $x$ = 0.0 as a function of pressure, we find a
surprising result that the non-Fermi liquid behavior persists over a nonzero
range of doping concentration, $x_c<x<0.9$. In this doping range, at the lowest
measured temperatures, the temperature dependence of the specific-heat
coefficient is logarithmically divergent and that of the electrical resistivity
is linear. We discuss the properties of
CeNi$_{2-\delta}$(As$_{1-x}$P$_{x}$)$_{2}$ in comparison with those of its 1111
counterpart, CeNi(As$_{1-x}$P$_{x}$)O. Our results indicate a non-Fermi liquid
phase in the global phase diagram of heavy fermion metals.
",0,1,0,0,0,0
11636,High-resolution photoelectron-spectroscopic investigation of the H$_2$O$^+$ cation in its ${\mathrm {\tilde A^+}}$ electronic state,"  The photoelectron spectrum of water has been recorded in the vicinity of the
${\mathrm {\tilde A^+}}$ $\leftarrow$ $\tilde{\mathrm{X}}$ transition between
112 000 and 116 000 cm$^{-1}$ (13.89-14.38 eV). The high-resolution allowed the
observation of the rotational structure of several bands. Rotational
assignments of the transitions involving the $\Pi(080)$, $\Sigma(070)$ and
$\Pi(060)$ vibronic states of the $\tilde{\mathrm{A}}^+$ electronic state are
deduced from previous studies of the $\tilde{\mathrm{A}}^+ -
\tilde{\mathrm{X}}^+$ band system of H$_2$O$^+$ (Lew, Can. J. Phys. 54, 2028
(1976) and Huet et al., J. Chem. Phys. 107, 5645 (1997)) and photoionization
selection rules. The transition to the $\Sigma(030)$ vibronic state is
tentatively assigned.
",0,1,0,0,0,0
13344,On Invariant Random Subgroups of Block-Diagonal Limits of Symmetric Groups,"  We classify the ergodic invariant random subgroups of block-diagonal limits
of symmetric groups in the cases when the groups are simple and the associated
dimension groups have finite dimensional state spaces. These block-diagonal
limits arise as the transformation groups (full groups) of Bratteli diagrams
that preserve the cofinality of infinite paths in the diagram. Given a simple
full group $G$ admitting only a finite number of ergodic measures on the
path-space $X$ of the associated Bratteli digram, we prove that every non-Dirac
ergodic invariant random subgroup of $G$ arises as the stabilizer distribution
of the diagonal action on $X^n$ for some $n\geq 1$. As a corollary, we
establish that every group character $\chi$ of $G$ has the form $\chi(g) =
Prob(g\in K)$, where $K$ is a conjugation-invariant random subgroup of $G$.
",0,0,1,0,0,0
7607,The structure of a minimal $n$-chart with two crossings II: Neighbourhoods of $Γ_1\cupΓ_{n-1}$,"  Given a 2-crossing minimal chart $\Gamma$, a minimal chart with two
crossings, set $\alpha=\min\{~i~|~$there exists an edge of label $i$ containing
a white vertex$\}$, and $\beta=\max\{~i~|~$there exists an edge of label $i$
containing a white vertex$\}$. In this paper we study the structure of a
neighbourhood of $\Gamma_\alpha\cup\Gamma_\beta$, and propose a normal form for
2-crossing minimal $n$-charts, here $\Gamma_\alpha$ and $\Gamma_\beta$ mean the
union of all the edges of label $\alpha$ and $\beta$ respectively.
",0,0,1,0,0,0
14274,Theoretical aspects of microscale acoustofluidics,"  Henrik Bruus is professor of lab-chip systems and theoretical physics at the
Technical University of Denmark. In this contribution, he summarizes some of
the recent results within theory and simulation of microscale acoustofluidic
systems that he has obtained in collaboration with his students and
international colleagues. The main emphasis is on three dynamical effects
induced by external ultrasound fields acting on aqueous solutions and particle
suspensions: The acoustic radiation force acting on suspended micro- and
nanoparticles, the acoustic streaming appearing in the fluid, and the newly
discovered acoustic body force acting on inhomogeneous solutions.
",0,0,0,0,1,0
5717,Predicting non-linear dynamics by stable local learning in a recurrent spiking neural network,"  Brains need to predict how the body reacts to motor commands. It is an open
question how networks of spiking neurons can learn to reproduce the non-linear
body dynamics caused by motor commands, using local, online and stable learning
rules. Here, we present a supervised learning scheme for the feedforward and
recurrent connections in a network of heterogeneous spiking neurons. The error
in the output is fed back through fixed random connections with a negative
gain, causing the network to follow the desired dynamics, while an online and
local rule changes the weights. The rule for Feedback-based Online Local
Learning Of Weights (FOLLOW) is local in the sense that weight changes depend
on the presynaptic activity and the error signal projected onto the
postsynaptic neuron. We provide examples of learning linear, non-linear and
chaotic dynamics, as well as the dynamics of a two-link arm. Using the Lyapunov
method, and under reasonable assumptions and approximations, we show that
FOLLOW learning is stable uniformly, with the error going to zero
asymptotically.
",1,0,0,0,0,0
1148,Anomaly Detection Using Optimally-Placed Micro-PMU Sensors in Distribution Grids,"  As the distribution grid moves toward a tightly-monitored network, it is
important to automate the analysis of the enormous amount of data produced by
the sensors to increase the operators situational awareness about the system.
In this paper, focusing on Micro-Phasor Measurement Unit ($\mu$PMU) data, we
propose a hierarchical architecture for monitoring the grid and establish a set
of analytics and sensor fusion primitives for the detection of abnormal
behavior in the control perimeter. Due to the key role of the $\mu$PMU devices
in our architecture, a source-constrained optimal $\mu$PMU placement is also
described that finds the best location of the devices with respect to our
rules. The effectiveness of the proposed methods are tested through the
synthetic and real $\mu$PMU data.
",1,0,0,0,0,0
12572,Singular sensitivity in a Keller-Segel-fluid system,"  In bounded smooth domains $\Omega\subset\mathbb{R}^N$, $N\in\{2,3\}$,
considering the chemotaxis--fluid system
\[ \begin{cases} \begin{split} & n_t + u\cdot \nabla n &= \Delta n - \chi
\nabla \cdot(\frac{n}{c}\nabla c) &\\ & c_t + u\cdot \nabla c &= \Delta c - c +
n &\\ & u_t + \kappa (u\cdot \nabla) u &= \Delta u + \nabla P + n\nabla \Phi &
\end{split}\end{cases} \] with singular sensitivity, we prove global existence
of classical solutions for given $\Phi\in C^2(\bar{\Omega})$, for $\kappa=0$
(Stokes-fluid) if $N=3$ and $\kappa\in\{0,1\}$ (Stokes- or Navier--Stokes
fluid) if $N=2$ and under the condition that \[
0<\chi<\sqrt{\frac{2}{N}}. \]
",0,0,1,0,0,0
8786,Using PCA and Factor Analysis for Dimensionality Reduction of Bio-informatics Data,"  Large volume of Genomics data is produced on daily basis due to the
advancement in sequencing technology. This data is of no value if it is not
properly analysed. Different kinds of analytics are required to extract useful
information from this raw data. Classification, Prediction, Clustering and
Pattern Extraction are useful techniques of data mining. These techniques
require appropriate selection of attributes of data for getting accurate
results. However, Bioinformatics data is high dimensional, usually having
hundreds of attributes. Such large a number of attributes affect the
performance of machine learning algorithms used for classification/prediction.
So, dimensionality reduction techniques are required to reduce the number of
attributes that can be further used for analysis. In this paper, Principal
Component Analysis and Factor Analysis are used for dimensionality reduction of
Bioinformatics data. These techniques were applied on Leukaemia data set and
the number of attributes was reduced from to.
",1,0,0,0,0,0
16249,An Overview of Recent Progress in Laser Wakefield Acceleration Experiments,"  The goal of this paper is to examine experimental progress in laser wakefield
acceleration over the past decade (2004-2014), and to use trends in the data to
understand some of the important physical processes. By examining a set of over
50 experiments, various trends concerning the relationship between plasma
density, accelerator length, laser power and the final electron beam en- ergy
are revealed. The data suggest that current experiments are limited by
dephasing and that current experiments typically require some pulse evolution
to reach the trapping threshold.
",0,1,0,0,0,0
1769,Carrier Diffusion in Thin-Film CH3NH3PbI3 Perovskite Measured using Four-Wave Mixing,"  We report the application of femtosecond four-wave mixing (FWM) to the study
of carrier transport in solution-processed CH3NH3PbI3. The diffusion
coefficient was extracted through direct detection of the lateral diffusion of
carriers utilizing the transient grating technique, coupled with simultaneous
measurement of decay kinetics exploiting the versatility of the boxcar
excitation beam geometry. The observation of exponential decay of the transient
grating versus interpulse delay indicates diffusive transport with negligible
trapping within the first nanosecond following excitation. The in-plane
transport geometry in our experiments enabled the diffusion length to be
compared directly with the grain size, indicating that carriers move across
multiple grain boundaries prior to recombination. Our experiments illustrate
the broad utility of FWM spectroscopy for rapid characterization of macroscopic
film transport properties.
",0,1,0,0,0,0
17058,High quality mesh generation using cross and asterisk fields: Application on coastal domains,"  This paper presents a method to generate high quality triangular or
quadrilateral meshes that uses direction fields and a frontal point insertion
strategy. Two types of direction fields are considered: asterisk fields and
cross fields. With asterisk fields we generate high quality triangulations,
while with cross fields we generate right-angled triangulations that are
optimal for transformation to quadrilateral meshes. The input of our algorithm
is an initial triangular mesh and a direction field calculated on it. The goal
is to compute the vertices of the final mesh by an advancing front strategy
along the direction field. We present an algorithm that enables to efficiently
generate the points using solely information from the base mesh. A
multi-threaded implementation of our algorithm is presented, allowing us to
achieve significant speedup of the point generation. Regarding the
quadrangulation process, we develop a quality criterion for right-angled
triangles with respect to the local cross field and an optimization process
based on it. Thus we are able to further improve the quality of the output
quadrilaterals. The algorithm is demonstrated on the sphere and examples of
high quality triangular and quadrilateral meshes of coastal domains are
presented.
",1,0,0,0,0,0
13199,Terrestrial effects of moderately nearby supernovae,"  Recent data indicate one or more moderately nearby supernovae in the early
Pleistocene, with additional events likely in the Miocene. This has motivated
more detailed computations, using new information about the nature of
supernovae and the distances of these events to describe in more detail the
sorts of effects that are indicated at the Earth. This short
communication/review is designed to describe some of these effects so that they
may possibly be related to changes in the biota around these times.
",0,1,0,0,0,0
18021,Synthetic dimensions in ultracold molecules: quantum strings and membranes,"  Synthetic dimensions alter one of the most fundamental properties in nature,
the dimension of space. They allow, for example, a real three-dimensional
system to act as effectively four-dimensional. Driven by such possibilities,
synthetic dimensions have been engineered in ongoing experiments with ultracold
matter. We show that rotational states of ultracold molecules can be used as
synthetic dimensions extending to many - potentially hundreds of - synthetic
lattice sites. Microwaves coupling rotational states drive fully controllable
synthetic inter-site tunnelings, enabling, for example, topological band
structures. Interactions leads to even richer behavior: when molecules are
frozen in a real space lattice with uniform synthetic tunnelings, dipole
interactions cause the molecules to aggregate to a narrow strip in the
synthetic direction beyond a critical interaction strength, resulting in a
quantum string or a membrane, with an emergent condensate that lives on this
string or membrane. All these phases can be detected using measurements of
rotational state populations.
",0,1,0,0,0,0
3572,Hydra: a C++11 framework for data analysis in massively parallel platforms,"  Hydra is a header-only, templated and C++11-compliant framework designed to
perform the typical bottleneck calculations found in common HEP data analyses
on massively parallel platforms. The framework is implemented on top of the
C++11 Standard Library and a variadic version of the Thrust library and is
designed to run on Linux systems, using OpenMP, CUDA and TBB enabled devices.
This contribution summarizes the main features of Hydra. A basic description of
the overall design, functionality and user interface is provided, along with
some code examples and measurements of performance.
",1,1,0,0,0,0
18903,"An integral formula for the powered sum of the independent, identically and normally distributed random variables","  The distribution of the sum of r-th power of standard normal random variables
is a generalization of the chi-squared distribution. In this paper, we
represent the probability density function of the random variable by an
one-dimensional absolutely convergent integral with the characteristic
function. Our integral formula is expected to be applied for evaluation of the
density function. Our integral formula is based on the inversion formula, and
we utilize a summation method. We also discuss on our formula in the view point
of hyperfunctions.
",0,0,1,0,0,0
20622,Learning Robust Representations for Computer Vision,"  Unsupervised learning techniques in computer vision often require learning
latent representations, such as low-dimensional linear and non-linear
subspaces. Noise and outliers in the data can frustrate these approaches by
obscuring the latent spaces.
Our main goal is deeper understanding and new development of robust
approaches for representation learning. We provide a new interpretation for
existing robust approaches and present two specific contributions: a new robust
PCA approach, which can separate foreground features from dynamic background,
and a novel robust spectral clustering method, that can cluster facial images
with high accuracy. Both contributions show superior performance to standard
methods on real-world test sets.
",1,0,0,1,0,0
14284,Investigating the Characteristics of One-Sided Matching Mechanisms Under Various Preferences and Risk Attitudes,"  One-sided matching mechanisms are fundamental for assigning a set of
indivisible objects to a set of self-interested agents when monetary transfers
are not allowed. Two widely-studied randomized mechanisms in multiagent
settings are the Random Serial Dictatorship (RSD) and the Probabilistic Serial
Rule (PS). Both mechanisms require only that agents specify ordinal preferences
and have a number of desirable economic and computational properties. However,
the induced outcomes of the mechanisms are often incomparable and thus there
are challenges when it comes to deciding which mechanism to adopt in practice.
In this paper, we first consider the space of general ordinal preferences and
provide empirical results on the (in)comparability of RSD and PS. We analyze
their respective economic properties under general and lexicographic
preferences. We then instantiate utility functions with the goal of gaining
insights on the manipulability, efficiency, and envyfreeness of the mechanisms
under different risk-attitude models. Our results hold under various preference
distribution models, which further confirm the broad use of RSD in most
practical applications.
",1,0,0,0,0,0
19453,Robust Statistics for Image Deconvolution,"  We present a blind multiframe image-deconvolution method based on robust
statistics. The usual shortcomings of iterative optimization of the likelihood
function are alleviated by minimizing the M-scale of the residuals, which
achieves more uniform convergence across the image. We focus on the
deconvolution of astronomical images, which are among the most challenging due
to their huge dynamic ranges and the frequent presence of large noise-dominated
regions in the images. We show that high-quality image reconstruction is
possible even in super-resolution and without the use of traditional
regularization terms. Using a robust \r{ho}-function is straightforward to
implement in a streaming setting and, hence our method is applicable to the
large volumes of astronomy images. The power of our method is demonstrated on
observations from the Sloan Digital Sky Survey (Stripe 82) and we briefly
discuss the feasibility of a pipeline based on Graphical Processing Units for
the next generation of telescope surveys.
",1,1,0,0,0,0
18641,Asymptotic and numerical analysis of a stochastic PDE model of volume transmission,"  Volume transmission is an important neural communication pathway in which
neurons in one brain region influence the neurotransmitter concentration in the
extracellular space of a distant brain region. In this paper, we apply
asymptotic analysis to a stochastic partial differential equation model of
volume transmission to calculate the neurotransmitter concentration in the
extracellular space. Our model involves the diffusion equation in a
three-dimensional domain with interior holes that randomly switch between being
either sources or sinks. These holes model nerve varicosities that alternate
between releasing and absorbing neurotransmitter, according to when they fire
action potentials. In the case that the holes are small, we compute
analytically the first two nonzero terms in an asymptotic expansion of the
average neurotransmitter concentration. The first term shows that the
concentration is spatially constant to leading order and that this constant is
independent of many details in the problem. Specifically, this constant first
term is independent of the number and location of nerve varicosities, neural
firing correlations, and the size and geometry of the extracellular space. The
second term shows how these factors affect the concentration at second order.
Interestingly, the second term is also spatially constant under some mild
assumptions. We verify our asymptotic results by high-order numerical
simulation using radial basis function-generated finite differences.
",0,0,0,0,1,0
12613,Experimental GHZ Entanglement beyond Qubits,"  The Greenberger-Horne-Zeilinger (GHZ) argument provides an all-or-nothing
contradiction between quantum mechanics and local-realistic theories. In its
original formulation, GHZ investigated three and four particles entangled in
two dimensions only. Very recently, higher dimensional contradictions
especially in three dimensions and three particles have been discovered but it
has remained unclear how to produce such states. In this article we
experimentally show how to generate a three-dimensional GHZ state from
two-photon orbital-angular-momentum entanglement. The first suggestion for a
setup which generates three-dimensional GHZ entanglement from these entangled
pairs came from using the computer algorithm Melvin. The procedure employs
novel concepts significantly beyond the qubit case. Our experiment opens up the
possibility of a truly high-dimensional test of the GHZ-contradiction which,
interestingly, employs non-Hermitian operators.
",0,1,0,0,0,0
20894,Efficient Contextual Bandits in Non-stationary Worlds,"  Most contextual bandit algorithms minimize regret against the best fixed
policy, a questionable benchmark for non-stationary environments that are
ubiquitous in applications. In this work, we develop several efficient
contextual bandit algorithms for non-stationary environments by equipping
existing methods for i.i.d. problems with sophisticated statistical tests so as
to dynamically adapt to a change in distribution.
We analyze various standard notions of regret suited to non-stationary
environments for these algorithms, including interval regret, switching regret,
and dynamic regret. When competing with the best policy at each time, one of
our algorithms achieves regret $\mathcal{O}(\sqrt{ST})$ if there are $T$ rounds
with $S$ stationary periods, or more generally
$\mathcal{O}(\Delta^{1/3}T^{2/3})$ where $\Delta$ is some non-stationarity
measure. These results almost match the optimal guarantees achieved by an
inefficient baseline that is a variant of the classic Exp4 algorithm. The
dynamic regret result is also the first one for efficient and fully adversarial
contextual bandit.
Furthermore, while the results above require tuning a parameter based on the
unknown quantity $S$ or $\Delta$, we also develop a parameter free algorithm
achieving regret $\min\{S^{1/4}T^{3/4}, \Delta^{1/5}T^{4/5}\}$. This improves
and generalizes the best existing result $\Delta^{0.18}T^{0.82}$ by Karnin and
Anava (2016) which only holds for the two-armed bandit problem.
",1,0,0,1,0,0
7809,Neon2: Finding Local Minima via First-Order Oracles,"  We propose a reduction for non-convex optimization that can (1) turn an
stationary-point finding algorithm into an local-minimum finding one, and (2)
replace the Hessian-vector product computations with only gradient
computations. It works both in the stochastic and the deterministic settings,
without hurting the algorithm's performance.
As applications, our reduction turns Natasha2 into a first-order method
without hurting its performance. It also converts SGD, GD, SCSG, and SVRG into
algorithms finding approximate local minima, outperforming some best known
results.
",1,0,0,1,0,0
18308,A Solution for Large-scale Multi-object Tracking,"  A large-scale multi-object tracker based on the generalised labeled
multi-Bernoulli (GLMB) filter is proposed. The algorithm is capable of tracking
a very large, unknown and time-varying number of objects simultaneously, in the
presence of a high number of false alarms, as well as misdetections and
measurement origin uncertainty due to closely spaced objects. The algorithm is
demonstrated on a simulated large-scale tracking scenario, where the peak
number objects appearing simultaneously exceeds one million. To evaluate the
performance of the proposed tracker, we also introduce a new method of applying
the optimal sub-pattern assignment (OSPA) metric, and an efficient strategy for
its evaluation in large-scale scenarios.
",0,0,0,1,0,0
6463,The Frequent Network Neighborhood Mapping of the Human Hippocampus Shows Much More Frequent Neighbor Sets in Males Than in Females,"  In the study of the human connectome, the vertices and the edges of the
network of the human brain are analyzed: the vertices of the graphs are the
anatomically identified gray matter areas of the subjects; this set is exactly
the same for all the subjects. The edges of the graphs correspond to the axonal
fibers, connecting these areas. In the biological applications of graph theory,
it happens very rarely that scientists examine numerous large graphs on the
very same, labeled vertex set. Exactly this is the case in the study of the
connectomes. Because of the particularity of these sets of graphs, novel,
robust methods need to be developed for their analysis. Here we introduce the
new method of the Frequent Network Neighborhood Mapping for the connectome,
which serves as a robust identification of the neighborhoods of given vertices
of special interest in the graph. We apply the novel method for mapping the
neighborhoods of the human hippocampus and discover strong statistical
asymmetries between the connectomes of the sexes, computed from the Human
Connectome Project. We analyze 413 braingraphs, each with 463 nodes. We show
that the hippocampi of men have much more significantly frequent neighbor sets
than women; therefore, in a sense, the connections of the hippocampi are more
regularly distributed in men and more varied in women. Our results are in
contrast to the volumetric studies of the human hippocampus, where it was shown
that the relative volume of the hippocampus is the same in men and women.
",0,0,0,0,1,0
1636,Effects of Degree Correlations in Interdependent Security: Good or Bad?,"  We study the influence of degree correlations or network mixing in
interdependent security. We model the interdependence in security among agents
using a dependence graph and employ a population game model to capture the
interaction among many agents when they are strategic and have various security
measures they can choose to defend themselves. The overall network security is
measured by what we call the average risk exposure (ARE) from neighbors, which
is proportional to the total (expected) number of attacks in the network.
We first show that there exists a unique pure-strategy Nash equilibrium of a
population game. Then, we prove that as the agents with larger degrees in the
dependence graph see higher risks than those with smaller degrees, the overall
network security deteriorates in that the ARE experienced by agents increases
and there are more attacks in the network. Finally, using this finding, we
demonstrate that the effects of network mixing on ARE depend on the (cost)
effectiveness of security measures available to agents; if the security
measures are not effective, increasing assortativity of dependence graph
results in higher ARE. On the other hand, if the security measures are
effective at fending off the damages and losses from attacks, increasing
assortativity reduces the ARE experienced by agents.
",1,1,0,0,0,0
7133,Underscreening in concentrated electrolytes,"  Screening of a surface charge by electrolyte and the resulting interaction
energy between charged objects is of fundamental importance in scenarios from
bio-molecular interactions to energy storage. The conventional wisdom is that
the interaction energy decays exponentially with object separation and the
decay length is a decreasing function of ion concentration; the interaction is
thus negligible in a concentrated electrolyte. Contrary to this conventional
wisdom, we have shown by surface force measurements that the decay length is an
increasing function of ion concentration and Bjerrum length for concentrated
electrolytes. In this paper we report surface force measurements to test
directly the scaling of the screening length with Bjerrum length. Furthermore,
we identify a relationship between the concentration dependence of this
screening length and empirical measurements of activity coefficient and
differential capacitance. The dependence of the screening length on the ion
concentration and the Bjerrum length can be explained by a simple scaling
conjecture based on the physical intuition that solvent molecules, rather than
ions, are charge carriers in a concentrated electrolyte.
",0,1,0,0,0,0
14201,Properties of Quasi-Assouad dimension,"  It is shown that for controlled Moran constructions in $\mathbb{R}$,
including the (sub) self-similar and more generally, (sub) self-conformal sets,
the quasi-Assouad dimension coincides with the upper box dimension. This can be
extended to some special classes of self-similar sets in higher dimensions. The
connections between quasi-Assouad dimension and tangents are studied. We show
that sets with decreasing gaps have quasi-Assouad dimension $0$ or $1$ and we
exhibit an example of a set in the plane whose quasi-Assouad dimension is
smaller than that of its projection onto the $x$-axis, showing that
quasi-Assouad dimension may increase under Lipschitz mappings.
",0,0,1,0,0,0
11343,Twitter and the Press: an Ego-Centred Analysis,"  Ego networks have proved to be a valuable tool for understanding the
relationships that individuals establish with their peers, both in offline and
online social networks. Particularly interesting are the cognitive constraints
associated with the interactions between the ego and the members of their ego
network, whereby individuals cannot maintain meaningful interactions with more
than 150 people, on average. In this work, we focus on the ego networks of
journalists on Twitter, and we investigate whether they feature the same
characteristics observed for other relevant classes of Twitter users, like
politicians and generic users. Our findings are that journalists are generally
more active and interact with more people than generic users. Their ego network
structure is very aligned with reference models derived from the social brain
hypothesis and observed in general human ego networks. Remarkably, the
similarity is even higher than the one of politicians and generic users ego
networks. This may imply a greater cognitive involvement with Twitter than with
other social interaction means. Moreover, the ego networks of journalists are
much stabler than those of politicians and generic users, and the ego-alter
ties are often information-driven.
",1,0,0,0,0,0
5854,"NVIDIA Tensor Core Programmability, Performance & Precision","  The NVIDIA Volta GPU microarchitecture introduces a specialized unit, called
""Tensor Core"" that performs one matrix-multiply-and-accumulate on 4x4 matrices
per clock cycle. The NVIDIA Tesla V100 accelerator, featuring the Volta
microarchitecture, provides 640 Tensor Cores with a theoretical peak
performance of 125 Tflops/s in mixed precision. In this paper, we investigate
current approaches to program NVIDIA Tensor Cores, their performances and the
precision loss due to computation in mixed precision.
Currently, NVIDIA provides three different ways of programming
matrix-multiply-and-accumulate on Tensor Cores: the CUDA Warp Matrix Multiply
Accumulate (WMMA) API, CUTLASS, a templated library based on WMMA, and cuBLAS
GEMM. After experimenting with different approaches, we found that NVIDIA
Tensor Cores can deliver up to 83 Tflops/s in mixed precision on a Tesla V100
GPU, seven and three times the performance in single and half precision
respectively. A WMMA implementation of batched GEMM reaches a performance of 4
Tflops/s. While precision loss due to matrix multiplication with half precision
input might be critical in many HPC applications, it can be considerably
reduced at the cost of increased computation. Our results indicate that HPC
applications using matrix multiplications can strongly benefit from using of
NVIDIA Tensor Cores.
",1,0,0,0,0,0
2278,Koszul duality for Lie algebroids,"  This paper studies the role of dg-Lie algebroids in derived deformation
theory. More precisely, we provide an equivalence between the homotopy theories
of formal moduli problems and dg-Lie algebroids over a commutative dg-algebra
of characteristic zero. At the level of linear objects, we show that the
category of representations of a dg-Lie algebroid is an extension of the
category of quasi-coherent sheaves on the corresponding formal moduli problem.
We describe this extension geometrically in terms of pro-coherent sheaves.
",0,0,1,0,0,0
2142,Wembedder: Wikidata entity embedding web service,"  I present a web service for querying an embedding of entities in the Wikidata
knowledge graph. The embedding is trained on the Wikidata dump using Gensim's
Word2Vec implementation and a simple graph walk. A REST API is implemented.
Together with the Wikidata API the web service exposes a multilingual resource
for over 600'000 Wikidata items and properties.
",1,0,0,1,0,0
4446,On a Formal Model of Safe and Scalable Self-driving Cars,"  In recent years, car makers and tech companies have been racing towards self
driving cars. It seems that the main parameter in this race is who will have
the first car on the road. The goal of this paper is to add to the equation two
additional crucial parameters. The first is standardization of safety assurance
--- what are the minimal requirements that every self-driving car must satisfy,
and how can we verify these requirements. The second parameter is scalability
--- engineering solutions that lead to unleashed costs will not scale to
millions of cars, which will push interest in this field into a niche academic
corner, and drive the entire field into a ""winter of autonomous driving"". In
the first part of the paper we propose a white-box, interpretable, mathematical
model for safety assurance, which we call Responsibility-Sensitive Safety
(RSS). In the second part we describe a design of a system that adheres to our
safety assurance requirements and is scalable to millions of cars.
",1,0,0,1,0,0
1892,Stochastic Canonical Correlation Analysis,"  We tightly analyze the sample complexity of CCA, provide a learning algorithm
that achieves optimal statistical performance in time linear in the required
number of samples (up to log factors), as well as a streaming algorithm with
similar guarantees.
",1,0,0,1,0,0
3214,Rapid micro fluorescence in situ hybridization in tissue sections,"  This paper describes a micro fluorescence in situ hybridization
({\mu}FISH)-based rapid detection of cytogenetic biomarkers on formalin-fixed
paraffin embedded (FFPE) tissue sections. We demonstrated this method in the
context of detecting human epidermal growth factor 2 (HER2) in breast tissue
sections. This method uses a non-contact microfluidic scanning probe (MFP),
which localizes FISH probes at the micrometer length-scale to selected cells of
the tissue section. The scanning ability of the MFP allows for a versatile
implementation of FISH on tissue sections. We demonstrated the use of
oligonucleotide FISH probes in ethylene carbonate-based buffer enabling rapid
hybridization within < 1 min for chromosome enumeration and 10-15 min for
assessment of the HER2 status in FFPE sections. We further demonstrated
recycling of FISH probes for multiple sequential tests using a defined volume
of probes by forming hierarchical hydrodynamic flow confinements. This
microscale method is compatible with the standard FISH protocols and with the
Instant Quality (IQ) FISH assay, reduces the FISH probe consumption ~100-fold
and the hybridization time 4-fold, resulting in an assay turnaround time of < 3
h. We believe rapid {\mu}FISH has the potential of being used in pathology
workflows as a standalone method or in combination with other molecular methods
for diagnostic and prognostic analysis of FFPE sections.
",0,0,0,0,1,0
16397,Detector sampling of optical/IR spectra: how many pixels per FWHM?,"  Most optical and IR spectra are now acquired using detectors with
finite-width pixels in a square array. This paper examines the effects of such
pixellation, using computed simulations to illustrate the effects which most
concern the astronomer end-user. Coarse sampling increases the random noise
errors in wavelength by typically 10 - 20% at 2 pixels/FWHM, but with wide
variation depending on the functional form of the instrumental Line Spread
Function (LSF) and on the pixel phase. Line widths are even more strongly
affected at low sampling frequencies. However, the noise in fitted peak
amplitudes is minimally affected. Pixellation has a substantial but complex
effect on the ability to see a relative minimum between two closely-spaced
peaks (or relative maximum between two absorption lines). The consistent scale
of resolving power presented by Robertson (2013) is extended to cover
pixellated spectra. The systematic bias errors in wavelength introduced by
pixellation are examined. While they may be negligible for smooth well-sampled
symmetric LSFs, they are very sensitive to asymmetry and high spatial frequency
substructure. The Modulation Transfer Function for sampled data is shown to
give a useful indication of the extent of improperly sampled signal in an LSF.
The common maxim that 2 pixels/FWHM is the Nyquist limit is incorrect and most
LSFs will exhibit some aliasing at this sample frequency. While 2 pixels/FWHM
is often an acceptable minimum for moderate signal/noise work, it is preferable
to carry out simulations for any actual or proposed LSF to find the effects of
sampling frequency. Where end-users have a choice of sampling frequencies,
through on-chip binning and/or spectrograph configurations, the instrument user
manual should include an examination of their effects. (Abridged)
",0,1,0,0,0,0
11143,Robust Kronecker-Decomposable Component Analysis for Low-Rank Modeling,"  Dictionary learning and component analysis are part of one of the most
well-studied and active research fields, at the intersection of signal and
image processing, computer vision, and statistical machine learning. In
dictionary learning, the current methods of choice are arguably K-SVD and its
variants, which learn a dictionary (i.e., a decomposition) for sparse coding
via Singular Value Decomposition. In robust component analysis, leading methods
derive from Principal Component Pursuit (PCP), which recovers a low-rank matrix
from sparse corruptions of unknown magnitude and support. However, K-SVD is
sensitive to the presence of noise and outliers in the training set.
Additionally, PCP does not provide a dictionary that respects the structure of
the data (e.g., images), and requires expensive SVD computations when solved by
convex relaxation. In this paper, we introduce a new robust decomposition of
images by combining ideas from sparse dictionary learning and PCP. We propose a
novel Kronecker-decomposable component analysis which is robust to gross
corruption, can be used for low-rank modeling, and leverages separability to
solve significantly smaller problems. We design an efficient learning algorithm
by drawing links with a restricted form of tensor factorization. The
effectiveness of the proposed approach is demonstrated on real-world
applications, namely background subtraction and image denoising, by performing
a thorough comparison with the current state of the art.
",1,0,0,1,0,0
7503,Approximate fixed points and B-amenable groups,"  A topological group $G$ is B-amenable if and only if every continuous affine
action of $G$ on a bounded convex subset of a locally convex space has an
approximate fixed point. Similar results hold more generally for slightly
uniformly continuous semigroup actions.
",0,0,1,0,0,0
6731,An investigation of pulsar searching techniques with the Fast Folding Algorithm,"  Here we present an in-depth study of the behaviour of the Fast Folding
Algorithm, an alternative pulsar searching technique to the Fast Fourier
Transform. Weaknesses in the Fast Fourier Transform, including a susceptibility
to red noise, leave it insensitive to pulsars with long rotational periods (P >
1 s). This sensitivity gap has the potential to bias our understanding of the
period distribution of the pulsar population. The Fast Folding Algorithm, a
time-domain based pulsar searching technique, has the potential to overcome
some of these biases. Modern distributed-computing frameworks now allow for the
application of this algorithm to all-sky blind pulsar surveys for the first
time. However, many aspects of the behaviour of this search technique remain
poorly understood, including its responsiveness to variations in pulse shape
and the presence of red noise. Using a custom CPU-based implementation of the
Fast Folding Algorithm, ffancy, we have conducted an in-depth study into the
behaviour of the Fast Folding Algorithm in both an ideal, white noise regime as
well as a trial on observational data from the HTRU-S Low Latitude pulsar
survey, including a comparison to the behaviour of the Fast Fourier Transform.
We are able to both confirm and expand upon earlier studies that demonstrate
the ability of the Fast Folding Algorithm to outperform the Fast Fourier
Transform under ideal white noise conditions, and demonstrate a significant
improvement in sensitivity to long-period pulsars in real observational data
through the use of the Fast Folding Algorithm.
",0,1,0,0,0,0
12960,A de Sitter limit analysis for dark energy and modified gravity models,"  The effective field theory of dark energy and modified gravity is supposed to
well describe, at low energies, the behaviour of the gravity modifications due
to one extra scalar degree of freedom. The usual curvature perturbation is very
useful when studying the conditions for the avoidance of ghost instabilities as
well as the positivity of the squared speeds of propagation for both the scalar
and tensor modes, or the Stückelberg field performs perfectly when
investigating the evolution of linear perturbations. We show that the viable
parameters space identified by requiring no-ghost instabilities and positive
squared speeds of propagation does not change by performing a field
redefinition, while the requirement of the avoidance of tachyonic instability
might instead be different. Therefore, we find interesting to associate to the
general modified gravity theory described in the effective field theory
framework, a perturbation field which will inherit the whole properties of the
theory. In the present paper we address the following questions: 1) how can we
define such a field? and 2) what is the mass of such a field as the background
approaches a final de Sitter state? We define a gauge invariant quantity which
identifies the density of the dark energy perturbation field valid for any
background. We derive the mass associated to the gauge invariant dark energy
field on a de Sitter background, which we retain to be still a good
approximation also at very low redshift ($z\simeq 0$). On this background we
also investigate the value of the speed of propagation and we find that there
exist classes of theories which admit a non-vanishing speed of propagation,
even among the Horndeski model, for which in literature it has previously been
found a zero speed. We finally apply our results to specific well known models.
",0,1,0,0,0,0
10481,Anisotropy of transport in bulk Rashba metals,"  The recent experimental discovery of three-dimensional (3D) materials hosting
a strong Rashba spin-orbit coupling calls for the theoretical investigation of
their transport properties. Here we study the zero temperature dc conductivity
of a 3D Rashba metal in the presence of static diluted impurities. We show
that, at variance with the two-dimensional case, in 3D systems spin-orbit
coupling affects dc charge transport in all density regimes. We find in
particular that the effect of spin-orbit interaction strongly depends on the
direction of the current, and we show that this yields strongly anisotropic
transport characteristics. In the dominant spin-orbit coupling regime where
only the lowest band is occupied, the SO-induced conductivity anisotropy is
governed entirely by the anomalous component of the renormalized current. We
propose that measurements of the conductivity anisotropy in bulk Rashba metals
may give a direct experimental assessment of the spin-orbit strength.
",0,1,0,0,0,0
14359,Nonautonomous Dynamics of Acute Cell Injury,"  Clinically-relevant forms of acute cell injury, which include stroke and
myocardial infarction, have been of long-lasting challenge in terms of
successful intervention and treatments. Although laboratory studies have shown
it is possible to decrease cell death after such injuries, human clinical
trials based on laboratory therapies have generally failed. We suggested these
failures are due, at least partially, to the lack of a quantitative theoretical
framework for acute cell injury. Here we provide a systematic study on a
nonlinear dynamical model of acute cell injury and characterize the global
dynamics of a nonautonomous version of the theory. The nonautonomous model
gives rise to four qualitative types of dynamical patterns that can be mapped
to the behavior of cells after clinical acute injuries. In addition, the
concept of a maximum total intrinsic stress response, $S_{max}^*$, emerges from
the nonautonomous theory. A continuous transition across the four qualitative
patterns has been observed, which sets a natural range for initial conditions.
Under these initial conditions in the parameter space tested, the total induced
stress response can be increased to 2.5-11 folds of $S_{max}^*$. This result
indicates that cells possess a reserve stress response capacity which provides
a theoretical explanation of how therapies can prevent cell death after lethal
injuries. This nonautonomous theory of acute cell injury thus provides a
quantitative framework for understanding cell death and recovery and developing
effective therapeutics for acute injury.
",0,0,0,0,1,0
10994,TIP: Typifying the Interpretability of Procedures,"  We provide a novel notion of what it means to be interpretable, looking past
the usual association with human understanding. Our key insight is that
interpretability is not an absolute concept and so we define it relative to a
target model, which may or may not be a human. We define a framework that
allows for comparing interpretable procedures by linking them to important
practical aspects such as accuracy and robustness. We characterize many of the
current state-of-the-art interpretable methods in our framework portraying its
general applicability. Finally, principled interpretable strategies are
proposed and empirically evaluated on synthetic data, as well as on the largest
public olfaction dataset that was made recently available \cite{olfs}. We also
experiment on MNIST with a simple target model and different oracle models of
varying complexity. This leads to the insight that the improvement in the
target model is not only a function of the oracle model's performance, but also
its relative complexity with respect to the target model. Further experiments
on CIFAR-10, a real manufacturing dataset and FICO dataset showcase the benefit
of our methods over Knowledge Distillation when the target models are simple
and the complex model is a neural network.
",1,0,0,1,0,0
17317,Gain-loss-driven travelling waves in PT-symmetric nonlinear metamaterials,"  In this work we investigate a one-dimensional parity-time (PT)-symmetric
magnetic metamaterial consisting of split-ring dimers having gain or loss.
Employing a Melnikov analysis we study the existence of localized travelling
waves, i.e. homoclinic or heteroclinic solutions. We find conditions under
which the homoclinic or heteroclinic orbits persist. Our analytical results are
found to be in good agreement with direct numerical computations. For the
particular nonlinearity admitting travelling kinks, numerically we observe
homoclinic snaking in the bifurcation diagram. The Melnikov analysis yields a
good approximation to one of the boundaries of the snaking profile.
",0,1,0,0,0,0
1351,Phase Congruency Parameter Optimization for Enhanced Detection of Image Features for both Natural and Medical Applications,"  Following the presentation and proof of the hypothesis that image features
are particularly perceived at points where the Fourier components are maximally
in phase, the concept of phase congruency (PC) is introduced. Subsequently, a
two-dimensional multi-scale phase congruency (2D-MSPC) is developed, which has
been an important tool for detecting and evaluation of image features. However,
the 2D-MSPC requires many parameters to be appropriately tuned for optimal
image features detection. In this paper, we defined a criterion for parameter
optimization of the 2D-MSPC, which is a function of its maximum and minimum
moments. We formulated the problem in various optimal and suboptimal
frameworks, and discussed the conditions and features of the suboptimal
solutions. The effectiveness of the proposed method was verified through
several examples, ranging from natural objects to medical images from patients
with a neurological disease, multiple sclerosis.
",1,0,1,0,0,0
16121,Improved Representation Learning for Predicting Commonsense Ontologies,"  Recent work in learning ontologies (hierarchical and partially-ordered
structures) has leveraged the intrinsic geometry of spaces of learned
representations to make predictions that automatically obey complex structural
constraints. We explore two extensions of one such model, the order-embedding
model for hierarchical relation learning, with an aim towards improved
performance on text data for commonsense knowledge representation. Our first
model jointly learns ordering relations and non-hierarchical knowledge in the
form of raw text. Our second extension exploits the partial order structure of
the training data to find long-distance triplet constraints among embeddings
which are poorly enforced by the pairwise training procedure. We find that both
incorporating free text and augmented training constraints improve over the
original order-embedding model and other strong baselines.
",1,0,0,1,0,0
3966,The phase retrieval problem for solutions of the Helmholtz equation,"  In this paper we consider the phase retrieval problem for Herglotz functions,
that is, solutions of the Helmholtz equation $\Delta u+\lambda^2u=0$ on domains
$\Omega\subset\mathbb{R}^d$, $d\geq2$. In dimension $d=2$, if $u,v$ are two
such solutions then $|u|=|v|$ implies that either $u=cv$ or $u=c\bar v$ for
some $c\in\mathbb{C}$ with $|c|=1$. In dimension $d\geq3$, the same conclusion
holds under some restriction on $u$ and $v$: either they are real valued or
zonal functions or have non vanishing mean.
",0,0,1,0,0,0
7569,Pulsar braking and the P-Pdot diagram,"  The location of radio pulsars in the period-period derivative (P-Pdot) plane
has been a key diagnostic tool since the early days of pulsar astronomy. Of
particular importance is how pulsars evolve through the P-Pdot diagram with
time. Here we show that the decay of the inclination angle (alpha-dot) between
the magnetic and rotation axes plays a critical role. In particular, alpha-dot
strongly impacts on the braking torque, an effect which has been largely
ignored in previous work. We carry out simulations which include a negative
alpha-dot term, and show that it is possible to reproduce the observational
P-Pdot diagram without the need for either pulsars with long birth periods or
magnetic field decay. Our best model indicates a birth rate of 1 radio pulsar
per century and a total Galactic population of ~20000 pulsars beaming towards
Earth.
",0,1,0,0,0,0
18956,Testing convexity of a discrete distribution,"  Based on the convex least-squares estimator, we propose two different
procedures for testing convexity of a probability mass function supported on N
with an unknown finite support. The procedures are shown to be asymptotically
calibrated.
",0,0,1,1,0,0
14267,Resilient Feedback Controller Design For Linear Model of Power Grids,"  In this paper, a resilient controller is designed for the linear
time-invariant (LTI) systems subject to attacks on the sensors and the
actuators. A novel probabilistic attack model is proposed to capture
vulnerabilities of the communication links from sensors to the controller and
from the controller to actuators. The observer and the controller formulation
under the attack are derived. Thereafter, By leveraging Lyapunov functional
methods, it is shown that exponential mean-square stability of the system under
the output feedback controller is guaranteed if a certain LMI is feasible. The
simulation results show the effectiveness and applicability of the proposed
controller design approach.
",1,0,0,0,0,0
2797,Polarization leakage in epoch of reionization windows: III. Wide-field effects of narrow-field arrays,"  Leakage of polarized Galactic diffuse emission into total intensity can
potentially mimic the 21-cm signal coming from the epoch of reionization (EoR),
as both of them might have fluctuating spectral structure. Although we are
sensitive to the EoR signal only in small fields of view, chromatic sidelobes
from further away can contaminate the inner region. Here, we explore the
effects of leakage into the 'EoR window' of the cylindrically averaged power
spectra (PS) within wide fields of view using both observation and simulation
of the 3C196 and NCP fields, two observing fields of the LOFAR-EoR project. We
present the polarization PS of two one-night observations of the two fields and
find that the NCP field has higher fluctuations along frequency, and
consequently exhibits more power at high-$k_\parallel$ that could potentially
leak to Stokes $I$. Subsequently, we simulate LOFAR observations of Galactic
diffuse polarized emission based on a model to assess what fraction of
polarized power leaks into Stokes $I$ because of the primary beam. We find that
the rms fractional leakage over the instrumental $k$-space is $0.35\%$ in the
3C196 field and $0.27\%$ in the NCP field, and it does not change significantly
within the diameters of $15^\circ$, $9^\circ$ and $4^\circ$. Based on the
observed PS and simulated fractional leakage, we show that a similar level of
leakage into Stokes $I$ is expected in the 3C196 and NCP fields, and the
leakage can be considered to be a bias in the PS.
",0,1,0,0,0,0
12515,Learning Overcomplete HMMs,"  We study the problem of learning overcomplete HMMs---those that have many
hidden states but a small output alphabet. Despite having significant practical
importance, such HMMs are poorly understood with no known positive or negative
results for efficient learning. In this paper, we present several new
results---both positive and negative---which help define the boundaries between
the tractable and intractable settings. Specifically, we show positive results
for a large subclass of HMMs whose transition matrices are sparse,
well-conditioned, and have small probability mass on short cycles. On the other
hand, we show that learning is impossible given only a polynomial number of
samples for HMMs with a small output alphabet and whose transition matrices are
random regular graphs with large degree. We also discuss these results in the
context of learning HMMs which can capture long-term dependencies.
",1,0,0,1,0,0
9977,gl2vec: Learning Feature Representation Using Graphlets for Directed Networks,"  Learning network representations has a variety of applications, such as
network classification. Most existing work in this area focuses on static
undirected networks and do not account for presence of directed edges or
temporarily changes. Furthermore, most work focuses on node representations
that do poorly on tasks like network classification. In this paper, we propose
a novel, flexible and scalable network embedding methodology, \emph{gl2vec},
for network classification in both static and temporal directed networks.
\emph{gl2vec} constructs vectors for feature representation using static or
temporal network graphlet distributions and a null model for comparing them
against random graphs. We argue that \emph{gl2vec} can be used to classify and
compare networks of varying sizes and time period with high accuracy. We
demonstrate the efficacy and usability of \emph{gl2vec} over existing
state-of-the-art methods on network classification tasks such as network type
classification and subgraph identification in several real-world static and
temporal directed networks. Experimental results further show that
\emph{gl2vec}, concatenated with a wide range of state-of-the-art methods,
improves classification accuracy by up to $10\%$ in real-world applications
such as detecting departments for subgraphs in an email network or identifying
mobile users given their app switching behaviors represented as static or
temporal directed networks.
",1,0,0,0,0,0
12826,The stifness of the supranuclear equation of state (once again),"  We revisit the present status of the stiffness of the supranuclear equations
of state, particularly the solutions that increase the stiffness in the
presence of hyperons, the putative transition to a quark matter phase and the
robustness of massive compact star observations.
",0,1,0,0,0,0
13789,Optimization Design of Decentralized Control for Complex Decentralized Systems,"  A new method is developed to deal with the problem that a complex
decentralized control system needs to keep centralized control performance. The
systematic procedure emphasizes quickly finding the decentralized
subcontrollers that matching the closed-loop performance and robustness
characteristics of the centralized controller, which is featured by the fact
that GA is used to optimize the design of centralized H-infinity controller
K(s) and decentralized engine subcontroller KT(s), and that only one interface
variable needs to satisfy decentralized control system requirement according to
the proposed selection principle. The optimization design is motivated by the
implementation issues where it is desirable to reduce the time in trial and
error process and accurately find the best decentralized subcontrollers. The
method is applied to decentralized control system design for a short takeoff
and landing fighter. By comparing the simulation results of the decentralized
control system with those of the centralized control system, the target of the
decentralized control attains the performance and robustness of centralized
control is validated.
",1,0,0,0,0,0
18176,Distributed Unknown-Input-Observers for Cyber Attack Detection and Isolation in Formation Flying UAVs,"  In this paper, cyber attack detection and isolation is studied on a network
of UAVs in a formation flying setup. As the UAVs communicate to reach consensus
on their states while making the formation, the communication network among the
UAVs makes them vulnerable to a potential attack from malicious adversaries.
Two types of attacks pertinent to a network of UAVs have been considered: a
node attack on the UAVs and a deception attack on the communication between the
UAVs. UAVs formation control presented using a consensus algorithm to reach a
pre-specified formation. A node and a communication path deception cyber
attacks on the UAV's network are considered with their respective models in the
formation setup. For these cyber attacks detection, a bank of Unknown Input
Observer (UIO) based distributed fault detection scheme proposed to detect and
identify the compromised UAV in the formation. A rule based on the residuals
generated using the bank of UIOs are used to detect attacks and identify the
compromised UAV in the formation. Further, an algorithm developed to remove the
faulty UAV from the network once an attack detected and the compromised UAV
isolated while maintaining the formation flight with a missing UAV node.
",1,0,0,0,0,0
20378,Discursive Landscapes and Unsupervised Topic Modeling in IR: A Validation of Text-As-Data Approaches through a New Corpus of UN Security Council Speeches on Afghanistan,"  The recent turn towards quantitative text-as-data approaches in IR brought
new ways to study the discursive landscape of world politics. Here seen as
complementary to qualitative approaches, quantitative assessments have the
advantage of being able to order and make comprehensible vast amounts of text.
However, the validity of unsupervised methods applied to the types of text
available in large quantities needs to be established before they can speak to
other studies relying on text and discourse as data. In this paper, we
introduce a new text corpus of United Nations Security Council (UNSC) speeches
on Afghanistan between 2001 and 2017; we study this corpus through unsupervised
topic modeling (LDA) with the central aim to validate the topic categories that
the LDA identifies; and we discuss the added value, and complementarity, of
quantitative text-as-data approaches. We set-up two tests using mixed- method
approaches. Firstly, we evaluate the identified topics by assessing whether
they conform with previous qualitative work on the development of the situation
in Afghanistan. Secondly, we use network analysis to study the underlying
social structures of what we will call 'speaker-topic relations' to see whether
they correspondent to know divisions and coalitions in the UNSC. In both cases
we find that the unsupervised LDA indeed provides valid and valuable outputs.
In addition, the mixed-method approaches themselves reveal interesting patterns
deserving future qualitative research. Amongst these are the coalition and
dynamics around the 'women and human rights' topic as part of the UNSC debates
on Afghanistan.
",1,0,0,0,0,0
18394,Learning Deep Visual Object Models From Noisy Web Data: How to Make it Work,"  Deep networks thrive when trained on large scale data collections. This has
given ImageNet a central role in the development of deep architectures for
visual object classification. However, ImageNet was created during a specific
period in time, and as such it is prone to aging, as well as dataset bias
issues. Moving beyond fixed training datasets will lead to more robust visual
systems, especially when deployed on robots in new environments which must
train on the objects they encounter there. To make this possible, it is
important to break free from the need for manual annotators. Recent work has
begun to investigate how to use the massive amount of images available on the
Web in place of manual image annotations. We contribute to this research thread
with two findings: (1) a study correlating a given level of noisily labels to
the expected drop in accuracy, for two deep architectures, on two different
types of noise, that clearly identifies GoogLeNet as a suitable architecture
for learning from Web data; (2) a recipe for the creation of Web datasets with
minimal noise and maximum visual variability, based on a visual and natural
language processing concept expansion strategy. By combining these two results,
we obtain a method for learning powerful deep object models automatically from
the Web. We confirm the effectiveness of our approach through object
categorization experiments using our Web-derived version of ImageNet on a
popular robot vision benchmark database, and on a lifelong object discovery
task on a mobile robot.
",1,0,0,0,0,0
3395,Characterization of catastrophic instabilities: Market crashes as paradigm,"  Catastrophic events, though rare, do occur and when they occur, they have
devastating effects. It is, therefore, of utmost importance to understand the
complexity of the underlying dynamics and signatures of catastrophic events,
such as market crashes. For deeper understanding, we choose the US and Japanese
markets from 1985 onward, and study the evolution of the cross-correlation
structures of stock return matrices and their eigenspectra over different short
time-intervals or ""epochs"". A slight non-linear distortion is applied to the
correlation matrix computed for any epoch, leading to the emerging spectrum of
eigenvalues. The statistical properties of the emerging spectrum display: (i)
the shape of the emerging spectrum reflects the market instability, (ii) the
smallest eigenvalue may be able to statistically distinguish the nature of a
market turbulence or crisis -- internal instability or external shock, and
(iii) the time-lagged smallest eigenvalue has a statistically significant
correlation with the mean market cross-correlation. The smallest eigenvalue
seems to indicate that the financial market has become more turbulent in a
similar way as the mean does. Yet we show features of the smallest eigenvalue
of the emerging spectrum that distinguish different types of market
instabilities related to internal or external causes. Based on the paradigmatic
character of financial time series for other complex systems, the capacity of
the emerging spectrum to understand the nature of instability may be a new
feature, which can be broadly applied.
",0,0,0,0,0,1
6548,A Reassessment of Absolute Energies of the X-ray L Lines of Lanthanide Metals,"  We introduce a new technique for determining x-ray fluorescence line energies
and widths, and we present measurements made with this technique of 22 x-ray L
lines from lanthanide-series elements. The technique uses arrays of
transition-edge sensors, microcalorimeters with high energy-resolving power
that simultaneously observe both calibrated x-ray standards and the x-ray
emission lines under study. The uncertainty in absolute line energies is
generally less than 0.4 eV in the energy range of 4.5 keV to 7.5 keV. Of the
seventeen line energies of neodymium, samarium, and holmium, thirteen are found
to be consistent with the available x-ray reference data measured after 1990;
only two of the four lines for which reference data predate 1980, however, are
consistent with our results. Five lines of terbium are measured with
uncertainties that improve on those of existing data by factors of two or more.
These results eliminate a significant discrepancy between measured and
calculated x-ray line energies for the terbium Ll line (5.551 keV). The line
widths are also measured, with uncertainties of 0.6 eV or less on the
full-width at half-maximum in most cases. These measurements were made with an
array of approximately one hundred superconducting x- ray microcalorimeters,
each sensitive to an energy band from 1 keV to 8 keV. No energy-dispersive
spectrometer has previously been used for absolute-energy estimation at this
level of accuracy. Future spectrometers, with superior linearity and energy
resolution, will allow us to improve on these results and expand the
measurements to more elements and a wider range of line energies.
",0,1,0,0,0,0
7331,Multiple Hypothesis Tracking Algorithm for Multi-Target Multi-Camera Tracking with Disjoint Views,"  In this study, a multiple hypothesis tracking (MHT) algorithm for
multi-target multi-camera tracking (MCT) with disjoint views is proposed. Our
method forms track-hypothesis trees, and each branch of them represents a
multi-camera track of a target that may move within a camera as well as move
across cameras. Furthermore, multi-target tracking within a camera is performed
simultaneously with the tree formation by manipulating a status of each track
hypothesis. Each status represents three different stages of a multi-camera
track: tracking, searching, and end-of-track. The tracking status means targets
are tracked by a single camera tracker. In the searching status, the
disappeared targets are examined if they reappear in other cameras. The
end-of-track status does the target exited the camera network due to its
lengthy invisibility. These three status assists MHT to form the
track-hypothesis trees for multi-camera tracking. Furthermore, they present a
gating technique for eliminating of unlikely observation-to-track association.
In the experiments, they evaluate the proposed method using two datasets,
DukeMTMC and NLPR-MCT, which demonstrates that the proposed method outperforms
the state-of-the-art method in terms of improvement of the accuracy. In
addition, they show that the proposed method can operate in real-time and
online.
",1,0,0,0,0,0
1064,Security Trust Zone in 5G Networks,"  Fifth Generation (5G) telecommunication system is going to deliver a flexible
radio access network (RAN). Security functions such as authorization,
authentication and accounting (AAA) are expected to be distributed from central
clouds to edge clouds. We propose a novel architectural security solution that
applies to 5G networks. It is called Trust Zone (TZ) that is designed as an
enhancement of the 5G AAA in the edge cloud. TZ also provides an autonomous and
decentralized security policy for different tenants under variable network
conditions. TZ also initiates an ability of disaster cognition and extends the
security functionalities to a set of flexible and highly available emergency
services in the edge cloud.
",1,0,0,0,0,0
20577,Cherednik algebras and Calogero-Moser cells,"  Using the representation theory of Cherednik algebras at $t=0$ and a Galois
covering of the Calogero-Moser space, we define the notions of left, right and
two-sided Calogero-Moser cells for any finite complex reflection group. To each
Caloger-Moser two-sided cell is associated a Calogero-Moser family, while to
each Calogero-Moser left cell is associated a Calogero-Moser cellular
representation. We study properties of these objects and we conjecture that,
whenever the reflection group is real (i.e. is a Coxeter group), these notions
coincide with the one of Kazhdan-Lusztig left, right and two-sided cells,
Kazhdan-Lusztig families and Kazhdan-Lusztig cellular representations.
",0,0,1,0,0,0
266,Magnus integrators on multicore CPUs and GPUs,"  In the present paper we consider numerical methods to solve the discrete
Schrödinger equation with a time dependent Hamiltonian (motivated by problems
encountered in the study of spin systems). We will consider both short-range
interactions, which lead to evolution equations involving sparse matrices, and
long-range interactions, which lead to dense matrices. Both of these settings
show very different computational characteristics. We use Magnus integrators
for time integration and employ a framework based on Leja interpolation to
compute the resulting action of the matrix exponential. We consider both
traditional Magnus integrators (which are extensively used for these types of
problems in the literature) as well as the recently developed commutator-free
Magnus integrators and implement them on modern CPU and GPU (graphics
processing unit) based systems.
We find that GPUs can yield a significant speed-up (up to a factor of $10$ in
the dense case) for these types of problems. In the sparse case GPUs are only
advantageous for large problem sizes and the achieved speed-ups are more
modest. In most cases the commutator-free variant is superior but especially on
the GPU this advantage is rather small. In fact, none of the advantage of
commutator-free methods on GPUs (and on multi-core CPUs) is due to the
elimination of commutators. This has important consequences for the design of
more efficient numerical methods.
",1,1,0,0,0,0
12580,Phase transition in the spiked random tensor with Rademacher prior,"  We consider the problem of detecting a deformation from a symmetric Gaussian
random $p$-tensor $(p\geq 3)$ with a rank-one spike sampled from the Rademacher
prior. Recently in Lesieur et al. (2017), it was proved that there exists a
critical threshold $\beta_p$ so that when the signal-to-noise ratio exceeds
$\beta_p$, one can distinguish the spiked and unspiked tensors and weakly
recover the prior via the minimal mean-square-error method. On the other side,
Perry, Wein, and Bandeira (2017) proved that there exists a $\beta_p'<\beta_p$
such that any statistical hypothesis test can not distinguish these two
tensors, in the sense that their total variation distance asymptotically
vanishes, when the signa-to-noise ratio is less than $\beta_p'$. In this work,
we show that $\beta_p$ is indeed the critical threshold that strictly separates
the distinguishability and indistinguishability between the two tensors under
the total variation distance. Our approach is based on a subtle analysis of the
high temperature behavior of the pure $p$-spin model with Ising spin, arising
initially from the field of spin glasses. In particular, we identify the
signal-to-noise criticality $\beta_p$ as the critical temperature,
distinguishing the high and low temperature behavior, of the Ising pure
$p$-spin mean-field spin glass model.
",0,0,1,0,0,0
9823,Connections between transport of intensity equation and two-dimensional phase unwrapping,"  In a recent publication [Appl. Opt. 55, 2418 (2016)], a method for
two-dimensional phase unwrapping based on the transport of intensity equation
(TIE) was studied. We wish to show that this approach is associated with the
standard least squares phase unwrapping algorithm, but with additional
numerical errors.
",0,1,0,0,0,0
16540,Multi-Agent Deep Reinforcement Learning with Human Strategies,"  Deep learning has enabled traditional reinforcement learning methods to deal
with high-dimensional problems. However, one of the disadvantages of deep
reinforcement learning methods is the limited exploration capacity of learning
agents. In this paper, we introduce an approach that integrates human
strategies to increase the exploration capacity of multiple deep reinforcement
learning agents. We also report the development of our own multi-agent
environment called Multiple Tank Defence to simulate the proposed approach. The
results show the significant performance improvement of multiple agents that
have learned cooperatively with human strategies. This implies that there is a
critical need for human intellect teamed with machines to solve complex
problems. In addition, the success of this simulation indicates that our
developed multi-agent environment can be used as a testbed platform to develop
and validate other multi-agent control algorithms. Details of the environment
implementation can be referred to
this http URL
",0,0,0,1,0,0
4200,Interval Exchange Transformations and Low-Discrepancy,"  In [Mas82] and [Vee78] it was proved independently that almost every interval
exchange transformation is uniquely ergodic. The Birkhoff ergodic theorem
implies that these maps mainly have uniformly distributed orbits. This raises
the question under which conditions the orbits yield low-discrepancy sequences.
The case of $n=2$ intervals corresponds to circle rotation, where conditions
for low-discrepancy are well-known. In this paper, we give corresponding
conditions in the case $n=3$. Furthermore, we construct infinitely many
interval exchange transformations with low-discrepancy orbits for $n \geq 4$.
We also show that these examples do not coincide with $LS$-sequences if $S \geq
2$.
",0,0,1,0,0,0
11519,DOC: Deep Open Classification of Text Documents,"  Traditional supervised learning makes the closed-world assumption that the
classes appeared in the test data must have appeared in training. This also
applies to text learning or text classification. As learning is used
increasingly in dynamic open environments where some new/test documents may not
belong to any of the training classes, identifying these novel documents during
classification presents an important problem. This problem is called open-world
classification or open classification. This paper proposes a novel deep
learning based approach. It outperforms existing state-of-the-art techniques
dramatically.
",1,0,0,0,0,0
16986,Robust Imitation of Diverse Behaviors,"  Deep generative models have recently shown great promise in imitation
learning for motor control. Given enough data, even supervised approaches can
do one-shot imitation learning; however, they are vulnerable to cascading
failures when the agent trajectory diverges from the demonstrations. Compared
to purely supervised methods, Generative Adversarial Imitation Learning (GAIL)
can learn more robust controllers from fewer demonstrations, but is inherently
mode-seeking and more difficult to train. In this paper, we show how to combine
the favourable aspects of these two approaches. The base of our model is a new
type of variational autoencoder on demonstration trajectories that learns
semantic policy embeddings. We show that these embeddings can be learned on a 9
DoF Jaco robot arm in reaching tasks, and then smoothly interpolated with a
resulting smooth interpolation of reaching behavior. Leveraging these policy
representations, we develop a new version of GAIL that (1) is much more robust
than the purely-supervised controller, especially with few demonstrations, and
(2) avoids mode collapse, capturing many diverse behaviors when GAIL on its own
does not. We demonstrate our approach on learning diverse gaits from
demonstration on a 2D biped and a 62 DoF 3D humanoid in the MuJoCo physics
environment.
",1,0,0,0,0,0
10743,How a small quantum bath can thermalize long localized chains,"  We investigate the stability of the many-body localized (MBL) phase for a
system in contact with a single ergodic grain, modelling a Griffiths region
with low disorder. Our numerical analysis provides evidence that even a small
ergodic grain consisting of only 3 qubits can delocalize a localized chain, as
soon as the localization length exceeds a critical value separating localized
and extended regimes of the whole system. We present a simple theory,
consistent with the arguments in [Phys. Rev. B 95, 155129 (2017)], that assumes
a system to be locally ergodic unless the local relaxation time, determined by
Fermi's Golden Rule, is larger than the inverse level spacing. This theory
predicts a critical value for the localization length that is perfectly
consistent with our numerical calculations. We analyze in detail the behavior
of local operators inside and outside the ergodic grain, and find excellent
agreement of numerics and theory.
",0,1,0,0,0,0
13664,Intrinsically motivated reinforcement learning for human-robot interaction in the real-world,"  For a natural social human-robot interaction, it is essential for a robot to
learn the human-like social skills. However, learning such skills is
notoriously hard due to the limited availability of direct instructions from
people to teach a robot. In this paper, we propose an intrinsically motivated
reinforcement learning framework in which an agent gets the intrinsic
motivation-based rewards through the action-conditional predictive model. By
using the proposed method, the robot learned the social skills from the
human-robot interaction experiences gathered in the real uncontrolled
environments. The results indicate that the robot not only acquired human-like
social skills but also took more human-like decisions, on a test dataset, than
a robot which received direct rewards for the task achievement.
",1,0,0,0,0,0
20814,Audio-Visual Speech Enhancement based on Multimodal Deep Convolutional Neural Network,"  Speech enhancement (SE) aims to reduce noise in speech signals. Most SE
techniques focus on addressing audio information only. In this work, inspired
by multimodal learning, which utilizes data from different modalities, and the
recent success of convolutional neural networks (CNNs) in SE, we propose an
audio-visual deep CNN (AVDCNN) SE model, which incorporates audio and visual
streams into a unified network model. In the proposed AVDCNN SE model, audio
and visual data are first processed using individual CNNs, and then, fused into
a joint network to generate enhanced speech at the output layer. The AVDCNN
model is trained in an end-to-end manner, and parameters are jointly learned
through back-propagation. We evaluate enhanced speech using five objective
criteria. Results show that the AVDCNN yields notably better performance,
compared with an audio-only CNN-based SE model and two conventional SE
approaches, confirming the effectiveness of integrating visual information into
the SE process.
",1,0,0,1,0,0
14291,Dimer correlation amplitudes and dimer excitation gap in spin-1/2 XXZ and Heisenberg chains,"  Correlation functions of dimer operators, the product operators of spins on
two adjacent sites, are studied in the spin-$\frac{1}{2}$ XXZ chain in the
critical regime. The amplitudes of the leading oscillating terms in the dimer
correlation functions are determined with high accuracy as functions of the
exchange anisotropy parameter and the external magnetic field, through the
combined use of bosonization and density-matrix renormalization group methods.
In particular, for the antiferromagnetic Heisenberg model with SU(2) symmetry,
logarithmic corrections to the dimer correlations due to the
marginally-irrelevant operator are studied, and the asymptotic form of the
dimer correlation function is obtained. The asymptotic form of the spin-Peierls
excitation gap including logarithmic corrections is also derived.
",0,1,0,0,0,0
19999,Radial Surface Density Profiles of Gas and Dust in the Debris Disk around 49 Ceti,"  We present ~0.4 resolution images of CO(3-2) and associated continuum
emission from the gas-bearing debris disk around the nearby A star 49 Ceti,
observed with the Atacama Large Millimeter/Submillimeter Array (ALMA). We
analyze the ALMA visibilities in tandem with the broad-band spectral energy
distribution to measure the radial surface density profiles of dust and gas
emission from the system. The dust surface density decreases with radius
between ~100 and 310 au, with a marginally significant enhancement of surface
density at a radius of ~110 au. The SED requires an inner disk of small grains
in addition to the outer disk of larger grains resolved by ALMA. The gas disk
exhibits a surface density profile that increases with radius, contrary to most
previous spatially resolved observations of circumstellar gas disks. While ~80%
of the CO flux is well described by an axisymmetric power-law disk in Keplerian
rotation about the central star, residuals at ~20% of the peak flux exhibit a
departure from axisymmetry suggestive of spiral arms or a warp in the gas disk.
The radial extent of the gas disk (~220 au) is smaller than that of the dust
disk (~300 au), consistent with recent observations of other gas-bearing debris
disks. While there are so far only three broad debris disks with well
characterized radial dust profiles at millimeter wavelengths, 49 Ceti's disk
shows a markedly different structure from two radially resolved gas-poor debris
disks, implying that the physical processes generating and sculpting the gas
and dust are fundamentally different.
",0,1,0,0,0,0
6604,The Block Point Process Model for Continuous-Time Event-Based Dynamic Networks,"  Many application settings involve the analysis of timestamped relations or
events between a set of entities, e.g. messages between users of an on-line
social network. Static and discrete-time network models are typically used as
analysis tools in these settings; however, they discard a significant amount of
information by aggregating events over time to form network snapshots. In this
paper, we introduce a block point process model (BPPM) for dynamic networks
evolving in continuous time in the form of events at irregular time intervals.
The BPPM is inspired by the well-known stochastic block model (SBM) for static
networks and is a simpler version of the recently-proposed Hawkes infinite
relational model (IRM). We show that networks generated by the BPPM follow an
SBM in the limit of a growing number of nodes and leverage this property to
develop an efficient inference procedure for the BPPM. We fit the BPPM to
several real network data sets, including a Facebook network with over 3, 500
nodes and 130, 000 events, several orders of magnitude larger than the Hawkes
IRM and other existing point process network models.
",1,0,0,1,0,0
11983,Development of verification system of socio-demographic data of virtual community member,"  The important task of developing verification system of data of virtual
community member on the basis of computer-linguistic analysis of the content of
a large sample of Ukrainian virtual communities is solved. The subject of
research is methods and tools for verification of web-members socio-demographic
characteristics based on computer-linguistic analysis of their communicative
interaction results. The aim of paper is to verifying web-user personal data on
the basis of computer-linguistic analysis of web-members information tracks.
The structure of verification software for web-user profile is designed for a
practical implementation of assigned tasks. The method of personal data
verification of web-members by analyzing information track of virtual community
member is conducted. For the first time the method for checking the
authenticity of web members personal data, which helped to design of
verification tool for socio-demographic characteristics of web-member is
developed. The verification system of data of web-members, which forms the
verified socio-demographic profiles of web-members, is developed as a result of
conducted experiments. Also the user interface of the developed verification
system web-members data is presented. Effectiveness and efficiency of use of
the developed methods and means for solving tasks in web-communities
administration is proved by their approbation. The number of false results of
verification system is 18%.
",1,0,0,0,0,0
6686,Unexpected Enhancement of Three-Dimensional Low-Energy Spin Correlations in Quasi-Two-Dimensional Fe$_{1+y}$Te$_{1-x}$Se$_{x}$ System at High Temperature,"  We report inelastic neutron scattering measurements of low energy ($\hbar
\omega < 10$ meV) magnetic excitations in the ""11"" system
Fe$_{1+y}$Te$_{1-x}$Se$_{x}$. The spin correlations are two-dimensional (2D) in
the superconducting samples at low temperature, but appear much more
three-dimensional when the temperature rises well above $T_c \sim 15$ K, with a
clear increase of the (dynamic) spin correlation length perpendicular to the Fe
planes. The spontaneous change of dynamic spin correlations from 2D to 3D on
warming is unexpected and cannot be naturally explained when only the spin
degree of freedom is considered. Our results suggest that the low temperature
physics in the ""11"" system, in particular the evolution of low energy spin
excitations towards %better satisfying the nesting condition for mediating
superconducting pairing, is driven by changes in orbital correlations.
",0,1,0,0,0,0
3377,Estimating Under Five Mortality in Space and Time in a Developing World Context,"  Accurate estimates of the under-5 mortality rate (U5MR) in a developing world
context are a key barometer of the health of a nation. This paper describes new
models to analyze survey data on mortality in this context. We are interested
in both spatial and temporal description, that is, wishing to estimate U5MR
across regions and years, and to investigate the association between the U5MR
and spatially-varying covariate surfaces. We illustrate the methodology by
producing yearly estimates for subnational areas in Kenya over the period 1980
- 2014 using data from demographic health surveys (DHS). We use a binomial
likelihood with fixed effects for the urban/rural stratification to account for
the complex survey design. We carry out smoothing using Bayesian hierarchical
models with continuous spatial and temporally discrete components. A key
component of the model is an offset to adjust for bias due to the effects of
HIV epidemics. Substantively, there has been a sharp decline in U5MR in the
period 1980 - 2014, but large variability in estimated subnational rates
remains. A priority for future research is understanding this variability.
Temperature, precipitation and a measure of malaria infection prevalence were
candidates for inclusion in the covariate model.
",0,0,0,1,0,0
5686,On algebraic branching programs of small width,"  In 1979 Valiant showed that the complexity class VP_e of families with
polynomially bounded formula size is contained in the class VP_s of families
that have algebraic branching programs (ABPs) of polynomially bounded size.
Motivated by the problem of separating these classes we study the topological
closure VP_e-bar, i.e. the class of polynomials that can be approximated
arbitrarily closely by polynomials in VP_e. We describe VP_e-bar with a
strikingly simple complete polynomial (in characteristic different from 2)
whose recursive definition is similar to the Fibonacci numbers. Further
understanding this polynomial seems to be a promising route to new formula
lower bounds.
Our methods are rooted in the study of ABPs of small constant width. In 1992
Ben-Or and Cleve showed that formula size is polynomially equivalent to width-3
ABP size. We extend their result (in characteristic different from 2) by
showing that approximate formula size is polynomially equivalent to approximate
width-2 ABP size. This is surprising because in 2011 Allender and Wang gave
explicit polynomials that cannot be computed by width-2 ABPs at all! The
details of our construction lead to the aforementioned characterization of
VP_e-bar.
As a natural continuation of this work we prove that the class VNP can be
described as the class of families that admit a hypercube summation of
polynomially bounded dimension over a product of polynomially many affine
linear forms. This gives the first separations of algebraic complexity classes
from their nondeterministic analogs.
",1,0,0,0,0,0
6934,Numerical Observation of Parafermion Zero Modes and their Stability in 2D Topological States,"  The possibility of realizing non-Abelian excitations (non-Abelions) in
two-dimensional (2D) Abelian states of matter has generated a lot of interest
recently. A well-known example of such non-Abelions are parafermion zeros modes
(PFZMs) which can be realized at the endpoints of the so called genons in
fractional quantum Hall (FQH) states or fractional Chern insulators (FCIs). In
this letter, we discuss some known signatures of PFZMs and also introduce some
novel ones. In particular, we show that the topological entanglement entropy
(TEE) shifts by a quantized value after crossing PFZMs. Utilizing those
signatures, we present the first large scale numerical study of PFZMs and their
stability against perturbations in both FQH states and FCIs within the
density-Matrix-Renormalization-Group (DMRG) framework. Our results can help
build a closer connection with future experiments on FQH states with genons.
",0,1,0,0,0,0
15116,Identifying networks with common organizational principles,"  Many complex systems can be represented as networks, and the problem of
network comparison is becoming increasingly relevant. There are many techniques
for network comparison, from simply comparing network summary statistics to
sophisticated but computationally costly alignment-based approaches. Yet it
remains challenging to accurately cluster networks that are of a different size
and density, but hypothesized to be structurally similar. In this paper, we
address this problem by introducing a new network comparison methodology that
is aimed at identifying common organizational principles in networks. The
methodology is simple, intuitive and applicable in a wide variety of settings
ranging from the functional classification of proteins to tracking the
evolution of a world trade network.
",1,1,0,1,0,0
3770,Particle-without-Particle: a practical pseudospectral collocation method for linear partial differential equations with distributional sources,"  Partial differential equations with distributional sources---in particular,
involving (derivatives of) delta distributions---have become increasingly
ubiquitous in numerous areas of physics and applied mathematics. It is often of
considerable interest to obtain numerical solutions for such equations, but any
singular (""particle""-like) source modeling invariably introduces nontrivial
computational obstacles. A common method to circumvent these is through some
form of delta function approximation procedure on the computational grid;
however, this often carries significant limitations on the efficiency of the
numerical convergence rates, or sometimes even the resolvability of the problem
at all.
In this paper, we present an alternative technique for tackling such
equations which avoids the singular behavior entirely: the
""Particle-without-Particle"" method. Previously introduced in the context of the
self-force problem in gravitational physics, the idea is to discretize the
computational domain into two (or more) disjoint pseudospectral
(Chebyshev-Lobatto) grids such that the ""particle"" is always at the interface
between them; thus, one only needs to solve homogeneous equations in each
domain, with the source effectively replaced by jump (boundary) conditions
thereon. We prove here that this method yields solutions to any linear PDE the
source of which is any linear combination of delta distributions and
derivatives thereof supported on a one-dimensional subspace of the problem
domain. We then implement it to numerically solve a variety of relevant PDEs:
hyperbolic (with applications to neuroscience and acoustics), parabolic (with
applications to finance), and elliptic. We generically obtain improved
convergence rates relative to typical past implementations relying on delta
function approximations.
",0,0,0,0,1,1
4575,Bäcklund Transformations for the Boussinesq Equation and Merging Solitons,"  The Bäcklund transformation (BT) for the ""good"" Boussinesq equation and its
superposition principles are presented and applied. Unlike many other standard
integrable equations, the Boussinesq equation does not have a strictly
algebraic superposition principle for 2 BTs, but it does for 3. We present
associated lattice systems. Applying the BT to the trivial solution generates
standard solitons but also what we call ""merging solitons"" --- solutions in
which two solitary waves (with related speeds) merge into a single one. We use
the superposition principles to generate a variety of interesting solutions,
including superpositions of a merging soliton with $1$ or $2$ regular solitons,
and solutions that develop a singularity in finite time which then disappears
at some later finite time. We prove a Wronskian formula for the solutions
obtained by applying a general sequence of BTs on the trivial solution.
Finally, we show how to obtain the standard conserved quantities of the
Boussinesq equation from the BT, and how the hierarchy of local symmetries
follows in a simple manner from the superposition principle for 3 BTs.
",0,1,1,0,0,0
5554,Adaptive p-value weighting with power optimality,"  Weighting the p-values is a well-established strategy that improves the power
of multiple testing procedures while dealing with heterogeneous data. However,
how to achieve this task in an optimal way is rarely considered in the
literature. This paper contributes to fill the gap in the case of
group-structured null hypotheses, by introducing a new class of procedures
named ADDOW (for Adaptive Data Driven Optimal Weighting) that adapts both to
the alternative distribution and to the proportion of true null hypotheses. We
prove the asymptotical FDR control and power optimality among all weighted
procedures of ADDOW, which shows that it dominates all existing procedures in
that framework. Some numerical experiments show that the proposed method
preserves its optimal properties in the finite sample setting when the number
of tests is moderately large.
",0,0,1,1,0,0
20447,The collapse of ecosystem engineer populations,"  Humans are the ultimate ecosystem engineers who have profoundly transformed
the world's landscapes in order to enhance their survival. Somewhat
paradoxically, however, sometimes the unforeseen effect of this ecosystem
engineering is the very collapse of the population it intended to protect. Here
we use a spatial version of a standard population dynamics model of ecosystem
engineers to study the colonization of unexplored virgin territories by a small
settlement of engineers. We find that during the expansion phase the population
density reaches values much higher than those the environment can support in
the equilibrium situation. When the colonization front reaches the boundary of
the available space, the population density plunges sharply and attains its
equilibrium value. The collapse takes place without warning and happens just
after the population reaches its peak number. We conclude that overpopulation
and the consequent collapse of an expanding population of ecosystem engineers
is a natural consequence of the nonlinear feedback between the population and
environment variables.
",0,1,0,0,0,0
55,An Effective Framework for Constructing Exponent Lattice Basis of Nonzero Algebraic Numbers,"  Computing a basis for the exponent lattice of algebraic numbers is a basic
problem in the field of computational number theory with applications to many
other areas. The main cost of a well-known algorithm
\cite{ge1993algorithms,kauers2005algorithms} solving the problem is on
computing the primitive element of the extended field generated by the given
algebraic numbers. When the extended field is of large degree, the problem
seems intractable by the tool implementing the algorithm. In this paper, a
special kind of exponent lattice basis is introduced. An important feature of
the basis is that it can be inductively constructed, which allows us to deal
with the given algebraic numbers one by one when computing the basis. Based on
this, an effective framework for constructing exponent lattice basis is
proposed. Through computing a so-called pre-basis first and then solving some
linear Diophantine equations, the basis can be efficiently constructed. A new
certificate for multiplicative independence and some techniques for decreasing
degrees of algebraic numbers are provided to speed up the computation. The new
algorithm has been implemented with Mathematica and its effectiveness is
verified by testing various examples. Moreover, the algorithm is applied to
program verification for finding invariants of linear loops.
",1,0,0,0,0,0
13232,Neural Networks retrieving Boolean patterns in a sea of Gaussian ones,"  Restricted Boltzmann Machines are key tools in Machine Learning and are
described by the energy function of bipartite spin-glasses. From a statistical
mechanical perspective, they share the same Gibbs measure of Hopfield networks
for associative memory. In this equivalence, weights in the former play as
patterns in the latter. As Boltzmann machines usually require real weights to
be trained with gradient descent like methods, while Hopfield networks
typically store binary patterns to be able to retrieve, the investigation of a
mixed Hebbian network, equipped with both real (e.g., Gaussian) and discrete
(e.g., Boolean) patterns naturally arises. We prove that, in the challenging
regime of a high storage of real patterns, where retrieval is forbidden, an
extra load of Boolean patterns can still be retrieved, as long as the ratio
among the overall load and the network size does not exceed a critical
threshold, that turns out to be the same of the standard
Amit-Gutfreund-Sompolinsky theory. Assuming replica symmetry, we study the case
of a low load of Boolean patterns combining the stochastic stability and
Hamilton-Jacobi interpolating techniques. The result can be extended to the
high load by a non rigorous but standard replica computation argument.
",0,1,0,0,0,0
15500,Stability Analysis of Piecewise Affine Systems with Multi-model Model Predictive Control,"  Constrained model predictive control (MPC) is a widely used control strategy,
which employs moving horizon-based on-line optimisation to compute the optimum
path of the manipulated variables. Nonlinear MPC can utilize detailed models
but it is computationally expensive; on the other hand linear MPC may not be
adequate. Piecewise affine (PWA) models can describe the underlying nonlinear
dynamics more accurately, therefore they can provide a viable trade-off through
their use in multi-model linear MPC configurations, which avoid integer
programming. However, such schemes may introduce uncertainty affecting the
closed loop stability. In this work, we propose an input to output stability
analysis for closed loop systems, consisting of PWA models, where an observer
and multi-model linear MPC are applied together, under unstructured
uncertainty. Integral quadratic constraints (IQCs) are employed to assess the
robustness of MPC under uncertainty. We create a model pool, by performing
linearisation on selected transient points. All the possible uncertainties and
nonlinearities (including the controller) can be introduced in the framework,
assuming that they admit the appropriate IQCs, whilst the dissipation
inequality can provide necessary conditions incorporating IQCs. We demonstrate
the existence of static multipliers, which can reduce the conservatism of the
stability analysis significantly. The proposed methodology is demonstrated
through two engineering case studies.
",1,0,0,0,0,0
16868,Liveness-Driven Random Program Generation,"  Randomly generated programs are popular for testing compilers and program
analysis tools, with hundreds of bugs in real-world C compilers found by random
testing. However, existing random program generators may generate large amounts
of dead code (computations whose result is never used). This leaves relatively
little code to exercise a target compiler's more complex optimizations.
To address this shortcoming, we introduce liveness-driven random program
generation. In this approach the random program is constructed bottom-up,
guided by a simultaneous structural data-flow analysis to ensure that the
generator never generates dead code.
The algorithm is implemented as a plugin for the Frama-C framework. We
evaluate it in comparison to Csmith, the standard random C program generator.
Our tool generates programs that compile to more machine code with a more
complex instruction mix.
",1,0,0,0,0,0
4565,Penalty Alternating Direction Methods for Mixed-Integer Optimization: A New View on Feasibility Pumps,"  Feasibility pumps are highly effective primal heuristics for mixed-integer
linear and nonlinear optimization. However, despite their success in practice
there are only few works considering their theoretical properties. We show that
feasibility pumps can be seen as alternating direction methods applied to
special reformulations of the original problem, inheriting the convergence
theory of these methods. Moreover, we propose a novel penalty framework that
encompasses this alternating direction method, which allows us to refrain from
random perturbations that are applied in standard versions of feasibility pumps
in case of failure. We present a convergence theory for the new penalty based
alternating direction method and compare the new variant of the feasibility
pump with existing versions in an extensive numerical study for mixed-integer
linear and nonlinear problems.
",0,0,1,0,0,0
20134,Easy High-Dimensional Likelihood-Free Inference,"  We introduce a framework using Generative Adversarial Networks (GANs) for
likelihood--free inference (LFI) and Approximate Bayesian Computation (ABC)
where we replace the black-box simulator model with an approximator network and
generate a rich set of summary features in a data driven fashion. On benchmark
data sets, our approach improves on others with respect to scalability, ability
to handle high dimensional data and complex probability distributions.
",1,0,0,1,0,0
696,Dynamics of the multi-soliton waves in the sine-Gordon model with two identical point impurities,"  The particular type of four-kink multi-solitons (or quadrons) adiabatic
dynamics of the sine-Gordon equation in a model with two identical point
attracting impurities has been studied. This model can be used for describing
magnetization localized waves in multilayer ferromagnet. The quadrons structure
and properties has been numerically investigated. The cases of both large and
small distances between impurities has been viewed. The dependence of the
localized in impurity region nonlinear high-amplitude waves frequencies on the
distance between the impurities has been found. For an analytical description
of two bound localized on impurities nonlinear waves dynamics, using
perturbation theory, the system of differential equations for harmonic
oscillators with elastic link has been found. The analytical model
qualitatively describes the results of the sine-Gordon equation numerical
simulation.
",0,1,0,0,0,0
6707,SEIRS epidemics in growing populations,"  An SEIRS epidemic with disease fatalities is introduced in a growing
population (modelled as a super-critical linear birth and death process). The
study of the initial phase of the epidemic is stochastic, while the analysis of
the major outbreaks is deterministic. Depending on the values of the
parameters, the following scenarios are possible. i) The disease dies out
quickly, only infecting few; ii) the epidemic takes off, the \textit{number} of
infected individuals grows exponentially, but the \textit{fraction} of infected
individuals remains negligible; iii) the epidemic takes off, the
\textit{number} of infected grows initially quicker than the population, the
disease fatalities diminish the growth rate of the population, but it remains
super critical, and the \emph{fraction} of infected go to an endemic
equilibrium; iv) the epidemic takes off, the \textit{number} of infected
individuals grows initially quicker than the population, the diseases
fatalities turn the exponential growth of the population to an exponential
decay.
",0,1,0,0,0,0
17984,Introducing SPAIN (SParse Audion INpainter),"  A novel sparsity-based algorithm for audio inpainting is proposed by
translating the SPADE algorithm by Kitić et. al.---the state-of-the-art for
audio declipping---into the task of audio inpainting. SPAIN (SParse Audio
INpainter) comes in synthesis and analysis variants. Experiments show that both
A-SPAIN and S-SPAIN outperform other sparsity-based inpainting algorithms and
that A-SPAIN performs on a par with the state-of-the-art method based on linear
prediction.
",1,0,0,0,0,0
3586,Learning Aided Optimization for Energy Harvesting Devices with Outdated State Information,"  This paper considers utility optimal power control for energy harvesting
wireless devices with a finite capacity battery. The distribution information
of the underlying wireless environment and harvestable energy is unknown and
only outdated system state information is known at the device controller. This
scenario shares similarity with Lyapunov opportunistic optimization and online
learning but is different from both. By a novel combination of Zinkevich's
online gradient learning technique and the drift-plus-penalty technique from
Lyapunov opportunistic optimization, this paper proposes a learning-aided
algorithm that achieves utility within $O(\epsilon)$ of the optimal, for any
desired $\epsilon>0$, by using a battery with an $O(1/\epsilon)$ capacity. The
proposed algorithm has low complexity and makes power investment decisions
based on system history, without requiring knowledge of the system state or its
probability distribution.
",1,0,0,0,0,0
14401,Labeled Memory Networks for Online Model Adaptation,"  Augmenting a neural network with memory that can grow without growing the
number of trained parameters is a recent powerful concept with many exciting
applications. We propose a design of memory augmented neural networks (MANNs)
called Labeled Memory Networks (LMNs) suited for tasks requiring online
adaptation in classification models. LMNs organize the memory with classes as
the primary key.The memory acts as a second boosted stage following a regular
neural network thereby allowing the memory and the primary network to play
complementary roles. Unlike existing MANNs that write to memory for every
instance and use LRU based memory replacement, LMNs write only for instances
with non-zero loss and use label-based memory replacement. We demonstrate
significant accuracy gains on various tasks including word-modelling and
few-shot learning. In this paper, we establish their potential in online
adapting a batch trained neural network to domain-relevant labeled data at
deployment time. We show that LMNs are better than other MANNs designed for
meta-learning. We also found them to be more accurate and faster than
state-of-the-art methods of retuning model parameters for adapting to
domain-specific labeled data.
",1,0,0,1,0,0
1262,Group-Server Queues,"  By analyzing energy-efficient management of data centers, this paper proposes
and develops a class of interesting {\it Group-Server Queues}, and establishes
two representative group-server queues through loss networks and impatient
customers, respectively. Furthermore, such two group-server queues are given
model descriptions and necessary interpretation. Also, simple mathematical
discussion is provided, and simulations are made to study the expected queue
lengths, the expected sojourn times and the expected virtual service times. In
addition, this paper also shows that this class of group-server queues are
often encountered in many other practical areas including communication
networks, manufacturing systems, transportation networks, financial networks
and healthcare systems. Note that the group-server queues are always used to
design effectively dynamic control mechanisms through regrouping and
recombining such many servers in a large-scale service system by means of, for
example, bilateral threshold control, and customers transfer to the buffer or
server groups. This leads to the large-scale service system that is divided
into several adaptive and self-organizing subsystems through scheduling of
batch customers and regrouping of service resources, which make the middle
layer of this service system more effectively managed and strengthened under a
dynamic, real-time and even reward optimal framework. Based on this,
performance of such a large-scale service system may be improved greatly in
terms of introducing and analyzing such group-server queues. Therefore, not
only analysis of group-server queues is regarded as a new interesting research
direction, but there also exists many theoretical challenges, basic
difficulties and open problems in the area of queueing networks.
",1,0,0,0,0,0
9877,Redundancy schemes for engineering coherent systems via a signature-based approach,"  This paper proposes a signature-based approach for solving redundancy
allocation problems when component lifetimes are not only heterogeneous but
also dependent. The two common schemes for allocations, that is active and
standby redundancies, are considered. If the component lifetimes are
independent, the proposed approach leads to simple manipulations. Various
illustrative examples are also analysed. This method can be implemented for
practical complex engineering systems.
",0,0,0,1,0,0
16145,Optimal continuous-time ALM for insurers: a martingale approach,"  We study a continuous-time asset-allocation problem for a firm in the
insurance industry that backs up the liabilities raised by the insurance
contracts with the underwriting profits and the income resulting from investing
in the financial market. Using the martingale approach and convex duality
techniques we characterize strategies that maximize expected utility from
consumption and final wealth under CRRA preferences. We present numerical
results for some distributions of claims/liabilities with policy limits.
",0,0,0,0,0,1
9930,A dequantized metaplectic knot invariant,"  Let $K\subset S^3$ be a knot, $X:= S^3\setminus K$ its complement, and
$\mathbb{T}$ the circle group identified with $\mathbb{R}/\mathbb{Z}$. To any
oriented long knot diagram of $K$, we associate a quadratic polynomial in
variables bijectively associated with the bridges of the diagram such that,
when the variables projected to $\mathbb{T}$ satisfy the linear equations
characterizing the first homology group $H_1(\tilde{X}_2)$ of the double cyclic
covering of $X$, the polynomial projects down to a well defined
$\mathbb{T}$-valued function on $T^1(\tilde{X}_2,\mathbb{T})$ (the dual of the
torsion part $T_1$ of $H_1$). This function is sensitive to knot chirality, for
example, it seems to confirm chirality of the knot $10_{71}$. It also
distinguishes the knots $7_4$ and $9_2$ known to have identical Alexander
polynomials and the knots $9_2$ and K11n13 known to have identical Jones
polynomials but does not distinguish $7_4$ and K11n13.
",0,0,1,0,0,0
10155,The challenge of decentralized marketplaces,"  Online trust systems are playing an important role in to-days world and face
various challenges in building them. Billions of dollars of products and
services are traded through electronic commerce, files are shared among large
peer-to-peer networks and smart contracts can potentially replace paper
contracts with digital contracts. These systems rely on trust mechanisms in
peer-to-peer networks like reputation systems or a trustless public ledger. In
most cases, reputation systems are build to determine the trustworthiness of
users and to provide incentives for users to make a fair contribution to the
peer-to-peer network. The main challenges are how to set up a good trust
system, how to deal with security issues and how to deal with strategic users
trying to cheat on the system. The Sybil attack, the most important attack on
reputation systems is discussed. At last match making in two sided markets and
the strategy proofness of these markets are discussed.
",1,0,0,0,0,0
11488,Kitting in the Wild through Online Domain Adaptation,"  Technological developments call for increasing perception and action
capabilities of robots. Among other skills, vision systems that can adapt to
any possible change in the working conditions are needed. Since these
conditions are unpredictable, we need benchmarks which allow to assess the
generalization and robustness capabilities of our visual recognition
algorithms. In this work we focus on robotic kitting in unconstrained
scenarios. As a first contribution, we present a new visual dataset for the
kitting task. Differently from standard object recognition datasets, we provide
images of the same objects acquired under various conditions where camera,
illumination and background are changed. This novel dataset allows for testing
the robustness of robot visual recognition algorithms to a series of different
domain shifts both in isolation and unified. Our second contribution is a novel
online adaptation algorithm for deep models, based on batch-normalization
layers, which allows to continuously adapt a model to the current working
conditions. Differently from standard domain adaptation algorithms, it does not
require any image from the target domain at training time. We benchmark the
performance of the algorithm on the proposed dataset, showing its capability to
fill the gap between the performances of a standard architecture and its
counterpart adapted offline to the given target domain.
",1,0,0,0,0,0
13312,The braid group for a quiver with superpotential,"  We survey and compare various generalizations of braid groups for quivers
with superpotential and focus on the cluster braid groups, which are introduced
in a joint work with A.~King. Our motivations come from the study of cluster
algebras, Calabi-Yau categories and Bridgeland stability conditions.
",0,0,1,0,0,0
20843,Design Considerations for Proposed Fermilab Integrable RCS,"  Integrable optics is an innovation in particle accelerator design that
provides strong nonlinear focusing while avoiding parametric resonances. One
promising application of integrable optics is to overcome the traditional
limits on accelerator intensity imposed by betatron tune-spread and collective
instabilities. The efficacy of high-intensity integrable accelerators will be
undergo comprehensive testing over the next several years at the Fermilab
Integrable Optics Test Accelerator (IOTA) and the University of Maryland
Electron Ring (UMER). We propose an integrable Rapid-Cycling Synchrotron (iRCS)
as a replacement for the Fermilab Booster to achieve multi-MW beam power for
the Fermilab high-energy neutrino program. We provide a overview of the machine
parameters and discuss an approach to lattice optimization. Integrable optics
requires arcs with integer-pi phase advance followed by drifts with matched
beta functions. We provide an example integrable lattice with features of a
modern RCS - long dispersion-free drifts, low momentum compaction,
superperiodicity, chromaticity correction, separate-function magnets, and
bounded beta functions.
",0,1,0,0,0,0
6904,The multi-resonant Lugiato-Lefever model,"  We introduce a new model describing multiple resonances in Kerr optical
cavities. It perfectly agrees quantitatively with the Ikeda map and predicts
complex phenomena such as super cavity solitons and coexistence of multiple
nonlinear states.
",0,1,0,0,0,0
11042,DeepProbe: Information Directed Sequence Understanding and Chatbot Design via Recurrent Neural Networks,"  Information extraction and user intention identification are central topics
in modern query understanding and recommendation systems. In this paper, we
propose DeepProbe, a generic information-directed interaction framework which
is built around an attention-based sequence to sequence (seq2seq) recurrent
neural network. DeepProbe can rephrase, evaluate, and even actively ask
questions, leveraging the generative ability and likelihood estimation made
possible by seq2seq models. DeepProbe makes decisions based on a derived
uncertainty (entropy) measure conditioned on user inputs, possibly with
multiple rounds of interactions. Three applications, namely a rewritter, a
relevance scorer and a chatbot for ad recommendation, were built around
DeepProbe, with the first two serving as precursory building blocks for the
third. We first use the seq2seq model in DeepProbe to rewrite a user query into
one of standard query form, which is submitted to an ordinary recommendation
system. Secondly, we evaluate DeepProbe's seq2seq model-based relevance
scoring. Finally, we build a chatbot prototype capable of making active user
interactions, which can ask questions that maximize information gain, allowing
for a more efficient user intention idenfication process. We evaluate first two
applications by 1) comparing with baselines by BLEU and AUC, and 2) human judge
evaluation. Both demonstrate significant improvements compared with current
state-of-the-art systems, proving their values as useful tools on their own,
and at the same time laying a good foundation for the ongoing chatbot
application.
",1,0,0,1,0,0
20676,Social Media Analysis For Organizations: Us Northeastern Public And State Libraries Case Study,"  Social networking sites such as Twitter have provided a great opportunity for
organizations such as public libraries to disseminate information for public
relations purposes. However, there is a need to analyze vast amounts of social
media data. This study presents a computational approach to explore the content
of tweets posted by nine public libraries in the northeastern United States of
America. In December 2017, this study extracted more than 19,000 tweets from
the Twitter accounts of seven state libraries and two urban public libraries.
Computational methods were applied to collect the tweets and discover
meaningful themes. This paper shows how the libraries have used Twitter to
represent their services and provides a starting point for different
organizations to evaluate the themes of their public tweets.
",1,0,0,1,0,0
13485,The Kinematics of the Permitted C II $λ$ 6578 Line in a Large Sample of Planetary Nebulae,"  We present spectroscopic observations of the C II $\lambda$6578 permitted
line for 83 lines of sight in 76 planetary nebulae at high spectral resolution,
most of them obtained with the Manchester Echelle Spectrograph on the 2.1\,m
telescope at the Observatorio Astronómico Nacional on the Sierra San Pedro
Mártir. We study the kinematics of the C II $\lambda$6578 permitted line with
respect to other permitted and collisionally-excited lines. Statistically, we
find that the kinematics of the C II $\lambda$6578 line are not those expected
if this line arises from the recombination of C$^{2+}$ ions or the fluorescence
of C$^+$ ions in ionization equilibrium in a chemically-homogeneous nebular
plasma, but instead its kinematics are those appropriate for a volume more
internal than expected. The planetary nebulae in this sample have well-defined
morphology and are restricted to a limited range in H$\alpha$ line widths (no
large values) compared to their counterparts in the Milky Way bulge, both of
which could be interpreted as the result of young nebular shells, an inference
that is also supported by nebular modeling. Concerning the long-standing
discrepancy between chemical abundances inferred from permitted and
collisionally-excited emission lines in photoionized nebulae, our results imply
that multiple plasma components occur commonly in planetary nebulae.
",0,1,0,0,0,0
1531,Rethinking generalization requires revisiting old ideas: statistical mechanics approaches and complex learning behavior,"  We describe an approach to understand the peculiar and counterintuitive
generalization properties of deep neural networks. The approach involves going
beyond worst-case theoretical capacity control frameworks that have been
popular in machine learning in recent years to revisit old ideas in the
statistical mechanics of neural networks. Within this approach, we present a
prototypical Very Simple Deep Learning (VSDL) model, whose behavior is
controlled by two control parameters, one describing an effective amount of
data, or load, on the network (that decreases when noise is added to the
input), and one with an effective temperature interpretation (that increases
when algorithms are early stopped). Using this model, we describe how a very
simple application of ideas from the statistical mechanics theory of
generalization provides a strong qualitative description of recently-observed
empirical results regarding the inability of deep neural networks not to
overfit training data, discontinuous learning and sharp transitions in the
generalization properties of learning algorithms, etc.
",1,0,0,1,0,0
2239,Peephole: Predicting Network Performance Before Training,"  The quest for performant networks has been a significant force that drives
the advancements of deep learning in recent years. While rewarding, improving
network design has never been an easy journey. The large design space combined
with the tremendous cost required for network training poses a major obstacle
to this endeavor. In this work, we propose a new approach to this problem,
namely, predicting the performance of a network before training, based on its
architecture. Specifically, we develop a unified way to encode individual
layers into vectors and bring them together to form an integrated description
via LSTM. Taking advantage of the recurrent network's strong expressive power,
this method can reliably predict the performances of various network
architectures. Our empirical studies showed that it not only achieved accurate
predictions but also produced consistent rankings across datasets -- a key
desideratum in performance prediction.
",1,0,0,1,0,0
13793,A Modified Sigma-Pi-Sigma Neural Network with Adaptive Choice of Multinomials,"  Sigma-Pi-Sigma neural networks (SPSNNs) as a kind of high-order neural
networks can provide more powerful mapping capability than the traditional
feedforward neural networks (Sigma-Sigma neural networks). In the existing
literature, in order to reduce the number of the Pi nodes in the Pi layer, a
special multinomial P_s is used in SPSNNs. Each monomial in P_s is linear with
respect to each particular variable sigma_i when the other variables are taken
as constants. Therefore, the monomials like sigma_i^n or sigma_i^n sigma_j with
n>1 are not included. This choice may be somehow intuitive, but is not
necessarily the best. We propose in this paper a modified Sigma-Pi-Sigma neural
network (MSPSNN) with an adaptive approach to find a better multinomial for a
given problem. To elaborate, we start from a complete multinomial with a given
order. Then we employ a regularization technique in the learning process for
the given problem to reduce the number of monomials used in the multinomial,
and end up with a new SPSNN involving the same number of monomials (= the
number of nodes in the Pi-layer) as in P_s. Numerical experiments on some
benchmark problems show that our MSPSNN behaves better than the traditional
SPSNN with P_s.
",0,0,0,1,0,0
1164,Replica analysis of overfitting in regression models for time-to-event data,"  Overfitting, which happens when the number of parameters in a model is too
large compared to the number of data points available for determining these
parameters, is a serious and growing problem in survival analysis. While modern
medicine presents us with data of unprecedented dimensionality, these data
cannot yet be used effectively for clinical outcome prediction. Standard error
measures in maximum likelihood regression, such as p-values and z-scores, are
blind to overfitting, and even for Cox's proportional hazards model (the main
tool of medical statisticians), one finds in literature only rules of thumb on
the number of samples required to avoid overfitting. In this paper we present a
mathematical theory of overfitting in regression models for time-to-event data,
which aims to increase our quantitative understanding of the problem and
provide practical tools with which to correct regression outcomes for the
impact of overfitting. It is based on the replica method, a statistical
mechanical technique for the analysis of heterogeneous many-variable systems
that has been used successfully for several decades in physics, biology, and
computer science, but not yet in medical statistics. We develop the theory
initially for arbitrary regression models for time-to-event data, and verify
its predictions in detail for the popular Cox model.
",0,1,0,1,0,0
8669,Structured Variational Inference for Coupled Gaussian Processes,"  Sparse variational approximations allow for principled and scalable inference
in Gaussian Process (GP) models. In settings where several GPs are part of the
generative model, theses GPs are a posteriori coupled. For many applications
such as regression where predictive accuracy is the quantity of interest, this
coupling is not crucial. Howewer if one is interested in posterior uncertainty,
it cannot be ignored. A key element of variational inference schemes is the
choice of the approximate posterior parameterization. When the number of latent
variables is large, mean field (MF) methods provide fast and accurate posterior
means while more structured posterior lead to inference algorithm of greater
computational complexity. Here, we extend previous sparse GP approximations and
propose a novel parameterization of variational posteriors in the multi-GP
setting allowing for fast and scalable inference capturing posterior
dependencies.
",1,0,0,1,0,0
7334,Belief Propagation Min-Sum Algorithm for Generalized Min-Cost Network Flow,"  Belief Propagation algorithms are instruments used broadly to solve graphical
model optimization and statistical inference problems. In the general case of a
loopy Graphical Model, Belief Propagation is a heuristic which is quite
successful in practice, even though its empirical success, typically, lacks
theoretical guarantees. This paper extends the short list of special cases
where correctness and/or convergence of a Belief Propagation algorithm is
proven. We generalize formulation of Min-Sum Network Flow problem by relaxing
the flow conservation (balance) constraints and then proving that the Belief
Propagation algorithm converges to the exact result.
",1,0,0,1,0,0
2389,The Stochastic Firefighter Problem,"  The dynamics of infectious diseases spread is crucial in determining their
risk and offering ways to contain them. We study sequential vaccination of
individuals in networks. In the original (deterministic) version of the
Firefighter problem, a fire breaks out at some node of a given graph. At each
time step, b nodes can be protected by a firefighter and then the fire spreads
to all unprotected neighbors of the nodes on fire. The process ends when the
fire can no longer spread. We extend the Firefighter problem to a probabilistic
setting, where the infection is stochastic. We devise a simple policy that only
vaccinates neighbors of infected nodes and is optimal on regular trees and on
general graphs for a sufficiently large budget. We derive methods for
calculating upper and lower bounds of the expected number of infected
individuals, as well as provide estimates on the budget needed for containment
in expectation. We calculate these explicitly on trees, d-dimensional grids,
and Erdős Rényi graphs. Finally, we construct a state-dependent budget
allocation strategy and demonstrate its superiority over constant budget
allocation on real networks following a first order acquaintance vaccination
policy.
",1,0,0,0,0,0
14649,Assessing the performance of self-consistent hybrid functional for band gap calculation in oxide semiconductors,"  In this paper we assess the predictive power of the self-consistent hybrid
functional scPBE0 in calculating the band gap of oxide semiconductors. The
computational procedure is based on the self-consistent evaluation of the
mixing parameter $\alpha$ by means of an iterative calculation of the static
dielectric constant using the perturbation expansion after discretization
(PEAD) method and making use of the relation $\alpha = 1/\epsilon_{\infty}$.
Our materials dataset is formed by 30 compounds covering a wide range of band
gaps and dielectric properties, and includes materials with a wide spectrum of
application as thermoelectrics, photocatalysis, photovoltaics, transparent
conducting oxides, and refractory materials. Our results show that the scPBE0
functional provides better band gaps than the non self-consistent hybrids PBE0
and HSE06, but scPBE0 does not show significant improvement on the description
of the static dielectric constants. Overall, the scPBE0 data exhibit a mean
absolute percentage error of 14 \% (band gaps) and 10 \% ($\epsilon_\infty$).
For materials with weak dielectric screening and large excitonic biding
energies scPBE0, unlike PBE0 and HSE06, overestimates the band gaps, but the
value of the gap become very close to the experimental value when excitonic
effects are included (e.g. for SiO$_2$). However, special caution must be given
to the compounds with small band gaps due to the tendency of scPBE0 to
overestimate the dielectric constant in proximity of the metallic limit.
",0,1,0,0,0,0
140,Evaluating openEHR for storing computable representations of electronic health record phenotyping algorithms,"  Electronic Health Records (EHR) are data generated during routine clinical
care. EHR offer researchers unprecedented phenotypic breadth and depth and have
the potential to accelerate the pace of precision medicine at scale. A main EHR
use-case is creating phenotyping algorithms to define disease status, onset and
severity. Currently, no common machine-readable standard exists for defining
phenotyping algorithms which often are stored in human-readable formats. As a
result, the translation of algorithms to implementation code is challenging and
sharing across the scientific community is problematic. In this paper, we
evaluate openEHR, a formal EHR data specification, for computable
representations of EHR phenotyping algorithms.
",1,0,0,0,0,0
6163,Sliced-Wasserstein Flows: Nonparametric Generative Modeling via Optimal Transport and Diffusions,"  By building up on the recent theory that established the connection between
implicit generative modeling and optimal transport, in this study, we propose a
novel parameter-free algorithm for learning the underlying distributions of
complicated datasets and sampling from them. The proposed algorithm is based on
a functional optimization problem, which aims at finding a measure that is
close to the data distribution as much as possible and also expressive enough
for generative modeling purposes. We formulate the problem as a gradient flow
in the space of probability measures. The connections between gradient flows
and stochastic differential equations let us develop a computationally
efficient algorithm for solving the optimization problem, where the resulting
algorithm resembles the recent dynamics-based Markov Chain Monte Carlo
algorithms. We provide formal theoretical analysis where we prove finite-time
error guarantees for the proposed algorithm. Our experimental results support
our theory and shows that our algorithm is able to capture the structure of
challenging distributions.
",0,0,0,1,0,0
5395,Evaluation of equity-based debt obligations,"  We consider a class of participation rights, i.e. obligations issued by a
company to investors who are interested in performance-based compensation.
Albeit having desirable economic properties equity-based debt obligations
(EbDO) pose challenges in accounting and contract pricing. We formulate and
solve the associated mathematical problem in a discrete time, as well as a
continuous time setting. In the latter case the problem is reduced to a
forward-backward stochastic differential equation (FBSDE) and solved using the
method of decoupling fields.
",0,0,0,0,0,1
2877,Limits to Arbitrage in Markets with Stochastic Settlement Latency,"  Distributed ledger technologies rely on consensus protocols confronting
traders with random waiting times until the transfer of ownership is
accomplished. This time-consuming settlement process exposes arbitrageurs to
price risk and imposes limits to arbitrage. We derive theoretical arbitrage
boundaries under general assumptions and show that they increase with expected
latency, latency uncertainty, spot volatility, and risk aversion. Using
high-frequency data from the Bitcoin network, we estimate arbitrage boundaries
due to settlement latency of on average 124 basis points, covering 88 percent
of the observed cross-exchange price differences. Settlement through
decentralized systems thus induces non-trivial frictions affecting market
efficiency and price formation.
",0,0,0,0,0,1
3234,Computational Thinking in Education: Where does it Fit? A systematic literary review,"  Computational Thinking (CT) has been described as an essential skill which
everyone should learn and can therefore include in their skill set. Seymour
Papert is credited as concretising Computational Thinking in 1980 but since
Wing popularised the term in 2006 and brought it to the international
community's attention, more and more research has been conducted on CT in
education. The aim of this systematic literary review is to give educators and
education researchers an overview of what work has been carried out in the
domain, as well as potential gaps and opportunities that still exist.
Overall it was found in this review that, although there is a lot of work
currently being done around the world in many different educational contexts,
the work relating to CT is still in its infancy. Along with the need to create
an agreed-upon definition of CT lots of countries are still in the process of,
or have not yet started, introducing CT into curriculums in all levels of
education. It was also found that Computer Science/Computing, which could be
the most obvious place to teach CT, has yet to become a mainstream subject in
some countries, although this is improving. Of encouragement to educators is
the wealth of tools and resources being developed to help teach CT as well as
more and more work relating to curriculum development. For those teachers
looking to incorporate CT into their schools or classes then there are
bountiful options which include programming, hands-on exercises and more. The
need for more detailed lesson plans and curriculum structure however, is
something that could be of benefit to teachers.
",1,1,0,0,0,0
19713,A Fully Convolutional Network for Semantic Labeling of 3D Point Clouds,"  When classifying point clouds, a large amount of time is devoted to the
process of engineering a reliable set of features which are then passed to a
classifier of choice. Generally, such features - usually derived from the
3D-covariance matrix - are computed using the surrounding neighborhood of
points. While these features capture local information, the process is usually
time-consuming, and requires the application at multiple scales combined with
contextual methods in order to adequately describe the diversity of objects
within a scene. In this paper we present a 1D-fully convolutional network that
consumes terrain-normalized points directly with the corresponding spectral
data,if available, to generate point-wise labeling while implicitly learning
contextual features in an end-to-end fashion. Our method uses only the
3D-coordinates and three corresponding spectral features for each point.
Spectral features may either be extracted from 2D-georeferenced images, as
shown here for Light Detection and Ranging (LiDAR) point clouds, or extracted
directly for passive-derived point clouds,i.e. from muliple-view imagery. We
train our network by splitting the data into square regions, and use a pooling
layer that respects the permutation-invariance of the input points. Evaluated
using the ISPRS 3D Semantic Labeling Contest, our method scored second place
with an overall accuracy of 81.6%. We ranked third place with a mean F1-score
of 63.32%, surpassing the F1-score of the method with highest accuracy by
1.69%. In addition to labeling 3D-point clouds, we also show that our method
can be easily extended to 2D-semantic segmentation tasks, with promising
initial results.
",1,0,0,1,0,0
4283,Origin of meteoritic stardust unveiled by a revised proton-capture rate of $^{17}$O,"  Stardust grains recovered from meteorites provide high-precision snapshots of
the isotopic composition of the stellar environment in which they formed.
Attributing their origin to specific types of stars, however, often proves
difficult. Intermediate-mass stars of 4-8 solar masses are expected to
contribute a large fraction of meteoritic stardust. However, no grains have
been found with characteristic isotopic compositions expected from such stars.
This is a long-standing puzzle, which points to serious gaps in our
understanding of the lifecycle of stars and dust in our Galaxy. Here we show
that the increased proton-capture rate of $^{17}$O reported by a recent
underground experiment leads to $^{17}$O/$^{16}$O isotopic ratios that match
those observed in a population of stardust grains, for proton-burning
temperatures of 60-80 million K. These temperatures are indeed achieved at the
base of the convective envelope during the late evolution of intermediate-mass
stars of 4-8 solar masses, which reveals them as the most likely site of origin
of the grains. This result provides the first direct evidence that these stars
contributed to the dust inventory from which the Solar System formed.
",0,1,0,0,0,0
9768,SONS: The JCMT legacy survey of debris discs in the submillimetre,"  Debris discs are evidence of the ongoing destructive collisions between
planetesimals, and their presence around stars also suggests that planets exist
in these systems. In this paper, we present submillimetre images of the thermal
emission from debris discs that formed the SCUBA-2 Observations of Nearby Stars
(SONS) survey, one of seven legacy surveys undertaken on the James Clerk
Maxwell telescope between 2012 and 2015. The overall results of the survey are
presented in the form of 850 microns (and 450 microns, where possible) images
and fluxes for the observed fields. Excess thermal emission, over that expected
from the stellar photosphere, is detected around 49 stars out of the 100
observed fields. The discs are characterised in terms of their flux density,
size (radial distribution of the dust) and derived dust properties from their
spectral energy distributions. The results show discs over a range of sizes,
typically 1-10 times the diameter of the Edgeworth-Kuiper Belt in our Solar
System. The mass of a disc, for particles up to a few millimetres in size, is
uniquely obtainable with submillimetre observations and this quantity is
presented as a function of the host stars' age, showing a tentative decline in
mass with age. Having doubled the number of imaged discs at submillimetre
wavelengths from ground-based, single dish telescope observations, one of the
key legacy products from the SONS survey is to provide a comprehensive target
list to observe at high angular resolution using submillimetre/millimetre
interferometers (e.g., ALMA, SMA).
",0,1,0,0,0,0
8645,A Note on Bayesian Model Selection for Discrete Data Using Proper Scoring Rules,"  We consider the problem of choosing between parametric models for a discrete
observable, taking a Bayesian approach in which the within-model prior
distributions are allowed to be improper. In order to avoid the ambiguity in
the marginal likelihood function in such a case, we apply a homogeneous scoring
rule. For the particular case of distinguishing between Poisson and Negative
Binomial models, we conduct simulations that indicate that, applied
prequentially, the method will consistently select the true model.
",0,0,1,1,0,0
11508,Coupled Electron-Ion Monte Carlo simulation of hydrogen molecular crystals,"  We performed simulations for solid molecular hydrogen at high pressures
(250GPa$\leq$P$\leq$500GPa) along two isotherms at T=200 K (phases III and VI)
and at T=414 K (phase IV). At T=200K we considered likely candidates for phase
III, the C2c and Cmca12 structures, while at T=414K in phase IV we studied the
Pc48 structure. We employed both Coupled Electron-Ion Monte Carlo (CEIMC) and
Path Integral Molecular Dynamics (PIMD) based on Density Functional Theory
(DFT) using the vdW-DF approximation. The comparison between the two methods
allows us to address the question of the accuracy of the xc approximation of
DFT for thermal and quantum protons without recurring to perturbation theories.
In general, we find that atomic and molecular fluctuations in PIMD are larger
than in CEIMC which suggests that the potential energy surface from vdW-DF is
less structured than the one from Quantum Monte Carlo. We find qualitatively
different behaviors for systems prepared in the C2c structure for increasing
pressure. Within PIMD the C2c structure is dynamically partially stable for
P$\leq$250GPa only: it retains the symmetry of the molecular centers but not
the molecular orientation; at intermediate pressures it develops layered
structures like Pbcn or Ibam and transforms to the metallic Cmca-4 structure at
P$\geq$450GPa. Instead, within CEIMC, the C2c structure is found to be
dynamically stable at least up to 450GPa; at increasing pressure the molecular
bond length increases and the nuclear correlation decreases. For the other two
structures the two methods are in qualitative agreement although quantitative
differences remain. We discuss various structural properties and the electrical
conductivity. We find these structures become conducting around 350GPa but the
metallic Drude-like behavior is reached only at around 500GPa, consistent with
recent experimental claims.
",0,1,0,0,0,0
6578,A sure independence screening procedure for ultra-high dimensional partially linear additive models,"  We introduce a two-step procedure, in the context of ultra-high dimensional
additive models, which aims to reduce the size of covariates vector and
distinguish linear and nonlinear effects among nonzero components. Our proposed
screening procedure, in the first step, is constructed based on the concept of
cumulative distribution function and conditional expectation of response in the
framework of marginal correlation. B-splines and empirical distribution
functions are used to estimate the two above measures. The sure property of
this procedure is also established. In the second step, a double penalization
based procedure is applied to identify nonzero and linear components,
simultaneously. The performance of the designed method is examined by several
test functions to show its capabilities against competitor methods when errors
distribution are varied. Simulation studies imply that the proposed screening
procedure can be applied to the ultra-high dimensional data and well detect the
in uential covariates. It is also demonstrate the superiority in comparison
with the existing methods. This method is also applied to identify most in
uential genes for overexpression of a G protein-coupled receptor in mice.
",0,0,1,1,0,0
11839,Ab initio design of drug carriers for zoledronate guest molecule using phosphonated and sulfonated calix[4]arene and calix[4]resorcinarene host molecules,"  Monomolecular drug carriers based on calix[n]-arenes and -resorcinarenes
containing the interior cavity can enhance the affinity and specificity of the
osteoporosis inhibitor drug zoledronate (ZOD). In this work we investigate the
suitability of nine different calix[4]-arenes and -resorcinarenes based
macrocycles as hosts for the ZOD guest molecule by conducting {\it ab initio}
density functional theory calculations for structures and energetics of
eighteen different host-guest complexes. For the optimized molecular structures
of the free, phosphonated, sulfonated calix[4]-arenes and -resorcinarenes, the
geometric sizes of their interior cavities are measured and compared with those
of the host-guest complexes in order to check the appropriateness for
host-guest complex formation. Our calculations of binding energies indicate
that in gaseous states some of the complexes might be unstable but in aqueous
states almost all of the complexes can be formed spontaneously. Of the two
different docking ways, the insertion of ZOD with the \ce{P-C-P} branch into
the cavity of host is easier than that with the nitrogen containing heterocycle
of ZOD. The work will open a way for developing effective drug delivering
systems for the ZOD drug and promote experimentalists to synthesize them.
",0,1,0,0,0,0
14279,An empirical evaluation of alternative methods of estimation for Permutation Entropy in time series with tied values,"  Bandt and Pompe introduced Permutation Entropy in 2002 for Time Series where
equal values, xt1 = xt2, t1 = t2, were neglected and only inequalities between
the xt were considered. Since then, this measure has been modified and
extended, in particular in cases when the amount of equal values in the series
can not be neglected, (i.e. heart rate variability (HRV) time series). We
review the different existing methodologies that treats this subject by
classifying them according to their different strategies. In addition, a novel
Bayesian Missing Data Imputation is presented that proves to outperform the
existing methodologies that deals with type of time series. All this facts are
illustrated by simulations and also by distinguishing patients suffering from
Congestive Heart Failure from a (healthy) control group using HRV time series
",0,0,0,1,0,0
4270,Generalization of Special Functions and its Applications to Multiplicative and Ordinary Fractional Derivatives,"  The goal of this paper is to extend the classical and multiplicative
fractional derivatives. For this purpose, it is introduced the new extended
modified Bessel function and also given an important relation between this new
function I(v,q;x) and the confluent hypergeometric function. Besides, it is
used to generalize the hypergeometric, the confluent hypergeometric and the
extended beta functions by using the new extended modified Bessel function.
Also, the asymptotic formulae and the generating function of the extended
modified Bessel function are obtained. The extensions of classical and
multiplicative fractional derivatives are defined via extended modified Bessel
function and, first time the fractional derivative of rational functions is
explicitly given via complex partial fraction decomposition.
",0,0,1,0,0,0
14927,The dimensionless dissipation rate and the Kolmogorov (1941) hypothesis of local stationarity in freely decaying isotropic turbulence,"  An expression for the dimensionless dissipation rate was derived from the
Karman-Howarth equation by asymptotic expansion of the second- and third- order
structure functions in powers of the inverse Reynolds number. The implications
of the time-derivative term for the assumption of local stationarity (or local
equilibrium) which underpins the derivation of the Kolmogorov `4/5' law for the
third-order structure function were studied. It was concluded that neglect of
the time-derivative cannot be justified by reason of restriction to certain
scales (the inertial range) nor to large Reynolds numbers. In principle,
therefore, the hypothesis cannot be correct, although it may be a good
approximation. It follows, at least in principle, that the quantitative aspects
of the hypothesis of local stationarity could be tested by a comparison of the
asymptotic dimensionless dissipation rate for free decay with that for the
stationary case. But in practice this is complicated by the absence of an
agreed evolution time for making the measurements during the decay. However, we
can assess the quantitative error involved in using the hypothesis by comparing
the exact asymptotic value of the dimensionless dissipation in free decay
calculated on the assumption of local stationarity to the experimentally
determined value (e.g. by means of direct numerical simulation), as this
relationship holds for all measuring times. Should the assumption of local
stationarity lead to significant error, then the `4/5' law needs to be
corrected. Despite this, scale invariance in wavenumber space appears to hold
in the formal limit of infinite Reynolds numbers, which implies that the `-5/3'
energy spectrum does not require correction in this limit.
",0,1,0,0,0,0
15502,End-to-End Monaural Multi-speaker ASR System without Pretraining,"  Recently, end-to-end models have become a popular approach as an alternative
to traditional hybrid models in automatic speech recognition (ASR). The
multi-speaker speech separation and recognition task is a central task in
cocktail party problem. In this paper, we present a state-of-the-art monaural
multi-speaker end-to-end automatic speech recognition model. In contrast to
previous studies on the monaural multi-speaker speech recognition, this
end-to-end framework is trained to recognize multiple label sequences
completely from scratch. The system only requires the speech mixture and
corresponding label sequences, without needing any indeterminate supervisions
obtained from non-mixture speech or corresponding labels/alignments. Moreover,
we exploited using the individual attention module for each separated speaker
and the scheduled sampling to further improve the performance. Finally, we
evaluate the proposed model on the 2-speaker mixed speech generated from the
WSJ corpus and the wsj0-2mix dataset, which is a speech separation and
recognition benchmark. The experiments demonstrate that the proposed methods
can improve the performance of the end-to-end model in separating the
overlapping speech and recognizing the separated streams. From the results, the
proposed model leads to ~10.0% relative performance gains in terms of CER and
WER respectively.
",1,0,0,0,0,0
4253,SVSGAN: Singing Voice Separation via Generative Adversarial Network,"  Separating two sources from an audio mixture is an important task with many
applications. It is a challenging problem since only one signal channel is
available for analysis. In this paper, we propose a novel framework for singing
voice separation using the generative adversarial network (GAN) with a
time-frequency masking function. The mixture spectra is considered to be a
distribution and is mapped to the clean spectra which is also considered a
distribtution. The approximation of distributions between mixture spectra and
clean spectra is performed during the adversarial training process. In contrast
with current deep learning approaches for source separation, the parameters of
the proposed framework are first initialized in a supervised setting and then
optimized by the training procedure of GAN in an unsupervised setting.
Experimental results on three datasets (MIR-1K, iKala and DSD100) show that
performance can be improved by the proposed framework consisting of
conventional networks.
",1,0,0,0,0,0
15171,Resonant inelastic x-ray scattering operators for $t_{2g}$ orbital systems,"  We derive general expressions for resonant inelastic x-ray scattering (RIXS)
operators for $t_{2g}$ orbital systems, which exhibit a rich array of
unconventional magnetism arising from unquenched orbital moments. Within the
fast collision approximation, which is valid especially for 4$d$ and 5$d$
transition metal compounds with short core-hole lifetimes, the RIXS operators
are expressed in terms of total spin and orbital angular momenta of the
constituent ions. We then map these operators onto pseudospins that represent
spin-orbit entangled magnetic moments in systems with strong spin-orbit
coupling. Applications of our theory to such systems as iridates and ruthenates
are discussed, with a particular focus on compounds based on $d^4$ ions with
Van Vleck-type nonmagnetic ground state.
",0,1,0,0,0,0
1713,Topology Estimation in Bulk Power Grids: Guarantees on Exact Recovery,"  The topology of a power grid affects its dynamic operation and settlement in
the electricity market. Real-time topology identification can enable faster
control action following an emergency scenario like failure of a line. This
article discusses a graphical model framework for topology estimation in bulk
power grids (both loopy transmission and radial distribution) using
measurements of voltage collected from the grid nodes. The graphical model for
the probability distribution of nodal voltages in linear power flow models is
shown to include additional edges along with the operational edges in the true
grid. Our proposed estimation algorithms first learn the graphical model and
subsequently extract the operational edges using either thresholding or a
neighborhood counting scheme. For grid topologies containing no three-node
cycles (two buses do not share a common neighbor), we prove that an exact
extraction of the operational topology is theoretically guaranteed. This
includes a majority of distribution grids that have radial topologies. For
grids that include cycles of length three, we provide sufficient conditions
that ensure existence of algorithms for exact reconstruction. In particular,
for grids with constant impedance per unit length and uniform injection
covariances, this observation leads to conditions on geographical placement of
the buses. The performance of algorithms is demonstrated in test case
simulations.
",1,0,1,1,0,0
17704,The Effects of Memory Replay in Reinforcement Learning,"  Experience replay is a key technique behind many recent advances in deep
reinforcement learning. Allowing the agent to learn from earlier memories can
speed up learning and break undesirable temporal correlations. Despite its
wide-spread application, very little is understood about the properties of
experience replay. How does the amount of memory kept affect learning dynamics?
Does it help to prioritize certain experiences? In this paper, we address these
questions by formulating a dynamical systems ODE model of Q-learning with
experience replay. We derive analytic solutions of the ODE for a simple
setting. We show that even in this very simple setting, the amount of memory
kept can substantially affect the agent's performance. Too much or too little
memory both slow down learning. Moreover, we characterize regimes where
prioritized replay harms the agent's learning. We show that our analytic
solutions have excellent agreement with experiments. Finally, we propose a
simple algorithm for adaptively changing the memory buffer size which achieves
consistently good empirical performance.
",1,0,0,1,0,0
8831,Sparse Rational Function Interpolation with Finitely Many Values for the Coefficients,"  In this paper, we give new sparse interpolation algorithms for black box
univariate and multivariate rational functions h=f/g whose coefficients are
integers with an upper bound. The main idea is as follows: choose a proper
integer beta and let h(beta) = a/b with gcd(a,b)=1. Then f and g can be
computed by solving the polynomial interpolation problems f(beta)=ka and
g(beta)=ka for some integer k. It is shown that the univariate interpolation
algorithm is almost optimal and multivariate interpolation algorithm has low
complexity in T but the data size is exponential in n.
",1,0,0,0,0,0
6142,Mastering Heterogeneous Behavioural Models,"  Heterogeneity is one important feature of complex systems, leading to the
complexity of their construction and analysis. Moving the heterogeneity at
model level helps in mastering the difficulty of composing heterogeneous models
which constitute a large system. We propose a method made of an algebra and
structure morphisms to deal with the interaction of behavioural models,
provided that they are compatible. We prove that heterogeneous models can
interact in a safe way, and therefore complex heterogeneous systems can be
built and analysed incrementally. The Uppaal tool is targeted for
experimentations.
",1,0,0,0,0,0
3352,Learning Light Transport the Reinforced Way,"  We show that the equations of reinforcement learning and light transport
simulation are related integral equations. Based on this correspondence, a
scheme to learn importance while sampling path space is derived. The new
approach is demonstrated in a consistent light transport simulation algorithm
that uses reinforcement learning to progressively learn where light comes from.
As using this information for importance sampling includes information about
visibility, too, the number of light transport paths with zero contribution is
dramatically reduced, resulting in much less noisy images within a fixed time
budget.
",1,0,0,0,0,0
10871,A spin-gapped Mott insulator with the dimeric arrangement of twisted molecules Zn(tmdt)$_{2}$,"  $^{13}$C nuclear magnetic resonance measurements were performed for a
single-component molecular material Zn(tmdt)$_{2}$, in which tmdt's form an
arrangement similar to the so-called ${\kappa}$-type molecular packing in
quasi-two-dimensional Mott insulators and superconductors. Detailed analysis of
the powder spectra uncovered local spin susceptibility in the tmdt ${\pi}$
orbitals. The obtained shift and relaxation rate revealed the singlet-triplet
excitations of the ${\pi}$ spins, indicating that Zn(tmdt)$_{2}$ is a
spin-gapped Mott insulator with exceptionally large electron correlations
compared to conventional molecular Mott systems.
",0,1,0,0,0,0
18177,Exponential Decay of the lengths of Spectral Gaps for Extended Harper's Model with Liouvillean Frequency,"  In this paper, we study the non-self dual extended Harper's model with
Liouvillean frequency. By establishing quantitative reducibility results
together with the averaging method, we prove that the lengths of spectral gaps
decay exponentially.
",0,0,1,0,0,0
20516,Blind source separation of tensor-valued time series,"  The blind source separation model for multivariate time series generally
assumes that the observed series is a linear transformation of an unobserved
series with temporally uncorrelated or independent components. Given the
observations, the objective is to find a linear transformation that recovers
the latent series. Several methods for accomplishing this exist and three
particular ones are the classic SOBI and the recently proposed generalized FOBI
(gFOBI) and generalized JADE (gJADE), each based on the use of joint lagged
moments. In this paper we generalize the methodologies behind these algorithms
for tensor-valued time series. We assume that our data consists of a tensor
observed at each time point and that the observations are linear
transformations of latent tensors we wish to estimate. The tensorial
generalizations are shown to have particularly elegant forms and we show that
each of them is Fisher consistent and orthogonal equivariant. Comparing the new
methods with the original ones in various settings shows that the tensorial
extensions are superior to both their vector-valued counterparts and to two
existing tensorial dimension reduction methods for i.i.d. data. Finally,
applications to fMRI-data and video processing show that the methods are
capable of extracting relevant information from noisy high-dimensional data.
",0,0,1,1,0,0
4266,Chaotic Dynamics of Inner Ear Hair Cells,"  Experimental records of active bundle motility are used to demonstrate the
presence of a low-dimensional chaotic attractor in hair cell dynamics.
Dimensionality tests from dynamic systems theory are applied to estimate the
number of independent variables sufficient for modeling the hair cell response.
Poincare maps are constructed to observe a quasiperiodic transition from chaos
to order with increasing amplitudes of mechanical forcing. The onset of this
transition is accompanied by a reduction of Kolmogorov entropy in the system
and an increase in mutual information between the stimulus and the hair bundle,
indicative of signal detection. A simple theoretical model is used to describe
the observed chaotic dynamics. The model exhibits an enhancement of sensitivity
to weak stimuli when the system is poised in the chaotic regime. We propose
that chaos may play a role in the hair cell's ability to detect low-amplitude
sounds.
",0,1,0,0,0,0
12599,Concentration of quadratic forms under a Bernstein moment assumption,"  A concentration result for quadratic form of independent subgaussian random
variables is derived. If the moments of the random variables satisfy a
""Bernstein condition"", then the variance term of the Hanson-Wright inequality
can be improved. The Bernstein condition is satisfied, for instance, by all
log-concave subgaussian distributions.
",0,0,1,1,0,0
10766,Sorted Concave Penalized Regression,"  The Lasso is biased. Concave penalized least squares estimation (PLSE) takes
advantage of signal strength to reduce this bias, leading to sharper error
bounds in prediction, coefficient estimation and variable selection. For
prediction and estimation, the bias of the Lasso can be also reduced by taking
a smaller penalty level than what selection consistency requires, but such
smaller penalty level depends on the sparsity of the true coefficient vector.
The sorted L1 penalized estimation (Slope) was proposed for adaptation to such
smaller penalty levels. However, the advantages of concave PLSE and Slope do
not subsume each other. We propose sorted concave penalized estimation to
combine the advantages of concave and sorted penalizations. We prove that
sorted concave penalties adaptively choose the smaller penalty level and at the
same time benefits from signal strength, especially when a significant
proportion of signals are stronger than the corresponding adaptively selected
penalty levels. A local convex approximation, which extends the local linear
and quadratic approximations to sorted concave penalties, is developed to
facilitate the computation of sorted concave PLSE and proven to possess desired
prediction and estimation error bounds. We carry out a unified treatment of
penalty functions in a general optimization setting, including the penalty
levels and concavity of the above mentioned sorted penalties and mixed
penalties motivated by Bayesian considerations. Our analysis of prediction and
estimation errors requires the restricted eigenvalue condition on the design,
not beyond, and provides selection consistency under a required minimum signal
strength condition in addition. Thus, our results also sharpens existing
results on concave PLSE by removing the upper sparse eigenvalue component of
the sparse Riesz condition.
",0,0,1,0,0,0
16949,Global optimization for low-dimensional switching linear regression and bounded-error estimation,"  The paper provides global optimization algorithms for two particularly
difficult nonconvex problems raised by hybrid system identification: switching
linear regression and bounded-error estimation. While most works focus on local
optimization heuristics without global optimality guarantees or with guarantees
valid only under restrictive conditions, the proposed approach always yields a
solution with a certificate of global optimality. This approach relies on a
branch-and-bound strategy for which we devise lower bounds that can be
efficiently computed. In order to obtain scalable algorithms with respect to
the number of data, we directly optimize the model parameters in a continuous
optimization setting without involving integer variables. Numerical experiments
show that the proposed algorithms offer a higher accuracy than convex
relaxations with a reasonable computational burden for hybrid system
identification. In addition, we discuss how bounded-error estimation is related
to robust estimation in the presence of outliers and exact recovery under
sparse noise, for which we also obtain promising numerical results.
",1,0,0,1,0,0
14614,Discovering Playing Patterns: Time Series Clustering of Free-To-Play Game Data,"  The classification of time series data is a challenge common to all
data-driven fields. However, there is no agreement about which are the most
efficient techniques to group unlabeled time-ordered data. This is because a
successful classification of time series patterns depends on the goal and the
domain of interest, i.e. it is application-dependent.
In this article, we study free-to-play game data. In this domain, clustering
similar time series information is increasingly important due to the large
amount of data collected by current mobile and web applications. We evaluate
which methods cluster accurately time series of mobile games, focusing on
player behavior data. We identify and validate several aspects of the
clustering: the similarity measures and the representation techniques to reduce
the high dimensionality of time series. As a robustness test, we compare
various temporal datasets of player activity from two free-to-play video-games.
With these techniques we extract temporal patterns of player behavior
relevant for the evaluation of game events and game-business diagnosis. Our
experiments provide intuitive visualizations to validate the results of the
clustering and to determine the optimal number of clusters. Additionally, we
assess the common characteristics of the players belonging to the same group.
This study allows us to improve the understanding of player dynamics and churn
behavior.
",1,0,0,1,0,0
11386,From rate distortion theory to metric mean dimension: variational principle,"  The purpose of this paper is to point out a new connection between
information theory and dynamical systems. In the information theory side, we
consider rate distortion theory, which studies lossy data compression of
stochastic processes under distortion constraints. In the dynamical systems
side, we consider mean dimension theory, which studies how many parameters per
second we need to describe a dynamical system. The main results are new
variational principles connecting rate distortion function to metric mean
dimension.
",1,0,1,0,0,0
10019,Matrix product states for topological phases with parafermions,"  In the Fock representation, we propose a framework to construct the
generalized matrix product states (MPS) for topological phases with $\mathbb{
Z}_{p}$ parafermions. Unlike the $\mathbb{Z}_{2}$ Majorana fermions, the $%
\mathbb{Z}_{p}$ parafermions form intrinsically interacting systems. Here we
explicitly construct two topologically distinct classes of irreducible $%
\mathbb{Z}_{3}$ parafermionic MPS wave functions, characterized by one or two
parafermionic zero modes at each end of an open chain. Their corresponding
parent Hamiltonians are found as the fixed point models of the single
$\mathbb{Z}_{3}$ parafermion chain and two-coupled parafermion chains with
$\mathbb{Z}_{3}\times \mathbb{Z}_{3}$ symmetry. Our results thus pave the road
to investigate all possible topological phases with $\mathbb{Z}_{p}$
parafermions within the matrix product representation in one dimension.
",0,1,0,0,0,0
4069,Hierarchical State Abstractions for Decision-Making Problems with Computational Constraints,"  In this semi-tutorial paper, we first review the information-theoretic
approach to account for the computational costs incurred during the search for
optimal actions in a sequential decision-making problem. The traditional (MDP)
framework ignores computational limitations while searching for optimal
policies, essentially assuming that the acting agent is perfectly rational and
aims for exact optimality. Using the free-energy, a variational principle is
introduced that accounts not only for the value of a policy alone, but also
considers the cost of finding this optimal policy. The solution of the
variational equations arising from this formulation can be obtained using
familiar Bellman-like value iterations from dynamic programming (DP) and the
Blahut-Arimoto (BA) algorithm from rate distortion theory. Finally, we
demonstrate the utility of the approach for generating hierarchies of state
abstractions that can be used to best exploit the available computational
resources. A numerical example showcases these concepts for a path-planning
problem in a grid world environment.
",1,0,0,1,0,0
7126,A two-dimensional data-driven model for traffic flow on highways,"  Based on experimental traffic data obtained from German and US highways, we
propose a novel two-dimensional first-order macroscopic traffic flow model. The
goal is to reproduce a detailed description of traffic dynamics for the real
road geometry. In our approach both the dynamic along the road and across the
lanes is continuous. The closure relations, being necessary to complete the
hydrodynamic equation, are obtained by regression on fundamental diagram data.
Comparison with prediction of one-dimensional models shows the improvement in
performance of the novel model.
",0,1,0,0,0,0
10649,Nef vector bundles on a projective space with first Chern class 3 and second Chern class 8,"  We describe nef vector bundles on a projective space with first Chern class
three and second Chern class eight over an algebraically closed field of
characteristic zero by giving them a minimal resolution in terms of a full
strong exceptional collection of line bundles.
",0,0,1,0,0,0
19474,Measuring heterogeneity in urban expansion via spatial entropy,"  The lack of efficiency in urban diffusion is a debated issue, important for
biologists, urban specialists, planners and statisticians, both in developed
and new developing countries. Many approaches have been considered to measure
urban sprawl, i.e. chaotic urban expansion; such idea of chaos is here linked
to the concept of entropy. Entropy, firstly introduced in information theory,
rapidly became a standard tool in ecology, biology and geography to measure the
degree of heterogeneity among observations; in these contexts, entropy measures
should include spatial information. The aim of this paper is to employ a
rigorous spatial entropy based approach to measure urban sprawl associated to
the diffusion of metropolitan cities. In order to assess the performance of the
considered measures, a comparative study is run over alternative urban
scenarios; afterwards, measures are used to quantify the degree of disorder in
the urban expansion of three cities in Europe. Results are easily interpretable
and can be used both as an absolute measure of urban sprawl and for comparison
over space and time.
",0,0,0,1,0,0
13583,LARNN: Linear Attention Recurrent Neural Network,"  The Linear Attention Recurrent Neural Network (LARNN) is a recurrent
attention module derived from the Long Short-Term Memory (LSTM) cell and ideas
from the consciousness Recurrent Neural Network (RNN). Yes, it LARNNs. The
LARNN uses attention on its past cell state values for a limited window size
$k$. The formulas are also derived from the Batch Normalized LSTM (BN-LSTM)
cell and the Transformer Network for its Multi-Head Attention Mechanism. The
Multi-Head Attention Mechanism is used inside the cell such that it can query
its own $k$ past values with the attention window. This has the effect of
augmenting the rank of the tensor with the attention mechanism, such that the
cell can perform complex queries to question its previous inner memories, which
should augment the long short-term effect of the memory. With a clever trick,
the LARNN cell with attention can be easily used inside a loop on the cell
state, just like how any other Recurrent Neural Network (RNN) cell can be
looped linearly through time series. This is due to the fact that its state,
which is looped upon throughout time steps within time series, stores the inner
states in a ""first in, first out"" queue which contains the $k$ most recent
states and on which it is easily possible to add static positional encoding
when the queue is represented as a tensor. This neural architecture yields
better results than the vanilla LSTM cells. It can obtain results of 91.92% for
the test accuracy, compared to the previously attained 91.65% using vanilla
LSTM cells. Note that this is not to compare to other research, where up to
93.35% is obtained, but costly using 18 LSTM cells rather than with 2 to 3
cells as analyzed here. Finally, an interesting discovery is made, such that
adding activation within the multi-head attention mechanism's linear layers can
yield better results in the context researched hereto.
",0,0,0,1,0,0
18599,Group Embeddings with Algorithmic Properties,"  We show that every countable group H with solvable word problem (=computable
group) can be subnormally embedded into a 2-generated group G which also has
solvable word problem. Moreover, the membership problem for H < G is also
solvable. We also give estimates of time and space complexity of the word
problem in G and of the membership problem for H < G.
",0,0,1,0,0,0
13617,Decomposition Strategies for Constructive Preference Elicitation,"  We tackle the problem of constructive preference elicitation, that is the
problem of learning user preferences over very large decision problems,
involving a combinatorial space of possible outcomes. In this setting, the
suggested configuration is synthesized on-the-fly by solving a constrained
optimization problem, while the preferences are learned itera tively by
interacting with the user. Previous work has shown that Coactive Learning is a
suitable method for learning user preferences in constructive scenarios. In
Coactive Learning the user provides feedback to the algorithm in the form of an
improvement to a suggested configuration. When the problem involves many
decision variables and constraints, this type of interaction poses a
significant cognitive burden on the user. We propose a decomposition technique
for large preference-based decision problems relying exclusively on inference
and feedback over partial configurations. This has the clear advantage of
drastically reducing the user cognitive load. Additionally, part-wise inference
can be (up to exponentially) less computationally demanding than inference over
full configurations. We discuss the theoretical implications of working with
parts and present promising empirical results on one synthetic and two
realistic constructive problems.
",1,0,0,1,0,0
18642,Mixtures of Hidden Truncation Hyperbolic Factor Analyzers,"  The mixture of factor analyzers model was first introduced over 20 years ago
and, in the meantime, has been extended to several non-Gaussian analogues. In
general, these analogues account for situations with heavy tailed and/or skewed
clusters. An approach is introduced that unifies many of these approaches into
one very general model: the mixture of hidden truncation hyperbolic factor
analyzers (MHTHFA) model. In the process of doing this, a hidden truncation
hyperbolic factor analysis model is also introduced. The MHTHFA model is
illustrated for clustering as well as semi-supervised classification using two
real datasets.
",0,0,0,1,0,0
5046,Timelike surfaces in Minkowski space with a canonical null direction,"  Given a constant vector field $Z$ in Minkowski space, a timelike surface is
said to have a canonical null direction with respect to $Z$ if the projection
of $Z$ on the tangent space of the surface gives a lightlike vector field. In
this paper we describe these surfaces in the ruled case. For example when the
Minkowski space has three dimensions then a surface with a canonical null
direction is minimal and flat. On the other hand, we describe several
properties in the non ruled case and we partially describe these surfaces in
four-dimensional Minkowski space. We give different ways for building these
surfaces in four-dimensional Minkowski space and we finally use the Gauss map
for describe another properties of these surfaces.
",0,0,1,0,0,0
3695,Specification properties on uniform spaces,"  In the following text we introduce specification property (stroboscopical
property) for dynamical systems on uniform space. We focus on two classes of
dynamical systems: generalized shifts and dynamical systems with Alexandroff
compactification of a discrete space as phase space. We prove that for a
discrete finite topological space $X$ with at least two elements, a nonempty
set $\Gamma$ and a self--map $\varphi:\Gamma\to\Gamma$ the generalized shift
dynamical system $(X^\Gamma,\sigma_\varphi)$: \begin{itemize} \item has
(almost) weak specification property if and only if $\varphi:\Gamma\to\Gamma$
does not have any periodic point,
\item has (uniform) stroboscopical property if and only if
$\varphi:\Gamma\to\Gamma$
is one-to-one. \end{itemize}
",0,0,1,0,0,0
6312,Multiplicative Convolution of Real Asymmetric and Real Antisymmetric Matrices,"  The singular values of products of standard complex Gaussian random matrices,
or sub-blocks of Haar distributed unitary matrices, have the property that
their probability distribution has an explicit, structured form referred to as
a polynomial ensemble. It is furthermore the case that the corresponding
bi-orthogonal system can be determined in terms of Meijer G-functions, and the
correlation kernel given as an explicit double contour integral. It has
recently been shown that the Hermitised product $X_M \cdots X_2 X_1A X_1^T
X_2^T \cdots X_M^T$, where each $X_i$ is a standard real complex Gaussian
matrix, and $A$ is real anti-symmetric shares exhibits analogous properties.
Here we use the theory of spherical functions and transforms to present a
theory which, for even dimensions, includes these properties of the latter
product as a special case. As an example we show that the theory also allows
for a treatment of this class of Hermitised product when the $X_i$ are chosen
as sub-blocks of Haar distributed real orthogonal matrices.
",0,0,1,0,0,0
5380,On The Communication Complexity of High-Dimensional Permutations,"  We study the multiparty communication complexity of high dimensional
permutations, in the Number On the Forehead (NOF) model. This model is due to
Chandra, Furst and Lipton (CFL) who also gave a nontrivial protocol for the
Exactly-n problem where three players receive integer inputs and need to decide
if their inputs sum to a given integer $n$. There is a considerable body of
literature dealing with the same problem, where $(\mathbb{N},+)$ is replaced by
some other abelian group. Our work can be viewed as a far-reaching extension of
this line of work.
We show that the known lower bounds for that group-theoretic problem apply to
all high dimensional permutations. We introduce new proof techniques that
appeal to recent advances in Additive Combinatorics and Ramsey theory. We
reveal new and unexpected connections between the NOF communication complexity
of high dimensional permutations and a variety of well known and thoroughly
studied problems in combinatorics.
Previous protocols for Exactly-n all rely on the construction of large sets
of integers without a 3-term arithmetic progression. No direct algorithmic
protocol was previously known for the problem, and we provide the first such
algorithm. This suggests new ways to significantly improve the CFL protocol.
Many new open questions are presented throughout.
",1,0,0,0,0,0
18175,Learning of Optimal Forecast Aggregation in Partial Evidence Environments,"  We consider the forecast aggregation problem in repeated settings, where the
forecasts are done on a binary event. At each period multiple experts provide
forecasts about an event. The goal of the aggregator is to aggregate those
forecasts into a subjective accurate forecast. We assume that experts are
Bayesian; namely they share a common prior, each expert is exposed to some
evidence, and each expert applies Bayes rule to deduce his forecast. The
aggregator is ignorant with respect to the information structure (i.e.,
distribution over evidence) according to which experts make their prediction.
The aggregator observes the experts' forecasts only. At the end of each period
the actual state is realized. We focus on the question whether the aggregator
can learn to aggregate optimally the forecasts of the experts, where the
optimal aggregation is the Bayesian aggregation that takes into account all the
information (evidence) in the system.
We consider the class of partial evidence information structures, where each
expert is exposed to a different subset of conditionally independent signals.
Our main results are positive; We show that optimal aggregation can be learned
in polynomial time in a quite wide range of instances of the partial evidence
environments. We provide a tight characterization of the instances where
learning is possible and impossible.
",0,0,0,1,0,0
15666,A characterisation of Lie algebras amongst anti-commutative algebras,"  Let $\mathbb{K}$ be an infinite field. We prove that if a variety of
anti-commutative $\mathbb{K}$-algebras - not necessarily associative, where
$xx=0$ is an identity - is locally algebraically cartesian closed, then it must
be a variety of Lie algebras over $\mathbb{K}$. In particular,
$\mathsf{Lie}_{\mathbb{K}}$ is the largest such. Thus, for a given variety of
anti-commutative $\mathbb{K}$-algebras, the Jacobi identity becomes equivalent
to a categorical condition: it is an identity in~$\mathcal{V}$ if and only if
$\mathcal{V}$ is a subvariety of a locally algebraically cartesian closed
variety of anti-commutative $\mathbb{K}$-algebras. This is based on a result
saying that an algebraically coherent variety of anti-commutative
$\mathbb{K}$-algebras is either a variety of Lie algebras or a variety of
anti-associative algebras over $\mathbb{K}$.
",0,0,1,0,0,0
8201,A critical analysis of string APIs: The case of Pharo,"  Most programming languages, besides C, provide a native abstraction for
character strings, but string APIs vary widely in size, expressiveness, and
subjective convenience across languages. In Pharo, while at first glance the
API of the String class seems rich, it often feels cumbersome in practice; to
improve its usability, we faced the challenge of assessing its design. However,
we found hardly any guideline about design forces and how they structure the
design space, and no comprehensive analysis of the expected string operations
and their different variations. In this article, we first analyse the Pharo 4
String library, then contrast it with its Haskell, Java, Python, Ruby, and Rust
counterparts. We harvest criteria to describe a string API, and reflect on
features and design tensions. This analysis should help language designers in
understanding the design space of strings, and will serve as a basis for a
future redesign of the string library in Pharo.
",1,0,0,0,0,0
8157,A Simple Reservoir Model of Working Memory with Real Values,"  The prefrontal cortex is known to be involved in many high-level cognitive
functions, in particular, working memory. Here, we study to what extent a group
of randomly connected units (namely an Echo State Network, ESN) can store and
maintain (as output) an arbitrary real value from a streamed input, i.e. can
act as a sustained working memory unit. Furthermore, we explore to what extent
such an architecture can take advantage of the stored value in order to produce
non-linear computations. Comparison between different architectures (with and
without feedback, with and without a working memory unit) shows that an
explicit memory improves the performances.
",0,0,0,0,1,0
10251,Reward Shaping via Meta-Learning,"  Reward shaping is one of the most effective methods to tackle the crucial yet
challenging problem of credit assignment in Reinforcement Learning (RL).
However, designing shaping functions usually requires much expert knowledge and
hand-engineering, and the difficulties are further exacerbated given multiple
similar tasks to solve. In this paper, we consider reward shaping on a
distribution of tasks, and propose a general meta-learning framework to
automatically learn the efficient reward shaping on newly sampled tasks,
assuming only shared state space but not necessarily action space. We first
derive the theoretically optimal reward shaping in terms of credit assignment
in model-free RL. We then propose a value-based meta-learning algorithm to
extract an effective prior over the optimal reward shaping. The prior can be
applied directly to new tasks, or provably adapted to the task-posterior while
solving the task within few gradient updates. We demonstrate the effectiveness
of our shaping through significantly improved learning efficiency and
interpretable visualizations across various settings, including notably a
successful transfer from DQN to DDPG.
",1,0,0,1,0,0
3530,Inductive Freeness of Ziegler's Canonical Multiderivations for Reflection Arrangements,"  Let $A$ be a free hyperplane arrangement. In 1989, Ziegler showed that the
restriction $A''$ of $A$ to any hyperplane endowed with the natural
multiplicity is then a free multiarrangement. We initiate a study of the
stronger freeness property of inductive freeness for these canonical free
multiarrangements and investigate them for the underlying class of reflection
arrangements.
More precisely, let $A = A(W)$ be the reflection arrangement of a complex
reflection group $W$. By work of Terao, each such reflection arrangement is
free. Thus so is Ziegler's canonical multiplicity on the restriction $A''$ of
$A$ to a hyperplane. We show that the latter is inductively free as a
multiarrangement if and only if $A''$ itself is inductively free.
",0,0,1,0,0,0
9394,Spin alignment of stars in old open clusters,"  Stellar clusters form by gravitational collapse of turbulent molecular
clouds, with up to several thousand stars per cluster. They are thought to be
the birthplace of most stars and therefore play an important role in our
understanding of star formation, a fundamental problem in astrophysics. The
initial conditions of the molecular cloud establish its dynamical history until
the stellar cluster is born. However, the evolution of the cloud's angular
momentum during cluster formation is not well understood. Current observations
have suggested that turbulence scrambles the angular momentum of the
cluster-forming cloud, preventing spin alignment amongst stars within a
cluster. Here we use asteroseismology to measure the inclination angles of spin
axes in 48 stars from the two old open clusters NGC~6791 and NGC~6819. The
stars within each cluster show strong alignment. Three-dimensional
hydrodynamical simulations of proto-cluster formation show that at least 50 %
of the initial proto-cluster kinetic energy has to be rotational in order to
obtain strong stellar-spin alignment within a cluster. Our result indicates
that the global angular momentum of the cluster-forming clouds was efficiently
transferred to each star and that its imprint has survived after several
gigayears since the clusters formed.
",0,1,0,0,0,0
16453,2-associahedra,"  For any $r\geq 1$ and $\mathbf{n} \in \mathbb{Z}_{\geq0}^r \setminus
\{\mathbf0\}$ we construct a poset $W_{\mathbf{n}}$ called a 2-associahedron.
The 2-associahedra arose in symplectic geometry, where they are expected to
control maps between Fukaya categories of different symplectic manifolds. We
prove that the completion $\widehat{W_{\mathbf{n}}}$ is an abstract polytope of
dimension $|\mathbf{n}|+r-3$. There are forgetful maps $W_{\mathbf{n}} \to
K_r$, where $K_r$ is the $(r-2)$-dimensional associahedron, and the
2-associahedra specialize to the associahedra (in two ways) and to the
multiplihedra. In an appendix, we work out the 2- and 3-dimensional
associahedra in detail.
",0,0,1,0,0,0
1228,Effective gravity and effective quantum equations in a system inspired by walking droplets experiments,"  In this paper we suggest a macroscopic toy system in which a potential-like
energy is generated by a non-uniform pulsation of the medium (i.e. pulsation of
transverse standing oscillations that the elastic medium of the system tends to
support at each point). This system is inspired by walking droplets experiments
with submerged barriers. We first show that a Poincaré-Lorentz covariant
formalization of the system causes inconsistency and contradiction. The
contradiction is solved by using a general covariant formulation and by
assuming a relation between the metric associated with the elastic medium and
the pulsation of the medium. (Calculations are performed in a Newtonian-like
metric, constant in time). We find ($i$) an effective Schrödinger equation
with external potential, ($ii$) an effective de Broglie-Bohm guidance formula
and ($iii$) an energy of the `particle' which has a direct counterpart in
general relativity as well as in quantum mechanics. We analyze the wave and the
`particle' in an effective free fall and with a harmonic potential. This
potential-like energy is an effective gravitational potential, rooted in the
pulsation of the medium at each point. The latter, also conceivable as a
natural clock, makes easy to understand why proper time varies from place to
place.
",0,1,0,0,0,0
20082,Using Deep Reinforcement Learning for the Continuous Control of Robotic Arms,"  Deep reinforcement learning enables algorithms to learn complex behavior,
deal with continuous action spaces and find good strategies in environments
with high dimensional state spaces. With deep reinforcement learning being an
active area of research and many concurrent inventions, we decided to focus on
a relatively simple robotic task to evaluate a set of ideas that might help to
solve recent reinforcement learning problems. We test a newly created
combination of two commonly used reinforcement learning methods, whether it is
able to learn more effectively than a baseline. We also compare different ideas
to preprocess information before it is fed to the reinforcement learning
algorithm. The goal of this strategy is to reduce training time and eventually
help the algorithm to converge. The concluding evaluation proves the general
applicability of the described concepts by testing them using a simulated
environment. These concepts might be reused for future experiments.
",1,0,0,0,0,0
11889,Effective mass of quasiparticles from thermodynamics,"  We discuss the potential advantages of calculating the effective mass of
quasiparticles in the interacting electron liquid from the low-temperature free
energy vis-a-vis the conventional approach, in which the effective mass is
obtained from approximate calculations of the self-energy, or from a quantum
Monte Carlo evaluation of the energy of a variational ""quasiparticle wave
function"". While raw quantum Monte Carlo data are presently too sparse to allow
for an accurate determination of the effective mass, the values estimated by
this method are numerically close to the ones obtained in previous calculations
using diagrammatic many-body theory. In contrast to this, a recently published
parametrization of quantum Monte Carlo data for the free energy of the
homogeneous electron liquid yields effective masses that considerably deviate
from previous calculations and even change sign for low densities, reflecting
an unphysical negative entropy. We suggest that this anomaly is related to the
treatment of the exchange energy at finite temperature.
",0,1,0,0,0,0
5702,Probabilistic PARAFAC2,"  The PARAFAC2 is a multimodal factor analysis model suitable for analyzing
multi-way data when one of the modes has incomparable observation units, for
example because of differences in signal sampling or batch sizes. A fully
probabilistic treatment of the PARAFAC2 is desirable in order to improve
robustness to noise and provide a well founded principle for determining the
number of factors, but challenging because the factor loadings are constrained
to be orthogonal. We develop two probabilistic formulations of the PARAFAC2
along with variational procedures for inference: In the one approach, the mean
values of the factor loadings are orthogonal leading to closed form variational
updates, and in the other, the factor loadings themselves are orthogonal using
a matrix Von Mises-Fisher distribution. We contrast our probabilistic
formulation to the conventional direct fitting algorithm based on maximum
likelihood. On simulated data and real fluorescence spectroscopy and gas
chromatography-mass spectrometry data, we compare our approach to the
conventional PARAFAC2 model estimation and find that the probabilistic
formulation is more robust to noise and model order misspecification. The
probabilistic PARAFAC2 thus forms a promising framework for modeling multi-way
data accounting for uncertainty.
",0,0,0,1,0,0
6358,Shape and fission instabilities of ferrofluids in non-uniform magnetic fields,"  We study static distributions of ferrofluid submitted to non-uniform magnetic
fields. We show how the normal-field instability is modified in the presence of
a weak magnetic field gradient. Then we consider a ferrofluid droplet and show
how the gradient affects its shape. A rich phase transitions phenomenology is
found. We also investigate the creation of droplets by successive splits when a
magnet is vertically approached from below and derive theoretical expressions
which are solved numerically to obtain the number of droplets and their aspect
ratio as function of the field configuration. A quantitative comparison is
performed with previous experimental results, as well as with our own
experiments, and yields good agreement with the theoretical modeling.
",0,1,0,0,0,0
15672,Multimodal Observation and Interpretation of Subjects Engaged in Problem Solving,"  In this paper we present the first results of a pilot experiment in the
capture and interpretation of multimodal signals of human experts engaged in
solving challenging chess problems. Our goal is to investigate the extent to
which observations of eye-gaze, posture, emotion and other physiological
signals can be used to model the cognitive state of subjects, and to explore
the integration of multiple sensor modalities to improve the reliability of
detection of human displays of awareness and emotion. We observed chess players
engaged in problems of increasing difficulty while recording their behavior.
Such recordings can be used to estimate a participant's awareness of the
current situation and to predict ability to respond effectively to challenging
situations. Results show that a multimodal approach is more accurate than a
unimodal one. By combining body posture, visual attention and emotion, the
multimodal approach can reach up to 93% of accuracy when determining player's
chess expertise while unimodal approach reaches 86%. Finally this experiment
validates the use of our equipment as a general and reproducible tool for the
study of participants engaged in screen-based interaction and/or problem
solving.
",1,0,0,1,0,0
4673,Entropy Production Rate is Maximized in Non-Contractile Actomyosin,"  The actin cytoskeleton is an active semi-flexible polymer network whose
non-equilibrium properties coordinate both stable and contractile behaviors to
maintain or change cell shape. While myosin motors drive the actin cytoskeleton
out-of-equilibrium, the role of myosin-driven active stresses in the
accumulation and dissipation of mechanical energy is unclear. To investigate
this, we synthesize an actomyosin material in vitro whose active stress content
can tune the network from stable to contractile. Each increment in activity
determines a characteristic spectrum of actin filament fluctuations which is
used to calculate the total mechanical work and the production of entropy in
the material. We find that the balance of work and entropy does not increase
monotonically and, surprisingly, the entropy production rate is maximized in
the non-contractile, stable state. Our study provides evidence that the origins
of system entropy production and activity-dependent dissipation arise from
disorder in the molecular interactions between actin and myosin
",0,0,0,0,1,0
5140,Non Relativistic Limit of Integrable QFT with fermionic excitations,"  The aim of this paper is to investigate the non-relativistic limit of
integrable quantum field theories with fermionic fields, such as the O(N)
Gross-Neveu model, the supersymmetric Sinh-Gordon and non-linear sigma models.
The non-relativistic limit of these theories is implemented by a double scaling
limit which consists of sending the speed of light c to infinity and rescaling
at the same time the relevant coupling constant of the model in such a way to
have finite energy excitations. For the general purpose of mapping the space of
continuous non-relativistic integrable models, this paper completes and
integrates the analysis done in Ref.[1] on the non-relativistic limit of purely
bosonic theories.
",0,1,0,0,0,0
6098,"Conditional Independence, Conditional Mean Independence, and Zero Conditional Covariance","  Investigation of the reversibility of the directional hierarchy in the
interdependency among the notions of conditional independence, conditional mean
independence, and zero conditional covariance, for two random variables X and Y
given a conditioning element Z which is not constrained by any topological
restriction on its range, reveals that if the first moments of X, Y, and XY
exist, then conditional independence implies conditional mean independence and
conditional mean independence implies zero conditional covariance, but the
direction of the hierarchy is not reversible in general. If the conditional
expectation of Y given X and Z is ""affine in X,"" which happens when X is
Bernoulli, then the ""intercept"" and ""slope"" of the conditional expectation
(that is, the nonparametric regression function) equal the ""intercept"" and
""slope"" of the ""least-squares linear regression function"", as a result of which
zero conditional covariance implies conditional mean independence.
",0,0,1,1,0,0
5837,Normal forms of dispersive scalar Poisson brackets with two independent variables,"  We classify the dispersive Poisson brackets with one dependent variable and
two independent variables, with leading order of hydrodynamic type, up to Miura
transformations. We show that, in contrast to the case of a single independent
variable for which a well known triviality result exists, the Miura equivalence
classes are parametrised by an infinite number of constants, which we call
numerical invariants of the brackets. We obtain explicit formulas for the first
few numerical invariants.
",0,1,1,0,0,0
9346,Taming the Signal-to-Noise Problem in Lattice QCD by Phase Reweighting,"  Path integrals describing quantum many-body systems can be calculated with
Monte Carlo sampling techniques, but average quantities are often subject to
signal-to-noise ratios that degrade exponentially with time. A
phase-reweighting technique inspired by recent observations of random walk
statistics in correlation functions is proposed that allows energy levels to be
extracted from late-time correlation functions with time-independent
signal-to-noise ratios. Phase reweighting effectively includes dynamical
refinement of source magnitudes but introduces a bias associated with the
phase. This bias can be removed by performing an extrapolation, but at the
expense of re-introducing a signal-to-noise problem. Lattice Quantum
Chromodynamics calculations of the $\rho$ and nucleon masses and of the
$\Xi\Xi$ binding energy show consistency between standard results obtained
using earlier-time correlation functions and phase-reweighted results using
late-time correlation functions inaccessible to standard statistical analysis
methods.
",0,1,0,0,0,0
10167,Signal propagation in sensing and reciprocating cellular systems with spatial and structural heterogeneity,"  Sensing and reciprocating cellular systems (SARs) are important for the
operation of many biological systems. Production in interferon (IFN) SARs is
achieved through activation of the Jak-Stat pathway, and downstream
upregulation of IFN regulatory factor (IRF)-3 and IFN transcription, but the
role that high and low affinity IFNs play in this process remains unclear. We
present a comparative between a minimal spatio-temporal partial differential
equation (PDE) model and a novel spatio-structural-temporal (SST) model for the
consideration of receptor, binding, and metabolic aspects of SAR behaviour.
Using the SST framework, we simulate single- and multi-cluster paradigms of IFN
communication. Simulations reveal a cyclic process between the binding of IFN
to the receptor, and the consequent increase in metabolism, decreasing the
propensity for binding due to the internal feed-back mechanism. One observes
the effect of heterogeneity between cellular clusters, allowing them to
individualise and increase local production, and within clusters, where we
observe `sub popular quiescence'; a process whereby intra-cluster
subpopulations reduce their binding and metabolism such that other such
subpopulations may augment their production. Finally, we observe the ability
for low affinity IFN to communicate a long range signal, where high affinity
cannot, and the breakdown of this relationship through the introduction of cell
motility. Biological systems may utilise cell motility where environments are
unrestrictive and may use fixed system, with low affinity communication, where
a localised response is desirable.
",0,0,0,0,1,0
10263,Urban Delay Tolerant Network Simulator (UDTNSim v0.1),"  Delay Tolerant Networking (DTN) is an approach to networking which handles
network disruptions and high delays that may occur in many kinds of
communication networks. The major reasons for high delay include partial
connectivity of networks as can be seen in many types of ad hoc wireless
networks with frequent network partitions, long propagation time as experienced
in inter-planetary and deep space networks, and frequent link disruptions due
to the mobility of nodes as observed in terrestrial wireless network
environments. Experimenting network architectures, protocols, and mobility
models in such real-world scenarios is difficult due to the complexities
involved in the network environment. Therefore, in this document, we present
the documentation of an Urban Delay Tolerant Network Simulator (UDTNSim)
version 0.1, capable of simulating urban road network environments with DTN
characteristics including mobility models and routing protocols. The mobility
models included in this version of UDTNSim are (i) Stationary Movement, (ii)
Simple Random Movement, (iii) Path Type Based Movememt, (iv) Path Memory Based
Movement, (v) Path Type with Restricted Movement, and (vi) Path Type with Wait
Movement. In addition to mobility models, we also provide three routing and
data hand-off protocols: (i) Epidemic Routing, (ii) Superior Only Handoff, and
(iii) Superior Peer Handoff. UDTNSim v0.1 is designed using object-oriented
programming approach in order to provide flexibility in addition of new
features to the DTN environment. UDTNSim v0.1 is distributed as an open source
simulator for the use of the research community.
",1,0,0,0,0,0
10820,Wigner functions for gauge equivalence classes of unitary irreducible representations of noncommutative quantum mechanics,"  While Wigner functions forming phase space representation of quantum states
is a well-known fact, their construction for noncommutative quantum mechanics
(NCQM) remains relatively lesser known, in particular with respect to gauge
dependencies. This paper deals with the construction of Wigner functions of
NCQM for a system of 2-degrees of freedom using 2-parameter families of gauge
equivalence classes of unitary irreducible representations (UIRs) of the Lie
group $\g$ which has been identified as the kinematical symmetry group of NCQM
in an earlier paper. This general construction of Wigner functions for NCQM, in
turn, yields the special cases of Landau and symmetric gauges of NCQM.
",0,0,1,0,0,0
3705,Prior-aware Dual Decomposition: Document-specific Topic Inference for Spectral Topic Models,"  Spectral topic modeling algorithms operate on matrices/tensors of word
co-occurrence statistics to learn topic-specific word distributions. This
approach removes the dependence on the original documents and produces
substantial gains in efficiency and provable topic inference, but at a cost:
the model can no longer provide information about the topic composition of
individual documents. Recently Thresholded Linear Inverse (TLI) is proposed to
map the observed words of each document back to its topic composition. However,
its linear characteristics limit the inference quality without considering the
important prior information over topics. In this paper, we evaluate Simple
Probabilistic Inverse (SPI) method and novel Prior-aware Dual Decomposition
(PADD) that is capable of learning document-specific topic compositions in
parallel. Experiments show that PADD successfully leverages topic correlations
as a prior, notably outperforming TLI and learning quality topic compositions
comparable to Gibbs sampling on various data.
",1,0,0,0,0,0
16761,FDTD: solving 1+1D delay PDE in parallel,"  We present a proof of concept for solving a 1+1D complex-valued, delay
partial differential equation (PDE) that emerges in the study of waveguide
quantum electrodynamics (QED) by adapting the finite-difference time-domain
(FDTD) method. The delay term is spatially non-local, rendering conventional
approaches such as the method of lines inapplicable. We show that by properly
designing the grid and by supplying the (partial) exact solution as the
boundary condition, the delay PDE can be numerically solved. In addition, we
demonstrate that while the delay imposes strong data dependency, multi-thread
parallelization can nevertheless be applied to such a problem. Our code
provides a numerically exact solution to the time-dependent multi-photon
scattering problem in waveguide QED.
",1,1,0,0,0,0
17868,Never Forget: Balancing Exploration and Exploitation via Learning Optical Flow,"  Exploration bonus derived from the novelty of the states in an environment
has become a popular approach to motivate exploration for deep reinforcement
learning agents in the past few years. Recent methods such as curiosity-driven
exploration usually estimate the novelty of new observations by the prediction
errors of their system dynamics models. Due to the capacity limitation of the
models and difficulty of performing next-frame prediction, however, these
methods typically fail to balance between exploration and exploitation in
high-dimensional observation tasks, resulting in the agents forgetting the
visited paths and exploring those states repeatedly. Such inefficient
exploration behavior causes significant performance drops, especially in large
environments with sparse reward signals. In this paper, we propose to introduce
the concept of optical flow estimation from the field of computer vision to
deal with the above issue. We propose to employ optical flow estimation errors
to examine the novelty of new observations, such that agents are able to
memorize and understand the visited states in a more comprehensive fashion. We
compare our method against the previous approaches in a number of experimental
experiments. Our results indicate that the proposed method appears to deliver
superior and long-lasting performance than the previous methods. We further
provide a set of comprehensive ablative analysis of the proposed method, and
investigate the impact of optical flow estimation on the learning curves of the
DRL agents.
",1,0,0,0,0,0
12789,The t-t'-J model in one dimension using extremely correlated Fermi liquid theory and time dependent density matrix renormalization group,"  We study the one dimensional t-t'-J model for generic couplings using two
complementary theories, the extremely correlated Fermi liquid theory and
time-dependent density matrix renormalization group over a broad energy scale.
The two methods provide a unique insight into the strong momentum dependence of
the self-energy of this prototypical non-Fermi liquid, described at low
energies as a Tomonaga-Luttinger liquid. We also demonstrate its intimate
relationship to spin-charge separation, i.e. the splitting of Landau
quasiparticles of higher dimensions into two constituents, driven by strong
quantum fluctuations inherent in one dimension. The momentum distribution
function, the spectral function, and the excitation dispersion of these two
methods also compare well.
",0,1,0,0,0,0
6075,V-cycle multigrid algorithms for discontinuous Galerkin methods on non-nested polytopic meshes,"  In this paper we analyse the convergence properties of V-cycle multigrid
algorithms for the numerical solution of the linear system of equations arising
from discontinuous Galerkin discretization of second-order elliptic partial
differential equations on polytopal meshes. Here, the sequence of spaces that
stands at the basis of the multigrid scheme is possibly non nested and is
obtained based on employing agglomeration with possible edge/face coarsening.
We prove that the method converges uniformly with respect to the granularity of
the grid and the polynomial approximation degree p, provided that the number of
smoothing steps, which depends on p, is chosen sufficiently large.
",1,0,0,0,0,0
5972,Observability of characteristic binary-induced structures in circumbinary disks,"  Context: A substantial fraction of protoplanetary disks forms around stellar
binaries. The binary system generates a time-dependent non-axisymmetric
gravitational potential, inducing strong tidal forces on the circumbinary disk.
This leads to a change in basic physical properties of the circumbinary disk,
which should in turn result in unique structures that are potentially
observable with the current generation of instruments.
Aims: The goal of this study is to identify these characteristic structures,
to constrain the physical conditions that cause them, and to evaluate the
feasibility to observe them in circumbinary disks.
Methods: To achieve this, at first two-dimensional hydrodynamic simulations
are performed. The resulting density distributions are post-processed with a 3D
radiative transfer code to generate re-emission and scattered light maps. Based
on these, we study the influence of various parameters, such as the mass of the
stellar components, the mass of the disk and the binary separation on
observable features in circumbinary disks.
Results: We find that the Atacama Large (sub-)Millimetre Array (ALMA) as well
as the European Extremely Large Telescope (E-ELT) are capable of tracing
asymmetries in the inner region of circumbinary disks which are affected most
by the binary-disk interaction. Observations at submillimetre/millimetre
wavelengths will allow the detection of the density waves at the inner rim of
the disk and the inner cavity. With the E-ELT one can partially resolve the
innermost parts of the disk in the infrared wavelength range, including the
disk's rim, accretion arms and potentially the expected circumstellar disks
around each of the binary components.
",0,1,0,0,0,0
19788,Spectral Graph Analysis: A Unified Explanation and Modern Perspectives,"  Complex networks or graphs are ubiquitous in sciences and engineering:
biological networks, brain networks, transportation networks, social networks,
and the World Wide Web, to name a few. Spectral graph theory provides a set of
useful techniques and models for understanding `patterns of interconnectedness'
in a graph. Our prime focus in this paper is on the following question: Is
there a unified explanation and description of the fundamental spectral graph
methods? There are at least two reasons to be interested in this question.
Firstly, to gain a much deeper and refined understanding of the basic
foundational principles, and secondly, to derive rich consequences with
practical significance for algorithm design. However, despite half a century of
research, this question remains one of the most formidable open issues, if not
the core problem in modern network science. The achievement of this paper is to
take a step towards answering this question by discovering a simple, yet
universal statistical logic of spectral graph analysis. The prescribed
viewpoint appears to be good enough to accommodate almost all existing spectral
graph techniques as a consequence of just one single formalism and algorithm.
",0,0,1,1,0,0
4388,Linear Additive Markov Processes,"  We introduce LAMP: the Linear Additive Markov Process. Transitions in LAMP
may be influenced by states visited in the distant history of the process, but
unlike higher-order Markov processes, LAMP retains an efficient
parametrization. LAMP also allows the specific dependence on history to be
learned efficiently from data. We characterize some theoretical properties of
LAMP, including its steady-state and mixing time. We then give an algorithm
based on alternating minimization to learn LAMP models from data. Finally, we
perform a series of real-world experiments to show that LAMP is more powerful
than first-order Markov processes, and even holds its own against deep
sequential models (LSTMs) with a negligible increase in parameter complexity.
",1,0,0,1,0,0
15751,Filamentary fragmentation in a turbulent medium,"  We present the results of smoothed particle hydrodynamic simulations
investigating the evolution and fragmentation of filaments that are accreting
from a turbulent medium. We show that the presence of turbulence, and the
resulting inhomogeneities in the accretion flow, play a significant role in the
fragmentation process. Filaments which experience a weakly turbulent accretion
flow fragment in a two-tier hierarchical fashion, similar to the fragmentation
pattern seen in the Orion Integral Shaped Filament. Increasing the energy in
the turbulent velocity field results in more sub-structure within the
filaments, and one sees a shift from gravity-dominated fragmentation to
turbulence-dominated fragmentation. The sub-structure formed in the filaments
is elongated and roughly parallel to the longitudinal axis of the filament,
similar to the fibres seen in observations of Taurus, and suggests that the
fray and fragment scenario is a possible mechanism for the production of
fibres. We show that the formation of these fibre-like structures is linked to
the vorticity of the velocity field inside the filament and the filament's
accretion from an inhomogeneous medium. Moreover, we find that accretion is
able to drive and sustain roughly sonic levels of turbulence inside the
filaments, but is not able to prevent radial collapse once the filaments become
supercritical. However, the supercritical filaments which contain fibre-like
structures do not collapse radially, suggesting that fibrous filaments may not
necessarily become radially unstable once they reach the critical line-density.
",0,1,0,0,0,0
5765,Composition by Conversation,"  Most musical programming languages are developed purely for coding virtual
instruments or algorithmic compositions. Although there has been some work in
the domain of musical query languages for music information retrieval, there
has been little attempt to unify the principles of musical programming and
query languages with cognitive and natural language processing models that
would facilitate the activity of composition by conversation. We present a
prototype framework, called MusECI, that merges these domains, permitting
score-level algorithmic composition in a text editor while also supporting
connectivity to existing natural language processing frameworks.
",1,0,0,0,0,0
9145,Graph Partitioning with Acyclicity Constraints,"  Graphs are widely used to model execution dependencies in applications. In
particular, the NP-complete problem of partitioning a graph under constraints
receives enormous attention by researchers because of its applicability in
multiprocessor scheduling. We identified the additional constraint of acyclic
dependencies between blocks when mapping computer vision and imaging
applications to a heterogeneous embedded multiprocessor. Existing algorithms
and heuristics do not address this requirement and deliver results that are not
applicable for our use-case. In this work, we show that this more constrained
version of the graph partitioning problem is NP-complete and present heuristics
that achieve a close approximation of the optimal solution found by an
exhaustive search for small problem instances and much better scalability for
larger instances. In addition, we can show a positive impact on the schedule of
a real imaging application that improves communication volume and execution
time.
",1,0,0,0,0,0
1523,Involutive bordered Floer homology,"  We give a bordered extension of involutive HF-hat and use it to give an
algorithm to compute involutive HF-hat for general 3-manifolds. We also explain
how the mapping class group action on HF-hat can be computed using bordered
Floer homology. As applications, we prove that involutive HF-hat satisfies a
surgery exact triangle and compute HFI-hat of the branched double covers of all
10-crossing knots.
",0,0,1,0,0,0
8676,Chaos in three coupled rotators: From Anosov dynamics to hyperbolic attractors,"  Starting from Anosov chaotic dynamics of geodesic flow on a surface of
negative curvature, we develop and consider a number of self-oscillatory
systems including those with hinged mechanical coupling of three rotators and a
system of rotators interacting through a potential function. These results are
used to design an electronic circuit for generation of rough (structurally
stable) chaos. Results of numerical integration of the model equations of
different degree of accuracy are presented and discussed. Also, circuit
simulation of the electronic generator is provided using the NI Multisim
environment. Portraits of attractors, waveforms of generated oscillations,
Lyapunov exponents, and spectra are considered and found to be in good
correspondence for the dynamics on the attractive sets of the self-oscillatory
systems and for the original Anosov geodesic flow. The hyperbolic nature of the
dynamics is tested numerically using a criterion based on statistics of angles
of intersection of stable and unstable subspaces of the perturbation vectors at
a reference phase trajectory on the attractor.
",0,1,0,0,0,0
5558,Linear Optimal Power Flow Using Cycle Flows,"  Linear optimal power flow (LOPF) algorithms use a linearization of the
alternating current (AC) load flow equations to optimize generator dispatch in
a network subject to the loading constraints of the network branches. Common
algorithms use the voltage angles at the buses as optimization variables, but
alternatives can be computationally advantageous. In this article we provide a
review of existing methods and describe a new formulation that expresses the
loading constraints directly in terms of the flows themselves, using a
decomposition of the network graph into a spanning tree and closed cycles. We
provide a comprehensive study of the computational performance of the various
formulations, in settings that include computationally challenging applications
such as multi-period LOPF with storage dispatch and generation capacity
expansion. We show that the new formulation of the LOPF solves up to 7 times
faster than the angle formulation using a commercial linear programming solver,
while another existing cycle-based formulation solves up to 20 times faster,
with an average speed-up of factor 3 for the standard networks considered here.
If generation capacities are also optimized, the average speed-up rises to a
factor of 12, reaching up to factor 213 in a particular instance. The speed-up
is largest for networks with many buses and decentral generators throughout the
network, which is highly relevant given the rise of distributed renewable
generation and the computational challenge of operation and planning in such
networks.
",1,1,0,0,0,0
9103,Are Deep Policy Gradient Algorithms Truly Policy Gradient Algorithms?,"  We study how the behavior of deep policy gradient algorithms reflects the
conceptual framework motivating their development. We propose a fine-grained
analysis of state-of-the-art methods based on key aspects of this framework:
gradient estimation, value prediction, optimization landscapes, and trust
region enforcement. We find that from this perspective, the behavior of deep
policy gradient algorithms often deviates from what their motivating framework
would predict. Our analysis suggests first steps towards solidifying the
foundations of these algorithms, and in particular indicates that we may need
to move beyond the current benchmark-centric evaluation methodology.
",1,0,0,0,0,0
12692,On the set of optimal homeomorphisms for the natural pseudo-distance associated with the Lie group S^1,"  If $\varphi$ and $\psi$ are two continuous real-valued functions defined on a
compact topological space $X$ and $G$ is a subgroup of the group of all
homeomorphisms of $X$ onto itself, the natural pseudo-distance
$d_G(\varphi,\psi)$ is defined as the infimum of $\mathcal{L}(g)=\|\varphi-\psi
\circ g \|_\infty$, as $g$ varies in $G$. In this paper, we make a first step
towards extending the study of this concept to the case of Lie groups, by
assuming $X=G=S^1$. In particular, we study the set of the optimal
homeomorphisms for $d_G$, i.e. the elements $\rho_\alpha$ of $S^1$ such that
$\mathcal{L}(\rho_\alpha)$ is equal to $d_G(\varphi,\psi)$. As our main
results, we give conditions that a homeomorphism has to meet in order to be
optimal, and we prove that the set of the optimal homeomorphisms is finite
under suitable conditions.
",1,0,1,0,0,0
2175,Formally continuous functions on Baire space,"  A function from Baire space to the natural numbers is called formally
continuous if it is induced by a morphism between the corresponding formal
spaces. We compare formal continuity to two other notions of continuity on
Baire space working in Bishop constructive mathematics: one is a function
induced by a Brouwer-operation (i.e. inductively defined neighbourhood
function); the other is a function uniformly continuous near every compact
image. We show that formal continuity is equivalent to the former while it is
strictly stronger than the latter.
",1,0,1,0,0,0
13230,On consistent vertex nomination schemes,"  Given a vertex of interest in a network $G_1$, the vertex nomination problem
seeks to find the corresponding vertex of interest (if it exists) in a second
network $G_2$. A vertex nomination scheme produces a list of the vertices in
$G_2$, ranked according to how likely they are judged to be the corresponding
vertex of interest in $G_2$. The vertex nomination problem and related
information retrieval tasks have attracted much attention in the machine
learning literature, with numerous applications to social and biological
networks. However, the current framework has often been confined to a
comparatively small class of network models, and the concept of statistically
consistent vertex nomination schemes has been only shallowly explored. In this
paper, we extend the vertex nomination problem to a very general statistical
model of graphs. Further, drawing inspiration from the long-established
classification framework in the pattern recognition literature, we provide
definitions for the key notions of Bayes optimality and consistency in our
extended vertex nomination framework, including a derivation of the Bayes
optimal vertex nomination scheme. In addition, we prove that no universally
consistent vertex nomination schemes exist. Illustrative examples are provided
throughout.
",0,0,0,1,0,0
2572,Angular momentum evolution of galaxies over the past 10-Gyr: A MUSE and KMOS dynamical survey of 400 star-forming galaxies from z=0.3-1.7,"  We present a MUSE and KMOS dynamical study 405 star-forming galaxies at
redshift z=0.28-1.65 (median redshift z=0.84). Our sample are representative of
star-forming, main-sequence galaxies, with star-formation rates of
SFR=0.1-30Mo/yr and stellar masses M=10^8-10^11Mo. For 49+/-4% of our sample,
the dynamics suggest rotational support, 24+/-3% are unresolved systems and
5+/-2% appear to be early-stage major mergers with components on 8-30kpc
scales. The remaining 22+/-5% appear to be dynamically complex, irregular (or
face-on systems). For galaxies whose dynamics suggest rotational support, we
derive inclination corrected rotational velocities and show these systems lie
on a similar scaling between stellar mass and specific angular momentum as
local spirals with j*=J/M*\propto M^(2/3) but with a redshift evolution that
scales as j*\propto M^{2/3}(1+z)^(-1). We identify a correlation between
specific angular momentum and disk stability such that galaxies with the
highest specific angular momentum, log(j*/M^(2/3))>2.5, are the most stable,
with Toomre Q=1.10+/-0.18, compared to Q=0.53+/-0.22 for galaxies with
log(j*/M^(2/3))<2.5. At a fixed mass, the HST morphologies of galaxies with the
highest specific angular momentum resemble spiral galaxies, whilst those with
low specific angular momentum are morphologically complex and dominated by
several bright star-forming regions. This suggests that angular momentum plays
a major role in defining the stability of gas disks: at z~1, massive galaxies
that have disks with low specific angular momentum, appear to be globally
unstable, clumpy and turbulent systems. In contrast, galaxies with high
specific angular have evolved in to stable disks with spiral structures.
",0,1,0,0,0,0
1203,"Closure operators, frames, and neatest representations","  Given a poset $P$ and a standard closure operator $\Gamma:\wp(P)\to\wp(P)$ we
give a necessary and sufficient condition for the lattice of $\Gamma$-closed
sets of $\wp(P)$ to be a frame in terms of the recursive construction of the
$\Gamma$-closure of sets. We use this condition to show that given a set
$\mathcal{U}$ of distinguished joins from $P$, the lattice of
$\mathcal{U}$-ideals of $P$ fails to be a frame if and only if it fails to be
$\sigma$-distributive, with $\sigma$ depending on the cardinalities of sets in
$\mathcal{U}$. From this we deduce that if a poset has the property that
whenever $a\wedge(b\vee c)$ is defined for $a,b,c\in P$ it is necessarily equal
to $(a\wedge b)\vee (a\wedge c)$, then it has an $(\omega,3)$-representation.
This answers a question from the literature.
",0,0,1,0,0,0
211,An Effective Way to Improve YouTube-8M Classification Accuracy in Google Cloud Platform,"  Large-scale datasets have played a significant role in progress of neural
network and deep learning areas. YouTube-8M is such a benchmark dataset for
general multi-label video classification. It was created from over 7 million
YouTube videos (450,000 hours of video) and includes video labels from a
vocabulary of 4716 classes (3.4 labels/video on average). It also comes with
pre-extracted audio & visual features from every second of video (3.2 billion
feature vectors in total). Google cloud recently released the datasets and
organized 'Google Cloud & YouTube-8M Video Understanding Challenge' on Kaggle.
Competitors are challenged to develop classification algorithms that assign
video-level labels using the new and improved Youtube-8M V2 dataset. Inspired
by the competition, we started exploration of audio understanding and
classification using deep learning algorithms and ensemble methods. We built
several baseline predictions according to the benchmark paper and public github
tensorflow code. Furthermore, we improved global prediction accuracy (GAP) from
base level 77% to 80.7% through approaches of ensemble.
",1,0,0,1,0,0
14569,Localized heat perturbation in harmonic 1D crystals. Solutions for an equation of anomalous heat conduction,"  In this work exact solutions for the equation that describes anomalous heat
propagation in 1D harmonic lattices are obtained. Rectangular, triangular, and
sawtooth initial perturbations of the temperature field are considered. The
solution for an initially rectangular temperature profile is investigated in
detail. It is shown that the decay of the solution near the wavefront is
proportional to $1/ \sqrt{t}$. In the center of the perturbation zone the decay
is proportional to $1/t$. Thus the solution decays slower near the wavefront,
leaving clearly visible peaks that can be detected experimentally.
",0,1,0,0,0,0
7666,The Elasticity of Puiseux Monoids,"  Let $M$ be an atomic monoid and let $x$ be a non-unit element of $M$. The
elasticity of $x$, denoted by $\rho(x)$, is the ratio of its largest
factorization length to its shortest factorization length, and it measures how
far is $x$ from having a unique factorization. The elasticity $\rho(M)$ of $M$
is the supremum of the elasticities of all non-unit elements of $M$. The monoid
$M$ has accepted elasticity if $\rho(M) = \rho(m)$ for some $m \in M$. In this
paper, we study the elasticity of Puiseux monoids (i.e., additive submonoids of
$\mathbb{Q}_{\ge 0}$). First, we characterize the Puiseux monoids $M$ having
finite elasticity and find a formula for $\rho(M)$. Then we classify the
Puiseux monoids having accepted elasticity in terms of their sets of atoms.
When $M$ is a primary Puiseux monoid, we describe the topology of the set of
elasticities of $M$, including a characterization of when $M$ is a bounded
factorization monoid. Lastly, we give an example of a Puiseux monoid that is
bifurcus (that is, every nonzero element has a factorization of length at most
$2$).
",0,0,1,0,0,0
4557,Phase Retrieval via Randomized Kaczmarz: Theoretical Guarantees,"  We consider the problem of phase retrieval, i.e. that of solving systems of
quadratic equations. A simple variant of the randomized Kaczmarz method was
recently proposed for phase retrieval, and it was shown numerically to have a
computational edge over state-of-the-art Wirtinger flow methods. In this paper,
we provide the first theoretical guarantee for the convergence of the
randomized Kaczmarz method for phase retrieval. We show that it is sufficient
to have as many Gaussian measurements as the dimension, up to a constant
factor. Along the way, we introduce a sufficient condition on measurement sets
for which the randomized Kaczmarz method is guaranteed to work. We show that
Gaussian sampling vectors satisfy this property with high probability; this is
proved using a chaining argument coupled with bounds on VC dimension and metric
entropy.
",1,0,1,1,0,0
7937,Relative Chern character number and super-connection,"  For two complex vector bundles admitting a homomorphism, whose singularity
locates in the disjoint union of some odd--dimensional spheres, we give a
formula to compute the relative Chern characteristic number of these two
complex vector bundles. In particular, for a spin manifold admitting some
sphere bundle structure, we give a formula to express the index of a special
twisted Dirac operator.
",0,0,1,0,0,0
2154,Dissolution of topological Fermi arcs in a dirty Weyl semimetal,"  Weyl semimetals (WSMs) have recently attracted a great deal of attention as
they provide condensed matter realization of chiral anomaly, feature
topologically protected Fermi arc surface states and sustain sharp chiral Weyl
quasiparticles up to a critical disorder at which a continuous quantum phase
transition (QPT) drives the system into a metallic phase. We here numerically
demonstrate that with increasing strength of disorder the Fermi arc gradually
looses its sharpness, and close to the WSM-metal QPT it completely dissolves
into the metallic bath of the bulk. Predicted topological nature of the
WSM-metal QPT and the resulting bulk-boundary correspondence across this
transition can directly be observed in
angle-resolved-photo-emmision-spectroscopy (ARPES) and Fourier transformed
scanning-tunneling-microscopy (STM) measurements by following the continuous
deformation of the Fermi arcs with increasing disorder in recently discovered
Weyl materials.
",0,1,0,0,0,0
1451,On Efficiently Detecting Overlapping Communities over Distributed Dynamic Graphs,"  Modern networks are of huge sizes as well as high dynamics, which challenges
the efficiency of community detection algorithms. In this paper, we study the
problem of overlapping community detection on distributed and dynamic graphs.
Given a distributed, undirected and unweighted graph, the goal is to detect
overlapping communities incrementally as the graph is dynamically changing. We
propose an efficient algorithm, called \textit{randomized Speaker-Listener
Label Propagation Algorithm} (rSLPA), based on the \textit{Speaker-Listener
Label Propagation Algorithm} (SLPA) by relaxing the probability distribution of
label propagation. Besides detecting high-quality communities, rSLPA can
incrementally update the detected communities after a batch of edge insertion
and deletion operations. To the best of our knowledge, rSLPA is the first
algorithm that can incrementally capture the same communities as those obtained
by applying the detection algorithm from the scratch on the updated graph.
Extensive experiments are conducted on both synthetic and real-world datasets,
and the results show that our algorithm can achieve high accuracy and
efficiency at the same time.
",1,0,0,0,0,0
16863,Motion of a rod pushed at one point in a weightless environment in space,"  We analyze the motion of a rod floating in a weightless environment in space
when a force is applied at some point on the rod in a direction perpendicular
to its length. If the force applied is at the centre of mass, then the rod gets
a linear motion perpendicular to its length. However, if the same force is
applied at a point other than the centre of mass, say, near one end of the rod,
thereby giving rise to a torque, then there will also be a rotation of the rod
about its centre of mass, in addition to the motion of the centre of mass
itself. If the force applied is for a very short duration, but imparting
nevertheless a finite impulse, like in a sudden (quick) hit at one end of the
rod, then the centre of mass will move with a constant linear speed and
superimposed on it will be a rotation of the rod with constant angular speed
about the centre of mass. However, if force is applied continuously, say by
strapping a tiny rocket at one end of the rod, then the rod will spin faster
and faster about the centre of mass, with angular speed increasing linearly
with time. As the direction of the applied force, as seen by an external
(inertial) observer, will be changing continuously with the rotation of the
rod, the acceleration of the centre of mass would also be not in one fixed
direction. However, it turns out that the locus of the velocity vector of the
centre of mass will describe a Cornu spiral, with the velocity vector reaching
a final constant value with time. The mean motion of the centre of mass will be
in a straight line, with superposed initial oscillations that soon die down.
",0,1,0,0,0,0
9399,Benchmarks and reliable DFT results for spin-crossover complexes,"  DFT is used throughout nanoscience, especially when modeling spin-dependent
properties that are important in spintronics. But standard quantum chemical
methods (both CCSD(T) and self-consistent semilocal density functional
calculations) fail badly for the spin adiabatic energy difference in Fe(II)
spin-crossover complexes. We show that all-electron fixed-node diffusion Monte
Carlo can be converged at significant computational cost, and that the B3LYP
single-determinant has sufficiently accurate nodes, providing benchmarks for
these systems. We also find that density-corrected DFT, using Hartree-Fock
densities (HF-DFT), greatly improves accuracy and reduces dependence on
approximations for these calculations. The small gap in the self-consistent DFT
calculations for the high-spin state is consistent with this. For the spin
adiabatic energy differences in these complexes, HF-DFT is both accurate and
reliable, and we make a strong prediction for the Fe-Porphyrin complex. The
""parameter-dilemma"" of needing different amounts of mixing for different
properties is eliminated by HF-DFT.
",0,1,0,0,0,0
4549,The Geometry of Nodal Sets and Outlier Detection,"  Let $(M,g)$ be a compact manifold and let $-\Delta \phi_k = \lambda_k \phi_k$
be the sequence of Laplacian eigenfunctions. We present a curious new
phenomenon which, so far, we only managed to understand in a few highly
specialized cases: the family of functions $f_N:M \rightarrow \mathbb{R}_{\geq
0}$ $$ f_N(x) = \sum_{k \leq N}{ \frac{1}{\sqrt{\lambda_k}}
\frac{|\phi_k(x)|}{\|\phi_k\|_{L^{\infty}(M)}}}$$ seems strangely suited for
the detection of anomalous points on the manifold. It may be heuristically
interpreted as the sum over distances to the nearest nodal line and potentially
hints at a new phenomenon in spectral geometry. We give rigorous statements on
the unit square $[0,1]^2$ (where minima localize in $\mathbb{Q}^2$) and on
Paley graphs (where $f_N$ recovers the geometry of quadratic residues of the
underlying finite field $\mathbb{F}_p$). Numerical examples show that the
phenomenon seems to arise on fairly generic manifolds.
",0,0,1,1,0,0
4511,Ranks of rational points of the Jacobian varieties of hyperelliptic curves,"  In this paper, we obtain bounds for the Mordell-Weil ranks over cyclotomic
extensions of a wide range of abelian varieties defined over a number field $F$
whose primes above $p$ are totally ramified over $F/\mathbb{Q}$. We assume that
the abelian varieties may have good non-ordinary reduction at those primes. Our
work is a generalization of \cite{Kim}, in which the second author generalized
Perrin-Riou's Iwasawa theory for elliptic curves over $\mathbb{Q}$ with
supersingular reduction (\cite{Perrin-Riou}) to elliptic curves defined over
the above-mentioned number field $F$. On top of non-ordinary reduction and the
ramification of the field $F$, we deal with the additional difficulty that the
dimensions of the abelian varieties can be any number bigger than 1 which
causes a variety of issues. As a result, we obtain bounds for the ranks over
cyclotomic extensions $\mathbb{Q}(\mu_{p^{\max(M,N)+n}})$ of the Jacobian
varieties of {\it ramified} hyperelliptic curves $y^{2p^M}=x^{3p^N}+ax^{p^N}+b$
among others.
",0,0,1,0,0,0
16047,Standard Galactic Field RR Lyrae. I. Optical to Mid-infrared Phased Photometry,"  We present a multi-wavelength compilation of new and previously-published
photometry for 55 Galactic field RR Lyrae variables. Individual studies,
spanning a time baseline of up to 30 years, are self-consistently phased to
produce light curves in 10 photometric bands covering the wavelength range from
0.4 to 4.5 microns. Data smoothing via the GLOESS technique is described and
applied to generate high-fidelity light curves, from which mean magnitudes,
amplitudes, rise-times, and times of minimum and maximum light are derived.
60,000 observations were acquired using the new robotic Three-hundred
MilliMeter Telescope (TMMT), which was first deployed at the Carnegie
Observatories in Pasadena, CA, and is now permanently installed and operating
at Las Campanas Observatory in Chile. We provide a full description of the TMMT
hardware, software, and data reduction pipeline. Archival photometry
contributed approximately 31,000 observations. Photometric data are given in
the standard Johnson UBV, Kron-Cousins RI, 2MASS JHK, and Spitzer [3.6] & [4.5]
bandpasses.
",0,1,0,0,0,0
425,Analytical solutions for the radial Scarf II potential,"  The real Scarf II potential is discussed as a radial problem. This potential
has been studied extensively as a one-dimensional problem, and now these
results are used to construct its bound and resonance solutions for $l=0$ by
setting the origin at some arbitrary value of the coordinate. The solutions
with appropriate boundary conditions are composed as the linear combination of
the two independent solutions of the Schrödinger equation. The asymptotic
expression of these solutions is used to construct the $S_0(k)$ s-wave
$S$-matrix, the poles of which supply the $k$ values corresponding to the
bound, resonance and anti-bound solutions. The location of the discrete energy
eigenvalues is analyzed, and the relation of the solutions of the radial and
one-dimensional Scarf II potentials is discussed. It is shown that the
generalized Woods--Saxon potential can be generated from the Rosen--Morse II
potential in the same way as the radial Scarf II potential is obtained from its
one-dimensional correspondent. Based on this analogy, possible applications are
also pointed out.
",0,1,0,0,0,0
8115,Adaptive Questionnaires for Direct Identification of Optimal Product Design,"  We consider the problem of identifying the most profitable product design
from a finite set of candidates under unknown consumer preference. A standard
approach to this problem follows a two-step strategy: First, estimate the
preference of the consumer population, represented as a point in part-worth
space, using an adaptive discrete-choice questionnaire. Second, integrate the
estimated part-worth vector with engineering feasibility and cost models to
determine the optimal design. In this work, we (1) demonstrate that accurate
preference estimation is neither necessary nor sufficient for identifying the
optimal design, (2) introduce a novel adaptive questionnaire that leverages
knowledge about engineering feasibility and manufacturing costs to directly
determine the optimal design, and (3) interpret product design in terms of a
nonlinear segmentation of part-worth space, and use this interpretation to
illuminate the intrinsic difficulty of optimal design in the presence of noisy
questionnaire responses. We establish the superiority of the proposed approach
using a well-documented optimal product design task. This study demonstrates
how the identification of optimal product design can be accelerated by
integrating marketing and manufacturing knowledge into the adaptive
questionnaire.
",1,0,0,1,0,0
3533,The proximal point algorithm in geodesic spaces with curvature bounded above,"  We investigate the asymptotic behavior of sequences generated by the proximal
point algorithm for convex functions in complete geodesic spaces with curvature
bounded above. Using the notion of resolvents of such functions, which was
recently introduced by the authors, we show the existence of minimizers of
convex functions under the boundedness assumptions on such sequences as well as
the convergence of such sequences to minimizers of given functions.
",0,0,1,0,0,0
16893,"Energy-efficient Analog Sensing for Large-scale, High-density Persistent Wireless Monitoring","  The research challenge of current Wireless Sensor Networks~(WSNs) is to
design energy-efficient, low-cost, high-accuracy, self-healing, and scalable
systems for applications such as environmental monitoring. Traditional WSNs
consist of low density, power-hungry digital motes that are expensive and
cannot remain functional for long periods on a single charge. In order to
address these challenges, a \textit{dumb-sensing and smart-processing}
architecture that splits sensing and computation capabilities among tiers is
proposed. Tier-1 consists of dumb sensors that only sense and transmit, while
the nodes in Tier-2 do all the smart processing on Tier-1 sensor data. A
low-power and low-cost solution for Tier-1 sensors has been proposed using
Analog Joint Source Channel Coding~(AJSCC). An analog circuit that realizes the
rectangular type of AJSCC has been proposed and realized on a Printed Circuit
Board for feasibility analysis. A prototype consisting of three Tier-1 sensors
(sensing temperature and humidity) communicating to a Tier-2 Cluster Head has
been demonstrated to verify the proposed approach. Results show that our
framework is indeed feasible to support large scale high density and persistent
WSN deployment.
",1,0,0,0,0,0
15066,Uniruledness of Strata of Holomorphic Differentials in Small Genus,"  We address the question concerning the birational geometry of the strata of
holomorphic and quadratic differentials. We show strata of holomorphic and
quadratic differentials to be uniruled in small genus by constructing rational
curves via pencils on K3 and del Pezzo surfaces respectively. Restricting to
genus $3\leq g\leq6$, we construct projective bundles over a rational varieties
that dominate the holomorphic strata with length at most $g-1$, hence showing
in addition that these strata are unirational.
",0,0,1,0,0,0
3019,Susceptibility of Methicillin Resistant Staphylococcus aureus to Vancomycin using Liposomal Drug Delivery System,"  Staphylococcus aureus responsible for nosocomial infections is a significant
threat to the public health. The increasing resistance of S.aureus to various
antibiotics has drawn it to a prime focus for research on designing an
appropriate drug delivery system. Emergence of Methicillin Resistant
Staphylococcus aureus (MRSA) in 1961, necessitated the use of vancomycin ""the
drug of last resort"" to treat these infections. Unfortunately, S.aureus has
already started gaining resistances to vancomycin. Liposome encapsulation of
drugs have been earlier shown to provide an efficient method of microbial
inhibition in many cases. We have studied the effect of liposome encapsulated
vancomycin on MRSA and evaluated the antibacterial activity of the
liposome-entrapped drug in comparison to that of the free drug based on the
minimum inhibitory concentration (MIC) of the drug. The MIC for liposomal
vancomycin was found to be about half of that of free vancomycin. The growth
response of MRSA showed that the liposomal vancomycin induced the culture to go
into bacteriostatic state and phagocytic killing was enhanced. Administration
of the antibiotic encapsulated in liposome thus was shown to greatly improve
the drug delivery as well as the drug resistance caused by MRSA.
",0,0,0,0,1,0
7975,Credit Risk Meets Random Matrices: Coping with Non-Stationary Asset Correlations,"  We review recent progress in modeling credit risk for correlated assets. We
start from the Merton model which default events and losses are derived from
the asset values at maturity. To estimate the time development of the asset
values, the stock prices are used whose correlations have a strong impact on
the loss distribution, particularly on its tails. These correlations are
non-stationary which also influences the tails. We account for the asset
fluctuations by averaging over an ensemble of random matrices that models the
truly existing set of measured correlation matrices. As a most welcome side
effect, this approach drastically reduces the parameter dependence of the loss
distribution, allowing us to obtain very explicit results which show
quantitatively that the heavy tails prevail over diversification benefits even
for small correlations. We calibrate our random matrix model with market data
and show how it is capable of grasping different market situations.
Furthermore, we present numerical simulations for concurrent portfolio risks,
i.e., for the joint probability densities of losses for two portfolios. For the
convenience of the reader, we give an introduction to the Wishart random matrix
model.
",0,0,0,0,0,1
7921,Extremely fast simulations of heat transfer in fluidized beds,"  Besides their huge technological importance, fluidized beds have attracted a
large amount of research because they are perfect playgrounds to investigate
highly dynamic particulate flows. Their over-all behavior is determined by
short-lasting particle collisions and the interaction between solid and gas
phase. Modern simulation techniques that combine computational fluid dynamics
(CFD) and discrete element methods (DEM) are capable of describing their
evolution and provide detailed information on what is happening on the particle
scale. However, these approaches are limited by small time steps and large
numerical costs, which inhibits the investigation of slower long-term processes
like heat transfer or chemical conversion.
In a recent study (Lichtenegger and Pirker, 2016), we have introduced
recurrence CFD (rCFD) as a way to decouple fast from slow degrees of freedom in
systems with recurring patterns: A conventional simulation is carried out to
capture such coherent structures. Their re-appearance is characterized with
recurrence plots that allow us to extrapolate their evolution far beyond the
simulated time. On top of these predicted flow fields, any passive or weakly
coupled process can then be investigated at fractions of the original
computational costs.
Here, we present the application of rCFD to heat transfer in a lab-scale
fluidized bed. Initially hot particles are fluidized with cool air and their
temperature evolution is recorded. In comparison to conventional CFD-DEM, we
observe speed-up factors of about two orders of magnitude at very good accuracy
with regard to recent measurements.
",0,1,0,0,0,0
14168,Unsupervised Ensemble Regression,"  Consider a regression problem where there is no labeled data and the only
observations are the predictions $f_i(x_j)$ of $m$ experts $f_{i}$ over many
samples $x_j$. With no knowledge on the accuracy of the experts, is it still
possible to accurately estimate the unknown responses $y_{j}$? Can one still
detect the least or most accurate experts? In this work we propose a framework
to study these questions, based on the assumption that the $m$ experts have
uncorrelated deviations from the optimal predictor. Assuming the first two
moments of the response are known, we develop methods to detect the best and
worst regressors, and derive U-PCR, a novel principal components approach for
unsupervised ensemble regression. We provide theoretical support for U-PCR and
illustrate its improved accuracy over the ensemble mean and median on a variety
of regression problems.
",1,0,0,1,0,0
1062,"Generalized Euler classes, differential forms and commutative DGAs","  In the context of commutative differential graded algebras over $\mathbb Q$,
we show that an iteration of ""odd spherical fibration"" creates a ""total space""
commutative differential graded algebra with only odd degree cohomology. Then
we show for such a commutative differential graded algebra that, for any of its
""fibrations"" with ""fiber"" of finite cohomological dimension, the induced map on
cohomology is injective.
",0,0,1,0,0,0
12057,"Time Reversal, SU(N) Yang-Mills and Cobordisms: Interacting Topological Superconductors/Insulators and Quantum Spin Liquids in 3+1D","  We introduce a web of strongly correlated interacting 3+1D topological
superconductors/insulators of 10 particular global symmetry groups of Cartan
classes, realizable in electronic condensed matter systems, and their new SU(N)
generalizations. The symmetries include SU(N), SU(2), U(1), fermion parity,
time reversal and relate to each other through symmetry embeddings. We overview
the lattice Hamiltonian formalism. We complete the list of field theories of
bulk symmetry-protected topological invariants (SPT invariants/partition
functions that exhibit boundary 't Hooft anomalies) via cobordism calculations,
matching their full classification. We also present explicit 4-manifolds that
detect these SPTs. On the other hand, once we dynamically gauge part of their
global symmetries, we arrive in various new phases of SU(N) Yang-Mills (YM)
gauge theories, analogous to quantum spin liquids with emergent gauge fields.
We discuss how coupling YM theories to time reversal-SPTs affects the strongly
coupled theories at low energy. For example, we point out a possibility of
having two deconfined gapless time-reversal symmetric SU(2) YM theories at
$\theta=\pi$ as two distinct conformal field theories, which although are
secretly indistinguishable by correlators of local operators on orientable
spacetimes nor by gapped SPT states, can be distinguished on non-orientable
spacetimes or potentially by correlators of extended operators.
",0,1,1,0,0,0
7086,The growth of carbon chains in IRC+10216 mapped with ALMA,"  Linear carbon chains are common in various types of astronomical molecular
sources. Possible formation mechanisms involve both bottom-up and top-down
routes. We have carried out a combined observational and modeling study of the
formation of carbon chains in the C-star envelope IRC+10216, where the
polymerization of acetylene and hydrogen cyanide induced by ultraviolet photons
can drive the formation of linear carbon chains of increasing length. We have
used ALMA to map the emission of 3 mm rotational lines of the hydrocarbon
radicals C2H, C4H, and C6H, and the CN-containing species CN, C3N, HC3N, and
HC5N with an angular resolution of 1"". The spatial distribution of all these
species is a hollow, 5-10"" wide, spherical shell located at a radius of 10-20""
from the star, with no appreciable emission close to the star. Our observations
resolve the broad shell of carbon chains into thinner sub-shells which are 1-2""
wide and not fully concentric, indicating that the mass loss process has been
discontinuous and not fully isotropic. The radial distributions of the species
mapped reveal subtle differences: while the hydrocarbon radicals have very
similar radial distributions, the CN-containing species show more diverse
distributions, with HC3N appearing earlier in the expansion and the radical CN
extending later than the rest of the species. The observed morphology can be
rationalized by a chemical model in which the growth of polyynes is mainly
produced by rapid gas-phase chemical reactions of C2H and C4H radicals with
unsaturated hydrocarbons, while cyanopolyynes are mainly formed from polyynes
in gas-phase reactions with CN and C3N radicals.
",0,1,0,0,0,0
6927,On multifractals: a non-linear study of actigraphy data,"  This work aimed, to determine the characteristics of activity series from
fractal geometry concepts application, in addition to evaluate the possibility
of identifying individuals with fibromyalgia. Activity level data were
collected from 27 healthy subjects and 27 fibromyalgia patients, with the use
of clock-like devices equipped with accelerometers, for about four weeks, all
day long. The activity series were evaluated through fractal and multifractal
methods. Hurst exponent analysis exhibited values according to other studies
($H>0.5$) for both groups ($H=0.98\pm0.04$ for healthy subjects and
$H=0.97\pm0.03$ for fibromyalgia patients), however, it is not possible to
distinguish between the two groups by such analysis. Activity time series also
exhibited a multifractal pattern. A paired analysis of the spectra indices for
the sleep and awake states revealed differences between healthy subjects and
fibromyalgia patients. The individuals feature differences between awake and
sleep states, having statistically significant differences for $\alpha_{q-} -
\alpha_{0}$ in healthy subjects ($p = 0.014$) and $D_{0}$ for patients with
fibromyalgia ($p = 0.013$). The approach has proven to be an option on the
characterisation of such kind of signals and was able to differ between both
healthy and fibromyalgia groups. This outcome suggests changes in the
physiologic mechanisms of movement control.
",0,1,0,1,0,0
6050,11 T Dipole for the Dispersion Suppressor Collimators,"  Chapter 11 in High-Luminosity Large Hadron Collider (HL-LHC) : Preliminary
Design Report. The Large Hadron Collider (LHC) is one of the largest scientific
instruments ever built. Since opening up a new energy frontier for exploration
in 2010, it has gathered a global user community of about 7,000 scientists
working in fundamental particle physics and the physics of hadronic matter at
extreme temperature and density. To sustain and extend its discovery potential,
the LHC will need a major upgrade in the 2020s. This will increase its
luminosity (rate of collisions) by a factor of five beyond the original design
value and the integrated luminosity (total collisions created) by a factor ten.
The LHC is already a highly complex and exquisitely optimised machine so this
upgrade must be carefully conceived and will require about ten years to
implement. The new configuration, known as High Luminosity LHC (HL-LHC), will
rely on a number of key innovations that push accelerator technology beyond its
present limits. Among these are cutting-edge 11-12 tesla superconducting
magnets, compact superconducting cavities for beam rotation with ultra-precise
phase control, new technology and physical processes for beam collimation and
300 metre-long high-power superconducting links with negligible energy
dissipation. The present document describes the technologies and components
that will be used to realise the project and is intended to serve as the basis
for the detailed engineering design of HL-LHC.
",0,1,0,0,0,0
20601,Discrete CMC surfaces in R^3 and discrete minimal surfaces in S^3. A discrete Lawson correspondence,"  The main result of this paper is a discrete Lawson correspondence between
discrete CMC surfaces in R^3 and discrete minimal surfaces in S^3. This is a
correspondence between two discrete isothermic surfaces. We show that this
correspondence is an isometry in the following sense: it preserves the metric
coefficients introduced previously by Bobenko and Suris for isothermic nets.
Exactly as in the smooth case, this is a correspondence between nets with the
same Lax matrices, and the immersion formulas also coincide with the smooth
case.
",0,1,1,0,0,0
17403,On stable solitons and interactions of the generalized Gross-Pitaevskii equation with PT-and non-PT-symmetric potentials,"  We report the bright solitons of the generalized Gross-Pitaevskii (GP)
equation with some types of physically relevant parity-time-(PT-) and
non-PT-symmetric potentials. We find that the constant momentum coefficient can
modulate the linear stability and complicated transverse power-flows (not
always from the gain toward loss) of nonlinear modes. However, the varying
momentum coefficient Gamma(x) can modulate both unbroken linear PT-symmetric
phases and stability of nonlinear modes. Particularly, the nonlinearity can
excite the unstable linear mode (i.e., broken linear PT-symmetric phase) to
stable nonlinear modes. Moreover, we also find stable bright solitons in the
presence of non-PT-symmetric harmonic-Gaussian potential. The interactions of
two bright solitons are also illustrated in PT-symmetric potentials. Finally,
we consider nonlinear modes and transverse power-flows in the three-dimensional
(3D) GP equation with the generalized PT-symmetric Scarf-II potential
",0,1,1,0,0,0
20227,Accelerating Kernel Classifiers Through Borders Mapping,"  Support vector machines (SVM) and other kernel techniques represent a family
of powerful statistical classification methods with high accuracy and broad
applicability. Because they use all or a significant portion of the training
data, however, they can be slow, especially for large problems. Piecewise
linear classifiers are similarly versatile, yet have the additional advantages
of simplicity, ease of interpretation and, if the number of component linear
classifiers is not too large, speed. Here we show how a simple, piecewise
linear classifier can be trained from a kernel-based classifier in order to
improve the classification speed. The method works by finding the root of the
difference in conditional probabilities between pairs of opposite classes to
build up a representation of the decision boundary. When tested on 17 different
datasets, it succeeded in improving the classification speed of a SVM for 9 of
them by factors as high as 88 times or more. The method is best suited to
problems with continuum features data and smooth probability functions. Because
the component linear classifiers are built up individually from an existing
classifier, rather than through a simultaneous optimization procedure, the
classifier is also fast to train.
",1,0,0,1,0,0
328,Graph Clustering using Effective Resistance,"  $ \def\vecc#1{\boldsymbol{#1}} $We design a polynomial time algorithm that
for any weighted undirected graph $G = (V, E,\vecc w)$ and sufficiently large
$\delta > 1$, partitions $V$ into subsets $V_1, \ldots, V_h$ for some $h\geq
1$, such that
$\bullet$ at most $\delta^{-1}$ fraction of the weights are between clusters,
i.e. \[ w(E - \cup_{i = 1}^h E(V_i)) \lesssim \frac{w(E)}{\delta};\]
$\bullet$ the effective resistance diameter of each of the induced subgraphs
$G[V_i]$ is at most $\delta^3$ times the average weighted degree, i.e. \[
\max_{u, v \in V_i} \mathsf{Reff}_{G[V_i]}(u, v) \lesssim \delta^3 \cdot
\frac{|V|}{w(E)} \quad \text{ for all } i=1, \ldots, h.\]
In particular, it is possible to remove one percent of weight of edges of any
given graph such that each of the resulting connected components has effective
resistance diameter at most the inverse of the average weighted degree.
Our proof is based on a new connection between effective resistance and low
conductance sets. We show that if the effective resistance between two vertices
$u$ and $v$ is large, then there must be a low conductance cut separating $u$
from $v$. This implies that very mildly expanding graphs have constant
effective resistance diameter. We believe that this connection could be of
independent interest in algorithm design.
",1,0,0,0,0,0
7828,A natural framework for isogeometric fluid-structure interaction based on BEM-shell coupling,"  The interaction between thin structures and incompressible Newtonian fluids
is ubiquitous both in nature and in industrial applications. In this paper we
present an isogeometric formulation of such problems which exploits a boundary
integral formulation of Stokes equations to model the surrounding flow, and a
non linear Kirchhoff-Love shell theory to model the elastic behaviour of the
structure. We propose three different coupling strategies: a monolithic, fully
implicit coupling, a staggered, elasticity driven coupling, and a novel
semi-implicit coupling, where the effect of the surrounding flow is
incorporated in the non-linear terms of the solid solver through its damping
characteristics. The novel semi-implicit approach is then used to demonstrate
the power and robustness of our method, which fits ideally in the isogeometric
paradigm, by exploiting only the boundary representation (B-Rep) of the thin
structure middle surface.
",0,1,1,0,0,0
9924,Including Uncertainty when Learning from Human Corrections,"  It is difficult for humans to efficiently teach robots how to correctly
perform a task. One intuitive solution is for the robot to iteratively learn
the human's preferences from corrections, where the human improves the robot's
current behavior at each iteration. When learning from corrections, we argue
that while the robot should estimate the most likely human preferences, it
should also know what it does not know, and integrate this uncertainty as it
makes decisions. We advance the state-of-the-art by introducing a Kalman filter
for learning from corrections: this approach obtains the uncertainty of the
estimated human preferences. Next, we demonstrate how the estimate uncertainty
can be leveraged for active learning and risk-sensitive deployment. Our results
indicate that obtaining and leveraging uncertainty leads to faster learning
from human corrections.
",1,0,0,0,0,0
9562,An energy-based analysis of reduced-order models of (networked) synchronous machines,"  Stability of power networks is an increasingly important topic because of the
high penetration of renewable distributed generation units. This requires the
development of advanced (typically model-based) techniques for the analysis and
controller design of power networks. Although there are widely accepted
reduced-order models to describe the dynamic behavior of power networks, they
are commonly presented without details about the reduction procedure, hampering
the understanding of the physical phenomena behind them. The present paper aims
to provide a modular model derivation of multi-machine power networks. Starting
from first-principle fundamental physics, we present detailed dynamical models
of synchronous machines and clearly state the underlying assumptions which lead
to some of the standard reduced-order multi-machine models, including the
classical second-order swing equations. In addition, the energy functions for
the reduced-order multi-machine models are derived, which allows to represent
the multi-machine systems as port-Hamiltonian systems. Moreover, the systems
are proven to be passive with respect to its steady states, which permits for a
power-preserving interconnection with other passive components, including
passive controllers. As a result, the corresponding energy function or
Hamiltonian can be used to provide a rigorous stability analysis of advanced
models for the power network without having to linearize the system.
",1,0,0,0,0,0
1963,Affine forward variance models,"  We introduce the class of affine forward variance (AFV) models of which both
the conventional Heston model and the rough Heston model are special cases. We
show that AFV models can be characterized by the affine form of their cumulant
generating function, which can be obtained as solution of a convolution Riccati
equation. We further introduce the class of affine forward order flow intensity
(AFI) models, which are structurally similar to AFV models, but driven by jump
processes, and which include Hawkes-type models. We show that the cumulant
generating function of an AFI model satisfies a generalized convolution Riccati
equation and that a high-frequency limit of AFI models converges in
distribution to the AFV model.
",0,0,0,0,0,1
8802,Accelerated Primal-Dual Proximal Block Coordinate Updating Methods for Constrained Convex Optimization,"  Block Coordinate Update (BCU) methods enjoy low per-update computational
complexity because every time only one or a few block variables would need to
be updated among possibly a large number of blocks. They are also easily
parallelized and thus have been particularly popular for solving problems
involving large-scale dataset and/or variables. In this paper, we propose a
primal-dual BCU method for solving linearly constrained convex program in
multi-block variables. The method is an accelerated version of a primal-dual
algorithm proposed by the authors, which applies randomization in selecting
block variables to update and establishes an $O(1/t)$ convergence rate under
weak convexity assumption. We show that the rate can be accelerated to
$O(1/t^2)$ if the objective is strongly convex. In addition, if one block
variable is independent of the others in the objective, we then show that the
algorithm can be modified to achieve a linear rate of convergence. The
numerical experiments show that the accelerated method performs stably with a
single set of parameters while the original method needs to tune the parameters
for different datasets in order to achieve a comparable level of performance.
",1,0,1,1,0,0
9124,Automated Detection of Serializability Violations under Weak Consistency,"  While a number of weak consistency mechanisms have been developed in recent
years to improve performance and ensure availability in distributed, replicated
systems, ensuring correctness of transactional applications running on top of
such systems remains a difficult and important problem. Serializability is a
well-understood correctness criterion for transactional programs; understanding
whether applications are serializable when executed in a weakly-consistent
environment, however remains a challenging exercise. In this work, we combine
the dependency graph-based characterization of serializability and the
framework of abstract executions to develop a fully automated approach for
statically finding bounded serializability violations under \emph{any} weak
consistency model. We reduce the problem of serializability to satisfiability
of a formula in First-Order Logic, which allows us to harness the power of
existing SMT solvers. We provide rules to automatically construct the FOL
encoding from programs written in SQL (allowing loops and conditionals) and the
consistency specification written as a formula in FOL. In addition to detecting
bounded serializability violations, we also provide two orthogonal schemes to
reason about unbounded executions by providing sufficient conditions (in the
form of FOL formulae) whose satisfiability would imply the absence of anomalies
in any arbitrary execution. We have applied the proposed technique on TPC-C, a
real world database program with complex application logic, and were able to
discover anomalies under Parallel Snapshot Isolation, and verify
serializability for unbounded executions under Snapshot Isolation, two
consistency mechanisms substantially weaker than serializability.
",1,0,0,0,0,0
8175,Passivation and Cooperative Control of Equilibrium-Independent Passivity-Short Systems,"  Maximal equilibrium-independent passivity (MEIP) is a recently introduced
system property which has acquired special attention in the study of networked
dynamical systems. MEIP requires a system to be passive with respect to any
forced equilibrium configuration and the associated steady-state input-output
map must be maximally monotone. In practice, however, most of the systems are
not well behaved and possess shortage of passivity or non-passiveness in their
operation. In this paper, we consider a class of passivity-short systems,
namely equilibrium-independent passivity-short (EIPS) systems, and presents an
input-output transformation based generalized passivation approach to ensure
their MEIP properties. We characterize the steady-state input-output relations
of the EIPS systems and establish their connection with that of the transformed
MEIP systems. We further study the diffusively-coupled networked interactions
of such EIPS systems and explore their connection to a pair of dual network
optimization problems, under the proposed matrix transformation. A simulation
example is given to illustrate the theoretical results.
",1,0,0,0,0,0
15582,A Thematic Study of Requirements Modeling and Analysis for Self-Adaptive Systems,"  Over the last decade, researchers and engineers have developed a vast body of
methodologies and technologies in requirements engineering for self-adaptive
systems. Although existing studies have explored various aspects of this topic,
few of them have categorized and summarized these areas of research in
require-ments modeling and analysis. This study aims to investigate the
research themes based on the utilized modeling methods and RE activities. We
conduct a thematic study in the systematic literature review. The results are
derived by synthesizing the extracted data with statistical methods. This paper
provides an updated review of the research literature, enabling researchers and
practitioners to better understand the research themes in these areas and
identify research gaps which need to be further studied.
",1,0,0,0,0,0
11936,Density matrix expansion based semi-local exchange hole applied to range separated density functional theory,"  Exchange hole is the principle constituent in density functional theory,
which can be used to accurately design exchange energy functional and range
separated hybrid functionals coupled with some appropriate correlation.
Recently, density matrix expansion (DME) based semi-local exchange hole
proposed by Tao-Mo gained attention due to its fulfillment of some exact
constraints. We propose a new long-range corrected (LC) scheme that combines
meta-generalized gradient approximation (meta-GGA) exchange functionals
designed from DME exchange hole coupled with the ab-initio Hartree-Fock (HF)
exchange integral by separating the Coulomb interaction operator using standard
error function. Associate with Lee-Yang-Parr (LYP) correlation functional,
assessment and benchmarking of our functional using well-known test set shows
that it performs remarkably well for a broad range of molecular properties,
such as thermochemistry, noncovalent interaction and barrier height of chemical
reactions.
",0,1,0,0,0,0
8117,Gate Tunable Magneto-resistance of Ultra-Thin WTe2 Devices,"  In this work, the magneto-resistance (MR) of ultra-thin WTe2/BN
heterostructures far away from electron-hole equilibrium is measured. The
change of MR of such devices is found to be determined largely by a single
tunable parameter, i.e. the amount of imbalance between electrons and holes. We
also found that the magnetoresistive behavior of ultra-thin WTe2 devices is
well-captured by a two-fluid model. According to the model, the change of MR
could be as large as 400,000%, the largest potential change of MR among all
materials known, if the ultra-thin samples are tuned to neutrality when
preserving the mobility of 167,000 cm2V-1s-1 observed in bulk samples. Our
findings show the prospects of ultra-thin WTe2 as a variable magnetoresistance
material in future applications such as magnetic field sensors, information
storage and extraction devices, and galvanic isolators. The results also
provide important insight into the electronic structure and the origin of the
large MR in ultra-thin WTe2 samples.
",0,1,0,0,0,0
89,Photo-Chemically Directed Self-Assembly of Carbon Nanotubes on Surfaces,"  Transistors incorporating single-wall carbon nanotubes (CNTs) as the channel
material are used in a variety of electronics applications. However, a
competitive CNT-based technology requires the precise placement of CNTs at
predefined locations of a substrate. One promising placement approach is to use
chemical recognition to bind CNTs from solution at the desired locations on a
surface. Producing the chemical pattern on the substrate is challenging. Here
we describe a one-step patterning approach based on a highly photosensitive
surface monolayer. The monolayer contains chromophopric group as light
sensitive body with heteroatoms as high quantum yield photolysis center. As
deposited, the layer will bind CNTs from solution. However, when exposed to
ultraviolet (UV) light with a low dose (60 mJ/cm2) similar to that used for
conventional photoresists, the monolayer cleaves and no longer binds CNTs.
These features allow standard, wafer-scale UV lithography processes to be used
to form a patterned chemical monolayer without the need for complex substrate
patterning or monolayer stamping.
",0,1,0,0,0,0
8328,Feature Model-to-Ontology for SPL Application Realisation,"  Feature model are widely used to capture commonalities and variabilities of
artefacts in Software Product Line (SPL). Several studies have discussed the
formal representation of feature diagram using ontologies with different styles
of mapping. However, they still focused on the ontology approach for problem
space and keep the solution space aside. In this paper, we present the
modelling of feature model using OWL ontology and produce an application based
on the ontology. Firstly, we map the features in a running example feature
diagram to OWL classes and properties. Secondly, we verify the consistency of
the OWL ontology by using reasoning engines. Finally, we use the ontology as an
input of Zotonic framework for application realisation.
",1,0,0,0,0,0
5991,Fast Characterization of Segmental Duplications in Genome Assemblies,"  Segmental duplications (SDs), or low-copy repeats (LCR), are segments of DNA
greater than 1 Kbp with high sequence identity that are copied to other regions
of the genome. SDs are among the most important sources of evolution, a common
cause of genomic structural variation, and several are associated with diseases
of genomic origin. Despite their functional importance, SDs present one of the
major hurdles for de novo genome assembly due to the ambiguity they cause in
building and traversing both state-of-the-art overlap-layout-consensus and de
Bruijn graphs. This causes SD regions to be misassembled, collapsed into a
unique representation, or completely missing from assembled reference genomes
for various organisms. In turn, this missing or incorrect information limits
our ability to fully understand the evolution and the architecture of the
genomes. Despite the essential need to accurately characterize SDs in
assemblies, there is only one tool that has been developed for this purpose,
called Whole Genome Assembly Comparison (WGAC). WGAC is comprised of several
steps that employ different tools and custom scripts, which makes it difficult
and time consuming to use. Thus there is still a need for algorithms to
characterize within-assembly SDs quickly, accurately, and in a user friendly
manner.
Here we introduce a SEgmental Duplication Evaluation Framework (SEDEF) to
rapidly detect SDs through sophisticated filtering strategies based on Jaccard
similarity and local chaining. We show that SEDEF accurately detects SDs while
maintaining substantial speed up over WGAC that translates into practical run
times of minutes instead of weeks. Notably, our algorithm captures up to 25%
pairwise error between segments, where previous studies focused on only 10%,
allowing us to more deeply track the evolutionary history of the genome.
SEDEF is available at this https URL
",0,0,0,0,1,0
2230,Smoothed GMM for quantile models,"  This paper develops theory for feasible estimators of finite-dimensional
parameters identified by general conditional quantile restrictions, under much
weaker assumptions than previously seen in the literature. This includes
instrumental variables nonlinear quantile regression as a special case. More
specifically, we consider a set of unconditional moments implied by the
conditional quantile restrictions, providing conditions for local
identification. Since estimators based on the sample moments are generally
impossible to compute numerically in practice, we study feasible estimators
based on smoothed sample moments. We propose a method of moments estimator for
exactly identified models, as well as a generalized method of moments estimator
for over-identified models. We establish consistency and asymptotic normality
of both estimators under general conditions that allow for weakly dependent
data and nonlinear structural models. Simulations illustrate the finite-sample
properties of the methods. Our in-depth empirical application concerns the
consumption Euler equation derived from quantile utility maximization.
Advantages of the quantile Euler equation include robustness to fat tails,
decoupling of risk attitude from the elasticity of intertemporal substitution,
and log-linearization without any approximation error. For the four countries
we examine, the quantile estimates of discount factor and elasticity of
intertemporal substitution are economically reasonable for a range of quantiles
above the median, even when two-stage least squares estimates are not
reasonable.
",0,0,1,1,0,0
13427,Protein Classification using Machine Learning and Statistical Techniques: A Comparative Analysis,"  In recent era prediction of enzyme class from an unknown protein is one of
the challenging tasks in bioinformatics. Day to day the number of proteins is
increases as result the prediction of enzyme class gives a new opportunity to
bioinformatics scholars. The prime objective of this article is to implement
the machine learning classification technique for feature selection and
predictions also find out an appropriate classification technique for function
prediction. In this article the seven different classification technique like
CRT, QUEST, CHAID, C5.0, ANN (Artificial Neural Network), SVM and Bayesian has
been implemented on 4368 protein data that has been extracted from UniprotKB
databank and categories into six different class. The proteins data is high
dimensional sequence data and contain a maximum of 48 features.To manipulate
the high dimensional sequential protein data with different classification
technique, the SPSS has been used as an experimental tool. Different
classification techniques give different results for every model and shows that
the data are imbalanced for class C4, C5 and C6. The imbalanced data affect the
performance of model. In these three classes the precision and recall value is
very less or negligible. The experimental results highlight that the C5.0
classification technique accuracy is more suited for protein feature
classification and predictions. The C5.0 classification technique gives 95.56%
accuracy and also gives high precision and recall value. Finally, we conclude
that the features that is selected can be used for function prediction.
",0,0,0,0,1,0
7224,Fixed-Parameter Tractable Sampling for RNA Design with Multiple Target Structures,"  The design of multi-stable RNA molecules has important applications in
biology, medicine, and biotechnology. Synthetic design approaches profit
strongly from effective in-silico methods, which can tremendously impact their
cost and feasibility. We revisit a central ingredient of most in-silico design
methods: the sampling of sequences for the design of multi-target structures,
possibly including pseudoknots. For this task, we present the efficient, tree
decomposition-based algorithm. Our fixed parameter tractable approach is
underpinned by establishing the P-hardness of uniform sampling. Modeling the
problem as a constraint network, our program supports generic
Boltzmann-weighted sampling for arbitrary additive RNA energy models; this
enables the generation of RNA sequences meeting specific goals like expected
free energies or \GCb-content. Finally, we empirically study general properties
of the approach and generate biologically relevant multi-target
Boltzmann-weighted designs for a common design benchmark. Generating seed
sequences with our program, we demonstrate significant improvements over the
previously best multi-target sampling strategy (uniform sampling).Our software
is freely available at: this https URL .
",0,0,0,0,1,0
2617,Global existence in the 1D quasilinear parabolic-elliptic chemotaxis system with critical nonlinearity,"  The paper should be viewed as complement of an earlier result in [8]. In the
paper just mentioned it is shown that 1d case of a quasilinear
parabolic-elliptic Keller-Segel system is very special. Namely, unlike in
higher dimensions, there is no critical nonlinearity. Indeed, for the nonlinear
diffusion of the form 1/u all the solutions, independently on the magnitude of
initial mass, stay bounded. However, the argument presented in [8] deals with
the Jager-Luckhaus type system. And is very sensitive to this restriction.
Namely, the change of variables introduced in [8], being a main step of the
method, works only for the Jager-Luckhaus modification. It does not seem to be
applicable in the usual version of the parabolic-elliptic Keller-Segel system.
The present paper fulfils this gap and deals with the case of the usual
parabolic-elliptic version. To handle it we establish a new Lyapunov-like
functional (it is related to what was done in [8]), which leads to global
existence of the initial-boundary value problem for any initial mass.
",0,0,1,0,0,0
5117,Noise-gating to clean astrophysical image data,"  I present a family of algorithms to reduce noise in astrophysical im- ages
and image sequences, preserving more information from the original data than is
retained by conventional techniques. The family uses locally adaptive filters
(""noise gates"") in the Fourier domain, to separate coherent image structure
from background noise based on the statistics of local neighborhoods in the
image. Processing of solar data limited by simple shot noise or by additive
noise reveals image structure not easily visible in the originals, preserves
photometry of observable features, and reduces shot noise by a factor of 10 or
more with little to no apparent loss of resolution, revealing faint features
that were either not directly discernible or not sufficiently strongly detected
for quantitative analysis. The method works best on image sequences containing
related subjects, for example movies of solar evolution, but is also applicable
to single images provided that there are enough pixels. The adaptive filter
uses the statistical properties of noise and of local neighborhoods in the
data, to discriminate between coherent features and incoherent noise without
reference to the specific shape or evolution of the those features. The
technique can potentially be modified in a straightforward way to exploit
additional a priori knowledge about the functional form of the noise.
",0,1,0,0,0,0
17556,DeepPainter: Painter Classification Using Deep Convolutional Autoencoders,"  In this paper we describe the problem of painter classification, and propose
a novel approach based on deep convolutional autoencoder neural networks. While
previous approaches relied on image processing and manual feature extraction
from paintings, our approach operates on the raw pixel level, without any
preprocessing or manual feature extraction. We first train a deep convolutional
autoencoder on a dataset of paintings, and subsequently use it to initialize a
supervised convolutional neural network for the classification phase.
The proposed approach substantially outperforms previous methods, improving
the previous state-of-the-art for the 3-painter classification problem from
90.44% accuracy (previous state-of-the-art) to 96.52% accuracy, i.e., a 63%
reduction in error rate.
",1,0,0,1,0,0
1573,Quantum Paramagnet and Frustrated Quantum Criticality in a Spin-One Diamond Lattice Antiferromagnet,"  Motivated by the proposal of topological quantum paramagnet in the diamond
lattice antiferromagnet NiRh$_2$O$_4$, we propose a minimal model to describe
the magnetic interaction and properties of the diamond material with the
spin-one local moments. Our model includes the first and second neighbor
Heisenberg interactions as well as a local single-ion spin anisotropy that is
allowed by the spin-one nature of the local moment and the tetragonal symmetry
of the system. We point out that there exists a quantum phase transition from a
trivial quantum paramagnet when the single-ion spin anisotropy is dominant to
the magnetic ordered states when the exchange is dominant. Due to the
frustrated spin interaction, the magnetic excitation in the quantum
paramagnetic state supports extensively degenerate band minima in the spectra.
As the system approaches the transition, extensively degenerate bosonic modes
become critical at the criticality, giving rise to unusual magnetic properties.
Our phase diagram and experimental predictions for different phases provide a
guildeline for the identification of the ground state for NiRh$_2$O$_4$.
Although our results are fundamentally different from the proposal of
topological quantum paramagnet, it represents interesting possibilities for
spin-one diamond lattice antiferromagnets.
",0,1,0,0,0,0
18508,A numerical study of the homogeneous elliptic equation with fractional order boundary conditions,"  We consider the homogeneous equation ${\mathcal A} u=0$, where ${\mathcal A}$
is a symmetric and coercive elliptic operator in $H^1(\Omega)$ with $\Omega$
bounded domain in ${\mathbb R}^d$. The boundary conditions involve fractional
power $\alpha$, $ 0 < \alpha <1$, of the Steklov spectral operator arising in
Dirichlet to Neumann map. For such problems we discuss two different numerical
methods: (1) a computational algorithm based on an approximation of the
integral representation of the fractional power of the operator and (2)
numerical technique involving an auxiliary Cauchy problem for an
ultra-parabolic equation and its subsequent approximation by a time stepping
technique. For both methods we present numerical experiment for a model
two-dimensional problem that demonstrate the accuracy, efficiency, and
stability of the algorithms.
",0,0,1,0,0,0
10153,Trapping and displacement of liquid collars and plugs in rough-walled tubes,"  A liquid film wetting the interior of a long circular cylinder redistributes
under the action of surface tension to form annular collars or occlusive plugs.
These equilibrium structures are invariant under axial translation within a
perfectly smooth uniform tube and therefore can be displaced axially by very
weak external forcing. We consider how this degeneracy is disrupted when the
tube wall is rough, and determine threshold conditions under which collars or
plugs resist displacement under forcing. Wall roughness is modelled as a
non-axisymmetric Gaussian random field of prescribed correlation length and
small variance, mimicking some of the geometric irregularities inherent in
applications such as lung airways. The thin film coating this surface is
modelled using lubrication theory. When the roughness is weak, we show how the
locations of equilibrium collars and plugs can be identified in terms of the
azimuthally averaged tube radius; we derive conditions specifying equilibrium
collar locations under an externally imposed shear flow, and plug locations
under an imposed pressure gradient. We use these results to determine the
probability of external forcing being sufficient to displace a collar or plug
from a rough-walled tube, when the tube roughness is defined only in
statistical terms.
",0,1,0,0,0,0
15783,Impact of Detour-Aware Policies on Maximizing Profit in Ridesharing,"  This paper provides efficient solutions to maximize profit for commercial
ridesharing services, under a pricing model with detour-based discounts for
passengers. We propose greedy heuristics for real-time ride matching that offer
different trade-offs between optimality and speed. Simulations on New York City
(NYC) taxi trip data show that our heuristics are up to 90% optimal and 10^5
times faster than the (necessarily) exponential-time optimal algorithm.
Commercial ridesharing service providers generate significant savings by
matching multiple ride requests using heuristic methods. The resulting savings
are typically shared between the service provider (in the form of increased
profit) and the ridesharing passengers (in the form of discounts). It is not
clear a priori how this split should be effected, since higher discounts would
encourage more ridesharing, thereby increasing total savings, but the fraction
of savings taken as profit is reduced. We simulate a scenario where the
decisions of the passengers to opt for ridesharing depend on the discount
offered by the service provider. We provide an adaptive learning algorithm
IDFLA that learns the optimal profit-maximizing discount factor for the
provider. An evaluation over NYC data shows that IDFLA, on average, learns the
optimal discount factor in under 16 iterations.
Finally, we investigate the impact of imposing a detour-aware routing policy
based on sequential individual rationality, a recently proposed concept. Such
restricted policies offer a better ride experience, increasing the provider's
market share, but at the cost of decreased average per-ride profit due to the
reduced number of matched rides. We construct a model that captures these
opposing effects, wherein simulations based on NYC data show that a 7% increase
in market share would suffice to offset the decreased average per-ride profit.
",1,0,1,0,0,0
11816,"Atmospheric stellar parameters for large surveys using FASMA, a new spectral synthesis package","  In the era of vast spectroscopic surveys focusing on Galactic stellar
populations, astronomers want to exploit the large quantity and good quality of
data to derive their atmospheric parameters without losing precision from
automatic procedures. In this work, we developed a new spectral package, FASMA,
to estimate the stellar atmospheric parameters (namely effective temperature,
surface gravity, and metallicity) in a fast and robust way. This method is
suitable for spectra of FGK-type stars in medium and high resolution. The
spectroscopic analysis is based on the spectral synthesis technique using the
radiative transfer code, MOOG. The line list is comprised of mainly iron lines
in the optical spectrum. The atomic data are calibrated after the Sun and
Arcturus. We use two comparison samples to test our method, i) a sample of 451
FGK-type dwarfs from the high resolution HARPS spectrograph, and ii) the
Gaia-ESO benchmark stars using both high and medium resolution spectra. We
explore biases in our method from the analysis of synthetic spectra covering
the parameter space of our interest. We show that our spectral package is able
to provide reliable results for a wide range of stellar parameters, different
rotational velocities, different instrumental resolutions, and for different
spectral regions of the VLT-GIRAFFE spectrographs, used among others for the
Gaia-ESO survey. FASMA estimates stellar parameters in less than 15 min for
high resolution and 3 min for medium resolution spectra. The complete package
is publicly available to the community.
",0,1,0,0,0,0
1057,Study of the Magnetizing Relationship of the Kickers for CSNS,"  The extraction system of CSNS mainly consists of two kinds of magnets: eight
kickers and one lambertson magnet. In this paper, firstly, the magnetic test
results of the eight kickers were introduced and then the filed uniformity and
magnetizing relationship of the kickers were given. Secondly, during the beam
commissioning in the future, in order to obtain more accurate magnetizing
relationship, a new method to measure the magnetizing coefficients of the
kickers by the real extraction beam was given and the data analysis would also
be processed.
",0,1,0,0,0,0
6341,Synthesis and analysis in total variation regularization,"  We generalize the bridge between analysis and synthesis estimators by Elad,
Milanfar and Rubinstein (2007) to rank deficient cases. This is a starting
point for the study of the connection between analysis and synthesis for total
variation regularized estimators. In particular, the case of first order total
variation regularized estimators over general graphs and their synthesis form
are studied.
We give a definition of the discrete graph derivative operator based on the
notion of line graph and provide examples of the synthesis form of
$k^{\text{th}}$ order total variation regularized estimators over a range of
graphs.
",0,0,1,1,0,0
1085,"Fast, Better Training Trick -- Random Gradient","  In this paper, we will show an unprecedented method to accelerate training
and improve performance, which called random gradient (RG). This method can be
easier to the training of any model without extra calculation cost, we use
Image classification, Semantic segmentation, and GANs to confirm this method
can improve speed which is training model in computer vision. The central idea
is using the loss multiplied by a random number to random reduce the
back-propagation gradient. We can use this method to produce a better result in
Pascal VOC, Cifar, Cityscapes datasets.
",0,0,0,1,0,0
3351,Adversarial Variational Optimization of Non-Differentiable Simulators,"  Complex computer simulators are increasingly used across fields of science as
generative models tying parameters of an underlying theory to experimental
observations. Inference in this setup is often difficult, as simulators rarely
admit a tractable density or likelihood function. We introduce Adversarial
Variational Optimization (AVO), a likelihood-free inference algorithm for
fitting a non-differentiable generative model incorporating ideas from
generative adversarial networks, variational optimization and empirical Bayes.
We adapt the training procedure of generative adversarial networks by replacing
the differentiable generative network with a domain-specific simulator. We
solve the resulting non-differentiable minimax problem by minimizing
variational upper bounds of the two adversarial objectives. Effectively, the
procedure results in learning a proposal distribution over simulator
parameters, such that the JS divergence between the marginal distribution of
the synthetic data and the empirical distribution of observed data is
minimized. We evaluate and compare the method with simulators producing both
discrete and continuous data.
",1,0,0,1,0,0
2027,Discontinuous Hamiltonian Monte Carlo for discrete parameters and discontinuous likelihoods,"  Hamiltonian Monte Carlo has emerged as a standard tool for posterior
computation. In this article, we present an extension that can efficiently
explore target distributions with discontinuous densities, which in turn
enables efficient sampling from ordinal parameters though embedding of
probability mass functions into continuous spaces. We motivate our approach
through a theory of discontinuous Hamiltonian dynamics and develop a numerical
solver of discontinuous dynamics. The proposed numerical solver is the first of
its kind, with a remarkable ability to exactly preserve the Hamiltonian and
thus yield a type of rejection-free proposals. We apply our algorithm to
challenging posterior inference problems to demonstrate its wide applicability
and competitive performance.
",0,0,0,1,0,0
15770,A mathematical characterization of confidence as valid belief,"  Confidence is a fundamental concept in statistics, but there is a tendency to
misinterpret it as probability. In this paper, I argue that an intuitively and
mathematically more appropriate interpretation of confidence is through
belief/plausibility functions, in particular, those that satisfy a certain
validity property. Given their close connection with confidence, it is natural
to ask how a valid belief/plausibility function can be constructed directly.
The inferential model (IM) framework provides such a construction, and here I
prove a complete-class theorem stating that, for every nominal confidence
region, there exists a valid IM whose plausibility regions are contained by the
given confidence region. This characterization has implications for statistics
understanding and communication, and highlights the importance of belief
functions and the IM framework.
",0,0,1,1,0,0
12591,Orientably-regular maps on twisted linear fractional groups,"  We present an enumeration of orientably-regular maps with automorphism group
isomorphic to the twisted linear fractional group $M(q^2)$ for any odd prime
power $q$.
",0,0,1,0,0,0
12275,Can the Journal Impact Factor Be Used as a Criterion for the Selection of Junior Researchers? A Large-Scale Empirical Study Based on ResearcherID Data,"  Early in researchers' careers, it is difficult to assess how good their work
is or how important or influential the scholars will eventually be. Hence,
funding agencies, academic departments, and others often use the Journal Impact
Factor (JIF) of where the authors have published to assess their work and
provide resources and rewards for future work. The use of JIFs in this way has
been heavily criticized, however. Using a large data set with many thousands of
publication profiles of individual researchers, this study tests the ability of
the JIF (in its normalized variant) to identify, at the beginning of their
careers, those candidates who will be successful in the long run. Instead of
bare JIFs and citation counts, the metrics used here are standardized according
to Web of Science subject categories and publication years. The results of the
study indicate that the JIF (in its normalized variant) is able to discriminate
between researchers who published papers later on with a citation impact above
or below average in a field and publication year - not only in the short term,
but also in the long term. However, the low to medium effect sizes of the
results also indicate that the JIF (in its normalized variant) should not be
used as the sole criterion for identifying later success: other criteria, such
as the novelty and significance of the specific research, academic
distinctions, and the reputation of previous institutions, should also be
considered.
",1,1,0,0,0,0
10690,cGAN-based Manga Colorization Using a Single Training Image,"  The Japanese comic format known as Manga is popular all over the world. It is
traditionally produced in black and white, and colorization is time consuming
and costly. Automatic colorization methods generally rely on greyscale values,
which are not present in manga. Furthermore, due to copyright protection,
colorized manga available for training is scarce. We propose a manga
colorization method based on conditional Generative Adversarial Networks
(cGAN). Unlike previous cGAN approaches that use many hundreds or thousands of
training images, our method requires only a single colorized reference image
for training, avoiding the need of a large dataset. Colorizing manga using
cGANs can produce blurry results with artifacts, and the resolution is limited.
We therefore also propose a method of segmentation and color-correction to
mitigate these issues. The final results are sharp, clear, and in high
resolution, and stay true to the character's original color scheme.
",1,0,0,0,0,0
18578,"Componentwise different tail solutions for bivariate stochastic recurrence equations -- with application to GARCH(1,1) processes --","  We study bivariate stochastic recurrence equations (SREs) motivated by
applications to GARCH(1,1) processes. If coefficient matrices of SREs have
strictly positive entries, then the Kesten result applies and it gives
solutions with regularly varying tails. Moreover, the tail indices are the same
for all coordinates. However, for applications, this framework is too
restrictive. We study SREs when coefficients are triangular matrices and prove
that the coordinates of the solution may exhibit regularly varying tails with
different indices. We also specify each tail index together with its constant.
The results are used to characterize regular variations of bivariate stationary
GARCH(1,1) processes.
",0,0,1,1,0,0
13295,Comparing distributions by multiple testing across quantiles or CDF values,"  When comparing two distributions, it is often helpful to learn at which
quantiles or values there is a statistically significant difference. This
provides more information than the binary ""reject"" or ""do not reject"" decision
of a global goodness-of-fit test. Framing our question as multiple testing
across the continuum of quantiles $\tau\in(0,1)$ or values $r\in\mathbb{R}$, we
show that the Kolmogorov--Smirnov test (interpreted as a multiple testing
procedure) achieves strong control of the familywise error rate. However, its
well-known flaw of low sensitivity in the tails remains. We provide an
alternative method that retains such strong control of familywise error rate
while also having even sensitivity, i.e., equal pointwise type I error rates at
each of $n\to\infty$ order statistics across the distribution. Our one-sample
method computes instantly, using our new formula that also instantly computes
goodness-of-fit $p$-values and uniform confidence bands. To improve power, we
also propose stepdown and pre-test procedures that maintain control of the
asymptotic familywise error rate. One-sample and two-sample cases are
considered, as well as extensions to regression discontinuity designs and
conditional distributions. Simulations, empirical examples, and code are
provided.
",0,0,1,1,0,0
20192,Semianalytical calculation of the zonal-flow oscillation frequency in stellarators,"  Due to their capability to reduce turbulent transport in magnetized plasmas,
understanding the dynamics of zonal flows is an important problem in the fusion
programme. Since the pioneering work by Rosenbluth and Hinton in axisymmetric
tokamaks, it is known that studying the linear and collisionless relaxation of
zonal flow perturbations gives valuable information and physical insight.
Recently, the problem has been investigated in stellarators and it has been
found that in these devices the relaxation process exhibits a characteristic
feature: a damped oscillation. The frequency of this oscillation might be a
relevant parameter in the regulation of turbulent transport, and therefore its
efficient and accurate calculation is important. Although an analytical
expression can be derived for the frequency, its numerical evaluation is not
simple and has not been exploited systematically so far. Here, a numerical
method for its evaluation is considered, and the results are compared with
those obtained by calculating the frequency from gyrokinetic simulations. This
""semianalytical"" approach for the determination of the zonal-flow frequency
reveals accurate and faster than the one based on gyrokinetic simulations.
",0,1,0,0,0,0
1206,Jamming-Resistant Receivers for the Massive MIMO Uplink,"  We design a jamming-resistant receiver scheme to enhance the robustness of a
massive MIMO uplink system against jamming. We assume that a jammer attacks the
system both in the pilot and data transmission phases. The key feature of the
proposed scheme is that, in the pilot phase, we estimate not only the
legitimate channel, but also the jamming channel by exploiting a purposely
unused pilot sequence. The jamming channel estimate is used to constructed
linear receive filters that reject the impact of the jamming signal. The
performance of the proposed scheme is analytically evaluated using asymptotic
properties of massive MIMO. The optimal regularized zero-forcing receiver and
the optimal power allocation are also studied. Numerical results are provided
to verify our analysis and show that the proposed scheme greatly improves the
achievable rates, as compared to conventional receivers. Interestingly, the
proposed scheme works particularly well under strong jamming attacks, since the
improved estimate of the jamming channel outweighs the extra jamming power.
",1,0,0,0,0,0
8301,A spectral approach to transit timing variations,"  The high planetary multiplicity revealed by Kepler implies that Transit Time
Variations (TTVs) are intrinsically common. The usual procedure for detecting
these TTVs is biased to long-period, deep transit planets whereas most
transiting planets have short periods and shallow transits. Here we introduce
the Spectral Approach to TTVs technique that allows expanding the TTVs catalog
towards lower TTV amplitude, shorter orbital period, and shallower transit
depth. In the Spectral Approach we assume that a sinusoidal TTV exists in the
data and then calculate the improvement to $\chi^2$ this model allows over that
of linear ephemeris model. This enables detection of TTVs even in cases where
the transits are too shallow so individual transits cannot be timed. The
Spectral Approach is more sensitive due to the reduced number of free
parameters in its model. Using the Spectral Approach, we: (a) detect 131 new
periodic TTVs in Kepler data (an increase of ~2/3 over a previous TTV catalog);
(b) Constrain the TTV periods of 34 long-period TTVs and reduce amplitude
errors of known TTVs; (c) Identify cases of multi-periodic TTVs, for which
absolute planetary mass determination may be possible. We further extend our
analysis by using perturbation theory assuming small TTV amplitude at the
detection stage, which greatly speeds up our detection (to a level of few
seconds per star). Our extended TTVs sample shows no deficit of short period or
low amplitude transits, in contrast to previous surveys in which the detection
schemes were significantly biased against such systems.
",0,1,0,0,0,0
1444,Anisotropic Exchange in ${\bf LiCu_2O_2}$,"  We investigate the magnetic properties of the multiferroic quantum-spin
system LiCu$_2$O$_2$ by electron spin resonance (ESR) measurements at $X$- and
$Q$-band frequencies in a wide temperature range $(T_{\rm N1} \leq T \leq
300$\,K). The observed anisotropies of the $g$ tensor and the ESR linewidth in
untwinned single crystals result from the crystal-electric field and from local
exchange geometries acting on the magnetic Cu$^{2+}$ ions in the zigzag-ladder
like structure of LiCu$_2$O$_2$. Supported by a microscopic analysis of the
exchange paths involved, we show that both the symmetric anisotropic exchange
interaction and the antisymmetric Dzyaloshinskii-Moriya interaction provide the
dominant spin-spin relaxation channels in this material.
",0,1,0,0,0,0
11978,Invariant Gibbs measures for the 2-d defocusing nonlinear wave equations,"  We consider the defocusing nonlinear wave equations (NLW) on the
two-dimensional torus. In particular, we construct invariant Gibbs measures for
the renormalized so-called Wick ordered NLW. We then prove weak universality of
the Wick ordered NLW, showing that the Wick ordered NLW naturally appears as a
suitable scaling limit of non-renormalized NLW with Gaussian random initial
data.
",0,0,1,0,0,0
2458,CardiacNET: Segmentation of Left Atrium and Proximal Pulmonary Veins from MRI Using Multi-View CNN,"  Anatomical and biophysical modeling of left atrium (LA) and proximal
pulmonary veins (PPVs) is important for clinical management of several cardiac
diseases. Magnetic resonance imaging (MRI) allows qualitative assessment of LA
and PPVs through visualization. However, there is a strong need for an advanced
image segmentation method to be applied to cardiac MRI for quantitative
analysis of LA and PPVs. In this study, we address this unmet clinical need by
exploring a new deep learning-based segmentation strategy for quantification of
LA and PPVs with high accuracy and heightened efficiency. Our approach is based
on a multi-view convolutional neural network (CNN) with an adaptive fusion
strategy and a new loss function that allows fast and more accurate convergence
of the backpropagation based optimization. After training our network from
scratch by using more than 60K 2D MRI images (slices), we have evaluated our
segmentation strategy to the STACOM 2013 cardiac segmentation challenge
benchmark. Qualitative and quantitative evaluations, obtained from the
segmentation challenge, indicate that the proposed method achieved the
state-of-the-art sensitivity (90%), specificity (99%), precision (94%), and
efficiency levels (10 seconds in GPU, and 7.5 minutes in CPU).
",1,0,0,1,0,0
11877,Positive scalar curvature and the Euler class,"  We prove the following generalization of the classical Lichnerowicz vanishing
theorem: if $F$ is an oriented flat vector bundle over a closed spin manifold
$M$ such that $TM$ carries a metric of positive scalar curvature, then
$<\widehat A(TM)e(F),[M]>=0$, where $e(F)$ is the Euler class of $F$.
",0,0,1,0,0,0
9503,Spatial Risk Measure for Max-Stable and Max-Mixture Processes,"  In this paper, we consider isotropic and stationary max-stable, inverse
max-stable and max-mixture processes $X=(X(s))\_{s\in\bR^2}$ and the damage
function $\cD\_X^{\nu}= |X|^\nu$ with $0<\nu<1/2$. We study the quantitative
behavior of a risk measure which is the variance of the average of
$\cD\_X^{\nu}$ over a region $\mathcal{A}\subset \bR^2$.} This kind of risk
measure has already been introduced and studied for \vero{some} max-stable
processes in \cite{koch2015spatial}. %\textcolor{red}{In this study, we
generalised this risk measure to be applicable for several models: asymptotic
dependence represented by max-stable, asymptotic independence represented by
inverse max-stable and mixing between of them.} We evaluated the proposed risk
measure by a simulation study.
",0,0,1,1,0,0
8181,Experiment Segmentation in Scientific Discourse as Clause-level Structured Prediction using Recurrent Neural Networks,"  We propose a deep learning model for identifying structure within experiment
narratives in scientific literature. We take a sequence labeling approach to
this problem, and label clauses within experiment narratives to identify the
different parts of the experiment. Our dataset consists of paragraphs taken
from open access PubMed papers labeled with rhetorical information as a result
of our pilot annotation. Our model is a Recurrent Neural Network (RNN) with
Long Short-Term Memory (LSTM) cells that labels clauses. The clause
representations are computed by combining word representations using a novel
attention mechanism that involves a separate RNN. We compare this model against
LSTMs where the input layer has simple or no attention and a feature rich CRF
model. Furthermore, we describe how our work could be useful for information
extraction from scientific literature.
",1,0,0,0,0,0
17412,A general method for calculating lattice Green functions on the branch cut,"  We present a method for calculating the complex Green function $G_{ij}
(\omega)$ at any real frequency $\omega$ between any two sites $i$ and $j$ on a
lattice. Starting from numbers of walks on square, cubic, honeycomb,
triangular, bcc, fcc, and diamond lattices, we derive Chebyshev expansion
coefficients for $G_{ij} (\omega)$. The convergence of the Chebyshev series can
be accelerated by constructing functions $f(\omega)$ that mimic the van Hove
singularities in $G_{ij} (\omega)$ and subtracting their Chebyshev coefficients
from the original coefficients. We demonstrate this explicitly for the square
lattice and bcc lattice. Our algorithm achieves typical accuracies of 6--9
significant figures using 1000 series terms.
",0,1,0,0,0,0
19860,On the link between column density distribution and density scaling relation in star formation regions,"  We present a method to derive the density scaling relation $\langle n\rangle
\propto L^{-\alpha}$ in regions of star formation or in their turbulent
vicinities from straightforward binning of the column-density distribution
($N$-pdf). The outcome of the method is studied for three types of $N$-pdf:
power law ($7/5\le\alpha\le5/3$), lognormal ($0.7\lesssim\alpha\lesssim1.4$)
and combination of lognormals. In the last case, the method of Stanchev et al.
(2015) was also applied for comparison and a very weak (or close to zero)
correlation was found. We conclude that the considered `binning approach'
reflects rather the local morphology of the $N$-pdf with no reference to the
physical conditions in a considered region. The rough consistency of the
derived slopes with the widely adopted Larson's (1981) value $\alpha\sim1.1$ is
suggested to support claims that the density-size relation in molecular clouds
is indeed an artifact of the observed $N$-pdf.
",0,1,0,0,0,0
5349,Go with the Flow: Compositional Abstractions for Concurrent Data Structures (Extended Version),"  Concurrent separation logics have helped to significantly simplify
correctness proofs for concurrent data structures. However, a recurring problem
in such proofs is that data structure abstractions that work well in the
sequential setting are much harder to reason about in a concurrent setting due
to complex sharing and overlays. To solve this problem, we propose a novel
approach to abstracting regions in the heap by encoding the data structure
invariant into a local condition on each individual node. This condition may
depend on a quantity associated with the node that is computed as a fixpoint
over the entire heap graph. We refer to this quantity as a flow. Flows can
encode both structural properties of the heap (e.g. the reachable nodes from
the root form a tree) as well as data invariants (e.g. sortedness). We then
introduce the notion of a flow interface, which expresses the relies and
guarantees that a heap region imposes on its context to maintain the local flow
invariant with respect to the global heap. Our main technical result is that
this notion leads to a new semantic model of separation logic. In this model,
flow interfaces provide a general abstraction mechanism for describing complex
data structures. This abstraction mechanism admits proof rules that generalize
over a wide variety of data structures. To demonstrate the versatility of our
approach, we show how to extend the logic RGSep with flow interfaces. We have
used this new logic to prove linearizability and memory safety of nontrivial
concurrent data structures. In particular, we obtain parametric linearizability
proofs for concurrent dictionary algorithms that abstract from the details of
the underlying data structure representation. These proofs cannot be easily
expressed using the abstraction mechanisms provided by existing separation
logics.
",1,0,0,0,0,0
11210,On Convergence of Extended Dynamic Mode Decomposition to the Koopman Operator,"  Extended Dynamic Mode Decomposition (EDMD) is an algorithm that approximates
the action of the Koopman operator on an $N$-dimensional subspace of the space
of observables by sampling at $M$ points in the state space. Assuming that the
samples are drawn either independently or ergodically from some measure $\mu$,
it was shown that, in the limit as $M\rightarrow\infty$, the EDMD operator
$\mathcal{K}_{N,M}$ converges to $\mathcal{K}_N$, where $\mathcal{K}_N$ is the
$L_2(\mu)$-orthogonal projection of the action of the Koopman operator on the
finite-dimensional subspace of observables. In this work, we show that, as $N
\rightarrow \infty$, the operator $\mathcal{K}_N$ converges in the strong
operator topology to the Koopman operator. This in particular implies
convergence of the predictions of future values of a given observable over any
finite time horizon, a fact important for practical applications such as
forecasting, estimation and control. In addition, we show that accumulation
points of the spectra of $\mathcal{K}_N$ correspond to the eigenvalues of the
Koopman operator with the associated eigenfunctions converging weakly to an
eigenfunction of the Koopman operator, provided that the weak limit of
eigenfunctions is nonzero. As a by-product, we propose an analytic version of
the EDMD algorithm which, under some assumptions, allows one to construct
$\mathcal{K}_N$ directly, without the use of sampling. Finally, under
additional assumptions, we analyze convergence of $\mathcal{K}_{N,N}$ (i.e.,
$M=N$), proving convergence, along a subsequence, to weak eigenfunctions (or
eigendistributions) related to the eigenmeasures of the Perron-Frobenius
operator. No assumptions on the observables belonging to a finite-dimensional
invariant subspace of the Koopman operator are required throughout.
",0,0,1,0,0,0
6505,On the interpretability and computational reliability of frequency-domain Granger causality,"  This is a comment to the paper 'A study of problems encountered in Granger
causality analysis from a neuroscience perspective'. We agree that
interpretation issues of Granger Causality in Neuroscience exist (partially due
to the historical unfortunate use of the name 'causality', as nicely described
in previous literature). On the other hand we think that the paper uses a
formulation of Granger causality which is outdated (albeit still used), and in
doing so it dismisses the measure based on a suboptimal use of it. Furthermore,
since data from simulated systems are used, the pitfalls that are found with
the used formulation are intended to be general, and not limited to
neuroscience. It would be a pity if this paper, even written in good faith,
became a wildcard against all possible applications of Granger Causality,
regardless of the hard work of colleagues aiming to seriously address the
methodological and interpretation pitfalls. In order to provide a balanced
view, we replicated their simulations used the updated State Space
implementation, proposed already some years ago, in which the pitfalls are
mitigated or directly solved.
",0,0,1,1,0,0
13909,Coinfection in a stochastic model for bacteriophage systems,"  A system modeling bacteriophage treatments with coinfections in a noisy
context is analyzed. We prove that in a small noise regime, the system
converges in the long term to a bacteria free equilibrium. Moreover, we compare
the treatment with coinfection with the treatment without coinfection, showing
how the coinfection affects the dose of bacteriophages that is needed to
eliminate the bacteria and the velocity of convergence to the free bacteria
equilibrium.
",0,0,1,0,0,0
20090,Optimal control of diffuser shapes for confined turbulent shear flows,"  A model for the development of turbulent shear flows, created by non-uniform
parallel flows in a confining channel, is used to identify the diffuser shape
that maximises pressure recovery when the inflow is non-uniform. Wide diffuser
angles tend to accentuate the non- uniform flow, causing poor pressure
recovery. On the other hand, shallow diffuser angles create longer regions with
large wall drag, which is also detrimental to pressure recovery. Thus, optimal
diffuser shapes strike a balance between the two effects. We use a simple model
which describes the evolution of an approximate flow profile and pressure in
the diffuser. The model equations form the dynamics of an optimal control
problem where the control is the diffuser channel shape. A numerical
optimisation approach is used to solve the optimal control problem and we use
analytical results to interpret the numerics in some limiting cases. The
results of the optimisation are compared to calculations from computational
fluid dynamics.
",0,1,0,0,0,0
15656,Anomaly Detection via Minimum Likelihood Generative Adversarial Networks,"  Anomaly detection aims to detect abnormal events by a model of normality. It
plays an important role in many domains such as network intrusion detection,
criminal activity identity and so on. With the rapidly growing size of
accessible training data and high computation capacities, deep learning based
anomaly detection has become more and more popular. In this paper, a new
domain-based anomaly detection method based on generative adversarial networks
(GAN) is proposed. Minimum likelihood regularization is proposed to make the
generator produce more anomalies and prevent it from converging to normal data
distribution. Proper ensemble of anomaly scores is shown to improve the
stability of discriminator effectively. The proposed method has achieved
significant improvement than other anomaly detection methods on Cifar10 and UCI
datasets.
",0,0,0,1,0,0
11226,On the Ergodic Control of Ensembles,"  Across smart-grid and smart-city applications, there are problems where an
ensemble of agents is to be controlled such that both the aggregate behaviour
and individual-level perception of the system's performance are acceptable. In
many applications, traditional PI control is used to regulate aggregate
ensemble performance. Our principal contribution in this note is to demonstrate
that PI control may not be always suitable for this purpose, and in some
situations may lead to a loss of ergodicity for closed-loop systems. Building
on this observation, a theoretical framework is proposed to both analyse and
design control systems for the regulation of large scale ensembles of agents
with a probabilistic intent. Examples are given to illustrate our results.
",1,0,0,0,0,0
19216,Promoting Saving for College Through Data Science,"  The cost of attending college has been steadily rising and in 10 years is
estimated to reach $140,000 for a 4-year public university. Recent surveys
estimate just over half of US families are saving for college. State-operated
529 college savings plans are an effective way for families to plan and save
for future college costs, but only 3% of families currently use them. The
Office of the Illinois State Treasurer (Treasurer) administers two 529 plans to
help its residents save for college. In order to increase the number of
families saving for college, the Treasurer and Civis Analytics used data
science techniques to identify the people most likely to sign up for a college
savings plan. In this paper, we will discuss the use of person matching to join
accountholder data from the Treasurer to the Civis National File, as well as
the use of lookalike modeling to identify new potential signups. In order to
avoid reinforcing existing demographic imbalances in who saves for college, the
lookalike models used were ensured to be racially and economically balanced. We
will also discuss how these new signup targets were then individually served
digital ads to encourage opening college savings accounts.
",1,0,0,0,0,0
13481,The altmetric performance of publications authored by Brazilian researchers: analysis of CNPq productivity scholarship holders,"  The present work seeks to analyse the altmetric performance of Brazilian
publications authored by researchers who are productivity scholarship holders
(PQ) of the National Council of Scientific and Technological Development
(CNPq). It was considered, within the scope of this research, the PQs in
activity in October, 2017 (n = 14.609). The scientific production registered on
Lattes was collected via GetLattesData and filtered by articles from academic
journals published between 2016 and October 2017 that hold the Digital Object
Identifier (n = 99064). The online attention data are analysed according to
their distribution by density and variation; language of the publication and
field of knowledge; and by average performance of the type of source that has
provided its altmetric values. The density evidences the long tail behavior of
the variable, with most part of the articles with altmetrics score = 0, while
few articles have a high index. The average of the online attention indicates a
better performance of articles written in English and belonging to the Health
and Biological Sciences field of knowledge. As for the sources, there was a
good performance from Mendeley, followed by Twitter and a low coverage from
Facebook
",1,0,0,0,0,0
11025,Deep Neural Networks - A Brief History,"  Introduction to deep neural networks and their history.
",1,0,0,0,0,0
10753,Large covers and sharp resonances of hyperbolic surfaces,"  Let $\Gamma$ be a convex co-compact discrete group of isometries of the
hyperbolic plane $\mathbb{H}^2$, and $X=\Gamma\backslash \mathbb{H}^2$ the
associated surface. In this paper we investigate the behaviour of resonances of
the Laplacian for large degree covers of $X$ given by a finite index normal
subgroup of $\Gamma$. Using various techniques of thermodynamical formalism and
representation theory, we prove two new existence results of ""sharp non-trivial
resonances"" close to $\Re(s)=\delta_\Gamma$, both in the large degree limit,
for abelian covers and also infinite index congruence subgroups of
$SL2(\mathbb{Z})$.
",0,0,1,0,0,0
3311,Determinant structure for tau-function of holonomic deformation of linear differential equations,"  In our previous works, a relationship between Hermite's two approximation
problems and Schlesinger transformations of linear differential equations has
been clarified. In this paper, we study tau-functions associated with holonomic
deformations of linear differential equations by using Hermite's two
approximation problems. As a result, we present a determinant formula for the
ratio of tau-functions (tau-quotient).
",0,1,1,0,0,0
9137,Developing a machine learning framework for estimating soil moisture with VNIR hyperspectral data,"  In this paper, we investigate the potential of estimating the soil-moisture
content based on VNIR hyperspectral data combined with LWIR data. Measurements
from a multi-sensor field campaign represent the benchmark dataset which
contains measured hyperspectral, LWIR, and soil-moisture data conducted on
grassland site. We introduce a regression framework with three steps consisting
of feature selection, preprocessing, and well-chosen regression models. The
latter are mainly supervised machine learning models. An exception are the
self-organizing maps which combine unsupervised and supervised learning. We
analyze the impact of the distinct preprocessing methods on the regression
results. Of all regression models, the extremely randomized trees model without
preprocessing provides the best estimation performance. Our results reveal the
potential of the respective regression framework combined with the VNIR
hyperspectral data to estimate soil moisture measured under real-world
conditions. In conclusion, the results of this paper provide a basis for
further improvements in different research directions.
",0,0,0,1,0,0
20269,The Multivariate Hawkes Process in High Dimensions: Beyond Mutual Excitation,"  The Hawkes process is a class of point processes whose future depends on its
own history. Previous theoretical work on the Hawkes process is limited to the
case of a mutually-exciting process, in which a past event can only increase
the occurrence of future events. However, in neuronal networks and other
real-world applications, inhibitory relationships may be present. In this
paper, we develop a new approach for establishing the properties of the Hawkes
process without the restriction to mutual excitation. To this end, we employ a
thinning process representation and a coupling construction to bound the
dependence coefficient of the Hawkes process. Using recent developments on
weakly dependent sequences, we establish a concentration inequality for
second-order statistics of the Hawkes process. We apply this concentration
inequality in order to establish theoretical results for penalized regression
and clustering analysis in the high-dimensional regime. Our theoretical results
are corroborated by simulation studies and an application to a neuronal spike
train data set.
",0,0,0,1,0,0
10713,The Impact of Small-Cell Bandwidth Requirements on Strategic Operators,"  Small-cell deployment in licensed and unlicensed spectrum is considered to be
one of the key approaches to cope with the ongoing wireless data demand
explosion. Compared to traditional cellular base stations with large
transmission power, small-cells typically have relatively low transmission
power, which makes them attractive for some spectrum bands that have strict
power regulations, for example, the 3.5GHz band [1]. In this paper we consider
a heterogeneous wireless network consisting of one or more service providers
(SPs). Each SP operates in both macro-cells and small-cells, and provides
service to two types of users: mobile and fixed. Mobile users can only
associate with macro-cells whereas fixed users can connect to either macro- or
small-cells. The SP charges a price per unit rate for each type of service.
Each SP is given a fixed amount of bandwidth and splits it between macro- and
small-cells. Motivated by bandwidth regulations, such as those for the 3.5Gz
band, we assume a minimum amount of bandwidth has to be set aside for
small-cells. We study the optimal pricing and bandwidth allocation strategies
in both monopoly and competitive scenarios. In the monopoly scenario the
strategy is unique. In the competitive scenario there exists a unique Nash
equilibrium, which depends on the regulatory constraints. We also analyze the
social welfare achieved, and compare it to that without the small-cell
bandwidth constraints. Finally, we discuss implications of our results on the
effectiveness of the minimum bandwidth constraint on influencing small-cell
deployments.
",1,0,1,0,0,0
6389,Topic supervised non-negative matrix factorization,"  Topic models have been extensively used to organize and interpret the
contents of large, unstructured corpora of text documents. Although topic
models often perform well on traditional training vs. test set evaluations, it
is often the case that the results of a topic model do not align with human
interpretation. This interpretability fallacy is largely due to the
unsupervised nature of topic models, which prohibits any user guidance on the
results of a model. In this paper, we introduce a semi-supervised method called
topic supervised non-negative matrix factorization (TS-NMF) that enables the
user to provide labeled example documents to promote the discovery of more
meaningful semantic structure of a corpus. In this way, the results of TS-NMF
better match the intuition and desired labeling of the user. The core of TS-NMF
relies on solving a non-convex optimization problem for which we derive an
iterative algorithm that is shown to be monotonic and convergent to a local
optimum. We demonstrate the practical utility of TS-NMF on the Reuters and
PubMed corpora, and find that TS-NMF is especially useful for conceptual or
broad topics, where topic key terms are not well understood. Although
identifying an optimal latent structure for the data is not a primary objective
of the proposed approach, we find that TS-NMF achieves higher weighted Jaccard
similarity scores than the contemporary methods, (unsupervised) NMF and latent
Dirichlet allocation, at supervision rates as low as 10% to 20%.
",1,0,0,1,0,0
11861,Recurrent Environment Simulators,"  Models that can simulate how environments change in response to actions can
be used by agents to plan and act efficiently. We improve on previous
environment simulators from high-dimensional pixel observations by introducing
recurrent neural networks that are able to make temporally and spatially
coherent predictions for hundreds of time-steps into the future. We present an
in-depth analysis of the factors affecting performance, providing the most
extensive attempt to advance the understanding of the properties of these
models. We address the issue of computationally inefficiency with a model that
does not need to generate a high-dimensional image at each time-step. We show
that our approach can be used to improve exploration and is adaptable to many
diverse environments, namely 10 Atari games, a 3D car racing environment, and
complex 3D mazes.
",1,0,0,1,0,0
20857,Robust Wald-type test in GLM with random design based on minimum density power divergence estimators,"  We consider the problem of robust inference under the important generalized
linear model (GLM) with stochastic covariates. We derive the properties of the
minimum density power divergence estimator of the parameters in GLM with random
design and used this estimator to propose a robust Wald-type test for testing
any general composite null hypothesis about the GLM. The asymptotic and
robustness properties of the proposed test are also examined for the GLM with
random design. Application of the proposed robust inference procedures to the
popular Poisson regression model for analyzing count data is discussed in
detail both theoretically and numerically with some interesting real data
examples.
",0,0,0,1,0,0
17301,Deep-HiTS: Rotation Invariant Convolutional Neural Network for Transient Detection,"  We introduce Deep-HiTS, a rotation invariant convolutional neural network
(CNN) model for classifying images of transients candidates into artifacts or
real sources for the High cadence Transient Survey (HiTS). CNNs have the
advantage of learning the features automatically from the data while achieving
high performance. We compare our CNN model against a feature engineering
approach using random forests (RF). We show that our CNN significantly
outperforms the RF model reducing the error by almost half. Furthermore, for a
fixed number of approximately 2,000 allowed false transient candidates per
night we are able to reduce the miss-classified real transients by
approximately 1/5. To the best of our knowledge, this is the first time CNNs
have been used to detect astronomical transient events. Our approach will be
very useful when processing images from next generation instruments such as the
Large Synoptic Survey Telescope (LSST). We have made all our code and data
available to the community for the sake of allowing further developments and
comparisons at this https URL.
",1,1,0,0,0,0
5107,A character of Siegel modular group of level 2 from theta constants,"  Given a characteristic, we define a character of the Siegel modular group of
level 2, the computations of their values are also obtained. By using our
theorems, some key theorems of Igusa [1] can be recovered.
",0,0,1,0,0,0
8248,Interpretation of Neural Networks is Fragile,"  In order for machine learning to be deployed and trusted in many
applications, it is crucial to be able to reliably explain why the machine
learning algorithm makes certain predictions. For example, if an algorithm
classifies a given pathology image to be a malignant tumor, then the doctor may
need to know which parts of the image led the algorithm to this classification.
How to interpret black-box predictors is thus an important and active area of
research. A fundamental question is: how much can we trust the interpretation
itself? In this paper, we show that interpretation of deep learning predictions
is extremely fragile in the following sense: two perceptively indistinguishable
inputs with the same predicted label can be assigned very different
interpretations. We systematically characterize the fragility of several
widely-used feature-importance interpretation methods (saliency maps, relevance
propagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that
even small random perturbation can change the feature importance and new
systematic perturbations can lead to dramatically different interpretations
without changing the label. We extend these results to show that
interpretations based on exemplars (e.g. influence functions) are similarly
fragile. Our analysis of the geometry of the Hessian matrix gives insight on
why fragility could be a fundamental challenge to the current interpretation
approaches.
",1,0,0,1,0,0
3048,Efficient Nearest-Neighbor Search for Dynamical Systems with Nonholonomic Constraints,"  Nearest-neighbor search dominates the asymptotic complexity of sampling-based
motion planning algorithms and is often addressed with k-d tree data
structures. While it is generally believed that the expected complexity of
nearest-neighbor queries is $O(log(N))$ in the size of the tree, this paper
reveals that when a classic k-d tree approach is used with sub-Riemannian
metrics, the expected query complexity is in fact $\Theta(N^p \log(N))$ for a
number $p \in [0, 1)$ determined by the degree of nonholonomy of the system.
These metrics arise naturally in nonholonomic mechanical systems, including
classic wheeled robot models. To address this negative result, we propose novel
k-d tree build and query strategies tailored to sub-Riemannian metrics and
demonstrate significant improvements in the running time of nearest-neighbor
search queries.
",1,0,0,0,0,0
19482,On Quitting: Performance and Practice in Online Game Play,"  We study the relationship between performance and practice by analyzing the
activity of many players of a casual online game. We find significant
heterogeneity in the improvement of player performance, given by score, and
address this by dividing players into similar skill levels and segmenting each
player's activity into sessions, i.e., sequence of game rounds without an
extended break. After disaggregating data, we find that performance improves
with practice across all skill levels. More interestingly, players are more
likely to end their session after an especially large improvement, leading to a
peak score in their very last game of a session. In addition, success is
strongly correlated with a lower quitting rate when the score drops, and only
weakly correlated with skill, in line with psychological findings about the
value of persistence and ""grit"": successful players are those who persist in
their practice despite lower scores. Finally, we train an epsilon-machine, a
type of hidden Markov model, and find a plausible mechanism of game play that
can predict player performance and quitting the game. Our work raises the
possibility of real-time assessment and behavior prediction that can be used to
optimize human performance.
",1,0,0,0,0,0
12627,Quasi-Steady Model of a Pumping Kite Power System,"  The traction force of a kite can be used to drive a cyclic motion for
extracting wind energy from the atmosphere. This paper presents a novel
quasi-steady modelling framework for predicting the power generated over a full
pumping cycle. The cycle is divided into traction, retraction and transition
phases, each described by an individual set of analytic equations. The effect
of gravity on the airborne system components is included in the framework. A
trade-off is made between modelling accuracy and computation speed such that
the model is specifically useful for system optimisation and scaling in
economic feasibility studies. Simulation results are compared to experimental
measurements of a 20 kW kite power system operated up to a tether length of 720
m. Simulation and experiment agree reasonably well, both for moderate and for
strong wind conditions, indicating that the effect of gravity has to be taken
into account for a predictive performance simulation.
",1,0,1,0,0,0
7202,One-Shot Reinforcement Learning for Robot Navigation with Interactive Replay,"  Recently, model-free reinforcement learning algorithms have been shown to
solve challenging problems by learning from extensive interaction with the
environment. A significant issue with transferring this success to the robotics
domain is that interaction with the real world is costly, but training on
limited experience is prone to overfitting. We present a method for learning to
navigate, to a fixed goal and in a known environment, on a mobile robot. The
robot leverages an interactive world model built from a single traversal of the
environment, a pre-trained visual feature encoder, and stochastic environmental
augmentation, to demonstrate successful zero-shot transfer under real-world
environmental variations without fine-tuning.
",1,0,0,0,0,0
18263,Candidate exoplanet host HD131399A: a nascent Am star,"  Direct imaging suggests that there is a Jovian exoplanet around the primary
A-star in the triple-star system HD131399. We investigate a high-quality
spectrum of the primary component HD131399A obtained with FEROS on the ESO/MPG
2.2m telescope, aiming to characterise the star's atmospheric and fundamental
parameters, and to determine elemental abundances at high precision and
accuracy. The aim is to constrain the chemical composition of the birth cloud
of the system and therefore the bulk composition of the putative planet. A
hybrid non-local thermal equilibrium (non-LTE) model atmosphere technique is
adopted for the quantitative spectral analysis. Comparison with the most recent
stellar evolution models yields the fundamental parameters. The atmospheric and
fundamental stellar parameters of HD131399A are constrained to Teff=9200+-100
K, log g=4.37+-0.10, M=1.95+0.08-0.06 Msun, R=1.51+0.13-0.10 Rsun, and log
L/Lsun=1.17+-0.07, locating the star on the zero-age main sequence. Non-LTE
effects on the derived metal abundances are often smaller than 0.1dex, but can
reach up to ~0.8dex for individual lines. The observed lighter elements up to
calcium are overall consistent with present-day cosmic abundances, with a C/O
ratio of 0.45$\pm$0.07 by number, while the heavier elements show mild
overabundances. We conclude that the birth cloud of the system had a standard
chemical composition, but we witness the onset of the Am phenomenon in the
slowly rotating star. We furthermore show that non-LTE analyses have the
potential to solve the remaining discrepancies between observed abundances and
predictions by diffusion models for Am stars. Moreover, the present case allows
mass loss, not turbulent mixing, to be identified as the main transport process
competing with diffusion in very young Am stars.
",0,1,0,0,0,0
1944,Resonant particle production during inflation: a full analytical study,"  We revisit the study of the phenomenology associated to a burst of particle
production of a field whose mass is controlled by the inflaton field and
vanishes at one given instance during inflation. This generates a bump in the
correlators of the primordial scalar curvature. We provide a unified formalism
to compute various effects that have been obtained in the literature and
confirm that the dominant effects are due to the rescattering of the produced
particles on the inflaton condensate. We improve over existing results (based
on numerical fits) by providing exact analytic expressions for the shape and
height of the bump, both in the power spectrum and the equilateral bispectrum.
We then study the regime of validity of the perturbative computations of this
signature. Finally, we extend these computations to the case of a burst of
particle production in a sector coupled only gravitationally to the inflaton.
",0,1,0,0,0,0
7342,An Annotated Corpus of Relational Strategies in Customer Service,"  We create and release the first publicly available commercial customer
service corpus with annotated relational segments. Human-computer data from
three live customer service Intelligent Virtual Agents (IVAs) in the domains of
travel and telecommunications were collected, and reviewers marked all text
that was deemed unnecessary to the determination of user intention. After
merging the selections of multiple reviewers to create highlighted texts, a
second round of annotation was done to determine the classes of language
present in the highlighted sections such as the presence of Greetings,
Backstory, Justification, Gratitude, Rants, or Emotions. This resulting corpus
is a valuable resource for improving the quality and relational abilities of
IVAs. As well as discussing the corpus itself, we compare the usage of such
language in human-human interactions on TripAdvisor forums. We show that
removal of this language from task-based inputs has a positive effect on IVA
understanding by both an increase in confidence and improvement in responses,
demonstrating the need for automated methods of its discovery.
",1,0,0,0,0,0
14642,Optimizing deep video representation to match brain activity,"  The comparison of observed brain activity with the statistics generated by
artificial intelligence systems is useful to probe brain functional
organization under ecological conditions. Here we study fMRI activity in ten
subjects watching color natural movies and compute deep representations of
these movies with an architecture that relies on optical flow and image
content. The association of activity in visual areas with the different layers
of the deep architecture displays complexity-related contrasts across visual
areas and reveals a striking foveal/peripheral dichotomy.
",0,0,0,0,1,0
3920,Asymptotic Enumeration of Compacted Binary Trees,"  A compacted tree is a graph created from a binary tree such that repeatedly
occurring subtrees in the original tree are represented by pointers to existing
ones, and hence every subtree is unique. Such representations form a special
class of directed acyclic graphs. We are interested in the asymptotic number of
compacted trees of given size, where the size of a compacted tree is given by
the number of its internal nodes. Due to its superexponential growth this
problem poses many difficulties. Therefore we restrict our investigations to
compacted trees of bounded right height, which is the maximal number of edges
going to the right on any path from the root to a leaf.
We solve the asymptotic counting problem for this class as well as a closely
related, further simplified class.
For this purpose, we develop a calculus on exponential generating functions
for compacted trees of bounded right height and for relaxed trees of bounded
right height, which differ from compacted trees by dropping the above described
uniqueness condition. This enables us to derive a recursively defined sequence
of differential equations for the exponential generating functions. The
coefficients can then be determined by performing a singularity analysis of the
solutions of these differential equations.
Our main results are the computation of the asymptotic numbers of relaxed as
well as compacted trees of bounded right height and given size, when the size
tends to infinity.
",1,0,0,0,0,0
16472,Efficient implementations of the modified Gram-Schmidt orthogonalization with a non-standard inner product,"  The modified Gram-Schmidt (MGS) orthogonalization is one of the most
well-used algorithms for computing the thin QR factorization. MGS can be
straightforwardly extended to a non-standard inner product with respect to a
symmetric positive definite matrix $A$. For the thin QR factorization of an $m
\times n$ matrix with the non-standard inner product, a naive implementation of
MGS requires $2n$ matrix-vector multiplications (MV) with respect to $A$. In
this paper, we propose $n$-MV implementations: a high accuracy (HA) type and a
high performance (HP) type, of MGS. We also provide error bounds of the HA-type
implementation. Numerical experiments and analysis indicate that the proposed
implementations have competitive advantages over the naive implementation in
terms of both computational cost and accuracy.
",0,0,1,0,0,0
12230,The dependence of cluster galaxy properties on the central entropy of their host cluster,"  We present a study of the connection between brightest cluster galaxies
(BCGs) and their host galaxy clusters. Using galaxy clusters at $0.1<z<0.3$
from the Hectospec Cluster Survey (HeCS) with X-ray information from the
Archive of {\it Chandra} Cluster Entropy Profile Tables (ACCEPT), we confirm
that BCGs in low central entropy clusters are well aligned with the X-ray
center. Additionally, the magnitude difference between BCG and the 2nd
brightest one also correlates with the central entropy of the intracluster
medium. From the red-sequence (RS) galaxies, we cannot find significant
dependence of RS color scatter and stellar population on the central entropy of
the intracluster medium of their host cluster. However, BCGs in low entropy
clusters are systematically less massive than those in high entropy clusters,
although this is dependent on the method used to derive the stellar mass of
BCGs. In contrast, the stellar velocity dispersion of BCGs shows no dependence
on BCG activity and cluster central entropy. This implies that the potential of
the BCG is established earlier and the activity leading to optical emission
lines is dictated by the properties of the intracluster medium in the cluster
core.
",0,1,0,0,0,0
15256,Path-integral formalism for stochastic resetting: Exactly solved examples and shortcuts to confinement,"  We study the dynamics of overdamped Brownian particles diffusing in
conservative force fields and undergoing stochastic resetting to a given
location with a generic space-dependent rate of resetting. We present a
systematic approach involving path integrals and elements of renewal theory
that allows to derive analytical expressions for a variety of statistics of the
dynamics such as (i) the propagator prior to first reset; (ii) the distribution
of the first-reset time, and (iii) the spatial distribution of the particle at
long times. We apply our approach to several representative and hitherto
unexplored examples of resetting dynamics. A particularly interesting example
for which we find analytical expressions for the statistics of resetting is
that of a Brownian particle trapped in a harmonic potential with a rate of
resetting that depends on the instantaneous energy of the particle. We find
that using energy-dependent resetting processes is more effective in achieving
spatial confinement of Brownian particles on a faster timescale than by
performing quenches of parameters of the harmonic potential.
",0,1,0,0,0,0
2609,Towards a More Reliable Privacy-preserving Recommender System,"  This paper proposes a privacy-preserving distributed recommendation
framework, Secure Distributed Collaborative Filtering (SDCF), to preserve the
privacy of value, model and existence altogether. That says, not only the
ratings from the users to the items, but also the existence of the ratings as
well as the learned recommendation model are kept private in our framework. Our
solution relies on a distributed client-server architecture and a two-stage
Randomized Response algorithm, along with an implementation on the popular
recommendation model, Matrix Factorization (MF). We further prove SDCF to meet
the guarantee of Differential Privacy so that clients are allowed to specify
arbitrary privacy levels. Experiments conducted on numerical rating prediction
and one-class rating action prediction exhibit that SDCF does not sacrifice too
much accuracy for privacy.
",1,0,0,0,0,0
3324,Improved Point Source Detection in Crowded Fields using Probabilistic Cataloging,"  Cataloging is challenging in crowded fields because sources are extremely
covariant with their neighbors and blending makes even the number of sources
ambiguous. We present the first optical probabilistic catalog, cataloging a
crowded (~0.1 sources per pixel brighter than 22nd magnitude in F606W) Sloan
Digital Sky Survey r band image from M2. Probabilistic cataloging returns an
ensemble of catalogs inferred from the image and thus can capture source-source
covariance and deblending ambiguities. By comparing to a traditional catalog of
the same image and a Hubble Space Telescope catalog of the same region, we show
that our catalog ensemble better recovers sources from the image. It goes more
than a magnitude deeper than the traditional catalog while having a lower false
discovery rate brighter than 20th magnitude. We also present an algorithm for
reducing this catalog ensemble to a condensed catalog that is similar to a
traditional catalog, except it explicitly marginalizes over source-source
covariances and nuisance parameters. We show that this condensed catalog has a
similar completeness and false discovery rate to the catalog ensemble. Future
telescopes will be more sensitive, and thus more of their images will be
crowded. Probabilistic cataloging performs better than existing software in
crowded fields and so should be considered when creating photometric pipelines
in the Large Synoptic Space Telescope era.
",0,1,0,0,0,0
5550,A repulsive skyrmion chain as guiding track for a race track memory,"  A skyrmion racetrack design is proposed that allows for thermally stable
skyrmions to code information and dynamical pinning sites that move with the
applied current. This concept solves the problem of intrinsic distributions of
pinning times and pinning currents of skyrmions at static geometrical or
magnetic pinning sites. The dynamical pinning sites are realized by a skyrmion
carrying wire, where the skyrmion repulsion is used in order to keep the
skyrmions at equal distances. The information is coded by an additional layer
where the presence and absence of a skyrmion is used to code the information.
The lowest energy barrier for a data loss is calculated to be DE = 55 kBT300
which is sufficient for long time thermal stability.
",0,1,0,0,0,0
12136,Data-Augmented Contact Model for Rigid Body Simulation,"  Accurately modeling contact behaviors for real-world, near-rigid materials
remains a grand challenge for existing rigid-body physics simulators. This
paper introduces a data-augmented contact model that incorporates analytical
solutions with observed data to predict the 3D contact impulse which could
result in rigid bodies bouncing, sliding or spinning in all directions. Our
method enhances the expressiveness of the standard Coulomb contact model by
learning the contact behaviors from the observed data, while preserving the
fundamental contact constraints whenever possible. For example, a classifier is
trained to approximate the transitions between static and dynamic frictions,
while non-penetration constraint during collision is enforced analytically. Our
method computes the aggregated effect of contact for the entire rigid body,
instead of predicting the contact force for each contact point individually,
removing the exponential decline in accuracy as the number of contact points
increases.
",1,0,0,0,0,0
12199,Percent Change Estimation in Large Scale Online Experiments,"  Online experiments are a fundamental component of the development of
web-facing products. Given the large user-base, even small product improvements
can have a large impact on an absolute scale. As a result, accurately
estimating the relative impact of these changes is extremely important. I
propose an approach based on an objective Bayesian model to improve the
sensitivity of percent change estimation in A/B experiments. Leveraging
pre-period information, this approach produces more robust and accurate point
estimates and up to 50% tighter credible intervals than traditional methods.
The R package abpackage provides an implementation of the approach.
",0,0,0,1,0,0
6287,Deep Learning for Computational Chemistry,"  The rise and fall of artificial neural networks is well documented in the
scientific literature of both computer science and computational chemistry. Yet
almost two decades later, we are now seeing a resurgence of interest in deep
learning, a machine learning algorithm based on multilayer neural networks.
Within the last few years, we have seen the transformative impact of deep
learning in many domains, particularly in speech recognition and computer
vision, to the extent that the majority of expert practitioners in those field
are now regularly eschewing prior established models in favor of deep learning
models. In this review, we provide an introductory overview into the theory of
deep neural networks and their unique properties that distinguish them from
traditional machine learning algorithms used in cheminformatics. By providing
an overview of the variety of emerging applications of deep neural networks, we
highlight its ubiquity and broad applicability to a wide range of challenges in
the field, including QSAR, virtual screening, protein structure prediction,
quantum chemistry, materials design and property prediction. In reviewing the
performance of deep neural networks, we observed a consistent outperformance
against non-neural networks state-of-the-art models across disparate research
topics, and deep neural network based models often exceeded the ""glass ceiling""
expectations of their respective tasks. Coupled with the maturity of
GPU-accelerated computing for training deep neural networks and the exponential
growth of chemical data on which to train these networks on, we anticipate that
deep learning algorithms will be a valuable tool for computational chemistry.
",1,0,0,1,0,0
14629,The California-Kepler Survey. III. A Gap in the Radius Distribution of Small Planets,"  The size of a planet is an observable property directly connected to the
physics of its formation and evolution. We used precise radius measurements
from the California-Kepler Survey (CKS) to study the size distribution of 2025
$\textit{Kepler}$ planets in fine detail. We detect a factor of $\geq$2 deficit
in the occurrence rate distribution at 1.5-2.0 R$_{\oplus}$. This gap splits
the population of close-in ($P$ < 100 d) small planets into two size regimes:
R$_P$ < 1.5 R$_{\oplus}$ and R$_P$ = 2.0-3.0 R$_{\oplus}$, with few planets in
between. Planets in these two regimes have nearly the same intrinsic frequency
based on occurrence measurements that account for planet detection
efficiencies. The paucity of planets between 1.5 and 2.0 R$_{\oplus}$ supports
the emerging picture that close-in planets smaller than Neptune are composed of
rocky cores measuring 1.5 R$_{\oplus}$ or smaller with varying amounts of
low-density gas that determine their total sizes.
",0,1,0,0,0,0
5000,Existence of closed geodesics through a regular point on translation surfaces,"  We show that on any translation surface, if a regular point is contained in a
simple closed geodesic, then it is contained in infinitely many simple closed
geodesics, whose directions are dense in the unit circle. Moreover, the set of
points that are not contained in any simple closed geodesic is finite. We also
construct explicit examples showing that such points exist. For a surface in
any hyperelliptic component, we show that this finite exceptional set is
actually empty. The proofs of our results use Apisa's classifications of
periodic points and of $\GL(2,\R)$ orbit closures in hyperelliptic components,
as well as a recent result of Eskin-Filip-Wright.
",0,0,1,0,0,0
16583,Online Structure Learning for Sum-Product Networks with Gaussian Leaves,"  Sum-product networks have recently emerged as an attractive representation
due to their dual view as a special type of deep neural network with clear
semantics and a special type of probabilistic graphical model for which
inference is always tractable. Those properties follow from some conditions
(i.e., completeness and decomposability) that must be respected by the
structure of the network. As a result, it is not easy to specify a valid
sum-product network by hand and therefore structure learning techniques are
typically used in practice. This paper describes the first online structure
learning technique for continuous SPNs with Gaussian leaves. We also introduce
an accompanying new parameter learning technique.
",1,0,0,1,0,0
9799,Effects of Arrival Type and Degree of Saturation on Queue Length Estimation at Signalized Intersections,"  Purpose of this study is evaluation of the relationship between different
arrival types and degree of saturation (X) with overestimations of HCM 2010
procedure for estimating the back of queue within a study area. Further
analysis is performed to establish the relationship between queue length and
delay and also between each of them individually and X in cases with
overestimation. The analyses are based on the 50th percentile queue lengths for
data collected at four signalized intersections along a corridor in 4 time
periods (off peak period and AM, Noon and PM peak periods). Based on the
statistical test results, arrival type did not play a role in overestimations.
However, there is a significant relationship between the overestimations on
minor and major street and different ranges of X. On minor streets, about 59%
of the overestimations are at X values less than half; while near 23% of the
overestimations are at oversaturation condition with X values greater than 1.
The relationship between amount of overestimations and degree of saturation
should be established based on the numerical amount of overestimations versus X
values rather than the relative amounts; since the statistical comparison
between the relative amount of overestimations and X values, resulted in a
wrong idea of the real world condition. There was a significant correlation
between field queue and delay data of the cases with overestimated queue length
in all cases on major and minor streets. Also, field queue is correlated to X,
in all cases on minor and major streets.
",0,0,0,1,0,0
7110,Multiplicity of solutions for a nonhomogeneous quasilinear elliptic problem with critical growth,"  It is established some existence and multiplicity of solution results for a
quasilinear elliptic problem driven by $\Phi$-Laplacian operator. One of these
solutions is built as a ground state solution. In order to prove our main
results we apply the Nehari method combined with the concentration compactness
theorem in an Orlicz-Sobolev framework. One of the difficulties in dealing with
this kind of operator is the lost of homogeneity properties.
",0,0,1,0,0,0
19081,Demonstration of an ac Josephson junction laser,"  Superconducting electronic devices have re-emerged as contenders for both
classical and quantum computing due to their fast operation speeds, low
dissipation and long coherence times. An ultimate demonstration of coherence is
lasing. We use one of the fundamental aspects of superconductivity, the ac
Josephson effect, to demonstrate a laser made from a Josephson junction
strongly coupled to a multi-mode superconducting cavity. A dc voltage bias to
the junction provides a source of microwave photons, while the circuit's
nonlinearity allows for efficient down-conversion of higher order Josephson
frequencies down to the cavity's fundamental mode. The simple fabrication and
operation allows for easy integration with a range of quantum devices, allowing
for efficient on-chip generation of coherent microwave photons at low
temperatures.
",0,1,0,0,0,0
222,"SPIRou Input Catalog: Activity, Rotation and Magnetic Field of Cool Dwarfs","  Based on optical high-resolution spectra obtained with CFHT/ESPaDOnS, we
present new measurements of activity and magnetic field proxies of 442 low-mass
K5-M7 dwarfs. The objects were analysed as potential targets to search for
planetary-mass companions with the new spectropolarimeter and high-precision
velocimeter, SPIRou. We have analysed their high-resolution spectra in an
homogeneous way: circular polarisation, chromospheric features, and Zeeman
broadening of the FeH infrared line. The complex relationship between these
activity indicators is analysed: while no strong connection is found between
the large-scale and small-scale magnetic fields, the latter relates with the
non-thermal flux originating in the chromosphere.
We then examine the relationship between various activity diagnostics and the
optical radial-velocity jitter available in the literature, especially for
planet host stars. We use this to derive for all stars an activity merit
function (higher for quieter stars) with the goal of identifying the most
favorable stars where the radial-velocity jitter is low enough for planet
searches. We find that the main contributors to the RV jitter are the
large-scale magnetic field and the chromospheric non-thermal emission.
In addition, three stars (GJ 1289, GJ 793, and GJ 251) have been followed
along their rotation using the spectropolarimetric mode, and we derive their
magnetic topology. These very slow rotators are good representatives of future
SPIRou targets. They are compared to other stars where the magnetic topology is
also known. The poloidal component of the magnetic field is predominent in all
three stars.
",0,1,0,0,0,0
3422,"Toric Codes, Multiplicative Structure and Decoding","  Long linear codes constructed from toric varieties over finite fields, their
multiplicative structure and decoding. The main theme is the inherent
multiplicative structure on toric codes. The multiplicative structure allows
for \emph{decoding}, resembling the decoding of Reed-Solomon codes and aligns
with decoding by error correcting pairs. We have used the multiplicative
structure on toric codes to construct linear secret sharing schemes with
\emph{strong multiplication} via Massey's construction generalizing the Shamir
Linear secret sharing shemes constructed from Reed-Solomon codes. We have
constructed quantum error correcting codes from toric surfaces by the
Calderbank-Shor-Steane method.
",1,0,1,0,0,0
3278,"Random characters under the $L$-measure, I : Dirichlet characters","  We define the $L$-measure on the set of Dirichlet characters as an analogue
of the Plancherel measure, once considered as a measure on the irreducible
characters of the symmetric group.
We compare the two measures and study the limit in distribution of characters
evaluations when the size of the underlying group grows. These evaluations are
proven to converge in law to imaginary exponentials of a Cauchy distribution in
the same way as the rescaled windings of the complex Brownian motion. This
contrasts with the case of the symmetric group where the renormalised
characters converge in law to Gaussians after rescaling (Kerov Central Limit
Theorem).
",0,0,1,0,0,0
17784,On Local Optimizers of Acquisition Functions in Bayesian Optimization,"  Bayesian optimization is a sample-efficient method for finding a global
optimum of an expensive-to-evaluate black-box function. A global solution is
found by accumulating a pair of query point and corresponding function value,
repeating these two procedures: (i) learning a surrogate model for the
objective function using the data observed so far; (ii) the maximization of an
acquisition function to determine where next to query the objective function.
Convergence guarantees are only valid when the global optimizer of the
acquisition function is found and selected as the next query point. In
practice, however, local optimizers of acquisition functions are also used,
since searching the exact optimizer of the acquisition function is often a
non-trivial or time-consuming task. In this paper we present an analysis on the
behavior of local optimizers of acquisition functions, in terms of
instantaneous regrets over global optimizers. We also present the performance
analysis when multi-started local optimizers are used to find the maximum of
the acquisition function. Numerical experiments confirm the validity of our
theoretical analysis.
",1,0,0,1,0,0
577,Far-field theory for trajectories of magnetic ellipsoids in rectangular and circular channels,"  We report a method to control the positions of ellipsoidal magnets in flowing
channels of rectangular or circular cross section at low Reynolds number.A
static uniform magnetic field is used to pin the particle orientation, and the
particles move with translational drift velocities resulting from hydrodynamic
interactions with the channel walls which can be described using Blake's image
tensor.Building on his insights, we are able to present a far-field theory
predicting the particle motion in rectangular channels, and validate the
accuracy of the theory by comparing to numerical solutions using the boundary
element method.We find that, by changing the direction of the applied magnetic
field, the motion can be controlled so that particles move either to a curved
focusing region or to the channel walls.We also use simulations to show that
the particles are focused to a single line in a circular channel.Our results
suggest ways to focus and segregate magnetic particles in lab-on-a-chip
devices.
",0,1,0,0,0,0
7218,Digital Advertising Traffic Operation: Flow Management Analysis,"  In a Web Advertising Traffic Operation the Trafficking Routing Problem (TRP)
consists in scheduling the management of Web Advertising (Adv) campaign between
Trafficking campaigns in the most efficient way to oversee and manage
relationship with partners and internal teams, managing expectations through
integration and post-launch in order to ensure success for every stakeholders
involved. For our own interest we did that independent research projects also
through specific innovative tasks validate towards average working time
declared on ""specification required"" by the main worldwide industry leading
Advertising Agency. We present a Mixed Integer Linear Programming (MILP)
formulation for end-to-end management of campaign workflow along a
predetermined path and generalize it to include alternative path to oversee and
manage detail-oriented relationship with partners and internal teams to achieve
the goals above mentioned. To meet clients' KPIs, we consider an objective
function that includes the punctuality indicators (the average waiting time and
completion times) but also the main punctuality indicators (the average delay
and the on time performance). Then we investigate their analytical
relationships in the advertising domain through experiments based on real data
from a Traffic Office. We show that the classic punctuality indicators are in
contradiction with the task of reducing waiting times. We propose new
indicators used for a synthesize analysis on projects or process changes for
the wider team that are more sustainable, but also more relevant for
stakeholders. We also show that the flow of a campaign (adv-ways) is the main
bottleneck of a Traffic Office and that alternate paths cannot improve the
performance indicators.
",1,0,1,0,0,0
10915,Millimeter-scale layered MoSe2 grown on sapphire and evidence for negative magnetoresistance,"  Molecular beam epitaxy technique has been used to deposit a single layer and
a bilayer of MoSe 2 on sapphire. Extensive characterizations including in-situ
and ex-situ measurements show that the layered MoSe 2 grows in a scalable
manner on the substrate and reveals characteristics of a stoichiometric
2H-phase. The layered MoSe 2 exhibits polycrystalline features with domains
separated by defects and boundaries. Temperature and magnetic field dependent
resistivity measurements unveil a carrier hopping character described within
two-dimensional variable range hopping mechanism. Moreover, a negative
magnetoresistance was observed, stressing a fascinating feature of the charge
transport under the application of a magnetic field in the layered MoSe 2
system. This negative magnetoresistance observed at millimeter-scale is similar
to that observed recently at room temperature inWS2 flakes at a micrometer
scale [Zhang et al., Appl. Phys. Lett. 108, 153114 (2016)]. This scalability
highlights the fact that the underlying physical mechanism is intrinsic to
these two-dimensional materials and occurs at very short scale.
",0,1,0,0,0,0
10920,Collective Dynamics of Self-propelled Semiflexible Filaments,"  The collective behavior of active semiflexible filaments is studied with a
model of tangentially driven self-propelled worm-like chains. The combination
of excluded-volume interactions and self-propulsion leads to several distinct
dynamic phases as a function of bending rigidity, activity, and aspect ratio of
individual filaments. We consider first the case of intermediate filament
density. For high-aspect-ratio filaments, we identify a transition with
increasing propulsion from a state of free-swimming filaments to a state of
spiraled filaments with nearly frozen translational motion. For lower aspect
ratios, this gas-of-spirals phase is suppressed with growing density due to
filament collisions; instead, filaments form clusters similar to self-propelled
rods, as activity increases. Finite bending rigidity strongly effects the
dynamics and phase behavior. Flexible filaments form small and transient
clusters, while stiffer filaments organize into giant clusters, similarly as
self-propelled rods, but with a reentrant phase behavior from giant to smaller
clusters as activity becomes large enough to bend the filaments. For high
filament densities, we identify a nearly frozen jamming state at low
activities, a nematic laning state at intermediate activities, and an
active-turbulence state at high activities. The latter state is characterized
by a power-law decay of the energy spectrum as a function of wave number. The
resulting phase diagrams encapsulate tunable non-equilibrium steady states that
can be used in the organization of living matter.
",0,0,0,0,1,0
14810,Uniform cohomological expansion of uniformly quasiregular mappings,"  Let $f\colon M \to M$ be a uniformly quasiregular self-mapping of a compact,
connected, and oriented Riemannian $n$-manifold $M$ without boundary, $n\ge 2$.
We show that, for $k \in \{0,\ldots, n\}$, the induced homomorphism $f^* \colon
H^k(M;\mathbb{R}) \to H^k(M;\mathbb{R})$, where $H^k(M;\mathbb{R})$ is the
$k$:th singular cohomology of $M$, is complex diagonalizable and the
eigenvalues of $f^*$ have modulus $(\mathrm{deg}\ f)^{k/n}$. As an application,
we obtain a degree restriction for uniformly quasiregular self-mappings of
closed manifolds. In the proof of the main theorem, we use a Sobolev--de Rham
cohomology based on conformally invariant differential forms and an induced
push-forward operator.
",0,0,1,0,0,0
15560,Program Language Translation Using a Grammar-Driven Tree-to-Tree Model,"  The task of translating between programming languages differs from the
challenge of translating natural languages in that programming languages are
designed with a far more rigid set of structural and grammatical rules.
Previous work has used a tree-to-tree encoder/decoder model to take advantage
of the inherent tree structure of programs during translation. Neural decoders,
however, by default do not exploit known grammar rules of the target language.
In this paper, we describe a tree decoder that leverages knowledge of a
language's grammar rules to exclusively generate syntactically correct
programs. We find that this grammar-based tree-to-tree model outperforms the
state of the art tree-to-tree model in translating between two programming
languages on a previously used synthetic task.
",1,0,0,1,0,0
7815,Quadratic twists of abelian varieties and disparity in Selmer ranks,"  We study the parity of 2-Selmer ranks in the family of quadratic twists of a
fixed principally polarised abelian variety over a number field. Specifically,
we determine the proportion of twists having odd (resp. even) 2-Selmer rank.
This generalises work of Klagsbrun--Mazur--Rubin for elliptic curves and Yu for
Jacobians of hyperelliptic curves. Several differences in the statistics arise
due to the possibility that the Shafarevich--Tate group (if finite) may have
order twice a square. In particular, the statistics for parities of 2-Selmer
ranks and 2-infinity Selmer ranks need no longer agree and we describe both.
",0,0,1,0,0,0
500,Approximately certifying the restricted isometry property is hard,"  A matrix is said to possess the Restricted Isometry Property (RIP) if it acts
as an approximate isometry when restricted to sparse vectors. Previous work has
shown it to be NP-hard to determine whether a matrix possess this property, but
only in a narrow range of parameters. In this work, we show that it is NP-hard
to make this determination for any accuracy parameter, even when we restrict
ourselves to instances which are either RIP or far from being RIP. This result
implies that it is NP-hard to approximate the range of parameters for which a
matrix possesses the Restricted Isometry Property with accuracy better than
some constant. Ours is the first work to prove such a claim without any
additional assumptions.
",1,0,0,0,0,0
19168,GSAE: an autoencoder with embedded gene-set nodes for genomics functional characterization,"  Bioinformatics tools have been developed to interpret gene expression data at
the gene set level, and these gene set based analyses improve the biologists'
capability to discover functional relevance of their experiment design. While
elucidating gene set individually, inter gene sets association is rarely taken
into consideration. Deep learning, an emerging machine learning technique in
computational biology, can be used to generate an unbiased combination of gene
set, and to determine the biological relevance and analysis consistency of
these combining gene sets by leveraging large genomic data sets. In this study,
we proposed a gene superset autoencoder (GSAE), a multi-layer autoencoder model
with the incorporation of a priori defined gene sets that retain the crucial
biological features in the latent layer. We introduced the concept of the gene
superset, an unbiased combination of gene sets with weights trained by the
autoencoder, where each node in the latent layer is a superset. Trained with
genomic data from TCGA and evaluated with their accompanying clinical
parameters, we showed gene supersets' ability of discriminating tumor subtypes
and their prognostic capability. We further demonstrated the biological
relevance of the top component gene sets in the significant supersets. Using
autoencoder model and gene superset at its latent layer, we demonstrated that
gene supersets retain sufficient biological information with respect to tumor
subtypes and clinical prognostic significance. Superset also provides high
reproducibility on survival analysis and accurate prediction for cancer
subtypes.
",0,0,0,1,1,0
8674,Towards equation of state for a market: A thermodynamical paradigm of economics,"  Foundations of equilibrium thermodynamics are the equation of state (EoS) and
four postulated laws of thermodynamics. We use equilibrium thermodynamics
paradigms in constructing the EoS for microeconomics system that is a market.
This speculation is hoped to be first step towards whole pictures of
thermodynamical paradigm of economics.
",0,0,0,0,0,1
16032,A Note on Cyclotomic Integers,"  In this note, we present a new proof that the cyclotomic integers constitute
the full ring of integers in the cyclotomic field.
",0,0,1,0,0,0
14403,Smart Mining for Deep Metric Learning,"  To solve deep metric learning problems and producing feature embeddings,
current methodologies will commonly use a triplet model to minimise the
relative distance between samples from the same class and maximise the relative
distance between samples from different classes. Though successful, the
training convergence of this triplet model can be compromised by the fact that
the vast majority of the training samples will produce gradients with
magnitudes that are close to zero. This issue has motivated the development of
methods that explore the global structure of the embedding and other methods
that explore hard negative/positive mining. The effectiveness of such mining
methods is often associated with intractable computational requirements. In
this paper, we propose a novel deep metric learning method that combines the
triplet model and the global structure of the embedding space. We rely on a
smart mining procedure that produces effective training samples for a low
computational cost. In addition, we propose an adaptive controller that
automatically adjusts the smart mining hyper-parameters and speeds up the
convergence of the training process. We show empirically that our proposed
method allows for fast and more accurate training of triplet ConvNets than
other competing mining methods. Additionally, we show that our method achieves
new state-of-the-art embedding results for CUB-200-2011 and Cars196 datasets.
",1,0,0,0,0,0
10756,A Dynamic Model for Traffic Flow Prediction Using Improved DRN,"  Real-time traffic flow prediction can not only provide travelers with
reliable traffic information so that it can save people's time, but also assist
the traffic management agency to manage traffic system. It can greatly improve
the efficiency of the transportation system. Traditional traffic flow
prediction approaches usually need a large amount of data but still give poor
performances. With the development of deep learning, researchers begin to pay
attention to artificial neural networks (ANNs) such as RNN and LSTM. However,
these ANNs are very time-consuming. In our research, we improve the Deep
Residual Network and build a dynamic model which previous researchers hardly
use. We firstly integrate the input and output of the $i^{th}$ layer to the
input of the $i+1^{th}$ layer and prove that each layer will fit a simpler
function so that the error rate will be much smaller. Then, we use the concept
of online learning in our model to update pre-trained model during prediction.
Our result shows that our model has higher accuracy than some state-of-the-art
models. In addition, our dynamic model can perform better in practical
applications.
",0,0,0,1,0,0
14495,"Potential kernel, hitting probabilities and distributional asymptotics","  Z^d-extensions of probability-preserving dynamical systems are themselves
dynamical systems preserving an infinite measure, and generalize random walks.
Using the method of moments, we prove a generalized central limit theorem for
additive functionals of the extension of integral zero, under spectral
assumptions. As a corollary, we get the fact that Green-Kubo's formula is
invariant under induction. This allows us to relate the hitting probability of
sites with the symmetrized potential kernel, giving an alternative proof and
generalizing a theorem of Spitzer. Finally, this relation is used to improve in
turn the asumptions of the generalized central limit theorem. Applications to
Lorentz gases in finite horizon and to the geodesic flow on abelian covers of
compact manifolds of negative curvature are discussed.
",0,0,1,0,0,0
3172,Self-adjointness and spectral properties of Dirac operators with magnetic links,"  We define Dirac operators on $\mathbb{S}^3$ (and $\mathbb{R}^3$) with
magnetic fields supported on smooth, oriented links and prove self-adjointness
of certain (natural) extensions. We then analyze their spectral properties and
show, among other things, that these operators have discrete spectrum. Certain
examples, such as circles in $\mathbb{S}^3$, are investigated in detail and we
compute the dimension of the zero-energy eigenspace.
",0,0,1,0,0,0
6106,Newton slopes for twisted Artin--Schreier--Witt Towers,"  We fix a monic polynomial $f(x) \in \mathbb F_q[x]$ over a finite field of
characteristic $p$ of degree relatively prime to $p$. Let $a\mapsto \omega(a)$
be the Teichmüller lift of $\mathbb F_q$, and let $\chi:\mathbb{Z}\to \mathbb
C_p^\times$ be a finite character of $\mathbb Z_p$. The $L$-function associated
to the polynomial $f$ and the so-called twisted character $\omega^u\times \chi$
is denoted by $L_f(\omega^u,\chi,s)$. We prove that, when the conductor of the
character is large enough, the $p$-adic Newton slopes of this $L$-function form
arithmetic progressions.
",0,0,1,0,0,0
14324,Agent Failures in All-Pay Auctions,"  All-pay auctions, a common mechanism for various human and agent
interactions, suffers, like many other mechanisms, from the possibility of
players' failure to participate in the auction. We model such failures, and
fully characterize equilibrium for this class of games, we present a symmetric
equilibrium and show that under some conditions the equilibrium is unique. We
reveal various properties of the equilibrium, such as the lack of influence of
the most-likely-to-participate player on the behavior of the other players. We
perform this analysis with two scenarios: the sum-profit model, where the
auctioneer obtains the sum of all submitted bids, and the max-profit model of
crowdsourcing contests, where the auctioneer can only use the best submissions
and thus obtains only the winning bid.
Furthermore, we examine various methods of influencing the probability of
participation such as the effects of misreporting one's own probability of
participating, and how influencing another player's participation chances
changes the player's strategy.
",1,0,0,0,0,0
15382,Profile of a coherent vortex in two-dimensional turbulence at static pumping,"  We examine the velocity profile of coherent vortices appearing as a
consequence of the inverse cascade of two-dimensional turbulence in a finite
box in the case of static pumping. We demonstrate that in the passive regime
the flat velocity profile is realized, as in the case of pumping short
correlated in time. However, in the static case the energy flux to large scales
is dependent on the system parameters. We demonstrate that it is proportional
to $f^{4/3}$ where $f$ is the characteristic force exciting turbulence.
",0,1,0,0,0,0
4978,Linearity of stability conditions,"  We study different concepts of stability for modules over a finite
dimensional algebra: linear stability, given by a ""central charge"", and
nonlinear stability given by the wall-crossing sequence of a ""green path"". Two
other concepts, finite Harder-Narasimhan stratification of the module category
and maximal forward hom-orthogonal sequences of Schurian modules, which are
always equivalent to each other, are shown to be equivalent to nonlinear
stability and to a maximal green sequence, defined using Fomin-Zelevinsky
quiver mutation, in the case the algebra is hereditary.
This is the first of a series of three papers whose purpose is to determine
all maximal green sequences of maximal length for quivers of affine type
$\tilde A$ and determine which are linear. The complete answer will be given in
the final paper [1].
",0,0,1,0,0,0
15448,Normalized Total Gradient: A New Measure for Multispectral Image Registration,"  Image registration is a fundamental issue in multispectral image processing.
In filter wheel based multispectral imaging systems, the non-coplanar placement
of the filters always causes the misalignment of multiple channel images. The
selective characteristic of spectral response in multispectral imaging raises
two challenges to image registration. First, the intensity levels of a local
region may be different in individual channel images. Second, the local
intensity may vary rapidly in some channel images while keeps stationary in
others. Conventional multimodal measures, such as mutual information,
correlation coefficient, and correlation ratio, can register images with
different regional intensity levels, but will fail in the circumstance of
severe local intensity variation. In this paper, a new measure, namely
normalized total gradient (NTG), is proposed for multispectral image
registration. The NTG is applied on the difference between two channel images.
This measure is based on the key assumption (observation) that the gradient of
difference image between two aligned channel images is sparser than that
between two misaligned ones. A registration framework, which incorporates image
pyramid and global/local optimization, is further introduced for rigid
transform. Experimental results validate that the proposed method is effective
for multispectral image registration and performs better than conventional
methods.
",1,0,0,0,0,0
16244,Efficient Covariance Approximations for Large Sparse Precision Matrices,"  The use of sparse precision (inverse covariance) matrices has become popular
because they allow for efficient algorithms for joint inference in
high-dimensional models. Many applications require the computation of certain
elements of the covariance matrix, such as the marginal variances, which may be
non-trivial to obtain when the dimension is large. This paper introduces a fast
Rao-Blackwellized Monte Carlo sampling based method for efficiently
approximating selected elements of the covariance matrix. The variance and
confidence bounds of the approximations can be precisely estimated without
additional computational costs. Furthermore, a method that iterates over
subdomains is introduced, and is shown to additionally reduce the approximation
errors to practically negligible levels in an application on functional
magnetic resonance imaging data. Both methods have low memory requirements,
which is typically the bottleneck for competing direct methods.
",0,0,0,1,0,0
3503,Allocation strategies for high fidelity models in the multifidelity regime,"  We propose a novel approach to allocating resources for expensive simulations
of high fidelity models when used in a multifidelity framework. Allocation
decisions that distribute computational resources across several simulation
models become extremely important in situations where only a small number of
expensive high fidelity simulations can be run. We identify this allocation
decision as a problem in optimal subset selection, and subsequently regularize
this problem so that solutions can be computed. Our regularized formulation
yields a type of group lasso problem that has been studied in the literature to
accomplish subset selection. Our numerical results compare performance of
algorithms that solve the group lasso problem for algorithmic allocation
against a variety of other strategies, including those based on classical
linear algebraic pivoting routines and those derived from more modern machine
learning-based methods. We demonstrate on well known synthetic problems and
more difficult real-world simulations that this group lasso solution to the
relaxed optimal subset selection problem performs better than the alternatives.
",1,0,0,0,0,0
18138,Bayesian uncertainty quantification for epidemic spread on networks,"  While there exist a number of mathematical approaches to modeling the spread
of disease on a network, analyzing such systems in the presence of uncertainty
introduces significant complexity. In scenarios where system parameters must be
inferred from limited observations, general approaches to uncertainty
quantification can generate approximate distributions of the unknown
parameters, but these methods often become computationally expensive if the
underlying disease model is complex. In this paper, we apply the recent
massively parallelizable Bayesian uncertainty quantification framework $\Pi4U$
to a model of a disease spreading on a network of communities, showing that the
method can accurately and tractably recover system parameters and select
optimal models in this setting.
",0,0,0,1,0,0
15343,Integrating self-efficacy into a gamified approach to thwart phishing attacks,"  Security exploits can include cyber threats such as computer programs that
can disturb the normal behavior of computer systems (viruses), unsolicited
e-mail (spam), malicious software (malware), monitoring software (spyware),
attempting to make computer resources unavailable to their intended users
(Distributed Denial-of-Service or DDoS attack), the social engineering, and
online identity theft (phishing). One such cyber threat, which is particularly
dangerous to computer users is phishing. Phishing is well known as online
identity theft, which targets to steal victims' sensitive information such as
username, password and online banking details. This paper focuses on designing
an innovative and gamified approach to educate individuals about phishing
attacks. The study asks how one can integrate self-efficacy, which has a
co-relation with the user's knowledge, into an anti-phishing educational game
to thwart phishing attacks? One of the main reasons would appear to be a lack
of user knowledge to prevent from phishing attacks. Therefore, this research
investigates the elements that influence (in this case, either conceptual or
procedural knowledge or their interaction effect) and then integrate them into
an anti-phishing educational game to enhance people's phishing prevention
behaviour through their motivation.
",1,0,0,0,0,0
14831,Avoiding Communication in Proximal Methods for Convex Optimization Problems,"  The fast iterative soft thresholding algorithm (FISTA) is used to solve
convex regularized optimization problems in machine learning. Distributed
implementations of the algorithm have become popular since they enable the
analysis of large datasets. However, existing formulations of FISTA communicate
data at every iteration which reduces its performance on modern distributed
architectures. The communication costs of FISTA, including bandwidth and
latency costs, is closely tied to the mathematical formulation of the
algorithm. This work reformulates FISTA to communicate data at every k
iterations and reduce data communication when operating on large data sets. We
formulate the algorithm for two different optimization methods on the Lasso
problem and show that the latency cost is reduced by a factor of k while
bandwidth and floating-point operation costs remain the same. The convergence
rates and stability properties of the reformulated algorithms are similar to
the standard formulations. The performance of communication-avoiding FISTA and
Proximal Newton methods is evaluated on 1 to 1024 nodes for multiple benchmarks
and demonstrate average speedups of 3-10x with scaling properties that
outperform the classical algorithms.
",1,0,0,0,0,0
7622,Long-range p-d exchange interaction in a ferromagnet-semiconductor Co/CdMgTe/CdTe quantum well hybrid structure,"  The exchange interaction between magnetic ions and charge carriers in
semiconductors is considered as prime tool for spin control. Here, we solve a
long-standing problem by uniquely determining the magnitude of the long-range
$p-d$ exchange interaction in a ferromagnet-semiconductor (FM-SC) hybrid
structure where a 10~nm thick CdTe quantum well is separated from the FM Co
layer by a CdMgTe barrier with a thickness on the order of 10~nm. The exchange
interaction is manifested by the spin splitting of acceptor bound holes in the
effective magnetic field induced by the FM. The exchange splitting is directly
evaluated using spin-flip Raman scattering by analyzing the dependence of the
Stokes shift $\Delta_S$ on the external magnetic field $B$. We show that in
strong magnetic field $\Delta_S$ is a linear function of $B$ with an offset of
$\Delta_{pd} = 50-100~\mu$eV at zero field from the FM induced effective
exchange field. On the other hand, the $s-d$ exchange interaction between
conduction band electrons and FM, as well as the $p-d$ contribution for free
valence band holes, are negligible. The results are well described by the model
of indirect exchange interaction between acceptor bound holes in the CdTe
quantum well and the FM layer mediated by elliptically polarized phonons in the
hybrid structure.
",0,1,0,0,0,0
6133,Updating the silent speech challenge benchmark with deep learning,"  The 2010 Silent Speech Challenge benchmark is updated with new results
obtained in a Deep Learning strategy, using the same input features and
decoding strategy as in the original article. A Word Error Rate of 6.4% is
obtained, compared to the published value of 17.4%. Additional results
comparing new auto-encoder-based features with the original features at reduced
dimensionality, as well as decoding scenarios on two different language models,
are also presented. The Silent Speech Challenge archive has been updated to
contain both the original and the new auto-encoder features, in addition to the
original raw data.
",1,0,0,0,0,0
20334,Gap structure of FeSe determined by field-angle-resolved specific heat measurements,"  Quasiparticle excitations in FeSe were studied by means of specific heat
($C$) measurements on a high-quality single crystal under rotating magnetic
fields. The field dependence of $C$ shows three-stage behavior with different
slopes, indicating the existence of three gaps ($\Delta_1$, $\Delta_2$, and
$\Delta_3$). In the low-temperature and low-field region, the azimuthal-angle
($\phi$) dependence of $C$ shows a four-fold symmetric oscillation with sign
change. On the other hand, the polar-angle ($\theta$) dependence manifests as
an anisotropy-inverted two-fold symmetry with unusual shoulder behavior.
Combining the angle-resolved results and the theoretical calculation, the
smaller gap $\Delta_1$ is proved to have two vertical-line nodes or gap minima
along the $k_z$ direction, and is determined to reside on the electron-type
$\varepsilon$ band. $\Delta_2$ is found to be related to the electron-type
$\delta$ band, and is isotropic in the $ab$-plane but largely anisotropic out
of the plane. $\Delta_3$ residing on the hole-type $\alpha$ band shows a small
out-of-plane anisotropy with a strong Pauli-paramagnetic effect.
",0,1,0,0,0,0
12121,Inference for Multiple Change-points in Linear and Non-linear Time Series Models,"  In this paper we develop a generalized likelihood ratio scan method (GLRSM)
for multiple change-points inference in piecewise stationary time series, which
estimates the number and positions of change-points and provides a confidence
interval for each change-point. The computational complexity of using GLRSM for
multiple change-points detection is as low as $O(n(\log n)^3)$ for a series of
length $n$. Consistency of the estimated numbers and positions of the
change-points is established. Extensive simulation studies are provided to
demonstrate the effectiveness of the proposed methodology under different
scenarios.
",0,0,1,1,0,0
13759,Constant-Time Predictive Distributions for Gaussian Processes,"  One of the most compelling features of Gaussian process (GP) regression is
its ability to provide well-calibrated posterior distributions. Recent advances
in inducing point methods have sped up GP marginal likelihood and posterior
mean computations, leaving posterior covariance estimation and sampling as the
remaining computational bottlenecks. In this paper we address these
shortcomings by using the Lanczos algorithm to rapidly approximate the
predictive covariance matrix. Our approach, which we refer to as LOVE (LanczOs
Variance Estimates), substantially improves time and space complexity. In our
experiments, LOVE computes covariances up to 2,000 times faster and draws
samples 18,000 times faster than existing methods, all without sacrificing
accuracy.
",0,0,0,1,0,0
4167,Validation of the 3-under-2 principle of cell wall growth in Gram-positive bacteria by simulation of a simple coarse-grained model,"  The aim of this work is to propose a first coarse-grained model of Bacillus
subtilis cell wall, handling explicitly the existence of multiple layers of
peptidoglycans. In this first work, we aim at the validation of the recently
proposed ""three under two"" principle.
",0,1,0,0,0,0
19746,Synchronization of Kuramoto oscillators in a bidirectional frequency-dependent tree network,"  This paper studies the synchronization of a finite number of Kuramoto
oscillators in a frequency-dependent bidirectional tree network. We assume that
the coupling strength of each link in each direction is equal to the product of
a common coefficient and the exogenous frequency of its corresponding head
oscillator. We derive a sufficient condition for the common coupling strength
in order to guarantee frequency synchronization in tree networks. Moreover, we
discuss the dependency of the obtained bound on both the graph structure and
the way that exogenous frequencies are distributed. Further, we present an
application of the obtained result by means of an event-triggered algorithm for
achieving frequency synchronization in a star network assuming that the common
coupling coefficient is given.
",1,0,0,0,0,0
467,Non-Asymptotic Analysis of Fractional Langevin Monte Carlo for Non-Convex Optimization,"  Recent studies on diffusion-based sampling methods have shown that Langevin
Monte Carlo (LMC) algorithms can be beneficial for non-convex optimization, and
rigorous theoretical guarantees have been proven for both asymptotic and
finite-time regimes. Algorithmically, LMC-based algorithms resemble the
well-known gradient descent (GD) algorithm, where the GD recursion is perturbed
by an additive Gaussian noise whose variance has a particular form. Fractional
Langevin Monte Carlo (FLMC) is a recently proposed extension of LMC, where the
Gaussian noise is replaced by a heavy-tailed {\alpha}-stable noise. As opposed
to its Gaussian counterpart, these heavy-tailed perturbations can incur large
jumps and it has been empirically demonstrated that the choice of
{\alpha}-stable noise can provide several advantages in modern machine learning
problems, both in optimization and sampling contexts. However, as opposed to
LMC, only asymptotic convergence properties of FLMC have been yet established.
In this study, we analyze the non-asymptotic behavior of FLMC for non-convex
optimization and prove finite-time bounds for its expected suboptimality. Our
results show that the weak-error of FLMC increases faster than LMC, which
suggests using smaller step-sizes in FLMC. We finally extend our results to the
case where the exact gradients are replaced by stochastic gradients and show
that similar results hold in this setting as well.
",1,0,0,1,0,0
6225,Exploiting ITO colloidal nanocrystals for ultrafast pulse generation,"  Dynamical materials that capable of responding to optical stimuli have always
been pursued for designing novel photonic devices and functionalities, of which
the response speed and amplitude as well as integration adaptability and energy
effectiveness are especially critical. Here we show ultrafast pulse generation
by exploiting the ultrafast and sensitive nonlinear dynamical processes in
tunably solution-processed colloidal epsilon-near-zero (ENZ) transparent
conducting oxide (TCO) nanocrystals (NCs), of which the potential respond
response speed is >2 THz and modulation depth is ~23% pumped at ~0.7 mJ/cm2,
benefiting from the highly confined geometry in addition to the ENZ enhancement
effect. These ENZ NCs may offer a scalable and printable material solution for
dynamic photonic and optoelectronic devices.
",0,1,0,0,0,0
60,The Query Complexity of Cake Cutting,"  We study the query complexity of cake cutting and give lower and upper bounds
for computing approximately envy-free, perfect, and equitable allocations with
the minimum number of cuts. The lower bounds are tight for computing connected
envy-free allocations among n=3 players and for computing perfect and equitable
allocations with minimum number of cuts between n=2 players.
We also formalize moving knife procedures and show that a large subclass of
this family, which captures all the known moving knife procedures, can be
simulated efficiently with arbitrarily small error in the Robertson-Webb query
model.
",1,0,0,0,0,0
9564,A Generative Model for Dynamic Networks with Applications,"  Networks observed in real world like social networks, collaboration networks
etc., exhibit temporal dynamics, i.e. nodes and edges appear and/or disappear
over time. In this paper, we propose a generative, latent space based,
statistical model for such networks (called dynamic networks). We consider the
case where the number of nodes is fixed, but the presence of edges can vary
over time. Our model allows the number of communities in the network to be
different at different time steps. We use a neural network based methodology to
perform approximate inference in the proposed model and its simplified version.
Experiments done on synthetic and real world networks for the task of community
detection and link prediction demonstrate the utility and effectiveness of our
model as compared to other similar existing approaches.
",1,0,0,1,0,0
5305,The Momentum Distribution of Liquid $^4$He,"  We report high-resolution neutron Compton scattering measurements of liquid
$^4$He under saturated vapor pressure. There is excellent agreement between the
observed scattering and ab initio predictions of its lineshape. Quantum Monte
Carlo calculations predict that the Bose condensate fraction is zero in the
normal fluid, builds up rapidly just below the superfluid transition
temperature, and reaches a value of approximately $7.5\%$ below 1 K. We also
used model fit functions to obtain from the scattering data empirical estimates
for the average atomic kinetic energy and Bose condensate fraction. These
quantities are also in excellent agreement with ab initio calculations. The
convergence between the scattering data and Quantum Monte Carlo calculations is
strong evidence for a Bose broken symmetry in superfluid $^4$He.
",0,1,0,0,0,0
4743,Twisted Quantum Double Model of Topological Orders with Boundaries,"  We generalize the twisted quantum double model of topological orders in two
dimensions to the case with boundaries by systematically constructing the
boundary Hamiltonians. Given the bulk Hamiltonian defined by a gauge group $G$
and a three-cocycle in the third cohomology group of $G$ over $U(1)$, a
boundary Hamiltonian can be defined by a subgroup $K$ of $G$ and a two-cochain
in the second cochain group of $K$ over $U(1)$. The consistency between the
bulk and boundary Hamiltonians is dictated by what we call the Frobenius
condition that constrains the two-cochain given the three-cocyle. We offer a
closed-form formula computing the ground state degeneracy of the model on a
cylinder in terms of the input data only, which can be naturally generalized to
surfaces with more boundaries. We also explicitly write down the ground-state
wavefunction of the model on a disk also in terms of the input data only.
",0,1,1,0,0,0
4032,Learning Context-Sensitive Convolutional Filters for Text Processing,"  Convolutional neural networks (CNNs) have recently emerged as a popular
building block for natural language processing (NLP). Despite their success,
most existing CNN models employed in NLP share the same learned (and static)
set of filters for all input sentences. In this paper, we consider an approach
of using a small meta network to learn context-sensitive convolutional filters
for text processing. The role of meta network is to abstract the contextual
information of a sentence or document into a set of input-aware filters. We
further generalize this framework to model sentence pairs, where a
bidirectional filter generation mechanism is introduced to encapsulate
co-dependent sentence representations. In our benchmarks on four different
tasks, including ontology classification, sentiment analysis, answer sentence
selection, and paraphrase identification, our proposed model, a modified CNN
with context-sensitive filters, consistently outperforms the standard CNN and
attention-based CNN baselines. By visualizing the learned context-sensitive
filters, we further validate and rationalize the effectiveness of proposed
framework.
",1,0,0,1,0,0
3571,Consistency Analysis for Massively Inconsistent Datasets in Bound-to-Bound Data Collaboration,"  Bound-to-Bound Data Collaboration (B2BDC) provides a natural framework for
addressing both forward and inverse uncertainty quantification problems. In
this approach, QOI (quantity of interest) models are constrained by related
experimental observations with interval uncertainty. A collection of such
models and observations is termed a dataset and carves out a feasible region in
the parameter space. If a dataset has a nonempty feasible set, it is said to be
consistent. In real-world applications, it is often the case that collections
of experiments and observations are inconsistent. Revealing the source of this
inconsistency, i.e., identifying which models and/or observations are
problematic, is essential before a dataset can be used for prediction. To
address this issue, we introduce a constraint relaxation-based approach,
entitled the vector consistency measure, for investigating datasets with
numerous sources of inconsistency. The benefits of this vector consistency
measure over a previous method of consistency analysis are demonstrated in two
realistic gas combustion examples.
",1,0,1,0,0,0
19549,Davenport-Heilbronn Theorems for Quotients of Class Groups,"  We prove a generalization of the Davenport-Heilbronn theorem to quotients of
ideal class groups of quadratic fields by the primes lying above a fixed set of
rational primes $S$. Additionally, we obtain average sizes for the relaxed
Selmer group $\mathrm{Sel}_3^S(K)$ and for
$\mathcal{O}_{K,S}^\times/(\mathcal{O}_{K,S}^\times)^3$ as $K$ varies among
quadratic fields with a fixed signature ordered by discriminant.
",0,0,1,0,0,0
13557,Local properties of Riesz minimal energy configurations and equilibrium measures,"  We investigate separation properties of $N$-point configurations that
minimize discrete Riesz $s$-energy on a compact set $A\subset \mathbb{R}^p$.
When $A$ is a smooth $(p-1)$-dimensional manifold without boundary and $s\in
[p-2, p-1)$, we prove that the order of separation (as $N\to \infty$) is the
best possible. The same conclusions hold for the points that are a fixed
positive distance from the boundary of $A$ whenever $A$ is any $p$-dimensional
set. These estimates extend a result of Dahlberg for certain smooth
$(p-1)$-dimensional surfaces when $s=p-2$ (the harmonic case). Furthermore, we
obtain the same separation results for `greedy' $s$-energy points. We deduce
our results from an upper regularity property of the $s$-equilibrium measure
(i.e., the measure that solves the continuous minimal Riesz $s$-energy
problem), and we show that this property holds under a local smoothness
assumption on the set $A$.
",0,0,1,0,0,0
345,Deep Learning for Classification Tasks on Geospatial Vector Polygons,"  In this paper, we evaluate the accuracy of deep learning approaches on
geospatial vector geometry classification tasks. The purpose of this evaluation
is to investigate the ability of deep learning models to learn from geometry
coordinates directly. Previous machine learning research applied to geospatial
polygon data did not use geometries directly, but derived properties thereof.
These are produced by way of extracting geometry properties such as Fourier
descriptors. Instead, our introduced deep neural net architectures are able to
learn on sequences of coordinates mapped directly from polygons. In three
classification tasks we show that the deep learning architectures are
competitive with common learning algorithms that require extracted features.
",0,0,0,1,0,0
19242,Spontaneous currents in superconducting systems with strong spin-orbit coupling,"  We show that Rashba spin-orbit coupling at the interface between a
superconductor and a ferromagnet should produce a spontaneous current in the
atomic thickness region near the interface. This current is counter-balanced by
the superconducting screening current flowing in the region of the width of the
London penetration depth near the interface. Such current carrying state
creates a magnetic field near the superconductor surface, generates a stray
magnetic field outside the sample edges, changes the slope of the temperature
dependence of the critical field $H_{c3}$ and may generate the spontaneous
Abrikosov vortices near the interface.
",0,1,0,0,0,0
3430,Tight Analysis for the 3-Majority Consensus Dynamics,"  We present a tight analysis for the well-studied randomized 3-majority
dynamics of stabilizing consensus, hence answering the main open question of
Becchetti et al. [SODA'16].
Consider a distributed system of n nodes, each initially holding an opinion
in {1, 2, ..., k}. The system should converge to a setting where all
(non-corrupted) nodes hold the same opinion. This consensus opinion should be
\emph{valid}, meaning that it should be among the initially supported opinions,
and the (fast) convergence should happen even in the presence of a malicious
adversary who can corrupt a bounded number of nodes per round and in particular
modify their opinions. A well-studied distributed algorithm for this problem is
the 3-majority dynamics, which works as follows: per round, each node gathers
three opinions --- say by taking its own and two of other nodes sampled at
random --- and then it sets its opinion equal to the majority of this set; ties
are broken arbitrarily, e.g., towards the node's own opinion.
Becchetti et al. [SODA'16] showed that the 3-majority dynamics converges to
consensus in O((k^2\sqrt{\log n} + k\log n)(k+\log n)) rounds, even in the
presence of a limited adversary. We prove that, even with a stronger adversary,
the convergence happens within O(k\log n) rounds. This bound is known to be
optimal.
",1,0,0,0,0,0
4198,HornDroid: Practical and Sound Static Analysis of Android Applications by SMT Solving,"  We present HornDroid, a new tool for the static analysis of information flow
properties in Android applications. The core idea underlying HornDroid is to
use Horn clauses for soundly abstracting the semantics of Android applications
and to express security properties as a set of proof obligations that are
automatically discharged by an off-the-shelf SMT solver. This approach makes it
possible to fine-tune the analysis in order to achieve a high degree of
precision while still using off-the-shelf verification tools, thereby
leveraging the recent advances in this field. As a matter of fact, HornDroid
outperforms state-of-the-art Android static analysis tools on benchmarks
proposed by the community. Moreover, HornDroid is the first static analysis
tool for Android to come with a formal proof of soundness, which covers the
core of the analysis technique: besides yielding correctness assurances, this
proof allowed us to identify some critical corner-cases that affect the
soundness guarantees provided by some of the previous static analysis tools for
Android.
",1,0,0,0,0,0
10495,Strong anisotropy effect in iron-based superconductor CaFe$_{0.882}$Co$_{0.118}$AsF,"  The anisotropy of the Fe-based superconductors is much smaller than that of
the cuprates and the theoretical calculations. A credible understanding for
this experimental fact is still lacking up to now. Here we experimentally study
the magnetic-field-angle dependence of electronic resistivity in the
superconducting phase of iron-based superconductor
CaFe$_{0.882}$Co$_{0.118}$AsF, and find the strongest anisotropy effect of the
upper critical field among the iron-based superconductors based on the
framework of Ginzburg-Landau theory. The evidences of energy band structure and
charge density distribution from electronic structure calculations demonstrate
that the observed strong anisotropic effect mainly comes from the strong ionic
bonding in between the ions of Ca$^{2+}$ and F$^-$, which weakens the
interlayer coupling between the layers of FeAs and CaF. This finding provides a
significant insight into the nature of experimentally observed strong
anisotropic effect of electronic resistivity, and also paves an avenue to
design exotic two dimensional artificial unconventional superconductors in
future.
",0,1,0,0,0,0
17751,A Bayesian model for lithology/fluid class prediction using a Markov mesh prior fitted from a training image,"  We consider a Bayesian model for inversion of observed amplitude variation
with offset (AVO) data into lithology/fluid classes, and study in particular
how the choice of prior distribution for the lithology/fluid classes influences
the inversion results. Two distinct prior distributions are considered, a
simple manually specified Markov random field prior with a first order
neighborhood and a Markov mesh model with a much larger neighborhood estimated
from a training image. They are chosen to model both horisontal connectivity
and vertical thickness distribution of the lithology/fluid classes, and are
compared on an offshore clastic oil reservoir in the North Sea. We combine both
priors with the same linearised Gaussian likelihood function based on a
convolved linearised Zoeppritz relation and estimate properties of the
resulting two posterior distributions by simulating from these distributions
with the Metropolis-Hastings algorithm.
The influence of the prior on the marginal posterior probabilities for the
lithology/fluid classes is clearly observable, but modest. The importance of
the prior on the connectivity properties in the posterior realisations,
however, is much stronger. The larger neighborhood of the Markov mesh prior
enables it to identify and model connectivity and curvature much better than
what can be done by the first order neighborhood Markov random field prior. As
a result, we conclude that the posterior realisations based on the Markov mesh
prior appear with much higher lateral connectivity, which is geologically
plausible.
",0,0,0,1,0,0
10608,FML-based Dynamic Assessment Agent for Human-Machine Cooperative System on Game of Go,"  In this paper, we demonstrate the application of Fuzzy Markup Language (FML)
to construct an FML-based Dynamic Assessment Agent (FDAA), and we present an
FML-based Human-Machine Cooperative System (FHMCS) for the game of Go. The
proposed FDAA comprises an intelligent decision-making and learning mechanism,
an intelligent game bot, a proximal development agent, and an intelligent
agent. The intelligent game bot is based on the open-source code of Facebook
Darkforest, and it features a representational state transfer application
programming interface mechanism. The proximal development agent contains a
dynamic assessment mechanism, a GoSocket mechanism, and an FML engine with a
fuzzy knowledge base and rule base. The intelligent agent contains a GoSocket
engine and a summarization agent that is based on the estimated win rate,
real-time simulation number, and matching degree of predicted moves.
Additionally, the FML for player performance evaluation and linguistic
descriptions for game results commentary are presented. We experimentally
verify and validate the performance of the FDAA and variants of the FHMCS by
testing five games in 2016 and 60 games of Google Master Go, a new version of
the AlphaGo program, in January 2017. The experimental results demonstrate that
the proposed FDAA can work effectively for Go applications.
",1,0,0,0,0,0
12365,Lexical Features in Coreference Resolution: To be Used With Caution,"  Lexical features are a major source of information in state-of-the-art
coreference resolvers. Lexical features implicitly model some of the linguistic
phenomena at a fine granularity level. They are especially useful for
representing the context of mentions. In this paper we investigate a drawback
of using many lexical features in state-of-the-art coreference resolvers. We
show that if coreference resolvers mainly rely on lexical features, they can
hardly generalize to unseen domains. Furthermore, we show that the current
coreference resolution evaluation is clearly flawed by only evaluating on a
specific split of a specific dataset in which there is a notable overlap
between the training, development and test sets.
",1,0,0,0,0,0
18830,Consistency of Maximum Likelihood for Continuous-Space Network Models,"  Network analysis needs tools to infer distributions over graphs of arbitrary
size from a single graph. Assuming the distribution is generated by a
continuous latent space model which obeys certain natural symmetry and
smoothness properties, we establish three levels of consistency for
non-parametric maximum likelihood inference as the number of nodes grows: (i)
the estimated locations of all nodes converge in probability on their true
locations; (ii) the distribution over locations in the latent space converges
on the true distribution; and (iii) the distribution over graphs of arbitrary
size converges.
",0,0,1,1,0,0
12617,Pattern Search Multidimensional Scaling,"  We present a novel view of nonlinear manifold learning using derivative-free
optimization techniques. Specifically, we propose an extension of the classical
multi-dimensional scaling (MDS) method, where instead of performing gradient
descent, we sample and evaluate possible ""moves"" in a sphere of fixed radius
for each point in the embedded space. A fixed-point convergence guarantee can
be shown by formulating the proposed algorithm as an instance of General
Pattern Search (GPS) framework. Evaluation on both clean and noisy synthetic
datasets shows that pattern search MDS can accurately infer the intrinsic
geometry of manifolds embedded in high-dimensional spaces. Additionally,
experiments on real data, even under noisy conditions, demonstrate that the
proposed pattern search MDS yields state-of-the-art results.
",0,0,0,1,0,0
1633,Generalization Tower Network: A Novel Deep Neural Network Architecture for Multi-Task Learning,"  Deep learning (DL) advances state-of-the-art reinforcement learning (RL), by
incorporating deep neural networks in learning representations from the input
to RL. However, the conventional deep neural network architecture is limited in
learning representations for multi-task RL (MT-RL), as multiple tasks can refer
to different kinds of representations. In this paper, we thus propose a novel
deep neural network architecture, namely generalization tower network (GTN),
which can achieve MT-RL within a single learned model. Specifically, the
architecture of GTN is composed of both horizontal and vertical streams. In our
GTN architecture, horizontal streams are used to learn representation shared in
similar tasks. In contrast, the vertical streams are introduced to be more
suitable for handling diverse tasks, which encodes hierarchical shared
knowledge of these tasks. The effectiveness of the introduced vertical stream
is validated by experimental results. Experimental results further verify that
our GTN architecture is able to advance the state-of-the-art MT-RL, via being
tested on 51 Atari games.
",1,0,0,1,0,0
20422,Efficient Version-Space Reduction for Visual Tracking,"  Discrminative trackers, employ a classification approach to separate the
target from its background. To cope with variations of the target shape and
appearance, the classifier is updated online with different samples of the
target and the background. Sample selection, labeling and updating the
classifier is prone to various sources of errors that drift the tracker. We
introduce the use of an efficient version space shrinking strategy to reduce
the labeling errors and enhance its sampling strategy by measuring the
uncertainty of the tracker about the samples. The proposed tracker, utilize an
ensemble of classifiers that represents different hypotheses about the target,
diversify them using boosting to provide a larger and more consistent coverage
of the version-space and tune the classifiers' weights in voting. The proposed
system adjusts the model update rate by promoting the co-training of the
short-memory ensemble with a long-memory oracle. The proposed tracker
outperformed state-of-the-art trackers on different sequences bearing various
tracking challenges.
",1,0,0,0,0,0
8026,High Speed Elephant Flow Detection Under Partial Information,"  In this paper we introduce a new framework to detect elephant flows at very
high speed rates and under uncertainty. The framework provides exact
mathematical formulas to compute the detection likelihood and introduces a new
flow reconstruction lemma under partial information. These theoretical results
lead to the design of BubbleCache, a new elephant flow detection algorithm
designed to operate near the optimal tradeoff between computational scalability
and accuracy by dynamically tracking the traffic's natural cutoff sampling
rate. We demonstrate on a real world 100 Gbps network that the BubbleCache
algorithm helps reduce the computational cost by a factor of 1000 and the
memory requirements by a factor of 100 while detecting the top flows on the
network with very high probability.
",1,0,0,0,0,0
15667,Characteristics of a magneto-optical trap of molecules,"  We present the properties of a magneto-optical trap (MOT) of CaF molecules.
We study the process of loading the MOT from a decelerated buffer-gas-cooled
beam, and how best to slow this molecular beam in order to capture the most
molecules. We determine how the number of molecules, the photon scattering
rate, the oscillation frequency, damping constant, temperature, cloud size and
lifetime depend on the key parameters of the MOT, especially the intensity and
detuning of the main cooling laser. We compare our results to analytical and
numerical models, to the properties of standard atomic MOTs, and to MOTs of SrF
molecules. We load up to $2 \times 10^4$ molecules, and measure a maximum
scattering rate of $2.5 \times 10^6$ s$^{-1}$ per molecule, a maximum
oscillation frequency of 100 Hz, a maximum damping constant of 500 s$^{-1}$,
and a minimum MOT rms radius of 1.5 mm. A minimum temperature of 730 $\mu$K is
obtained by ramping down the laser intensity to low values. The lifetime,
typically about 100 ms, is consistent with a leak out of the cooling cycle with
a branching ratio of about $6 \times 10^{-6}$. The MOT has a capture velocity
of about 11 m/s.
",0,1,0,0,0,0
8938,Mechanism Deduction from Noisy Chemical Reaction Networks,"  We introduce KiNetX, a fully automated meta-algorithm for the kinetic
analysis of complex chemical reaction networks derived from semi-accurate but
efficient electronic structure calculations. It is designed to (i) accelerate
the automated exploration of such networks, and (ii) cope with model-inherent
errors in electronic structure calculations on elementary reaction steps. We
developed and implemented KiNetX to possess three features. First, KiNetX
evaluates the kinetic relevance of every species in a (yet incomplete) reaction
network to confine the search for new elementary reaction steps only to those
species that are considered possibly relevant. Second, KiNetX identifies and
eliminates all kinetically irrelevant species and elementary reactions to
reduce a complex network graph to a comprehensible mechanism. Third, KiNetX
estimates the sensitivity of species concentrations toward changes in
individual rate constants (derived from relative free energies), which allows
us to systematically select the most efficient electronic structure model for
each elementary reaction given a predefined accuracy. The novelty of KiNetX
consists in the rigorous propagation of correlated free-energy uncertainty
through all steps of our kinetic analyis. To examine the performance of KiNetX,
we developed AutoNetGen. It semirandomly generates chemistry-mimicking reaction
networks by encoding chemical logic into their underlying graph structure.
AutoNetGen allows us to consider a vast number of distinct chemistry-like
scenarios and, hence, to discuss assess the importance of rigorous uncertainty
propagation in a statistical context. Our results reveal that KiNetX reliably
supports the deduction of product ratios, dominant reaction pathways, and
possibly other network properties from semi-accurate electronic structure data.
",0,0,0,0,1,0
5454,ZhuSuan: A Library for Bayesian Deep Learning,"  In this paper we introduce ZhuSuan, a python probabilistic programming
library for Bayesian deep learning, which conjoins the complimentary advantages
of Bayesian methods and deep learning. ZhuSuan is built upon Tensorflow. Unlike
existing deep learning libraries, which are mainly designed for deterministic
neural networks and supervised tasks, ZhuSuan is featured for its deep root
into Bayesian inference, thus supporting various kinds of probabilistic models,
including both the traditional hierarchical Bayesian models and recent deep
generative models. We use running examples to illustrate the probabilistic
programming on ZhuSuan, including Bayesian logistic regression, variational
auto-encoders, deep sigmoid belief networks and Bayesian recurrent neural
networks.
",1,0,0,1,0,0
879,Landau Collision Integral Solver with Adaptive Mesh Refinement on Emerging Architectures,"  The Landau collision integral is an accurate model for the small-angle
dominated Coulomb collisions in fusion plasmas. We investigate a high order
accurate, fully conservative, finite element discretization of the nonlinear
multi-species Landau integral with adaptive mesh refinement using the PETSc
library (www.mcs.anl.gov/petsc). We develop algorithms and techniques to
efficiently utilize emerging architectures with an approach that minimizes
memory usage and movement and is suitable for vector processing. The Landau
collision integral is vectorized with Intel AVX-512 intrinsics and the solver
sustains as much as 22% of the theoretical peak flop rate of the Second
Generation Intel Xeon Phi, Knights Landing, processor.
",1,0,0,0,0,0
11577,Improved Regularization Techniques for End-to-End Speech Recognition,"  Regularization is important for end-to-end speech models, since the models
are highly flexible and easy to overfit. Data augmentation and dropout has been
important for improving end-to-end models in other domains. However, they are
relatively under explored for end-to-end speech models. Therefore, we
investigate the effectiveness of both methods for end-to-end trainable, deep
speech recognition models. We augment audio data through random perturbations
of tempo, pitch, volume, temporal alignment, and adding random noise.We further
investigate the effect of dropout when applied to the inputs of all layers of
the network. We show that the combination of data augmentation and dropout give
a relative performance improvement on both Wall Street Journal (WSJ) and
LibriSpeech dataset of over 20%. Our model performance is also competitive with
other end-to-end speech models on both datasets.
",1,0,0,1,0,0
18015,Controllability of temporal networks: An analysis using higher-order networks,"  The control of complex networks is a significant challenge, especially when
the network topology of the system to be controlled is dynamic. Addressing this
challenge, here we introduce a novel approach which allows exploring the
controllability of temporal networks. Studying six empirical data sets, we
particularly show that order correlations in the sequence of interactions can
both increase or decrease the time needed to achieve full controllability.
Counter-intuitively, we find that this effect can be opposite than the effect
of order correlations on other dynamical processes. Specifically, we show that
order correlations that speed up a diffusion process in a given system can slow
down the control of the same system, and vice-versa. Building on the
higher-order graphical modeling framework introduced in recent works, we
further demonstrate that spectral properties of higher-order network topologies
can be used to analytically explain this phenomenon.
",1,1,0,0,0,0
14648,Local Descriptor for Robust Place Recognition using LiDAR Intensity,"  Place recognition is a challenging problem in mobile robotics, especially in
unstructured environments or under viewpoint and illumination changes. Most
LiDAR-based methods rely on geometrical features to overcome such challenges,
as generally scene geometry is invariant to these changes, but tend to affect
camera-based solutions significantly. Compared to cameras, however, LiDARs lack
the strong and descriptive appearance information that imaging can provide.
To combine the benefits of geometry and appearance, we propose coupling the
conventional geometric information from the LiDAR with its calibrated intensity
return. This strategy extracts extremely useful information in the form of a
new descriptor design, coined ISHOT, outperforming popular state-of-art
geometric-only descriptors by significant margin in our local descriptor
evaluation. To complete the framework, we furthermore develop a probabilistic
keypoint voting place recognition algorithm, leveraging the new descriptor and
yielding sublinear place recognition performance. The efficacy of our approach
is validated in challenging global localization experiments in large-scale
built-up and unstructured environments.
",1,0,0,0,0,0
8475,Consistency of Dirichlet Partitions,"  A Dirichlet $k$-partition of a domain $U \subseteq \mathbb{R}^d$ is a
collection of $k$ pairwise disjoint open subsets such that the sum of their
first Laplace-Dirichlet eigenvalues is minimal. A discrete version of Dirichlet
partitions has been posed on graphs with applications in data analysis. Both
versions admit variational formulations: solutions are characterized by
minimizers of the Dirichlet energy of mappings from $U$ into a singular space
$\Sigma_k \subseteq \mathbb{R}^k$. In this paper, we extend results of N.\
García Trillos and D.\ Slepčev to show that there exist solutions of the
continuum problem arising as limits to solutions of a sequence of discrete
problems. Specifically, a sequence of points $\{x_i\}_{i \in \mathbb{N}}$ from
$U$ is sampled i.i.d.\ with respect to a given probability measure $\nu$ on $U$
and for all $n \in \mathbb{N}$, a geometric graph $G_n$ is constructed from the
first $n$ points $x_1, x_2, \ldots, x_n$ and the pairwise distances between the
points. With probability one with respect to the choice of points $\{x_i\}_{i
\in \mathbb{N}}$, we show that as $n \to \infty$ the discrete Dirichlet
energies for functions $G_n \to \Sigma_k$ $\Gamma$-converge to (a scalar
multiple of) the continuum Dirichlet energy for functions $U \to \Sigma_k$ with
respect to a metric coming from the theory of optimal transport. This, along
with a compactness property for the aforementioned energies that we prove,
implies the convergence of minimizers. When $\nu$ is the uniform distribution,
our results also imply the statistical consistency statement that Dirichlet
partitions of geometric graphs converge to partitions of the sampled space in
the Hausdorff sense.
",0,0,1,1,0,0
16699,On the k-Means/Median Cost Function,"  In this work, we study the $k$-means cost function. The (Euclidean) $k$-means
problem can be described as follows: given a dataset $X \subseteq \mathbb{R}^d$
and a positive integer $k$, find a set of $k$ centers $C \subseteq
\mathbb{R}^d$ such that $\Phi(C, X) \stackrel{def}{=} \sum_{x \in X} \min_{c
\in C} ||x - c||^2$ is minimized. Let $\Delta_k(X) \stackrel{def}{=} \min_{C
\subseteq \mathbb{R}^d} \Phi(C, X)$ denote the cost of the optimal $k$-means
solution. It is simple to observe that for any dataset $X$, $\Delta_k(X)$
decreases as $k$ increases. We try to understand this behaviour more precisely.
For any dataset $X \subseteq \mathbb{R}^d$, integer $k \geq 1$, and a small
precision parameter $\varepsilon > 0$, let $\mathcal{L}_{X}^{k, \varepsilon}$
denote the smallest integer such that $\Delta_{\mathcal{L}_{X}^{k,
\varepsilon}}(X) \leq \varepsilon \cdot \Delta_{k}(X)$. We show upper and lower
bounds on this quantity. Our techniques generalize for the metric $k$-median
problem in arbitrary metrics and we give bounds in terms of the doubling
dimension of the metric. Finally, we observe that for any dataset $X$, we can
compute a set $S$ of size $O \left(\mathcal{L}_{X}^{k, \frac{\varepsilon}{c}}
\right)$ such that $\Delta_{S}(X) \leq \varepsilon \cdot \Delta_k(X)$ using the
$D^2$-sampling algorithm which is also known as the $k$-means++ seeding
procedure. In the previous statement, $c$ is some fixed constant. We also
discuss some applications of our bounds.
",1,0,0,0,0,0
6276,Topological Terms and Phases of Sigma Models,"  We study boundary conditions of topological sigma models with the goal of
generalizing the concepts of anomalous symmetry and symmetry protected
topological order. We find a version of 't Hooft's anomaly matching conditions
on the renormalization group flow of boundaries of invertible topological sigma
models and discuss several examples of anomalous boundary theories. We also
comment on bulk topological transitions in dynamical sigma models and argue
that one can, with care, use topological data to draw sigma model phase
diagrams.
",0,1,0,0,0,0
12525,Moments and non-vanishing of Hecke $L$-functions with quadratic characters in $\mathbb{Q}(i)$ at the central point,"  In this paper, we study the moments of central values of Hecke $L$-functions
associated with quadratic characters in $\mq(i)$, and establish quantitative
non-vanishing result for the $L$-values.
",0,0,1,0,0,0
3888,Invariant submanifolds of (LCS)n-Manifolds with respect to quarter symmetric metric connection,"  The object of the present paper is to study invariant submanifolds of
(LCS)n-manifolds with respect to quarter symmetric metric connection. It is
shown that the mean curvature of an invariant submanifold of (LCS)n-manifold
with respect to quarter symmetric metric connection and Levi-Civita connection
are equal. An example is constructed to illustrate the results of the paper. We
also obtain some equivalent conditions of such notion.
",0,0,1,0,0,0
14628,"A Detailed Observational Analysis of V1324 Sco, the Most Gamma-Ray Luminous Classical Nova to Date","  It has recently been discovered that some, if not all, classical novae emit
GeV gamma rays during outburst, but the mechanisms involved in the production
of the gamma rays are still not well understood. We present here a
comprehensive multi-wavelength dataset---from radio to X-rays---for the most
gamma-ray luminous classical nova to-date, V1324 Sco. Using this dataset, we
show that V1324 Sco is a canonical dusty Fe-II type nova, with a maximum ejecta
velocity of 2600 km s$^{-1}$ and an ejecta mass of few $\times 10^{-5}$
M$_{\odot}$. There is also evidence for complex shock interactions, including a
double-peaked radio light curve which shows high brightness temperatures at
early times. To explore why V1324~Sco was so gamma-ray luminous, we present a
model of the nova ejecta featuring strong internal shocks, and find that higher
gamma-ray luminosities result from higher ejecta velocities and/or mass-loss
rates. Comparison of V1324~Sco with other gamma-ray detected novae does not
show clear signatures of either, and we conclude that a larger sample of
similarly well-observed novae is needed to understand the origin and variation
of gamma rays in novae.
",0,1,0,0,0,0
15930,Lifted Polymatroid Inequalities for Mean-Risk Optimization with Indicator Variables,"  We investigate a mixed 0-1 conic quadratic optimization problem with
indicator variables arising in mean-risk optimization. The indicator variables
are often used to model non-convexities such as fixed charges or cardinality
constraints. Observing that the problem reduces to a submodular function
minimization for its binary restriction, we derive three classes of strong
convex valid inequalities by lifting the polymatroid inequalities on the binary
variables. Computational experiments demonstrate the effectiveness of the
inequalities in strengthening the convex relaxations and, thereby, improving
the solution times for mean-risk problems with fixed charges and cardinality
constraints significantly.
",0,0,1,0,0,0
11263,A propagation tool to connect remote-sensing observations with in-situ measurements of heliospheric structures,"  The remoteness of the Sun and the harsh conditions prevailing in the solar
corona have so far limited the observational data used in the study of solar
physics to remote-sensing observations taken either from the ground or from
space. In contrast, the `solar wind laboratory' is directly measured in situ by
a fleet of spacecraft measuring the properties of the plasma and magnetic
fields at specific points in space. Since 2007, the solar-terrestrial relations
observatory (STEREO) has been providing images of the solar wind that flows
between the solar corona and spacecraft making in-situ measurements. This has
allowed scientists to directly connect processes imaged near the Sun with the
subsequent effects measured in the solar wind. This new capability prompted the
development of a series of tools and techniques to track heliospheric
structures through space. This article presents one of these tools, a web-based
interface called the 'Propagation Tool' that offers an integrated research
environment to study the evolution of coronal and solar wind structures, such
as Coronal Mass Ejections (CMEs), Corotating Interaction Regions (CIRs) and
Solar Energetic Particles (SEPs). These structures can be propagated from the
Sun outwards to or alternatively inwards from planets and spacecraft situated
in the inner and outer heliosphere. In this paper, we present the global
architecture of the tool, discuss some of the assumptions made to simulate the
evolution of the structures and show how the tool connects to different
databases.
",0,1,0,0,0,0
1403,"Sample, computation vs storage tradeoffs for classification using tensor subspace models","  In this paper, we exhibit the tradeoffs between the (training) sample,
computation and storage complexity for the problem of supervised classification
using signal subspace estimation. Our main tool is the use of tensor subspaces,
i.e. subspaces with a Kronecker structure, for embedding the data into lower
dimensions. Among the subspaces with a Kronecker structure, we show that using
subspaces with a hierarchical structure for representing data leads to improved
tradeoffs. One of the main reasons for the improvement is that embedding data
into these hierarchical Kronecker structured subspaces prevents overfitting at
higher latent dimensions.
",1,0,0,1,0,0
14164,Tunneling of the hard-core model on finite triangular lattices,"  We consider the hard-core model on finite triangular lattices with Metropolis
dynamics. Under suitable conditions on the triangular lattice dimensions, this
interacting particle system has three maximum-occupancy configurations and we
investigate its high-fugacity behavior by studying tunneling times, i.e., the
first hitting times between between these maximum-occupancy configurations, and
the mixing time. The proof method relies on the analysis of the corresponding
state space using geometrical and combinatorial properties of the hard-core
configurations on finite triangular lattices, in combination with known results
for first hitting times of Metropolis Markov chains in the equivalent
zero-temperature limit. In particular, we show how the order of magnitude of
the expected tunneling times depends on the triangular lattice dimensions in
the low-temperature regime and prove the asymptotic exponentiality of the
rescaled tunneling time leveraging the intrinsic symmetry of the state space.
",0,1,1,0,0,0
665,Multi-dimensional Graph Fourier Transform,"  Many signals on Cartesian product graphs appear in the real world, such as
digital images, sensor observation time series, and movie ratings on Netflix.
These signals are ""multi-dimensional"" and have directional characteristics
along each factor graph. However, the existing graph Fourier transform does not
distinguish these directions, and assigns 1-D spectra to signals on product
graphs. Further, these spectra are often multi-valued at some frequencies. Our
main result is a multi-dimensional graph Fourier transform that solves such
problems associated with the conventional GFT. Using algebraic properties of
Cartesian products, the proposed transform rearranges 1-D spectra obtained by
the conventional GFT into the multi-dimensional frequency domain, of which each
dimension represents a directional frequency along each factor graph. Thus, the
multi-dimensional graph Fourier transform enables directional frequency
analysis, in addition to frequency analysis with the conventional GFT.
Moreover, this rearrangement resolves the multi-valuedness of spectra in some
cases. The multi-dimensional graph Fourier transform is a foundation of novel
filterings and stationarities that utilize dimensional information of graph
signals, which are also discussed in this study. The proposed methods are
applicable to a wide variety of data that can be regarded as signals on
Cartesian product graphs. This study also notes that multivariate graph signals
can be regarded as 2-D univariate graph signals. This correspondence provides
natural definitions of the multivariate graph Fourier transform and the
multivariate stationarity based on their 2-D univariate versions.
",1,0,0,1,0,0
7291,Bayesian inference for Stable Levy driven Stochastic Differential Equations with high-frequency data,"  In this article we consider parametric Bayesian inference for stochastic
differential equations (SDE) driven by a pure-jump stable Levy process, which
is observed at high frequency. In most cases of practical interest, the
likelihood function is not available, so we use a quasi-likelihood and place an
associated prior on the unknown parameters. It is shown under regularity
conditions that there is a Bernstein-von Mises theorem associated to the
posterior. We then develop a Markov chain Monte Carlo (MCMC) algorithm for
Bayesian inference and assisted by our theoretical results, we show how to
scale Metropolis-Hastings proposals when the frequency of the data grows, in
order to prevent the acceptance ratio going to zero in the large data limit.
Our algorithm is presented on numerical examples that help to verify our
theoretical findings.
",0,0,1,1,0,0
10865,The Hidden Vulnerability of Distributed Learning in Byzantium,"  While machine learning is going through an era of celebrated success,
concerns have been raised about the vulnerability of its backbone: stochastic
gradient descent (SGD). Recent approaches have been proposed to ensure the
robustness of distributed SGD against adversarial (Byzantine) workers sending
poisoned gradients during the training phase. Some of these approaches have
been proven Byzantine-resilient: they ensure the convergence of SGD despite the
presence of a minority of adversarial workers.
We show in this paper that convergence is not enough. In high dimension $d
\gg 1$, an adver\-sary can build on the loss function's non-convexity to make
SGD converge to ineffective models. More precisely, we bring to light that
existing Byzantine-resilient schemes leave a margin of poisoning of
$\Omega\left(f(d)\right)$, where $f(d)$ increases at least like $\sqrt{d~}$.
Based on this leeway, we build a simple attack, and experimentally show its
strong to utmost effectivity on CIFAR-10 and MNIST.
We introduce Bulyan, and prove it significantly reduces the attackers leeway
to a narrow $O( \frac{1}{\sqrt{d~}})$ bound. We empirically show that Bulyan
does not suffer the fragility of existing aggregation rules and, at a
reasonable cost in terms of required batch size, achieves convergence as if
only non-Byzantine gradients had been used to update the model.
",0,0,0,1,0,0
4822,An Agile Software Engineering Method to Design Blockchain Applications,"  Cryptocurrencies and their foundation technology, the Blockchain, are
reshaping finance and economics, allowing a decentralized approach enabling
trusted applications with no trusted counterpart. More recently, the Blockchain
and the programs running on it, called Smart Contracts, are also finding more
and more applications in all fields requiring trust and sound certifications.
Some people have come to the point of saying that the ""Blockchain revolution""
can be compared to that of the Internet and the Web in their early days. As a
result, all the software development revolving around the Blockchain technology
is growing at a staggering rate. The feeling of many software engineers about
such huge interest in Blockchain technologies is that of unruled and hurried
software development, a sort of competition on a first-come-first-served basis
which does not assure neither software quality, nor that the basic concepts of
software engineering are taken into account. This paper tries to cope with this
issue, proposing a software development process to gather the requirement,
analyze, design, develop, test and deploy Blockchain applications. The process
is based on several Agile practices, such as User Stories and iterative and
incremental development based on them. However, it makes also use of more
formal notations, such as some UML diagrams describing the design of the
system, with additions to represent specific concepts found in Blockchain
development. The method is described in good detail, and an example is given to
show how it works.
",1,0,0,0,0,0
5246,Personalized and Private Peer-to-Peer Machine Learning,"  The rise of connected personal devices together with privacy concerns call
for machine learning algorithms capable of leveraging the data of a large
number of agents to learn personalized models under strong privacy
requirements. In this paper, we introduce an efficient algorithm to address the
above problem in a fully decentralized (peer-to-peer) and asynchronous fashion,
with provable convergence rate. We show how to make the algorithm
differentially private to protect against the disclosure of information about
the personal datasets, and formally analyze the trade-off between utility and
privacy. Our experiments show that our approach dramatically outperforms
previous work in the non-private case, and that under privacy constraints, we
can significantly improve over models learned in isolation.
",1,0,0,1,0,0
6144,Driver Distraction Identification with an Ensemble of Convolutional Neural Networks,"  The World Health Organization (WHO) reported 1.25 million deaths yearly due
to road traffic accidents worldwide and the number has been continuously
increasing over the last few years. Nearly fifth of these accidents are caused
by distracted drivers. Existing work of distracted driver detection is
concerned with a small set of distractions (mostly, cell phone usage).
Unreliable ad-hoc methods are often used.In this paper, we present the first
publicly available dataset for driver distraction identification with more
distraction postures than existing alternatives. In addition, we propose a
reliable deep learning-based solution that achieves a 90% accuracy. The system
consists of a genetically-weighted ensemble of convolutional neural networks,
we show that a weighted ensemble of classifiers using a genetic algorithm
yields in a better classification confidence. We also study the effect of
different visual elements in distraction detection by means of face and hand
localizations, and skin segmentation. Finally, we present a thinned version of
our ensemble that could achieve 84.64% classification accuracy and operate in a
real-time environment.
",1,0,0,1,0,0
16711,Rigidity for von Neumann algebras given by locally compact groups and their crossed products,"  We prove the first rigidity and classification theorems for crossed product
von Neumann algebras given by actions of non-discrete, locally compact groups.
We prove that for arbitrary free probability measure preserving actions of
connected simple Lie groups of real rank one, the crossed product has a unique
Cartan subalgebra up to unitary conjugacy. We then deduce a W* strong rigidity
theorem for irreducible actions of products of such groups. More generally, our
results hold for products of locally compact groups that are nonamenable,
weakly amenable and that belong to Ozawa's class S.
",0,0,1,0,0,0
19341,Approximation Dynamics,"  We describe the approximation of a continuous dynamical system on a p. l.
manifold or Cantor set by a tractable system. A system is tractable when it has
a finite number of chain components and, with respect to a given full
background measure, almost every point is generic for one of a finite number of
ergodic invariant measures with non-overlapping supports. The approximations
use non-degenerate simplicial dynamical systems for p. l. manifolds and
shift-like dynamical systems for Cantor Sets.
",0,0,1,0,0,0
6333,Symmetry and the Geometric Phase in Ultracold Hydrogen-Exchange Reactions,"  Quantum reactive scattering calculations are reported for the ultracold
hydrogen-exchange reaction and its non-reactive atom-exchange isotopic
counterparts, proceeding from excited rotational states. It is shown that while
the geometric phase (GP) does not necessarily control the reaction to all final
states one can always find final states where it does. For the isotopic
counterpart reactions these states can be used to make a measurement of the GP
effect by separately measuring the even and odd symmetry contributions, which
experimentally requires nuclear-spin final-state resolution. This follows from
symmetry considerations that make the even and odd identical-particle exchange
symmetry wavefunctions which include the GP locally equivalent to the opposite
symmetry wavefunctions which do not. This equivalence reflects the important
role discrete symmetries play in ultracold chemistry generally and highlights
the key role ultracold reactions can play in understanding fundamental aspects
of chemical reactivity.
",0,1,0,0,0,0
5251,Spectral Norm Regularization for Improving the Generalizability of Deep Learning,"  We investigate the generalizability of deep learning based on the sensitivity
to input perturbation. We hypothesize that the high sensitivity to the
perturbation of data degrades the performance on it. To reduce the sensitivity
to perturbation, we propose a simple and effective regularization method,
referred to as spectral norm regularization, which penalizes the high spectral
norm of weight matrices in neural networks. We provide supportive evidence for
the abovementioned hypothesis by experimentally confirming that the models
trained using spectral norm regularization exhibit better generalizability than
other baseline methods.
",1,0,0,1,0,0
827,Asymptotic Confidence Regions for High-dimensional Structured Sparsity,"  In the setting of high-dimensional linear regression models, we propose two
frameworks for constructing pointwise and group confidence sets for penalized
estimators which incorporate prior knowledge about the organization of the
non-zero coefficients. This is done by desparsifying the estimator as in van de
Geer et al. [18] and van de Geer and Stucky [17], then using an appropriate
estimator for the precision matrix $\Theta$. In order to estimate the precision
matrix a corresponding structured matrix norm penalty has to be introduced.
After normalization the result is an asymptotic pivot.
The asymptotic behavior is studied and simulations are added to study the
differences between the two schemes.
",0,0,1,1,0,0
11298,Hausdorff dimension of the boundary of bubbles of additive Brownian motion and of the Brownian sheet,"  We first consider the additive Brownian motion process $(X(s_1,s_2),\
(s_1,s_2) \in \mathbb{R}^2)$ defined by $X(s_1,s_2) = Z_1(s_1) - Z_2 (s_2)$,
where $Z_1$ and $Z_2 $ are two independent (two-sided) Brownian motions. We
show that with probability one, the Hausdorff dimension of the boundary of any
connected component of the random set $\{(s_1,s_2)\in \mathbb{R}^2: X(s_1,s_2)
>0\}$ is equal to $$
\frac{1}{4}\left(1 + \sqrt{13 + 4 \sqrt{5}}\right) \simeq 1.421\, . $$ Then
the same result is shown to hold when $X$ is replaced by a standard Brownian
sheet indexed by the nonnegative quadrant.
",0,0,1,0,0,0
19138,"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset","  The paucity of videos in current action classification datasets (UCF-101 and
HMDB-51) has made it difficult to identify good video architectures, as most
methods obtain similar performance on existing small-scale benchmarks. This
paper re-evaluates state-of-the-art architectures in light of the new Kinetics
Human Action Video dataset. Kinetics has two orders of magnitude more data,
with 400 human action classes and over 400 clips per class, and is collected
from realistic, challenging YouTube videos. We provide an analysis on how
current architectures fare on the task of action classification on this dataset
and how much performance improves on the smaller benchmark datasets after
pre-training on Kinetics.
We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on
2D ConvNet inflation: filters and pooling kernels of very deep image
classification ConvNets are expanded into 3D, making it possible to learn
seamless spatio-temporal feature extractors from video while leveraging
successful ImageNet architecture designs and even their parameters. We show
that, after pre-training on Kinetics, I3D models considerably improve upon the
state-of-the-art in action classification, reaching 80.9% on HMDB-51 and 98.0%
on UCF-101.
",1,0,0,0,0,0
201,Jastrow form of the Ground State Wave Functions for Fractional Quantum Hall States,"  The topological morphology--order of zeros at the positions of electrons with
respect to a specific electron--of Laughlin state at filling fractions $1/m$
($m$ odd) is homogeneous as every electron feels zeros of order $m$ at the
positions of other electrons. Although fairly accurate ground state wave
functions for most of the other quantum Hall states in the lowest Landau level
are quite well-known, it had been an open problem in expressing the ground
state wave functions in terms of flux-attachment to particles, {\em a la}, this
morphology of Laughlin state. With a very general consideration of
flux-particle relations only, in spherical geometry, we here report a novel
method for determining morphologies of these states. Based on these, we
construct almost exact ground state wave-functions for the Coulomb interaction.
Although the form of interaction may change the ground state wave-function, the
same morphology constructs the latter irrespective of the nature of the
interaction between electrons.
",0,1,0,0,0,0
5020,Neural Face Editing with Intrinsic Image Disentangling,"  Traditional face editing methods often require a number of sophisticated and
task specific algorithms to be applied one after the other --- a process that
is tedious, fragile, and computationally intensive. In this paper, we propose
an end-to-end generative adversarial network that infers a face-specific
disentangled representation of intrinsic face properties, including shape (i.e.
normals), albedo, and lighting, and an alpha matte. We show that this network
can be trained on ""in-the-wild"" images by incorporating an in-network
physically-based image formation module and appropriate loss functions. Our
disentangling latent representation allows for semantically relevant edits,
where one aspect of facial appearance can be manipulated while keeping
orthogonal properties fixed, and we demonstrate its use for a number of facial
editing applications.
",1,0,0,0,0,0
2406,Dynamical patterns in individual trajectories toward extremism,"  Society faces a fundamental global problem of understanding which individuals
are currently developing strong support for some extremist entity such as ISIS
(Islamic State) -- even if they never end up doing anything in the real world.
The importance of online connectivity in developing intent has been confirmed
by recent case-studies of already convicted terrorists. Here we identify
dynamical patterns in the online trajectories that individuals take toward
developing a high level of extremist support -- specifically, for ISIS. Strong
memory effects emerge among individuals whose transition is fastest, and hence
may become 'out of the blue' threats in the real world. A generalization of
diagrammatic expansion theory helps quantify these characteristics, including
the impact of changes in geographical location, and can facilitate prediction
of future risks. By quantifying the trajectories that individuals follow on
their journey toward expressing high levels of pro-ISIS support -- irrespective
of whether they then carry out a real-world attack or not -- our findings can
help move safety debates beyond reliance on static watch-list identifiers such
as ethnic background or immigration status, and/or post-fact interviews with
already-convicted individuals. Given the broad commonality of social media
platforms, our results likely apply quite generally: for example, even on
Telegram where (like Twitter) there is no built-in group feature as in our
study, individuals tend to collectively build and pass through so-called
super-group accounts.
",1,1,0,0,0,0
7487,Revealing patterns in HIV viral load data and classifying patients via a novel machine learning cluster summarization method,"  HIV RNA viral load (VL) is an important outcome variable in studies of HIV
infected persons. There exists only a handful of methods which classify
patients by viral load patterns. Most methods place limits on the use of viral
load measurements, are often specific to a particular study design, and do not
account for complex, temporal variation. To address this issue, we propose a
set of four unambiguous computable characteristics (features) of time-varying
HIV viral load patterns, along with a novel centroid-based classification
algorithm, which we use to classify a population of 1,576 HIV positive clinic
patients into one of five different viral load patterns (clusters) often found
in the literature: durably suppressed viral load (DSVL), sustained low viral
load (SLVL), sustained high viral load (SHVL), high viral load suppression
(HVLS), and rebounding viral load (RVL). The centroid algorithm summarizes
these clusters in terms of their centroids and radii. We show that this allows
new viral load patterns to be assigned pattern membership based on the distance
from the centroid relative to its radius, which we term radial normalization
classification. This method has the benefit of providing an objective and
quantitative method to assign viral load pattern membership with a concise and
interpretable model that aids clinical decision making. This method also
facilitates meta-analyses by providing computably distinct HIV categories.
Finally we propose that this novel centroid algorithm could also be useful in
the areas of cluster comparison for outcomes research and data reduction in
machine learning.
",0,0,0,1,1,0
18969,Stability and Robust Regulation of Passive Linear Systems,"  We study the stability of coupled impedance passive regular linear systems
under power-preserving interconnections. We present new conditions for strong,
exponential, and non-uniform stability of the closed-loop system. We apply the
stability results to the construction of passive error feedback controllers for
robust output tracking and disturbance rejection for strongly stabilizable
passive systems. In the case of nonsmooth reference and disturbance signals we
present conditions for non-uniform rational and logarithmic rates of
convergence of the output. The results are illustrated with examples on
designing controllers for linear wave and heat equations, and on studying the
stability of a system of coupled partial differential equations.
",0,0,1,0,0,0
13220,Divide-and-Conquer Reinforcement Learning,"  Standard model-free deep reinforcement learning (RL) algorithms sample a new
initial state for each trial, allowing them to optimize policies that can
perform well even in highly stochastic environments. However, problems that
exhibit considerable initial state variation typically produce high-variance
gradient estimates for model-free RL, making direct policy or value function
optimization challenging. In this paper, we develop a novel algorithm that
instead partitions the initial state space into ""slices"", and optimizes an
ensemble of policies, each on a different slice. The ensemble is gradually
unified into a single policy that can succeed on the whole state space. This
approach, which we term divide-and-conquer RL, is able to solve complex tasks
where conventional deep RL methods are ineffective. Our results show that
divide-and-conquer RL greatly outperforms conventional policy gradient methods
on challenging grasping, manipulation, and locomotion tasks, and exceeds the
performance of a variety of prior methods. Videos of policies learned by our
algorithm can be viewed at this http URL
",1,0,0,0,0,0
11000,A Bayesian algorithm for detecting identity matches and fraud in image databases,"  A statistical algorithm for categorizing different types of matches and fraud
in image databases is presented. The approach is based on a generative model of
a graph representing images and connections between pairs of identities,
trained using properties of a matching algorithm between images.
",1,0,0,1,0,0
10791,Competing effects of Hund's splitting and symmetry-breaking perturbations on electronic order in Pb$_{1-x}$Sn$_{x}$Te,"  We study the effect of a uniform external magnetization on p-wave
superconductivity on the (001) surface of the crystalline topological
insulator(TCI) Pb$_{1-x}$Sn$_{x}$Te. It was shown by us in an earlier work that
a chiral p-wave finite momentum pairing (FFLO) state can be stabilized in this
system in the presence of weak repulsive interparticle interactions. In
particular, the superconducting instability is very sensitive to the Hund's
interaction in the multiorbital TCI, and no instabilities are found to be
possible for the ""wrong"" sign of the Hund's splitting. Here we show that for a
finite Hund's splitting of interactions, a significant value of the external
magnetization is needed to degrade the surface superconductivity, while in the
absence of the Hund's interaction, an arbitrarily small external magnetization
can destroy the superconductivity. This implies that multiorbital effects in
this system play an important role in stabilizing electronic order on the
surface.
",0,1,0,0,0,0
15467,On realizability of sign patterns by real polynomials,"  The classical Descartes' rule of signs limits the number of positive roots of
a real polynomial in one variable by the number of sign changes in the sequence
of its coefficients. One can ask the question which pairs of nonnegative
integers $(p,n)$, chosen in accordance with this rule and with some other
natural conditions, can be the pairs of numbers of positive and negative roots
of a real polynomial with prescribed signs of the coefficients. The paper
solves this problem for degree $8$ polynomials.
",0,0,1,0,0,0
7178,Oxygen Partial Pressure during Pulsed Laser Deposition: Deterministic Role on Thermodynamic Stability of Atomic Termination Sequence at SrRuO3/BaTiO3 Interface,"  With recent trends on miniaturizing oxide-based devices, the need for
atomic-scale control of surface/interface structures by pulsed laser deposition
(PLD) has increased. In particular, realizing uniform atomic termination at the
surface/interface is highly desirable. However, a lack of understanding on the
surface formation mechanism in PLD has limited a deliberate control of
surface/interface atomic stacking sequences. Here, taking the prototypical
SrRuO3/BaTiO3/SrRuO3 (SRO/BTO/SRO) heterostructure as a model system, we
investigated the formation of different interfacial termination sequences
(BaO-RuO2 or TiO2-SrO) with oxygen partial pressure (PO2) during PLD. We found
that a uniform SrO-TiO2 termination sequence at the SRO/BTO interface can be
achieved by lowering the PO2 to 5 mTorr, regardless of the total background gas
pressure (Ptotal), growth mode, or growth rate. Our results indicate that the
thermodynamic stability of the BTO surface at the low-energy kinetics stage of
PLD can play an important role in surface/interface termination formation. This
work paves the way for realizing termination engineering in functional oxide
heterostructures.
",0,1,0,0,0,0
2767,On permutation-invariance of limit theorems,"  By a classical principle of probability theory, sufficiently thin
subsequences of general sequences of random variables behave like i.i.d.\
sequences. This observation not only explains the remarkable properties of
lacunary trigonometric series, but also provides a powerful tool in many areas
of analysis, such the theory of orthogonal series and Banach space theory. In
contrast to i.i.d.\ sequences, however, the probabilistic structure of lacunary
sequences is not permutation-invariant and the analytic properties of such
sequences can change after rearrangement. In a previous paper we showed that
permutation-invariance of subsequences of the trigonometric system and related
function systems is connected with Diophantine properties of the index
sequence. In this paper we will study permutation-invariance of subsequences of
general r.v.\ sequences.
",0,0,1,0,0,0
899,Biocompatible Writing of Data into DNA,"  A simple DNA-based data storage scheme is demonstrated in which information
is written using ""addressing"" oligonucleotides. In contrast to other methods
that allow arbitrary code to be stored, the resulting DNA is suitable for
downstream enzymatic and biological processing. This capability is crucial for
DNA computers, and may allow for a diverse array of computational operations to
be carried out using this DNA. Although here we use gel-based methods for
information readout, we also propose more advanced methods involving
protein/DNA complexes and atomic force microscopy/nano-pore schemes for data
readout.
",1,1,0,0,0,0
6188,Learning graphs from data: A signal representation perspective,"  The construction of a meaningful graph topology plays a crucial role in the
effective representation, processing, analysis and visualization of structured
data. When a natural choice of the graph is not readily available from the data
sets, it is thus desirable to infer or learn a graph topology from the data. In
this tutorial overview, we survey solutions to the problem of graph learning,
including classical viewpoints from statistics and physics, and more recent
approaches that adopt a graph signal processing (GSP) perspective. We further
emphasize the conceptual similarities and differences between classical and
GSP-based graph inference methods, and highlight the potential advantage of the
latter in a number of theoretical and practical scenarios. We conclude with
several open issues and challenges that are keys to the design of future signal
processing and machine learning algorithms for learning graphs from data.
",1,0,0,1,0,0
9433,A Connection between Feed-Forward Neural Networks and Probabilistic Graphical Models,"  Two of the most popular modelling paradigms in computer vision are
feed-forward neural networks (FFNs) and probabilistic graphical models (GMs).
Various connections between the two have been studied in recent works, such as
e.g. expressing mean-field based inference in a GM as an FFN. This paper
establishes a new connection between FFNs and GMs. Our key observation is that
any FFN implements a certain approximation of a corresponding Bayesian network
(BN). We characterize various benefits of having this connection. In
particular, it results in a new learning algorithm for BNs. We validate the
proposed methods for a classification problem on CIFAR-10 dataset and for
binary image segmentation on Weizmann Horse dataset. We show that statistically
learned BNs improve performance, having at the same time essentially better
generalization capability, than their FFN counterparts.
",1,0,0,1,0,0
11231,Ergodic Theorems for Nonconventional Arrays and an Extension of the Szemeredi Theorem,"  The paper is primarily concerned with the asymptotic behavior as $N\to\infty$
of averages of nonconventional arrays having the form
$N^{-1}\sum_{n=1}^N\prod_{j=1}^\ell T^{P_j(n,N)}f_j$ where $f_j$'s are bounded
measurable functions, $T$ is an invertible measure preserving transformation
and $P_j$'s are polynomials of $n$ and $N$ taking on integer values on
integers. It turns out that when $T$ is weakly mixing and $P_j(n,N)=p_jn+q_jN$
are linear or, more generally, have the form $P_j(n,N)=P_j(n)+Q_j(N)$ for some
integer valued polynomials $P_j$ and $Q_j$ then the above averages converge in
$L^2$ but for general polynomials $P_j$ the $L^2$ convergence can be ensured
even in the case $\ell=1$ only when $T$ is strongly mixing. Studying also
weakly mixing and compact extensions and relying on Furstenberg's structure
theorem we derive an extension of Szemer\' edi's theorem saying that for any
subset of integers $\Lambda$ with positive upper density there exists a subset
$\mathcal N_\Lambda$ of positive integers having uniformly bounded gaps such
that for $N\in\mathcal N_\Lambda$ and at least $\varepsilon N,\,\varepsilon>0$
of $n$'s all numbers $p_jn+q_jN,\, j=1,...,\ell$ belong to $\Lambda$. We obtain
also a version of these results for several commuting transformations which
yields a corresponding extension of the multidimensional Szemer\' edi theorem.
",0,0,1,0,0,0
14732,Resilient Transmission Grid Design: AC Relaxation vs. DC approximation,"  As illustrated in recent years (Superstorm Sandy, the Northeast Ice Storm of
1998, etc.), extreme weather events pose an enormous threat to the electric
power transmission systems and the associated socio-economic systems that
depend on reliable delivery of electric power. Besides inevitable malfunction
of power grid components, deliberate malicious attacks can cause high risks to
the service. These threats motivate the need for approaches and methods that
improve the resilience of power systems. In this paper, we develop a model and
tractable methods for optimizing the upgrade of transmission systems through a
combination of hardening existing components, adding redundant lines, switches,
generators, and FACTS and phase-shifting devices. While many of these
controllable components are included in traditional design (expansion planning)
problems, we uniquely assess their benefits from a resiliency point of view.
More importantly, perhaps, we evaluate the suitability of using
state-of-the-art AC power flow relaxations versus the common DC approximation
in resilience improvement studies. The resiliency model and algorithms are
tested on a modified version of the RTS-96 (single area) system.
",1,0,1,0,0,0
15187,A Systematic Approach to Numerical Dispersion in Maxwell Solvers,"  The finite-difference time-domain (FDTD) method is a well established method
for solving the time evolution of Maxwell's equations. Unfortunately the scheme
introduces numerical dispersion and therefore phase and group velocities which
deviate from the correct values. The solution to Maxwell's equations in more
than one dimension results in non-physical predictions such as numerical
dispersion or numerical Cherenkov radiation emitted by a relativistic electron
beam propagating in vacuum.
Improved solvers, which keep the staggered Yee-type grid for electric and
magnetic fields, generally modify the spatial derivative operator in the
Maxwell-Faraday equation by increasing the computational stencil. These
modified solvers can be characterized by different sets of coefficients,
leading to different dispersion properties. In this work we introduce a norm
function to rewrite the choice of coefficients into a minimization problem. We
solve this problem numerically and show that the minimization procedure leads
to phase and group velocities that are considerably closer to $c$ as compared
to schemes with manually set coefficients available in the literature.
Depending on a specific problem at hand (e.g. electron beam propagation in
plasma, high-order harmonic generation from plasma surfaces, etc), the norm
function can be chosen accordingly, for example, to minimize the numerical
dispersion in a certain given propagation direction. Particle-in-cell
simulations of an electron beam propagating in vacuum using our solver are
provided.
",0,1,0,0,0,0
2852,Intrinsic entropies of log-concave distributions,"  The entropy of a random variable is well-known to equal the exponential
growth rate of the volumes of its typical sets. In this paper, we show that for
any log-concave random variable $X$, the sequence of the $\lfloor n\theta
\rfloor^{\text{th}}$ intrinsic volumes of the typical sets of $X$ in dimensions
$n \geq 1$ grows exponentially with a well-defined rate. We denote this rate by
$h_X(\theta)$, and call it the $\theta^{\text{th}}$ intrinsic entropy of $X$.
We show that $h_X(\theta)$ is a continuous function of $\theta$ over the range
$[0,1]$, thereby providing a smooth interpolation between the values 0 and
$h(X)$ at the endpoints 0 and 1, respectively.
",1,0,0,0,0,0
1454,Spatially distributed multipartite entanglement enables Einstein-Podolsky-Rosen steering of atomic clouds,"  A key resource for distributed quantum-enhanced protocols is entanglement
between spatially separated modes. Yet, the robust generation and detection of
nonlocal entanglement between spatially separated regions of an ultracold
atomic system remains a challenge. Here, we use spin mixing in a tightly
confined Bose-Einstein condensate to generate an entangled state of
indistinguishable particles in a single spatial mode. We show experimentally
that this local entanglement can be spatially distributed by self-similar
expansion of the atomic cloud. Spatially resolved spin read-out is used to
reveal a particularly strong form of quantum correlations known as
Einstein-Podolsky-Rosen steering between distinct parts of the expanded cloud.
Based on the strength of Einstein-Podolsky-Rosen steering we construct a
witness, which testifies up to genuine five-partite entanglement.
",0,1,0,0,0,0
4640,Fixing and almost fixing a planar convex body,"  A set of points a 1 ,. .. , a n fixes a planar convex body K if the points
are on bdK, the boundary of K, and if any small move of K brings some point of
the set in intK, the interior of K. The points a 1 ,. .. , a n $\in$ bdK almost
fix K if, for any neighbourhoods V i of a i (i = 1,. .. , n), there are pairs
of points a i , a i $\in$ V i $\cap$ bdK such that a 1 , a 1 ,. .. , a n fix K.
This note compares several definitions of these notions and gives first order
conditions for a 1 ,. .. , a n $\in$ bdK to fix, and to almost fix, K.
",0,0,1,0,0,0
2316,One pixel attack for fooling deep neural networks,"  Recent research has revealed that the output of Deep Neural Networks (DNN)
can be easily altered by adding relatively small perturbations to the input
vector. In this paper, we analyze an attack in an extremely limited scenario
where only one pixel can be modified. For that we propose a novel method for
generating one-pixel adversarial perturbations based on differential
evolution(DE). It requires less adversarial information(a black-box attack) and
can fool more types of networks due to the inherent features of DE. The results
show that 68.36% of the natural images in CIFAR-10 test dataset and 41.22% of
the ImageNet (ILSVRC 2012) validation images can be perturbed to at least one
target class by modifying just one pixel with 73.22% and 5.52% confidence on
average. Thus, the proposed attack explores a different take on adversarial
machine learning in an extreme limited scenario, showing that current DNNs are
also vulnerable to such low dimension attacks. Besides, we also illustrate an
important application of DE (or broadly speaking, evolutionary computation) in
the domain of adversarial machine learning: creating tools that can effectively
generate low-cost adversarial attacks against neural networks for evaluating
robustness. The code is available on:
this https URL
",1,0,0,1,0,0
17865,Emotion in Reinforcement Learning Agents and Robots: A Survey,"  This article provides the first survey of computational models of emotion in
reinforcement learning (RL) agents. The survey focuses on agent/robot emotions,
and mostly ignores human user emotions. Emotions are recognized as functional
in decision-making by influencing motivation and action selection. Therefore,
computational emotion models are usually grounded in the agent's decision
making architecture, of which RL is an important subclass. Studying emotions in
RL-based agents is useful for three research fields. For machine learning (ML)
researchers, emotion models may improve learning efficiency. For the
interactive ML and human-robot interaction (HRI) community, emotions can
communicate state and enhance user investment. Lastly, it allows affective
modelling (AM) researchers to investigate their emotion theories in a
successful AI agent class. This survey provides background on emotion theory
and RL. It systematically addresses 1) from what underlying dimensions (e.g.,
homeostasis, appraisal) emotions can be derived and how these can be modelled
in RL-agents, 2) what types of emotions have been derived from these
dimensions, and 3) how these emotions may either influence the learning
efficiency of the agent or be useful as social signals. We also systematically
compare evaluation criteria, and draw connections to important RL sub-domains
like (intrinsic) motivation and model-based RL. In short, this survey provides
both a practical overview for engineers wanting to implement emotions in their
RL agents, and identifies challenges and directions for future emotion-RL
research.
",1,0,0,1,0,0
20541,Dispersive optical detection of magnetic Feshbach resonances in ultracold gases,"  Magnetically tunable Feshbach resonances in ultracold atomic systems are
chiefly identified and characterized through time consuming atom loss
spectroscopy. We describe an off-resonant dispersive optical probing technique
to rapidly locate Feshbach resonances and demonstrate the method by locating
four resonances of $^{87}$Rb, between the $|\rm{F} = 1, \rm{m_F}=1 \rangle$ and
$|\rm{F} = 2, \rm{m_F}=0 \rangle$ states. Despite the loss features being
$\lesssim0.1$ G wide, we require only 21 experimental runs to explore a
magnetic field range >18 G, where $1~\rm{G}=10^{-4}$ T. The resonances consist
of two known s-wave features in the vicinity of 9 G and 18 G and two previously
unobserved p-wave features near 5 G and 10 G. We further utilize the dispersive
approach to directly characterize the two-body loss dynamics for each Feshbach
resonance.
",0,1,0,0,0,0
20786,Random Projections For Large-Scale Regression,"  Fitting linear regression models can be computationally very expensive in
large-scale data analysis tasks if the sample size and the number of variables
are very large. Random projections are extensively used as a dimension
reduction tool in machine learning and statistics. We discuss the applications
of random projections in linear regression problems, developed to decrease
computational costs, and give an overview of the theoretical guarantees of the
generalization error. It can be shown that the combination of random
projections with least squares regression leads to similar recovery as ridge
regression and principal component regression. We also discuss possible
improvements when averaging over multiple random projections, an approach that
lends itself easily to parallel implementation.
",0,0,1,1,0,0
17426,A taxonomy of learning dynamics in 2 x 2 games,"  Learning would be a convincing method to achieve coordination on an
equilibrium. But does learning converge, and to what? We answer this question
in generic 2-player, 2-strategy games, using Experience-Weighted Attraction
(EWA), which encompasses many extensively studied learning algorithms. We
exhaustively characterize the parameter space of EWA learning, for any payoff
matrix, and we understand the generic properties that imply convergent or
non-convergent behaviour in 2 x 2 games.
Irrational choice and lack of incentives imply convergence to a mixed
strategy in the centre of the strategy simplex, possibly far from the Nash
Equilibrium (NE). In the opposite limit, in which the players quickly modify
their strategies, the behaviour depends on the payoff matrix: (i) a strong
discrepancy between the pure strategies describes dominance-solvable games,
which show convergence to a unique fixed point close to the NE; (ii) a
preference towards profiles of strategies along the main diagonal describes
coordination games, with multiple stable fixed points corresponding to the NE;
(iii) a cycle of best responses defines discoordination games, which commonly
yield limit cycles or low-dimensional chaos.
While it is well known that mixed strategy equilibria may be unstable, our
approach is novel from several perspectives: we fully analyse EWA and provide
explicit thresholds that define the onset of instability; we find an emerging
taxonomy of the learning dynamics, without focusing on specific classes of
games ex-ante; we show that chaos can occur even in the simplest games; we make
a precise theoretical prediction that can be tested against data on
experimental learning of discoordination games.
",0,1,0,0,0,0
2506,Almost complex structures on connected sums of complex projective spaces,"  We show that the m-fold connected sum $m\#\mathbb{C}\mathbb{P}^{2n}$ admits
an almost complex structure if and only if m is odd.
",0,0,1,0,0,0
8591,Along the sun-drenched roadside: On the interplay between urban street orientation entropy and the buildings' solar potential,"  We explore the relation between urban road network characteristics
particularly circuitry, street orientation entropy and the city's topography on
the one hand and the building's orientation entropy on the other in order to
quantify their effect on the city's solar potential. These statistical measures
of the road network reveal the interplay between the built environment's design
and its sustainability.
",0,1,0,0,0,0
11840,Skeleton-based Action Recognition of People Handling Objects,"  In visual surveillance systems, it is necessary to recognize the behavior of
people handling objects such as a phone, a cup, or a plastic bag. In this
paper, to address this problem, we propose a new framework for recognizing
object-related human actions by graph convolutional networks using human and
object poses. In this framework, we construct skeletal graphs of reliable human
poses by selectively sampling the informative frames in a video, which include
human joints with high confidence scores obtained in pose estimation. The
skeletal graphs generated from the sampled frames represent human poses related
to the object position in both the spatial and temporal domains, and these
graphs are used as inputs to the graph convolutional networks. Through
experiments over an open benchmark and our own data sets, we verify the
validity of our framework in that our method outperforms the state-of-the-art
method for skeleton-based action recognition.
",1,0,0,0,0,0
12282,Near-Infrared Knots and Dense Fe Ejecta in the Cassiopeia A Supernova Remnant,"  We report the results of broadband (0.95--2.46 $\mu$m) near-infrared
spectroscopic observations of the Cassiopeia A supernova remnant. Using a
clump-finding algorithm in two-dimensional dispersed images, we identify 63
""knots"" from eight slit positions and derive their spectroscopic properties.
All of the knots emit [Fe II] lines together with other ionic forbidden lines
of heavy elements, and some of them also emit H and He lines. We identify 46
emission line features in total from the 63 knots and measure their fluxes and
radial velocities. The results of our analyses of the emission line features
based on principal component analysis show that the knots can be classified
into three groups: (1) He-rich, (2) S-rich, and (3) Fe-rich knots. The He-rich
knots have relatively small, $\lesssim 200~{\rm km~s}^{-1}$, line-of-sight
speeds and radiate strong He I and [Fe II] lines resembling closely optical
quasi-stationary flocculi of circumstellar medium, while the S-rich knots show
strong lines from O-burning material with large radial velocities up to $\sim
2000~{\rm km~s}^{-1}$ indicating that they are supernova ejecta material known
as fast-moving knots. The Fe-rich knots also have large radial velocities but
show no lines from O-burning material. We discuss the origin of the Fe-rich
knots and conclude that they are most likely ""pure"" Fe ejecta synthesized in
the innermost region during the supernova explosion. The comparison of [Fe II]
images with other waveband images shows that these dense Fe ejecta are mainly
distributed along the southwestern shell just outside the unshocked $^{44}$Ti
in the interior, supporting the presence of unshocked Fe associated with
$^{44}$Ti.
",0,1,0,0,0,0
1093,EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial Examples,"  Recent studies have highlighted the vulnerability of deep neural networks
(DNNs) to adversarial examples - a visually indistinguishable adversarial image
can easily be crafted to cause a well-trained model to misclassify. Existing
methods for crafting adversarial examples are based on $L_2$ and $L_\infty$
distortion metrics. However, despite the fact that $L_1$ distortion accounts
for the total variation and encourages sparsity in the perturbation, little has
been developed for crafting $L_1$-based adversarial examples. In this paper, we
formulate the process of attacking DNNs via adversarial examples as an
elastic-net regularized optimization problem. Our elastic-net attacks to DNNs
(EAD) feature $L_1$-oriented adversarial examples and include the
state-of-the-art $L_2$ attack as a special case. Experimental results on MNIST,
CIFAR10 and ImageNet show that EAD can yield a distinct set of adversarial
examples with small $L_1$ distortion and attains similar attack performance to
the state-of-the-art methods in different attack scenarios. More importantly,
EAD leads to improved attack transferability and complements adversarial
training for DNNs, suggesting novel insights on leveraging $L_1$ distortion in
adversarial machine learning and security implications of DNNs.
",1,0,0,1,0,0
4490,Multi-Period Flexibility Forecast for Low Voltage Prosumers,"  Near-future electric distribution grids operation will have to rely on
demand-side flexibility, both by implementation of demand response strategies
and by taking advantage of the intelligent management of increasingly common
small-scale energy storage. The Home energy management system (HEMS), installed
at low voltage residential clients, will play a crucial role on the flexibility
provision to both system operators and market players like aggregators.
Modeling and forecasting multi-period flexibility from residential prosumers,
such as battery storage and electric water heater, while complying with
internal constraints (comfort levels, data privacy) and uncertainty is a
complex task. This papers describes a computational method that is capable of
efficiently learn and define the feasibility flexibility space from
controllable resources connected to a HEMS. An Evolutionary Particle Swarm
Optimization (EPSO) algorithm is adopted and reshaped to derive a set of
feasible temporal trajectories for the residential net-load, considering
storage, flexible appliances, and predefined costumer preferences, as well as
load and photovoltaic (PV) forecast uncertainty. A support vector data
description (SVDD) algorithm is used to build models capable of classifying
feasible and non-feasible HEMS operating trajectories upon request from an
optimization/control algorithm operated by a DSO or market player.
",1,0,0,0,0,0
8110,The spin-Brauer diagram algebra,"  We investigate the spin-Brauer diagram algebra, denoted ${\bf SB}_n(\delta)$,
that arises from studying an analogous form of Schur-Weyl duality for the
action of the pin group on ${\bf V}^{\otimes n} \otimes \Delta$. Here ${\bf V}$
is the standard $N$-dimensional complex representation of ${\bf Pin}(N)$ and
$\Delta$ is the spin representation. When $\delta = N$ is a positive integer,
we define a surjective map ${\bf SB}_n(N) \twoheadrightarrow {\rm End}_{{\bf
Pin}(N)}({\bf V}^{\otimes n} \otimes \Delta)$ and show it is an isomorphism for
$N \geq 2n$. We show ${\bf SB}_n(\delta)$ is a cellular algebra and use
cellularity to characterize its irreducible representations.
",0,0,1,0,0,0
10133,Feature Engineering for Predictive Modeling using Reinforcement Learning,"  Feature engineering is a crucial step in the process of predictive modeling.
It involves the transformation of given feature space, typically using
mathematical functions, with the objective of reducing the modeling error for a
given target. However, there is no well-defined basis for performing effective
feature engineering. It involves domain knowledge, intuition, and most of all,
a lengthy process of trial and error. The human attention involved in
overseeing this process significantly influences the cost of model generation.
We present a new framework to automate feature engineering. It is based on
performance driven exploration of a transformation graph, which systematically
and compactly enumerates the space of given options. A highly efficient
exploration strategy is derived through reinforcement learning on past
examples.
",1,0,0,1,0,0
6199,Multi-Label Learning with Global and Local Label Correlation,"  It is well-known that exploiting label correlations is important to
multi-label learning. Existing approaches either assume that the label
correlations are global and shared by all instances; or that the label
correlations are local and shared only by a data subset. In fact, in the
real-world applications, both cases may occur that some label correlations are
globally applicable and some are shared only in a local group of instances.
Moreover, it is also a usual case that only partial labels are observed, which
makes the exploitation of the label correlations much more difficult. That is,
it is hard to estimate the label correlations when many labels are absent. In
this paper, we propose a new multi-label approach GLOCAL dealing with both the
full-label and the missing-label cases, exploiting global and local label
correlations simultaneously, through learning a latent label representation and
optimizing label manifolds. The extensive experimental studies validate the
effectiveness of our approach on both full-label and missing-label data.
",1,0,0,0,0,0
9230,Smith-Purcell Radiation,"  The simplest model of the magnetized infinitely thin electron beam is
considered. The basic equations that describe the periodic solutions for a
self-consistent system of a couple of Maxwell equations and equations for the
medium are obtained.
",0,1,0,0,0,0
11079,Transfer Learning for Speech Recognition on a Budget,"  End-to-end training of automated speech recognition (ASR) systems requires
massive data and compute resources. We explore transfer learning based on model
adaptation as an approach for training ASR models under constrained GPU memory,
throughput and training data. We conduct several systematic experiments
adapting a Wav2Letter convolutional neural network originally trained for
English ASR to the German language. We show that this technique allows faster
training on consumer-grade resources while requiring less training data in
order to achieve the same accuracy, thereby lowering the cost of training ASR
models in other languages. Model introspection revealed that small adaptations
to the network's weights were sufficient for good performance, especially for
inner layers.
",1,0,0,1,0,0
17169,Penalized Maximum Tangent Likelihood Estimation and Robust Variable Selection,"  We introduce a new class of mean regression estimators -- penalized maximum
tangent likelihood estimation -- for high-dimensional regression estimation and
variable selection. We first explain the motivations for the key ingredient,
maximum tangent likelihood estimation (MTE), and establish its asymptotic
properties. We further propose a penalized MTE for variable selection and show
that it is $\sqrt{n}$-consistent, enjoys the oracle property. The proposed
class of estimators consists penalized $\ell_2$ distance, penalized exponential
squared loss, penalized least trimmed square and penalized least square as
special cases and can be regarded as a mixture of minimum Kullback-Leibler
distance estimation and minimum $\ell_2$ distance estimation. Furthermore, we
consider the proposed class of estimators under the high-dimensional setting
when the number of variables $d$ can grow exponentially with the sample size
$n$, and show that the entire class of estimators (including the aforementioned
special cases) can achieve the optimal rate of convergence in the order of
$\sqrt{\ln(d)/n}$. Finally, simulation studies and real data analysis
demonstrate the advantages of the penalized MTE.
",0,0,0,1,0,0
14921,"Toda maps, cocycles, and canonical systems","  I present a discussion of the hierarchy of Toda flows that gives center stage
to the associated cocycles and the maps they induce on the $m$ functions. In
the second part, these ideas are then applied to canonical systems; an
important feature of this discussion will be my proposal that the role of the
shift on Jacobi matrices should now be taken over by the more general class of
twisted shifts.
",0,0,1,0,0,0
8374,RAFP-Pred: Robust Prediction of Antifreeze Proteins using Localized Analysis of n-Peptide Compositions,"  In extreme cold weather, living organisms produce Antifreeze Proteins (AFPs)
to counter the otherwise lethal intracellular formation of ice. Structures and
sequences of various AFPs exhibit a high degree of heterogeneity, consequently
the prediction of the AFPs is considered to be a challenging task. In this
research, we propose to handle this arduous manifold learning task using the
notion of localized processing. In particular an AFP sequence is segmented into
two sub-segments each of which is analyzed for amino acid and di-peptide
compositions. We propose to use only the most significant features using the
concept of information gain (IG) followed by a random forest classification
approach. The proposed RAFP-Pred achieved an excellent performance on a number
of standard datasets. We report a high Youden's index
(sensitivity+specificity-1) value of 0.75 on the standard independent test data
set outperforming the AFP-PseAAC, AFP\_PSSM, AFP-Pred and iAFP by a margin of
0.05, 0.06, 0.14 and 0.68 respectively. The verification rate on the UniProKB
dataset is found to be 83.19\% which is substantially superior to the 57.18\%
reported for the iAFP method.
",0,0,0,0,1,0
2266,Sample-Derived Disjunctive Rules for Secure Power System Operation,"  Machine learning techniques have been used in the past using Monte Carlo
samples to construct predictors of the dynamic stability of power systems. In
this paper we move beyond the task of prediction and propose a comprehensive
approach to use predictors, such as Decision Trees (DT), within a standard
optimization framework for pre- and post-fault control purposes. In particular,
we present a generalizable method for embedding rules derived from DTs in an
operation decision-making model. We begin by pointing out the specific
challenges entailed when moving from a prediction to a control framework. We
proceed with introducing the solution strategy based on generalized disjunctive
programming (GDP) as well as a two-step search method for identifying optimal
hyper-parameters for balancing cost and control accuracy. We showcase how the
proposed approach constructs security proxies that cover multiple contingencies
while facing high-dimensional uncertainty with respect to operating conditions
with the use of a case study on the IEEE 39-bus system. The method is shown to
achieve efficient system control at a marginal increase in system price
compared to an oracle model.
",0,0,0,1,0,0
18152,Hierarchical VampPrior Variational Fair Auto-Encoder,"  Decision making is a process that is extremely prone to different biases. In
this paper we consider learning fair representations that aim at removing
nuisance (sensitive) information from the decision process. For this purpose,
we propose to use deep generative modeling and adapt a hierarchical Variational
Auto-Encoder to learn these fair representations. Moreover, we utilize the
mutual information as a useful regularizer for enforcing fairness of a
representation. In experiments on two benchmark datasets and two scenarios
where the sensitive variables are fully and partially observable, we show that
the proposed approach either outperforms or performs on par with the current
best model.
",0,0,0,1,0,0
20825,"Deep Learning Scaling is Predictable, Empirically","  Deep learning (DL) creates impactful advances following a virtuous recipe:
model architecture search, creating large training data sets, and scaling
computation. It is widely believed that growing training sets and models should
improve accuracy and result in better products. As DL application domains grow,
we would like a deeper understanding of the relationships between training set
size, computational scale, and model accuracy improvements to advance the
state-of-the-art.
This paper presents a large scale empirical characterization of
generalization error and model size growth as training sets grow. We introduce
a methodology for this measurement and test four machine learning domains:
machine translation, language modeling, image processing, and speech
recognition. Our empirical results show power-law generalization error scaling
across a breadth of factors, resulting in power-law exponents---the ""steepness""
of the learning curve---yet to be explained by theoretical work. Further, model
improvements only shift the error but do not appear to affect the power-law
exponent. We also show that model size scales sublinearly with data size. These
scaling relationships have significant implications on deep learning research,
practice, and systems. They can assist model debugging, setting accuracy
targets, and decisions about data set growth. They can also guide computing
system design and underscore the importance of continued computational scaling.
",1,0,0,1,0,0
5509,An adelic arithmeticity theorem for lattices in products,"  We prove that, under mild assumptions, a lattice in a product of semi-simple
Lie group and a totally disconnected locally compact group is, in a certain
sense, arithmetic. We do not assume the lattice to be finitely generated or the
ambient group to be compactly generated.
",0,0,1,0,0,0
6294,Colored Image Encryption and Decryption Using Chaotic Lorenz System and DCT2,"  In this paper, a scheme for the encryption and decryption of colored images
by using the Lorenz system and the discrete cosine transform in two dimensions
(DCT2) is proposed. Although chaos is random, it has deterministic features
that can be used for encryption; further, the same sequences can be produced at
the transmitter and receiver under the same initial conditions. Another
property of DCT2 is that the energy is concentrated in some elements of the
coefficients. These two properties are used to efficiently encrypt and recover
the image at the receiver by using three different keys with three different
predefined number of shifts for each instance of key usage. Simulation results
and statistical analysis show that the scheme high performance in weakening the
correlation between the pixels of the image that resulted from the inverse of
highest energy values of DCT2 that form 99.9 % of the energy as well as those
of the difference image.
",1,0,0,0,0,0
10511,Multi-parametric sensitivity analysis of the band structure for tetrachiral acoustic metamaterials,"  Tetrachiral materials are characterized by a cellular microstructure made by
a periodic pattern of stiff rings and flexible ligaments. Their mechanical
behaviour can be described by a planar lattice of rigid massive bodies and
elastic massless beams. The periodic cell dynamics is governed by a monoatomic
structural model, conveniently reduced to the only active degrees-of-freedom.
The paper presents an explicit parametric description of the band structure
governing the free propagation of elastic waves. By virtue of multiparametric
perturbation techniques, sensitivity analyses are performed to achieve
analytical asymptotic approximation of the dispersion functions. The parametric
conditions for the existence of full band gaps in the low-frequency range are
established. Furthermore, the band gap amplitude is analytically assessed in
the admissible parameter range. In tetrachiral acoustic metamaterials, stop
bands can be opened by the introduction of intra-ring resonators. Perturbation
methods can efficiently deal with the consequent enlargement of the mechanical
parameter space. Indeed high-accuracy parametric approximations are achieved
for the band structure, enriched by the new optical branches related to the
resonator frequencies. In particular, target stop bands in the metamaterial
spectrum are analytically designed through the asymptotic solution of inverse
spectral problems.
",0,1,0,0,0,0
12750,The Global Optimization Geometry of Low-Rank Matrix Optimization,"  This paper considers general rank-constrained optimization problems that
minimize a general objective function $f(X)$ over the set of rectangular
$n\times m$ matrices that have rank at most $r$. To tackle the rank constraint
and also to reduce the computational burden, we factorize $X$ into $UV^T$ where
$U$ and $V$ are $n\times r$ and $m\times r$ matrices, respectively, and then
optimize over the small matrices $U$ and $V$. We characterize the global
optimization geometry of the nonconvex factored problem and show that the
corresponding objective function satisfies the robust strict saddle property as
long as the original objective function $f$ satisfies restricted strong
convexity and smoothness properties, ensuring global convergence of many local
search algorithms (such as noisy gradient descent) in polynomial time for
solving the factored problem. We also provide a comprehensive analysis for the
optimization geometry of a matrix factorization problem where we aim to find
$n\times r$ and $m\times r$ matrices $U$ and $V$ such that $UV^T$ approximates
a given matrix $X^\star$. Aside from the robust strict saddle property, we show
that the objective function of the matrix factorization problem has no spurious
local minima and obeys the strict saddle property not only for the
exact-parameterization case where $rank(X^\star) = r$, but also for the
over-parameterization case where $rank(X^\star) < r$ and the
under-parameterization case where $rank(X^\star) > r$. These geometric
properties imply that a number of iterative optimization algorithms (such as
gradient descent) converge to a global solution with random initialization.
",1,0,1,0,0,0
5622,Optimal Learning for Sequential Decision Making for Expensive Cost Functions with Stochastic Binary Feedbacks,"  We consider the problem of sequentially making decisions that are rewarded by
""successes"" and ""failures"" which can be predicted through an unknown
relationship that depends on a partially controllable vector of attributes for
each instance. The learner takes an active role in selecting samples from the
instance pool. The goal is to maximize the probability of success in either
offline (training) or online (testing) phases. Our problem is motivated by
real-world applications where observations are time-consuming and/or expensive.
We develop a knowledge gradient policy using an online Bayesian linear
classifier to guide the experiment by maximizing the expected value of
information of labeling each alternative. We provide a finite-time analysis of
the estimated error and show that the maximum likelihood estimator based
produced by the KG policy is consistent and asymptotically normal. We also show
that the knowledge gradient policy is asymptotically optimal in an offline
setting. This work further extends the knowledge gradient to the setting of
contextual bandits. We report the results of a series of experiments that
demonstrate its efficiency.
",0,0,0,1,0,0
7745,Efficient Antihydrogen Detection in Antimatter Physics by Deep Learning,"  Antihydrogen is at the forefront of antimatter research at the CERN
Antiproton Decelerator. Experiments aiming to test the fundamental CPT symmetry
and antigravity effects require the efficient detection of antihydrogen
annihilation events, which is performed using highly granular tracking
detectors installed around an antimatter trap. Improving the efficiency of the
antihydrogen annihilation detection plays a central role in the final
sensitivity of the experiments. We propose deep learning as a novel technique
to analyze antihydrogen annihilation data, and compare its performance with a
traditional track and vertex reconstruction method. We report that the deep
learning approach yields significant improvement, tripling event coverage while
simultaneously improving performance by over 5% in terms of Area Under Curve
(AUC).
",1,1,0,0,0,0
4339,Detecting singular weak-dissipation limit for flutter onset in reversible systems,"  A `flutter machine' is introduced for the investigation of a singular
interface between the classical and reversible Hopf bifurcations that is
theoretically predicted to be generic in nonconservative reversible systems
with vanishing dissipation. In particular, such a singular interface exists for
the Pflüger viscoelastic column moving in a resistive medium, which is proven
by means of the perturbation theory of multiple eigenvalues with the Jordan
block. The laboratory setup, consisting of a cantilevered viscoelastic rod
loaded by a positional force with non-zero curl produced by dry friction,
demonstrates high sensitivity of the classical Hopf bifurcation onset {to the
ratio between} the weak air drag and Kelvin-Voigt damping in the Pflüger
column. Thus, the Whitney umbrella singularity is experimentally confirmed,
responsible for discontinuities accompanying dissipation-induced instabilities
in a broad range of physical contexts.
",0,1,0,0,0,0
17814,Domain Adaptation for Infection Prediction from Symptoms Based on Data from Different Study Designs and Contexts,"  Acute respiratory infections have epidemic and pandemic potential and thus
are being studied worldwide, albeit in many different contexts and study
formats. Predicting infection from symptom data is critical, though using
symptom data from varied studies in aggregate is challenging because the data
is collected in different ways. Accordingly, different symptom profiles could
be more predictive in certain studies, or even symptoms of the same name could
have different meanings in different contexts. We assess state-of-the-art
transfer learning methods for improving prediction of infection from symptom
data in multiple types of health care data ranging from clinical, to home-visit
as well as crowdsourced studies. We show interesting characteristics regarding
six different study types and their feature domains. Further, we demonstrate
that it is possible to use data collected from one study to predict infection
in another, at close to or better than using a single dataset for prediction on
itself. We also investigate in which conditions specific transfer learning and
domain adaptation methods may perform better on symptom data. This work has the
potential for broad applicability as we show how it is possible to transfer
learning from one public health study design to another, and data collected
from one study may be used for prediction of labels for another, even collected
through different study designs, populations and contexts.
",0,0,0,1,1,0
6162,New face of multifractality: Multi-branched left-sidedness and phase transitions in multifractality of interevent times,"  We develop an extended multifractal analysis based on the Legendre-Fenchel
transform rather than the routinely used Legendre transform. We apply this
analysis to studying time series consisting of inter-event times. As a result,
we discern the non-monotonic behavior of the generalized Hurst exponent - the
fundamental exponent studied by us - and hence a multi-branched left-sided
spectrum of dimensions. This kind of multifractality is a direct result of the
non-monotonic behavior of the generalized Hurst exponent and is not caused by
non-analytic behavior as has been previously suggested. We examine the main
thermodynamic consequences of the existence of this type of multifractality
related to the thermal stable, metastable, and unstable phases within a
hierarchy of fluctuations, and also to the first and second order phase
transitions between them.
",0,0,0,0,0,1
2248,Advanced Bayesian Multilevel Modeling with the R Package brms,"  The brms package allows R users to easily specify a wide range of Bayesian
single-level and multilevel models, which are fitted with the probabilistic
programming language Stan behind the scenes. Several response distributions are
supported, of which all parameters (e.g., location, scale, and shape) can be
predicted at the same time thus allowing for distributional regression.
Non-linear relationships may be specified using non-linear predictor terms or
semi-parametric approaches such as splines or Gaussian processes. To make all
of these modeling options possible in a multilevel framework, brms provides an
intuitive and powerful formula syntax, which extends the well known formula
syntax of lme4. The purpose of the present paper is to introduce this syntax in
detail and to demonstrate its usefulness with four examples, each showing other
relevant aspects of the syntax.
",0,0,0,1,0,0
13433,When confidence and competence collide: Effects on online decision-making discussions,"  Group discussions are a way for individuals to exchange ideas and arguments
in order to reach better decisions than they could on their own. One of the
premises of productive discussions is that better solutions will prevail, and
that the idea selection process is mediated by the (relative) competence of the
individuals involved. However, since people may not know their actual
competence on a new task, their behavior is influenced by their self-estimated
competence --- that is, their confidence --- which can be misaligned with their
actual competence.
Our goal in this work is to understand the effects of confidence-competence
misalignment on the dynamics and outcomes of discussions. To this end, we
design a large-scale natural setting, in the form of an online team-based
geography game, that allows us to disentangle confidence from competence and
thus separate their effects.
We find that in task-oriented discussions, the more-confident individuals
have a larger impact on the group's decisions even when these individuals are
at the same level of competence as their teammates. Furthermore, this
unjustified role of confidence in the decision-making process often leads teams
to under-perform. We explore this phenomenon by investigating the effects of
confidence on conversational dynamics.
",1,1,0,0,0,0
2753,Nesterov's Acceleration For Approximate Newton,"  Optimization plays a key role in machine learning. Recently, stochastic
second-order methods have attracted much attention due to their low
computational cost in each iteration. However, these algorithms might perform
poorly especially if it is hard to approximate the Hessian well and
efficiently. As far as we know, there is no effective way to handle this
problem. In this paper, we resort to Nesterov's acceleration technique to
improve the convergence performance of a class of second-order methods called
approximate Newton. We give a theoretical analysis that Nesterov's acceleration
technique can improve the convergence performance for approximate Newton just
like for first-order methods. We accordingly propose an accelerated regularized
sub-sampled Newton. Our accelerated algorithm performs much better than the
original regularized sub-sampled Newton in experiments, which validates our
theory empirically. Besides, the accelerated regularized sub-sampled Newton has
good performance comparable to or even better than classical algorithms.
",1,0,0,0,0,0
10156,Radially resolved simulations of collapsing pebble clouds in protoplanetary discs,"  We study the collapse of pebble clouds with a statistical model to find the
internal structure of comet-sized planetesimals. Pebble-pebble collisions occur
during the collapse and the outcome of these collisions affect the resulting
structure of the planetesimal. We expand our previous models by allowing the
individual pebble sub-clouds to contract at different rates and by including
the effect of gas drag on the contraction speed and in energy dissipation. Our
results yield comets that are porous pebble-piles with particle sizes varying
with depth. In the surface layers there is a mixture of primordial pebbles and
pebble fragments. The interior, on the other hand, consists only of primordial
pebbles with a narrower size distribution, yielding higher porosity there. Our
results imply that the gas in the protoplanetary disc plays an important role
in determining the radial distribution of pebble sizes and porosity inside
planetesimals.
",0,1,0,0,0,0
4548,Belitskii's canonical forms of linear dynamical systems,"  In the note, all indecomposable canonical forms of linear systems with
dimension less than or equal to $4$ are determined based on Belitskii's
algorithm. As an application, an effective way to calculate dimensions of
equivalence classes of linear systems is given by using Belitskii's canonical
forms.
",0,0,1,0,0,0
11019,Virtual plane-wave imaging via Marchenko redatuming,"  Marchenko redatuming is a novel scheme used to retrieve up- and down-going
Green's functions in an unknown medium. Marchenko equations are based on
reciprocity theorems and are derived on the assumption of the existence of so
called focusing functions, i.e. functions which exhibit time-space focusing
properties once injected in the subsurface. In contrast to interferometry but
similarly to standard migration methods, Marchenko redatuming only requires an
estimate of the direct wave from the virtual source (or to the virtual
receiver), illumination from only one side of the medium, and no physical
sources (or receivers) inside the medium. In this contribution we consider a
different time-focusing condition within the frame of Marchenko redatuming and
show how this can lead to the retrieval of virtual plane-wave responses, thus
allowing multiple-free imaging using only a 1 dimensional sampling of the
targeted model. The potential of the new method is demonstrated on a 2D
synthetic model.
",0,1,0,0,0,0
16205,Supervised Speech Separation Based on Deep Learning: An Overview,"  Speech separation is the task of separating target speech from background
interference. Traditionally, speech separation is studied as a signal
processing problem. A more recent approach formulates speech separation as a
supervised learning problem, where the discriminative patterns of speech,
speakers, and background noise are learned from training data. Over the past
decade, many supervised separation algorithms have been put forward. In
particular, the recent introduction of deep learning to supervised speech
separation has dramatically accelerated progress and boosted separation
performance. This article provides a comprehensive overview of the research on
deep learning based supervised speech separation in the last several years. We
first introduce the background of speech separation and the formulation of
supervised separation. Then we discuss three main components of supervised
separation: learning machines, training targets, and acoustic features. Much of
the overview is on separation algorithms where we review monaural methods,
including speech enhancement (speech-nonspeech separation), speaker separation
(multi-talker separation), and speech dereverberation, as well as
multi-microphone techniques. The important issue of generalization, unique to
supervised learning, is discussed. This overview provides a historical
perspective on how advances are made. In addition, we discuss a number of
conceptual issues, including what constitutes the target source.
",1,0,0,0,0,0
16817,Transforming Sensor Data to the Image Domain for Deep Learning - an Application to Footstep Detection,"  Convolutional Neural Networks (CNNs) have become the state-of-the-art in
various computer vision tasks, but they are still premature for most sensor
data, especially in pervasive and wearable computing. A major reason for this
is the limited amount of annotated training data. In this paper, we propose the
idea of leveraging the discriminative power of pre-trained deep CNNs on
2-dimensional sensor data by transforming the sensor modality to the visual
domain. By three proposed strategies, 2D sensor output is converted into
pressure distribution imageries. Then we utilize a pre-trained CNN for transfer
learning on the converted imagery data. We evaluate our method on a gait
dataset of floor surface pressure mapping. We obtain a classification accuracy
of 87.66%, which outperforms the conventional machine learning methods by over
10%.
",1,0,0,0,0,0
19700,Boötes-HiZELS: an optical to near-infrared survey of emission-line galaxies at $\bf z=0.4-4.7$,"  We present a sample of $\sim 1000$ emission line galaxies at $z=0.4-4.7$ from
the $\sim0.7$deg$^2$ High-$z$ Emission Line Survey (HiZELS) in the Boötes
field identified with a suite of six narrow-band filters at $\approx 0.4-2.1$
$\mu$m. These galaxies have been selected on their Ly$\alpha$ (73), {\sc [Oii]}
(285), H$\beta$/{\sc [Oiii]} (387) or H$\alpha$ (362) emission-line, and have
been classified with optical to near-infrared colours. A subsample of 98
sources have reliable redshifts from multiple narrow-band (e.g. [O{\sc
ii}]-H$\alpha$) detections and/or spectroscopy. In this survey paper, we
present the observations, selection and catalogs of emitters. We measure number
densities of Ly$\alpha$, [O{\sc ii}], H$\beta$/{\sc [Oiii]} and H$\alpha$ and
confirm strong luminosity evolution in star-forming galaxies from $z\sim0.4$ to
$\sim 5$, in agreement with previous results. To demonstrate the usefulness of
dual-line emitters, we use the sample of dual [O{\sc ii}]-H$\alpha$ emitters to
measure the observed [O{\sc ii}]/H$\alpha$ ratio at $z=1.47$. The observed
[O{\sc ii}]/H$\alpha$ ratio increases significantly from 0.40$\pm0.01$ at
$z=0.1$ to 0.52$\pm0.05$ at $z=1.47$, which we attribute to either decreasing
dust attenuation with redshift, or due to a bias in the (typically)
fiber-measurements in the local Universe which only measure the central kpc
regions. At the bright end, we find that both the H$\alpha$ and Ly$\alpha$
number densities at $z\approx2.2$ deviate significantly from a Schechter form,
following a power-law. We show that this is driven entirely by an increasing
X-ray/AGN fraction with line-luminosity, which reaches $\approx 100$ \% at
line-luminosities $L\gtrsim3\times10^{44}$ erg s$^{-1}$.
",0,1,0,0,0,0
11016,Resource Sharing Among mmWave Cellular Service Providers in a Vertically Differentiated Duopoly,"  With the increasing interest in the use of millimeter wave bands for 5G
cellular systems comes renewed interest in resource sharing. Properties of
millimeter wave bands such as massive bandwidth, highly directional antennas,
high penetration loss, and susceptibility to shadowing, suggest technical
advantages to spectrum and infrastructure sharing in millimeter wave cellular
networks. However, technical advantages do not necessarily translate to
increased profit for service providers, or increased consumer surplus. In this
paper, detailed network simulations are used to better understand the economic
implications of resource sharing in a vertically differentiated duopoly market
for cellular service. The results suggest that resource sharing is less often
profitable for millimeter wave service providers compared to microwave cellular
service providers, and does not necessarily increase consumer surplus.
",1,0,0,0,0,0
12811,Finite Sample Inference for Targeted Learning,"  The Highly-Adaptive-Lasso(HAL)-TMLE is an efficient estimator of a pathwise
differentiable parameter in a statistical model that at minimal (and possibly
only) assumes that the sectional variation norm of the true nuisance parameters
are finite. It relies on an initial estimator (HAL-MLE) of the nuisance
parameters by minimizing the empirical risk over the parameter space under the
constraint that sectional variation norm is bounded by a constant, where this
constant can be selected with cross-validation. In the formulation of the
HALMLE this sectional variation norm corresponds with the sum of absolute value
of coefficients for an indicator basis. Due to its reliance on machine
learning, statistical inference for the TMLE has been based on its normal limit
distribution, thereby potentially ignoring a large second order remainder in
finite samples.
In this article, we present four methods for construction of a finite sample
0.95-confidence interval that use the nonparametric bootstrap to estimate the
finite sample distribution of the HAL-TMLE or a conservative distribution
dominating the true finite sample distribution. We prove that it consistently
estimates the optimal normal limit distribution, while its approximation error
is driven by the performance of the bootstrap for a well behaved empirical
process. We demonstrate our general inferential methods for 1) nonparametric
estimation of the average treatment effect based on observing on each unit a
covariate vector, binary treatment, and outcome, and for 2) nonparametric
estimation of the integral of the square of the multivariate density of the
data distribution.
",0,0,1,1,0,0
12424,Azumaya algebras and canonical components,"  Let $M$ be a compact 3-manifold and $\Gamma=\pi_1(M)$. The work of Thurston
and Culler--Shalen established the $\mathrm{SL}_2(\mathbb{C})$ character
variety $X(\Gamma)$ as fundamental tool in the study of the geometry and
topology of $M$. This is particularly so in the case when $M$ is the exterior
of a hyperbolic knot $K$ in $S^3$. The main goals of this paper are to bring to
bear tools from algebraic and arithmetic geometry to understand algebraic and
number theoretic properties of the so-called canonical component of $X(\Gamma)$
as well as distinguished points on the canonical component when $\Gamma$ is a
knot group. In particular, we study how the theory of quaternion Azumaya
algebras can be used to obtain algebraic and arithmetic information about Dehn
surgeries, and perhaps of most interest, to construct new knot invariants that
lie in the Brauer groups of curves over number fields.
",0,0,1,0,0,0
15699,Computational landscape of user behavior on social media,"  With the increasing abundance of 'digital footprints' left by human
interactions in online environments, e.g., social media and app use, the
ability to model complex human behavior has become increasingly possible. Many
approaches have been proposed, however, most previous model frameworks are
fairly restrictive. We introduce a new social modeling approach that enables
the creation of models directly from data with minimal a priori restrictions on
the model class. In particular, we infer the minimally complex, maximally
predictive representation of an individual's behavior when viewed in isolation
and as driven by a social input. We then apply this framework to a
heterogeneous catalog of human behavior collected from fifteen thousand users
on the microblogging platform Twitter. The models allow us to describe how a
user processes their past behavior and their social inputs. Despite the
diversity of observed user behavior, most models inferred fall into a small
subclass of all possible finite-state processes. Thus, our work demonstrates
that user behavior, while quite complex, belies simple underlying computational
structures.
",1,0,0,1,0,0
1283,Arimoto-Rényi Conditional Entropy and Bayesian $M$-ary Hypothesis Testing,"  This paper gives upper and lower bounds on the minimum error probability of
Bayesian $M$-ary hypothesis testing in terms of the Arimoto-Rényi conditional
entropy of an arbitrary order $\alpha$. The improved tightness of these bounds
over their specialized versions with the Shannon conditional entropy
($\alpha=1$) is demonstrated. In particular, in the case where $M$ is finite,
we show how to generalize Fano's inequality under both the conventional and
list-decision settings. As a counterpart to the generalized Fano's inequality,
allowing $M$ to be infinite, a lower bound on the Arimoto-Rényi conditional
entropy is derived as a function of the minimum error probability. Explicit
upper and lower bounds on the minimum error probability are obtained as a
function of the Arimoto-Rényi conditional entropy for both positive and
negative $\alpha$. Furthermore, we give upper bounds on the minimum error
probability as functions of the Rényi divergence. In the setup of discrete
memoryless channels, we analyze the exponentially vanishing decay of the
Arimoto-Rényi conditional entropy of the transmitted codeword given the
channel output when averaged over a random coding ensemble.
",1,0,1,1,0,0
9359,Step bunching with both directions of the current: Vicinal W(110) surfaces versus atomistic scale model,"  We report for the first time the observation of bunching of monoatomic steps
on vicinal W(110) surfaces induced by step up or step down currents across the
steps. Measurements reveal that the size scaling exponent {\gamma}, connecting
the maximal slope of a bunch with its height, differs depending on the current
direction. We provide a numerical perspective by using an atomistic scale model
with a conserved surface flux to mimic experimental conditions, and also for
the first time show that there is an interval of parameters in which the
vicinal surface is unstable against step bunching for both directions of the
adatom drift.
",0,1,0,0,0,0
18706,Subband adaptive filter trained by differential evolution for channel estimation,"  The normalized subband adaptive filter (NSAF) is widely accepted as a
preeminent adaptive filtering algorithm because of its efficiency under the
colored excitation. However, the convergence rate of NSAF is slow. To address
this drawback, in this paper, a variant of the NSAF, called the differential
evolution (DE)-NSAF (DE-NSAF), is proposed for channel estimation based on DE
strategy. It is worth noticing that there are several papers concerning
designing DE strategies for adaptive filter. But their signal models are still
the single adaptive filter model rather than the fullband adaptive filter model
considered in this paper. Thus, the problem considered in our work is quite
different from those. The proposed DE-NSAF algorithm is based on real-valued
manipulations and has fast convergence rate for searching the global solution
of optimized weight vector. Moreover, a design step of new algorithm is given
in detail. Simulation results demonstrate the improved performance of the
proposed DE-NSAF algorithm in terms of the convergence rate.
",1,0,0,0,0,0
13591,Sentence-level quality estimation by predicting HTER as a multi-component metric,"  This submission investigates alternative machine learning models for
predicting the HTER score on the sentence level. Instead of directly predicting
the HTER score, we suggest a model that jointly predicts the amount of the 4
distinct post-editing operations, which are then used to calculate the HTER
score. This also gives the possibility to correct invalid (e.g. negative)
predicted values prior to the calculation of the HTER score. Without any
feature exploration, a multi-layer perceptron with 4 outputs yields small but
significant improvements over the baseline.
",1,0,0,0,0,0
16438,"Inspiration, Captivation, and Misdirection: Emergent Properties in Networks of Online Navigation","  The World Wide Web (WWW) has fundamentally changed the ways billions of
people are able to access information. Thus, understanding how people seek
information online is an important issue of study. Wikipedia is a hugely
important part of information provision on the web, with hundreds of millions
of users browsing and contributing to its network of knowledge. The study of
navigational behaviour on Wikipedia, due to the site's popularity and breadth
of content, can reveal more general information seeking patterns that may be
applied beyond Wikipedia and the Web. Our work addresses the relative
shortcomings of existing literature in relating how information structure
influences patterns of navigation online. We study aggregated clickstream data
for articles on the English Wikipedia in the form of a weighted, directed
navigational network. We introduce two parameters that describe how articles
act to source and spread traffic through the network, based on their in/out
strength and entropy. From these, we construct a navigational phase space where
different article types occupy different, distinct regions, indicating how the
structure of information online has differential effects on patterns of
navigation. Finally, we go on to suggest applications for this analysis in
identifying and correcting deficiencies in the Wikipedia page network that may
also be adapted to more general information networks.
",1,0,0,0,0,0
6825,"Rapid, User-Transparent, and Trustworthy Device Pairing for D2D-Enabled Mobile Crowdsourcing","  Mobile Crowdsourcing is a promising service paradigm utilizing ubiquitous
mobile devices to facilitate largescale crowdsourcing tasks (e.g. urban sensing
and collaborative computing). Many applications in this domain require
Device-to-Device (D2D) communications between participating devices for
interactive operations such as task collaborations and file transmissions.
Considering the private participating devices and their opportunistic
encountering behaviors, it is highly desired to establish secure and
trustworthy D2D connections in a fast and autonomous way, which is vital for
implementing practical Mobile Crowdsourcing Systems (MCSs). In this paper, we
develop an efficient scheme, Trustworthy Device Pairing (TDP), which achieves
user-transparent secure D2D connections and reliable peer device selections for
trustworthy D2D communications. Through rigorous analysis, we demonstrate the
effectiveness and security intensity of TDP in theory. The performance of TDP
is evaluated based on both real-world prototype experiments and extensive
trace-driven simulations. Evaluation results verify our theoretical analysis
and show that TDP significantly outperforms existing approaches in terms of
pairing speed, stability, and security.
",1,0,0,0,0,0
6614,Generalized Coordinated Transaction Scheduling: A Market Approach to Seamless Interfaces,"  A generalization of the coordinated transaction scheduling (CTS)---the
state-of-the-art interchange scheduling---is proposed. Referred to as
generalized coordinated transaction scheduling (GCTS), the proposed approach
addresses major seams issues of CTS: the ad hoc use of proxy buses, the
presence of loop flow as a result of proxy bus approximation, and difficulties
in dealing with multiple interfaces. By allowing market participants to submit
bids across market boundaries, GCTS also generalizes the joint economic
dispatch that achieves seamless interchange without market participants. It is
shown that GCTS asymptotically achieves seamless interface under certain
conditions. GCTS is also shown to be revenue adequate in that each regional
market has a non-negative net revenue that is equal to its congestion rent.
Numerical examples are presented to illustrate the quantitative improvement of
the proposed approach.
",0,0,1,0,0,0
19861,Measuring Effectiveness of Video Advertisements,"  Advertisements are unavoidable in modern society. Times Square is notorious
for its incessant display of advertisements. Its popularity is worldwide and
smaller cities possess miniature versions of the display, such as Pittsburgh
and its digital works in Oakland on Forbes Avenue. Tokyo's Ginza district
recently rose to popularity due to its upscale shops and constant onslaught of
advertisements to pedestrians. Advertisements arise in other mediums as well.
For example, they help popular streaming services, such as Spotify, Hulu, and
Youtube TV gather significant streams of revenue to reduce the cost of monthly
subscriptions for consumers. Ads provide an additional source of money for
companies and entire industries to allocate resources toward alternative
business motives. They are attractive to companies and nearly unavoidable for
consumers. One challenge for advertisers is examining a advertisement's
effectiveness or usefulness in conveying a message to their targeted
demographics. Rather than constructing a single, static image of content, a
video advertisement possesses hundreds of frames of data with varying scenes,
actors, objects, and complexity. Therefore, measuring effectiveness of video
advertisements is important to impacting a billion-dollar industry. This paper
explores the combination of human-annotated features and common video
processing techniques to predict effectiveness ratings of advertisements
collected from Youtube. This task is seen as a binary (effective vs.
non-effective), four-way, and five-way machine learning classification task.
The first findings in terms of accuracy and inference on this dataset, as well
as some of the first ad research, on a small dataset are presented. Accuracies
of 84\%, 65\%, and 55\% are reached on the binary, four-way, and five-way tasks
respectively.
",1,0,0,0,0,0
6456,Britannia Rule the Waves,"  The students are introduced to navigation in general and the longitude
problem in particular. A few videos provide insight into scientific and
historical facts related to the issue. Then, the students learn in two steps
how longitude can be derived from time measurements. They first build a
Longitude Clock that visualises the math behind the concept. They use it to
determine the longitudes corresponding to five time measurements. In the second
step, they assume the position of James Cook's navigator and plot the location
of seven destinations on Cook's second voyage between 1772 and 1775.
",0,1,0,0,0,0
17162,Bootstrapping Generalization Error Bounds for Time Series,"  We consider the problem of finding confidence intervals for the risk of
forecasting the future of a stationary, ergodic stochastic process, using a
model estimated from the past of the process. We show that a bootstrap
procedure provides valid confidence intervals for the risk, when the data
source is sufficiently mixing, and the loss function and the estimator are
suitably smooth. Autoregressive (AR(d)) models estimated by least squares obey
the necessary regularity conditions, even when mis-specified, and simulations
show that the finite- sample coverage of our bounds quickly converges to the
theoretical, asymptotic level. As an intermediate step, we derive sufficient
conditions for asymptotic independence between empirical distribution functions
formed by splitting a realization of a stochastic process, of independent
interest.
",0,0,1,1,0,0
11536,Deep Learning-aided Application Scheduler for Vehicular Safety Communication,"  802.11p based V2X communication uses stochastic medium access control, which
cannot prevent broadcast packet collision, in particular during high channel
load. Wireless congestion control has been designed to keep the channel load at
an optimal point. However, vehicles' lack of precise and granular knowledge
about true channel activity, in time and space, makes it impossible to fully
avoid packet collisions. In this paper, we propose a machine learning approach
using deep neural network for learning the vehicles' transmit patterns, and as
such predicting future channel activity in space and time. We evaluate the
performance of our proposal via simulation considering multiple safety-related
V2X services involving heterogeneous transmit patterns. Our results show that
predicting channel activity, and transmitting accordingly, reduces collisions
and significantly improves communication performance.
",1,0,0,0,0,0
2677,Discriminants of complete intersection space curves,"  In this paper, we develop a new approach to the discrimi-nant of a complete
intersection curve in the 3-dimensional projective space. By relying on the
resultant theory, we first prove a new formula that allows us to define this
discrimi-nant without ambiguity and over any commutative ring, in particular in
any characteristic. This formula also provides a new method for evaluating and
computing this discrimi-nant efficiently, without the need to introduce new
variables as with the well-known Cayley trick. Then, we obtain new properties
and computational rules such as the covariance and the invariance formulas.
Finally, we show that our definition of the discriminant satisfies to the
expected geometric property and hence yields an effective smoothness criterion
for complete intersection space curves. Actually, we show that in the generic
setting, it is the defining equation of the discriminant scheme if the ground
ring is assumed to be a unique factorization domain.
",1,0,1,0,0,0
3779,Novel processes and metrics for a scientific evaluation rooted in the principles of science - Version 1,"  Scientific evaluation is a determinant of how scientists, institutions and
funders behave, and as such is a key element in the making of science. In this
article, we propose an alternative to the current norm of evaluating research
with journal rank. Following a well-defined notion of scientific value, we
introduce qualitative processes that can also be quantified and give rise to
meaningful and easy-to-use article-level metrics. In our approach, the goal of
a scientist is transformed from convincing an editorial board through a
vertical process to convincing peers through an horizontal one. We argue that
such an evaluation system naturally provides the incentives and logic needed to
constantly promote quality, reproducibility, openness and collaboration in
science. The system is legally and technically feasible and can gradually lead
to the self-organized reappropriation of the scientific process by the
scholarly community and its institutions. We propose an implementation of our
evaluation system with the platform ""the Self-Journals of Science""
(www.sjscience.org).
",1,0,0,0,0,0
20736,Intertwining operators among twisted modules associated to not-necessarily-commuting automorphisms,"  We introduce intertwining operators among twisted modules or twisted
intertwining operators associated to not-necessarily-commuting automorphisms of
a vertex operator algebra. Let $V$ be a vertex operator algebra and let
$g_{1}$, $g_{2}$ and $g_{3}$ be automorphisms of $V$. We prove that for
$g_{1}$-, $g_{2}$- and $g_{3}$-twisted $V$-modules $W_{1}$, $W_{2}$ and
$W_{3}$, respectively, such that the vertex operator map for $W_{3}$ is
injective, if there exists a twisted intertwining operator of type
${W_{3}\choose W_{1}W_{2}}$ such that the images of its component operators
span $W_{3}$, then $g_{3}=g_{1}g_{2}$. We also construct what we call the
skew-symmetry and contragredient isomorphisms between spaces of twisted
intertwining operators among twisted modules of suitable types. The proofs of
these results involve careful analysis of the analytic extensions corresponding
to the actions of the not-necessarily-commuting automorphisms of the vertex
operator algebra.
",0,0,1,0,0,0
17219,Using controlled disorder to probe the interplay between charge order and superconductivity in NbSe2,"  The interplay between superconductivity and charge density waves (CDW) in
$H$-NbSe2 is not fully understood despite decades of study. Artificially
introduced disorder can tip the delicate balance between two competing forms of
long-range order, and reveal the underlying interactions that give rise to
them. Here we introduce disorders by electron irradiation and measure in-plane
resistivity, Hall resistivity, X-ray scattering, and London penetration depth.
With increasing disorder, $T_{\textrm{c}}$ varies nonmonotonically, whereas
$T_{\textrm{CDW}}$ monotonically decreases and becomes unresolvable above a
critical irradiation dose where $T_{\textrm{c}}$ drops sharply. Our results
imply that CDW order initially competes with superconductivity, but eventually
assists it. We argue that at the transition where the long-range CDW order
disappears, the cooperation with superconductivity is dramatically suppressed.
X-ray scattering and Hall resistivity measurements reveal that the short-range
CDW survives above the transition. Superconductivity persists to much higher
dose levels, consistent with fully gapped superconductivity and moderate
interband pairing.
",0,1,0,0,0,0
15016,Low-temperature behavior of the multicomponent Widom-Rowlison model on finite square lattices,"  We consider the multicomponent Widom-Rowlison with Metropolis dynamics, which
describes the evolution of a particle system where $M$ different types of
particles interact subject to certain hard-core constraints. Focusing on the
scenario where the spatial structure is modeled by finite square lattices, we
study the asymptotic behavior of this interacting particle system in the
low-temperature regime, analyzing the tunneling times between its $M$
maximum-occupancy configurations, and the mixing time of the corresponding
Markov chain. In particular, we develop a novel combinatorial method that,
exploiting geometrical properties of the Widom-Rowlinson configurations on
finite square lattices, leads to the identification of the timescale at which
transitions between maximum-occupancy configurations occur and shows how this
depends on the chosen boundary conditions and the square lattice dimensions.
",0,1,1,0,0,0
9531,Non-Markovian Control with Gated End-to-End Memory Policy Networks,"  Partially observable environments present an important open challenge in the
domain of sequential control learning with delayed rewards. Despite numerous
attempts during the two last decades, the majority of reinforcement learning
algorithms and associated approximate models, applied to this context, still
assume Markovian state transitions. In this paper, we explore the use of a
recently proposed attention-based model, the Gated End-to-End Memory Network,
for sequential control. We call the resulting model the Gated End-to-End Memory
Policy Network. More precisely, we use a model-free value-based algorithm to
learn policies for partially observed domains using this memory-enhanced neural
network. This model is end-to-end learnable and it features unbounded memory.
Indeed, because of its attention mechanism and associated non-parametric
memory, the proposed model allows us to define an attention mechanism over the
observation stream unlike recurrent models. We show encouraging results that
illustrate the capability of our attention-based model in the context of the
continuous-state non-stationary control problem of stock trading. We also
present an OpenAI Gym environment for simulated stock exchange and explain its
relevance as a benchmark for the field of non-Markovian decision process
learning.
",1,0,0,1,0,0
7720,Magnifying the early episodes of star formation: super star clusters at cosmological distances,"  We study the spectrophotometric properties of a highly magnified (\mu~40-70)
pair of stellar systems identified at z=3.2222 behind the Hubble Frontier Field
galaxy cluster MACS~J0416. Five multiple images (out of six) have been
spectroscopically confirmed by means of VLT/MUSE and VLT/X-Shooter
observations. Each image includes two faint (m_uv~30.6), young (<100 Myr),
low-mass (<10^7 Msun), low-metallicity (12+Log(O/H)~7.7, or 1/10 solar) and
compact (30 pc effective radius) stellar systems separated by ~300pc, after
correcting for lensing amplification. We measured several rest-frame
ultraviolet and optical narrow (\sigma_v <~ 25 km/s) high-ionization lines.
These features may be the signature of very hot (T>50000 K) stars within dense
stellar clusters, whose dynamical mass is likely dominated by the stellar
component. Remarkably, the ultraviolet metal lines are not accompanied by Lya
emission (e.g., CIV / Lya > 15), despite the fact that the Lya line flux is
expected to be 150 times brighter (inferred from the Hbeta flux). A
spatially-offset, strongly-magnified (\mu>50) Lya emission with a spatial
extent <~7.6 kpc^2 is instead identified 2 kpc away from the system. The origin
of such a faint emission can be the result of fluorescent Lya induced by a
transverse leakage of ionizing radiation emerging from the stellar systems
and/or can be associated to an underlying and barely detected object (with m_uv
> 34 de-lensed). This is the first confirmed metal-line emitter at such
low-luminosity and redshift without Lya emission, suggesting that, at least in
some cases, a non-uniform covering factor of the neutral gas might hamper the
Lya detection.
",0,1,0,0,0,0
18970,Bayes Minimax Competitors of Preliminary Test Estimators in k Sample Problems,"  In this paper, we consider the estimation of a mean vector of a multivariate
normal population where the mean vector is suspected to be nearly equal to mean
vectors of $k-1$ other populations. As an alternative to the preliminary test
estimator based on the test statistic for testing hypothesis of equal means, we
derive empirical and hierarchical Bayes estimators which shrink the sample mean
vector toward a pooled mean estimator given under the hypothesis. The
minimaxity of those Bayesian estimators are shown, and their performances are
investigated by simulation.
",0,0,1,1,0,0
11989,Symbolic Music Genre Transfer with CycleGAN,"  Deep generative models such as Variational Autoencoders (VAEs) and Generative
Adversarial Networks (GANs) have recently been applied to style and domain
transfer for images, and in the case of VAEs, music. GAN-based models employing
several generators and some form of cycle consistency loss have been among the
most successful for image domain transfer. In this paper we apply such a model
to symbolic music and show the feasibility of our approach for music genre
transfer. Evaluations using separate genre classifiers show that the style
transfer works well. In order to improve the fidelity of the transformed music,
we add additional discriminators that cause the generators to keep the
structure of the original music mostly intact, while still achieving strong
genre transfer. Visual and audible results further show the potential of our
approach. To the best of our knowledge, this paper represents the first
application of GANs to symbolic music domain transfer.
",1,0,0,0,0,0
10691,Cooperation and Environment Characterize the Low-Lying Optical Spectrum of Liquid Water,"  The optical spectrum of liquid water is analyzed by subsystem time-dependent
density functional theory. We provide simple explanations for several important
(and so far elusive) features. Due to the disordered environment surrounding
each water molecule, the joint density of states of the liquid is much broader
than that of the vapor. This results in a red shifted Urbach tail. Confinement
effects provided by the first solvation shell are responsible for the blue
shift of the first absorption peak compared to the vapor. In addition, we also
characterize many-body excitonic effects. These dramatically affect the
spectral weights at low frequencies, contributing to the refractive index by a
small but significant amount.
",0,1,0,0,0,0
11828,A Novel Comprehensive Approach for Estimating Concept Semantic Similarity in WordNet,"  Computation of semantic similarity between concepts is an important
foundation for many research works. This paper focuses on IC computing methods
and IC measures, which estimate the semantic similarities between concepts by
exploiting the topological parameters of the taxonomy. Based on analyzing
representative IC computing methods and typical semantic similarity measures,
we propose a new hybrid IC computing method. Through adopting the parameter
dhyp and lch, we utilize the new IC computing method and propose a novel
comprehensive measure of semantic similarity between concepts. An experiment
based on WordNet ""is a"" taxonomy has been designed to test representative
measures and our measure on benchmark dataset R&G, and the results show that
our measure can obviously improve the similarity accuracy. We evaluate the
proposed approach by comparing the correlation coefficients between five
measures and the artificial data. The results show that our proposal
outperforms the previous measures.
",1,0,0,0,0,0
19391,Transfer Learning for Performance Modeling of Configurable Systems: An Exploratory Analysis,"  Modern software systems provide many configuration options which
significantly influence their non-functional properties. To understand and
predict the effect of configuration options, several sampling and learning
strategies have been proposed, albeit often with significant cost to cover the
highly dimensional configuration space. Recently, transfer learning has been
applied to reduce the effort of constructing performance models by transferring
knowledge about performance behavior across environments. While this line of
research is promising to learn more accurate models at a lower cost, it is
unclear why and when transfer learning works for performance modeling. To shed
light on when it is beneficial to apply transfer learning, we conducted an
empirical study on four popular software systems, varying software
configurations and environmental conditions, such as hardware, workload, and
software versions, to identify the key knowledge pieces that can be exploited
for transfer learning. Our results show that in small environmental changes
(e.g., homogeneous workload change), by applying a linear transformation to the
performance model, we can understand the performance behavior of the target
environment, while for severe environmental changes (e.g., drastic workload
change) we can transfer only knowledge that makes sampling more efficient,
e.g., by reducing the dimensionality of the configuration space.
",1,0,0,1,0,0
11597,A statistical model for aggregating judgments by incorporating peer predictions,"  We propose a probabilistic model to aggregate the answers of respondents
answering multiple-choice questions. The model does not assume that everyone
has access to the same information, and so does not assume that the consensus
answer is correct. Instead, it infers the most probable world state, even if
only a minority vote for it. Each respondent is modeled as receiving a signal
contingent on the actual world state, and as using this signal to both
determine their own answer and predict the answers given by others. By
incorporating respondent's predictions of others' answers, the model infers
latent parameters corresponding to the prior over world states and the
probability of different signals being received in all possible world states,
including counterfactual ones. Unlike other probabilistic models for
aggregation, our model applies to both single and multiple questions, in which
case it estimates each respondent's expertise. The model shows good
performance, compared to a number of other probabilistic models, on data from
seven studies covering different types of expertise.
",0,0,0,1,0,0
7469,Entire holomorphic curves into projective spaces intersecting a generic hypersurface of high degree,"  In this note, we establish the following Second Main Theorem type estimate
for every entire non-algebraically degenerate holomorphic curve
$f\colon\mathbb{C}\rightarrow\mathbb{P}^n(\mathbb{C})$, in present of a {\sl
generic} hypersuface $D\subset\mathbb{P}^n(\mathbb{C})$ of sufficiently high
degree $d\geq 15(5n+1)n^n$: \[ T_f(r) \leq \,N_f^{[1]}(r,D) + O\big(\log T_f(r)
+ \log r \big)\parallel, \] where $T_f(r)$ and $N_f^{[1]}(r,D)$ stand for the
order function and the $1$-truncated counting function in Nevanlinna theory.
This inequality quantifies recent results on the logarithmic Green--Griffiths
conjecture.
",0,0,1,0,0,0
10320,Multiresolution Coupled Vertical Equilibrium Model for Fast Flexible Simulation of CO$_2$ Storage,"  CO2 capture and storage is an important technology for mitigating climate
change. Design of efficient strategies for safe, long-term storage requires the
capability to efficiently simulate processes taking place on very different
temporal and spatial scales. The physical laws describing CO2 storage are the
same as for hydrocarbon recovery, but the characteristic spatial and temporal
scales are quite different. Petroleum reservoirs seldom extend more than tens
of kilometers and have operational horizons spanning decades. Injected CO2
needs to be safely contained for hundreds or thousands of years, during which
it can migrate hundreds or thousands of kilometers. Because of the vast scales
involved, conventional 3D reservoir simulation quickly becomes computationally
unfeasible. Large density difference between injected CO2 and resident brine
means that vertical segregation will take place relatively quickly, and
depth-integrated models assuming vertical equilibrium (VE) often represents a
better strategy to simulate long-term migration of CO2 in large-scale aquifer
systems. VE models have primarily been formulated for relatively simple rock
formations and have not been coupled to 3D simulation in a uniform way. In
particular, known VE simulations have not been applied to models of realistic
geology in which many flow compartments may exist in-between impermeable
layers. In this paper, we generalize the concept of VE models, formulated in
terms of well-proven reservoir simulation technology, to complex aquifer
systems with multiple layers and regions. We also introduce novel formulations
for multi-layered VE models by use of both direct spill and diffuse leakage
between individual layers. This new layered 3D model is then coupled to a
state-of-the-art, 3D black-oil type model.
",0,1,0,0,0,0
12018,Better Protocol for XOR Game using Communication Protocol and Nonlocal Boxes,"  Buhrman showed that an efficient communication protocol implies a reliable
XOR game protocol. This idea rederives Linial and Shraibman's lower bounds of
communication complexity, which was derived by using factorization norms, with
worse constant factor in much more intuitive way. In this work, we improve and
generalize Buhrman's idea, and obtain a class of lower bounds for classical
communication complexity including an exact Linial and Shraibman's lower bound
as a special case. In the proof, we explicitly construct a protocol for XOR
game from a classical communication protocol by using a concept of nonlocal
boxes and Paw{\l}owski et al.'s elegant protocol, which was used for showing
the violation of information causality in superquantum theories.
",1,0,1,0,0,0
3711,A note on the approximate admissibility of regularized estimators in the Gaussian sequence model,"  We study the problem of estimating an unknown vector $\theta$ from an
observation $X$ drawn according to the normal distribution with mean $\theta$
and identity covariance matrix under the knowledge that $\theta$ belongs to a
known closed convex set $\Theta$. In this general setting, Chatterjee (2014)
proved that the natural constrained least squares estimator is ""approximately
admissible"" for every $\Theta$. We extend this result by proving that the same
property holds for all convex penalized estimators as well. Moreover, we
simplify and shorten the original proof considerably. We also provide explicit
upper and lower bounds for the universal constant underlying the notion of
approximate admissibility.
",0,0,1,1,0,0
20539,Dynamics of Relaxed Inflation,"  The cosmological relaxation of the electroweak scale has been proposed as a
mechanism to address the hierarchy problem of the Standard Model. A field, the
relaxion, rolls down its potential and, in doing so, scans the squared mass
parameter of the Higgs, relaxing it to a parametrically small value. In this
work, we promote the relaxion to an inflaton. We couple it to Abelian gauge
bosons, thereby introducing the necessary dissipation mechanism which slows
down the field in the last stages. We describe a novel reheating mechanism,
which relies on the gauge-boson production leading to strong electromagnetic
fields, and proceeds via the vacuum production of electron-positron pairs
through the Schwinger effect. We refer to this mechanism as Schwinger
reheating. We discuss the cosmological dynamics of the model and the
phenomenological constraints from CMB and other experiments. We find that a
cutoff close to the Planck scale may be achieved. In its minimal form, the
model does not generate sufficient curvature perturbations and additional
ingredients, such as a curvaton field, are needed.
",0,1,0,0,0,0
9223,Automatic Rule Extraction from Long Short Term Memory Networks,"  Although deep learning models have proven effective at solving problems in
natural language processing, the mechanism by which they come to their
conclusions is often unclear. As a result, these models are generally treated
as black boxes, yielding no insight of the underlying learned patterns. In this
paper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new
approach for tracking the importance of a given input to the LSTM for a given
output. By identifying consistently important patterns of words, we are able to
distill state of the art LSTMs on sentiment analysis and question answering
into a set of representative phrases. This representation is then
quantitatively validated by using the extracted phrases to construct a simple,
rule-based classifier which approximates the output of the LSTM.
",1,0,0,1,0,0
5044,"Perturbative Thermodynamic Geometry of Nonextensive Ideal Classical, Bose and Fermi Gases","  We investigate perturbative thermodynamic geometry of nonextensive ideal
Classical, Bose and Fermi gases.We show that the intrinsic statistical
interaction of nonextensive Bose (Fermi) gas is attractive (repulsive) similar
to the extensive case but the value of thermodynamic curvature is changed by
nonextensive parameter. In contrary to the extensive ideal classical gas, the
nonextensive one may be divided to two different regimes. According to
deviation parameter of the system to the nonextensive case, one can find a
special value of fugacity, $z^{*}$, where the sign of thermodynamic curvature
is changed. Therefore, we argue that the nonextensive parameter induces an
attractive (repulsive) statistical interaction for $z<z^{*}$ ($z>z^{*}$) for an
ideal classical gas. Also, according to the singular point of thermodynamic
curvature, we consider the condensation of nonextensive Boson gas.
",0,1,0,0,0,0
19501,A Review on Quantile Regression for Stochastic Computer Experiments,"  We report on an empirical study of the main strategies for conditional
quantile estimation in the context of stochastic computer experiments. To
ensure adequate diversity, six metamodels are presented, divided into three
categories based on order statistics, functional approaches, and those of
Bayesian inspiration. The metamodels are tested on several problems
characterized by the size of the training set, the input dimension, the
quantile order and the value of the probability density function in the
neighborhood of the quantile. The metamodels studied reveal good contrasts in
our set of 480 experiments, enabling several patterns to be extracted. Based on
our results, guidelines are proposed to allow users to select the best method
for a given problem.
",1,0,0,1,0,0
11061,Spatial cytoskeleton organization supports targeted intracellular transport,"  The efficiency of intracellular cargo transport from specific source to
target locations is strongly dependent upon molecular motor-assisted motion
along the cytoskeleton. Radial transport along microtubules and lateral
transport along the filaments of the actin cortex underneath the cell membrane
are characteristic for cells with a centrosome. The interplay between the
specific cytoskeleton organization and the motor performance realizes a
spatially inhomogeneous intermittent search strategy. In order to analyze the
efficiency of such intracellular search strategies we formulate a random
velocity model with intermittent arrest states. We evaluate efficiency in terms
of mean first passage times for three different, frequently encountered
intracellular transport tasks: i) the narrow escape problem, which emerges
during cargo transport to a synapse or other specific region of the cell
membrane, ii) the reaction problem, which considers the binding time of two
particles within the cell, and iii) the reaction-escape problem, which arises
when cargo must be released at a synapse only after pairing with another
particle. Our results indicate that cells are able to realize efficient search
strategies for various intracellular transport tasks economically through a
spatial cytoskeleton organization that involves only a narrow actin cortex
rather than a cell body filled with randomly oriented actin filaments.
",0,1,0,0,0,0
7050,Deep Learning for Real Time Crime Forecasting,"  Accurate real time crime prediction is a fundamental issue for public safety,
but remains a challenging problem for the scientific community. Crime
occurrences depend on many complex factors. Compared to many predictable
events, crime is sparse. At different spatio-temporal scales, crime
distributions display dramatically different patterns. These distributions are
of very low regularity in both space and time. In this work, we adapt the
state-of-the-art deep learning spatio-temporal predictor, ST-ResNet [Zhang et
al, AAAI, 2017], to collectively predict crime distribution over the Los
Angeles area. Our models are two staged. First, we preprocess the raw crime
data. This includes regularization in both space and time to enhance
predictable signals. Second, we adapt hierarchical structures of residual
convolutional units to train multi-factor crime prediction models. Experiments
over a half year period in Los Angeles reveal highly accurate predictive power
of our models.
",1,0,0,1,0,0
10804,Proton-induced halo formation in charged meteors,"  Despite a very long history of meteor science, our understanding of meteor
ablation and its shocked plasma physics is still far from satisfactory as we
are still missing the microphysics of meteor shock formation and its plasma
dynamics. Here we argue that electrons and ions in the meteor plasma above
$\sim$100 km altitude undergo spatial separation due to electrons being trapped
by gyration in the Earth's magnetic field, while the ions are carried by the
meteor as their dynamics is dictated by collisions. This separation process
charges the meteor and creates a strong local electric field. We show how
acceleration of protons in this field leads to the collisional excitation of
ionospheric N$_2$ on the scale of many 100 m. This mechanism explains the
puzzling large halo detected around Leonid meteors, while it also fits into the
theoretical expectations of several other unexplained meteor related phenomena.
We expect our work to lead to more advanced models of meteor-ionosphere
interaction, combined with the electrodynamics of meteor trail evolution.
",0,1,0,0,0,0
